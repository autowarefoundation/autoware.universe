{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"autoware.universe","text":""},{"location":"#autowareuniverse","title":"autoware.universe","text":"<p>For Autoware's general documentation, see Autoware Documentation.</p> <p>For detailed documents of Autoware Universe components, see Autoware Universe Documentation.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at conduct@autoware.org. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing","title":"Contributing","text":"<p>See https://autowarefoundation.github.io/autoware-documentation/main/contributing/.</p>"},{"location":"DISCLAIMER/","title":"DISCLAIMER","text":"<p>DISCLAIMER</p> <p>\u201cAutoware\u201d will be provided by The Autoware Foundation under the Apache License 2.0. This \u201cDISCLAIMER\u201d will be applied to all users of Autoware (a \u201cUser\u201d or \u201cUsers\u201d) with the Apache License 2.0 and Users shall hereby approve and acknowledge all the contents specified in this disclaimer below and will be deemed to consent to this disclaimer without any objection upon utilizing or downloading Autoware.</p> <p>Disclaimer and Waiver of Warranties</p> <ol> <li> <p>AUTOWARE FOUNDATION MAKES NO REPRESENTATION OR WARRANTY OF ANY KIND,    EXPRESS OR IMPLIED, WITH RESPECT TO PROVIDING AUTOWARE (the \u201cService\u201d)    including but not limited to any representation or warranty (i) of fitness or    suitability for a particular purpose contemplated by the Users, (ii) of the    expected functions, commercial value, accuracy, or usefulness of the Service,    (iii) that the use by the Users of the Service complies with the laws and    regulations applicable to the Users or any internal rules established by    industrial organizations, (iv) that the Service will be free of interruption or    defects, (v) of the non-infringement of any third party's right and (vi) the    accuracy of the content of the Services and the software itself.</p> </li> <li> <p>The Autoware Foundation shall not be liable for any damage incurred by the    User that are attributable to the Autoware Foundation for any reasons    whatsoever. UNDER NO CIRCUMSTANCES SHALL THE AUTOWARE FOUNDATION BE LIABLE FOR    INCIDENTAL, INDIRECT, SPECIAL OR FUTURE DAMAGES OR LOSS OF PROFITS.</p> </li> <li> <p>A User shall be entirely responsible for the content posted by the User and    its use of any content of the Service or the Website. If the User is held    responsible in a civil action such as a claim for damages or even in a criminal    case, the Autoware Foundation and member companies, governments and academic &amp;    non-profit organizations and their directors, officers, employees and agents    (collectively, the \u201cIndemnified Parties\u201d) shall be completely discharged from    any rights or assertions the User may have against the Indemnified Parties, or    from any legal action, litigation or similar procedures.</p> </li> </ol> <p>Indemnity</p> <p>A User shall indemnify and hold the Indemnified Parties harmless from any of their damages, losses, liabilities, costs or expenses (including attorneys' fees or criminal compensation), or any claims or demands made against the Indemnified Parties by any third party, due to or arising out of, or in connection with utilizing Autoware (including the representations and warranties), the violation of applicable Product Liability Law of each country (including criminal case) or violation of any applicable laws by the Users, or the content posted by the User or its use of any content of the Service or the Website.</p>"},{"location":"common/autoware_ad_api_specs/","title":"autoware_adapi_specs","text":""},{"location":"common/autoware_ad_api_specs/#autoware_adapi_specs","title":"autoware_adapi_specs","text":"<p>This package is a specification of Autoware AD API.</p>"},{"location":"common/autoware_auto_common/design/comparisons/","title":"Comparisons","text":""},{"location":"common/autoware_auto_common/design/comparisons/#comparisons","title":"Comparisons","text":"<p>The <code>float_comparisons.hpp</code> library is a simple set of functions for performing approximate numerical comparisons. There are separate functions for performing comparisons using absolute bounds and relative bounds. Absolute comparison checks are prefixed with <code>abs_</code> and relative checks are prefixed with <code>rel_</code>.</p> <p>The <code>bool_comparisons.hpp</code> library additionally contains an XOR operator.</p> <p>The intent of the library is to improve readability of code and reduce likelihood of typographical errors when using numerical and boolean comparisons.</p>"},{"location":"common/autoware_auto_common/design/comparisons/#target-use-cases","title":"Target use cases","text":"<p>The approximate comparisons are intended to be used to check whether two numbers lie within some absolute or relative interval. The <code>exclusive_or</code> function will test whether two values cast to different boolean values.</p>"},{"location":"common/autoware_auto_common/design/comparisons/#assumptions","title":"Assumptions","text":"<ul> <li>The approximate comparisons all take an <code>epsilon</code> parameter.   The value of this parameter must be &gt;= 0.</li> <li>The library is only intended to be used with floating point types.   A static assertion will be thrown if the library is used with a non-floating point type.</li> </ul>"},{"location":"common/autoware_auto_common/design/comparisons/#example-usage","title":"Example Usage","text":"<pre><code>#include \"common/bool_comparisons.hpp\"\n#include \"common/float_comparisons.hpp\"\n\n#include &lt;iostream&gt;\n\n// using-directive is just for illustration; don't do this in practice\nusing namespace autoware::common::helper_functions::comparisons;\n\nstatic constexpr auto epsilon = 0.2;\nstatic constexpr auto relative_epsilon = 0.01;\n\nstd::cout &lt;&lt; exclusive_or(true, false) &lt;&lt; \"\\n\";\n// Prints: true\n\nstd::cout &lt;&lt; rel_eq(1.0, 1.1, relative_epsilon)) &lt;&lt; \"\\n\";\n// Prints: false\n\nstd::cout &lt;&lt; approx_eq(10000.0, 10010.0, epsilon, relative_epsilon)) &lt;&lt; \"\\n\";\n// Prints: true\n\nstd::cout &lt;&lt; abs_eq(4.0, 4.2, epsilon) &lt;&lt; \"\\n\";\n// Prints: true\n\nstd::cout &lt;&lt; abs_ne(4.0, 4.2, epsilon) &lt;&lt; \"\\n\";\n// Prints: false\n\nstd::cout &lt;&lt; abs_eq_zero(0.2, epsilon) &lt;&lt; \"\\n\";\n// Prints: false\n\nstd::cout &lt;&lt; abs_lt(4.0, 4.25, epsilon) &lt;&lt; \"\\n\";\n// Prints: true\n\nstd::cout &lt;&lt; abs_lte(1.0, 1.2, epsilon) &lt;&lt; \"\\n\";\n// Prints: true\n\nstd::cout &lt;&lt; abs_gt(1.25, 1.0, epsilon) &lt;&lt; \"\\n\";\n// Prints: true\n\nstd::cout &lt;&lt; abs_gte(0.75, 1.0, epsilon) &lt;&lt; \"\\n\";\n// Prints: false\n</code></pre>"},{"location":"common/autoware_auto_geometry/design/interval/","title":"Interval","text":""},{"location":"common/autoware_auto_geometry/design/interval/#interval","title":"Interval","text":"<p>The interval is a standard 1D real-valued interval. The class implements a representation and operations on the interval type and guarantees interval validity on construction. Basic operations and accessors are implemented, as well as other common operations. See 'Example Usage' below.</p>"},{"location":"common/autoware_auto_geometry/design/interval/#target-use-cases","title":"Target use cases","text":"<ul> <li>Range or containment checks.   The interval class simplifies code that involves checking membership of a value to a range, or intersecting two ranges.   It also provides consistent behavior and consistent handling of edge cases.</li> </ul>"},{"location":"common/autoware_auto_geometry/design/interval/#properties","title":"Properties","text":"<ul> <li>empty: An empty interval is equivalent to an empty set.   It contains no elements.   It is a valid interval, but because it is empty, the notion of measure (length) is undefined; the measure of an empty interval is not zero.   The implementation represents the measure of an empty interval with <code>NaN</code>.</li> <li>zero measure: An interval with zero measure is an interval whose bounds are exactly equal.   The measure is zero because the interval contains only a single point, and points have zero measure.   However, because it does contain a single element, the interval is not empty.</li> <li>valid: A valid interval is either empty or has min/max bounds such that (min &lt;= max). On construction, interval objects are guaranteed to be valid.   An attempt to construct an invalid interval results in a runtime_error exception being thrown.</li> <li>pseudo-immutable: Once constructed the only way to change the value of an interval is to overwrite it with a new one; an existing object cannot be modified.</li> </ul>"},{"location":"common/autoware_auto_geometry/design/interval/#conventions","title":"Conventions","text":"<ul> <li>All operations on interval objects are defined as static class methods on the interval class.   This is a functional-style of programming that basically turns the class into a namespace that grants functions access to private member variables of the object they operate on.</li> </ul>"},{"location":"common/autoware_auto_geometry/design/interval/#assumptions","title":"Assumptions","text":"<ul> <li>The interval is only intended for floating point types.   This is enforced via static assertion.</li> <li>The constructor for non-empty intervals takes two arguments 'min' and 'max', and they must be ordered (i.e., min &lt;= max).   If this assumption is violated, an exception is emitted and construction fails.</li> </ul>"},{"location":"common/autoware_auto_geometry/design/interval/#example-usage","title":"Example Usage","text":"<pre><code>#include \"geometry/interval.hpp\"\n\n#include &lt;iostream&gt;\n\n// using-directive is just for illustration; don't do this in practice\nusing namespace autoware::common::geometry;\n\n// bounds for example interval\nconstexpr auto MIN = 0.0;\nconstexpr auto MAX = 1.0;\n\n//\n// Try to construct an invalid interval. This will give the following error:\n// 'Attempted to construct an invalid interval: {\"min\": 1.0, \"max\": 0.0}'\n//\n\ntry {\nconst auto i = Interval_d(MAX, MIN);\n} catch (const std::runtime_error&amp; e) {\nstd::cerr &lt;&lt; e.what();\n}\n\n//\n// Construct a double precision interval from 0 to 1\n//\n\nconst auto i = Interval_d(MIN, MAX);\n\n//\n// Test accessors and properties\n//\n\nstd::cout &lt;&lt; Interval_d::min(i) &lt;&lt; \" \" &lt;&lt; Interval_d::max(i) &lt;&lt; \"\\n\";\n// Prints: 0.0 1.0\n\nstd::cout &lt;&lt; Interval_d::empty(i) &lt;&lt; \" \" &lt;&lt; Interval_d::length(i) &lt;&lt; \"\\n\";\n// Prints: false 1.0\n\nstd::cout &lt;&lt; Interval_d::contains(i, 0.3) &lt;&lt; \"\\n\";\n// Prints: true\n\nstd::cout &lt;&lt; Interval_d::is_subset_eq(Interval_d(0.2, 0.4), i) &lt;&lt; \"\\n\";\n// Prints: true\n\n//\n// Test operations.\n//\n\nstd::cout &lt;&lt; Interval_d::intersect(i, Interval(-1.0, 0.3)) &lt;&lt; \"\\n\";\n// Prints: {\"min\": 0.0, \"max\": 0.3}\n\nstd::cout &lt;&lt; Interval_d::project_to_interval(i, 0.5) &lt;&lt; \" \"\n&lt;&lt; Interval_d::project_to_interval(i, -1.3) &lt;&lt; \"\\n\";\n// Prints: 0.5 0.0\n\n//\n// Distinguish empty/zero measure\n//\n\nconst auto i_empty = Interval();\nconst auto i_zero_length = Interval(0.0, 0.0);\n\nstd::cout &lt;&lt; Interval_d::empty(i_empty) &lt;&lt; \" \"\n&lt;&lt; Interval_d::empty(i_zero_length) &lt;&lt; \"\\n\";\n// Prints: true false\n\nstd::cout &lt;&lt; Interval_d::zero_measure(i_empty) &lt;&lt; \" \"\n&lt;&lt; Interval_d::zero_measure(i_zero_length) &lt;&lt; \"\\n\";\n// Prints: false false\n</code></pre>"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/","title":"2D Convex Polygon Intersection","text":""},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#2d-convex-polygon-intersection","title":"2D Convex Polygon Intersection","text":"<p>Two convex polygon's intersection can be visualized on the image below as the blue area:</p>"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>Computing the intersection between two polygons can be useful in many applications of scene understanding. It can be used to estimate collision detection, shape alignment, shape association and in any application that deals with the objects around the perceiving agent.</p>"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#design","title":"Design","text":"<p>\\(Livermore, Calif, 1977\\) mention the following observations about convex polygon intersection:</p> <ul> <li>Intersection of two convex polygons is a convex polygon</li> <li>A vertex from a polygon that is contained in the other polygon is a vertex of the intersection   shape. (Vertices A, C, D in the shape above)</li> <li>An edge from a polygon that is contained in the other polygon is an edge in the intersection   shape. (edge C-D in the shape above)</li> <li>Edge intersections between two polygons are vertices in the intersection shape. (Vertices B,   E in the shape above.)</li> </ul>"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>With the observation mentioned above, the current algorithm operates in the following way:</p> <ul> <li>Compute and find the vertices from each polygon that is contained in the other polygon   (Vertices A, C, D)</li> <li>Compute and find the intersection points between each polygon (Vertices B, E)</li> <li>Compute the convex hull shaped by these vertices by ordering them CCW.</li> </ul>"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#inputs-outputs-api","title":"Inputs / Outputs / API","text":"<p>Inputs:</p> <ul> <li>Two iterables that contain vertices of the convex polygons ordered in the CCW direction.</li> </ul> <p>Outputs:</p> <ul> <li>A list of vertices of the intersection shape ordered in the CCW direction.</li> </ul>"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#future-work","title":"Future Work","text":""},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#1230-applying-efficient-algorithms","title":"1230: Applying efficient algorithms.","text":""},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#related-issues","title":"Related issues","text":""},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#983-integrate-vision-detections-in-object-tracker","title":"983: Integrate vision detections in object tracker","text":""},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/","title":"Spatial Hash","text":""},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#spatial-hash","title":"Spatial Hash","text":"<p>The spatial hash is a data structure designed for efficient fixed-radius near-neighbor queries in low dimensions.</p> <p>The fixed-radius near-neighbors problem is defined as follows:</p> <p><code>For point p, find all points p' s.t. d(p, p') &lt; r</code></p> <p>Where in this case <code>d(p, p')</code> is euclidean distance, and <code>r</code> is the fixed radius.</p> <p>For <code>n</code> points with an average of <code>k</code> neighbors each, this data structure can perform <code>m</code> near-neighbor queries (to generate lists of near-neighbors for <code>m</code> different points) in <code>O(mk)</code> time.</p> <p>By contrast, using a k-d tree for successive nearest-neighbor queries results in a running time of <code>O(m log n)</code>.</p> <p>The spatial hash works as follows:</p> <ul> <li>Each point is assigned to a bin in the predefined bounding area defined by   <code>x_min/x_max</code> and <code>y_min/y_max</code></li> <li>This can be done by converting x and y position into x and y index   respectively<ul> <li>For example with the bin containing <code>x_min</code> and <code>y_min</code> as index <code>(0, 0)</code></li> <li>The two (or more) indices can then be converted into a single index</li> </ul> </li> <li>Once every point of interest has been inserted into the hash, near-neighbor   queries can begin:<ul> <li>The bin of the reference point is first computed</li> <li>For each point in each adjacent bin, perform an explicit distance computation   between said point and the reference point. If the distance is below the given   radius, said point is considered to be a near-neighbor</li> </ul> </li> </ul> <p>Under the hood, an <code>std::unordered_multimap</code> is used, where the key is a bin/voxel index. The bin size was computed to be the same as the lookup distance.</p> <p>In addition, this data structure can support 2D or 3D queries. This is determined during configuration, and baked into the data structure via the configuration class. The purpose of this was to avoid if statements in tight loops. The configuration class specializations themselves use CRTP (Curiously Recurring Template Patterns) to do \"static polymorphism\", and avoid a dispatching call.</p>"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#performance-characterization","title":"Performance characterization","text":""},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#time","title":"Time","text":"<p>Insertion is <code>O(n)</code> because lookup time for the underlying hashmap is <code>O(n)</code> for hashmaps. In practice, lookup time for hashmaps and thus insertion time should be <code>O(1)</code>.</p> <p>Removing a point is <code>O(1)</code> because the current API only supports removal via direct reference to a node.</p> <p>Finding <code>k</code> near-neighbors is worst case <code>O(n)</code> in the case of an adversarial example, but in practice <code>O(k)</code>.</p>"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#space","title":"Space","text":"<p>The module consists of the following components:</p> <ul> <li>The internal hashmap is <code>O(n + n + A * n)</code>, where <code>A</code> is an arbitrary   constant (load factor)</li> <li>The other components of the spatial hash are <code>O(n + n)</code></li> </ul> <p>This results in <code>O(n)</code> space complexity.</p>"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#states","title":"States","text":"<p>The spatial hash's state is dictated by the status of the underlying unordered_multimap.</p> <p>The data structure is wholly configured by a config class. The constructor of the class determines in the data structure accepts strictly 2D or strictly 3D queries.</p>"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#inputs","title":"Inputs","text":"<p>The primary method of introducing data into the data structure is via the insert method.</p>"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#outputs","title":"Outputs","text":"<p>The primary method of retrieving data from the data structure is via the near\\(2D configuration\\) or near \\(3D configuration\\) method.</p> <p>The whole data structure can also be traversed using standard constant iterators.</p>"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#future-work","title":"Future Work","text":"<ul> <li>Performance tuning and optimization</li> </ul>"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#related-issues","title":"Related issues","text":""},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#28-port-to-autowareauto","title":"28: Port to autoware.Auto","text":""},{"location":"common/autoware_auto_perception_rviz_plugin/","title":"autoware_auto_perception_plugin","text":""},{"location":"common/autoware_auto_perception_rviz_plugin/#autoware_auto_perception_plugin","title":"autoware_auto_perception_plugin","text":""},{"location":"common/autoware_auto_perception_rviz_plugin/#purpose","title":"Purpose","text":"<p>It is an rviz plugin for visualizing the result from perception module. This package is based on the implementation of the rviz plugin developed by Autoware.Auto.</p> <p>See Autoware.Auto design documentation for the original design philosophy. [1]</p>"},{"location":"common/autoware_auto_perception_rviz_plugin/#input-types-visualization-results","title":"Input Types / Visualization Results","text":""},{"location":"common/autoware_auto_perception_rviz_plugin/#detectedobjects","title":"DetectedObjects","text":""},{"location":"common/autoware_auto_perception_rviz_plugin/#input-types","title":"Input Types","text":"Name Type Description <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> detection result array"},{"location":"common/autoware_auto_perception_rviz_plugin/#visualization-result","title":"Visualization Result","text":""},{"location":"common/autoware_auto_perception_rviz_plugin/#trackedobjects","title":"TrackedObjects","text":""},{"location":"common/autoware_auto_perception_rviz_plugin/#input-types_1","title":"Input Types","text":"Name Type Description <code>autoware_auto_perception_msgs::msg::TrackedObjects</code> tracking result array"},{"location":"common/autoware_auto_perception_rviz_plugin/#visualization-result_1","title":"Visualization Result","text":"<p>Overwrite tracking results with detection results.</p> <p></p>"},{"location":"common/autoware_auto_perception_rviz_plugin/#predictedobjects","title":"PredictedObjects","text":""},{"location":"common/autoware_auto_perception_rviz_plugin/#input-types_2","title":"Input Types","text":"Name Type Description <code>autoware_auto_perception_msgs::msg::PredictedObjects</code> prediction result array"},{"location":"common/autoware_auto_perception_rviz_plugin/#visualization-result_2","title":"Visualization Result","text":"<p>Overwrite prediction results with tracking results.</p> <p></p>"},{"location":"common/autoware_auto_perception_rviz_plugin/#referencesexternal-links","title":"References/External links","text":"<p>[1] https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/tree/master/src/tools/visualization/autoware_rviz_plugins</p>"},{"location":"common/autoware_auto_perception_rviz_plugin/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/","title":"autoware_auto_tf2","text":""},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#autoware_auto_tf2","title":"autoware_auto_tf2","text":"<p>This is the design document for the <code>autoware_auto_tf2</code> package.</p>"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>In general, users of ROS rely on tf (and its successor, tf2) for publishing and utilizing coordinate frame transforms. This is true even to the extent that the tf2 contains the packages <code>tf2_geometry_msgs</code> and <code>tf2_sensor_msgs</code> which allow for easy conversion to and from the message types defined in <code>geometry_msgs</code> and <code>sensor_msgs</code>, respectively. However, AutowareAuto contains some specialized message types which are not transformable between frames using the ROS 2 library. The <code>autoware_auto_tf2</code> package aims to provide developers with tools to transform applicable <code>autoware_auto_msgs</code> types. In addition to this, this package also provides transform tools for messages types in <code>geometry_msgs</code> missing in <code>tf2_geometry_msgs</code>.</p>"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#design","title":"Design","text":"<p>While writing <code>tf2_some_msgs</code> or contributing to <code>tf2_geometry_msgs</code>, compatibility and design intent was ensured with the following files in the existing tf2 framework:</p> <ul> <li><code>tf2/convert.h</code></li> <li><code>tf2_ros/buffer_interface.h</code></li> </ul> <p>For example:</p> <pre><code>void tf2::convert( const A &amp; a,B &amp; b)\n</code></pre> <p>The method <code>tf2::convert</code> is dependent on the following:</p> <pre><code>template&lt;typename A, typename B&gt;\nB tf2::toMsg(const A&amp; a);\ntemplate&lt;typename A, typename B&gt;\nvoid tf2::fromMsg(const A&amp;, B&amp; b);\n\n// New way to transform instead of using tf2::doTransform() directly\ntf2_ros::BufferInterface::transform(...)\n</code></pre> <p>Which, in turn, is dependent on the following:</p> <pre><code>void tf2::convert( const A &amp; a,B &amp; b)\nconst std::string&amp; tf2::getFrameId(const T&amp; t)\nconst ros::Time&amp; tf2::getTimestamp(const T&amp; t);\n</code></pre>"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#current-implementation-of-tf2_geometry_msgs","title":"Current Implementation of tf2_geometry_msgs","text":"<p>In both ROS 1 and ROS 2 stamped msgs like <code>Vector3Stamped</code>, <code>QuaternionStamped</code> have associated functions like:</p> <ul> <li><code>getTimestamp</code></li> <li><code>getFrameId</code></li> <li><code>doTransform</code></li> <li><code>toMsg</code></li> <li><code>fromMsg</code></li> </ul> <p>In ROS 1, to support <code>tf2::convert</code> and need in <code>doTransform</code> of the stamped data, non-stamped underlying data like <code>Vector3</code>, <code>Point</code>, have implementations of the following functions:</p> <ul> <li><code>toMsg</code></li> <li><code>fromMsg</code></li> </ul> <p>In ROS 2, much of the <code>doTransform</code> method is not using <code>toMsg</code> and <code>fromMsg</code> as data types from tf2 are not used. Instead <code>doTransform</code> is done using <code>KDL</code>, thus functions relating to underlying data were not added; such as <code>Vector3</code>, <code>Point</code>, or ported in this commit ros/geometry2/commit/6f2a82. The non-stamped data with <code>toMsg</code> and <code>fromMsg</code> are <code>Quaternion</code>, <code>Transform</code>. <code>Pose</code> has the modified <code>toMsg</code> and not used by <code>PoseStamped</code>.</p>"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#plan-for-autoware_auto_perception_msgsmsgboundingboxarray","title":"Plan for autoware_auto_perception_msgs::msg::BoundingBoxArray","text":"<p>The initial rough plan was to implement some of the common tf2 functions like <code>toMsg</code>, <code>fromMsg</code>, and <code>doTransform</code>, as needed for all the underlying data types in <code>BoundingBoxArray</code>. Examples of the data types include: <code>BoundingBox</code>, <code>Quaternion32</code>, and <code>Point32</code>. In addition, the implementation should be done such that upstream contributions could also be made to <code>geometry_msgs</code>.</p>"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Due to conflicts in a function signatures, the predefined template of <code>convert.h</code>/ <code>transform_functions.h</code> is not followed and compatibility with <code>tf2::convert(..)</code> is broken and <code>toMsg</code> is written differently.</p> <pre><code>// Old style\ngeometry_msgs::Vector3 toMsg(const tf2::Vector3&amp; in)\ngeometry_msgs::Point&amp; toMsg(const tf2::Vector3&amp; in)\n\n// New style\ngeometry_msgs::Point&amp; toMsg(const tf2::Vector3&amp; in, geometry_msgs::Point&amp; out)\n</code></pre>"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#inputs-outputs-api","title":"Inputs / Outputs / API","text":"<p>The library provides API <code>doTransform</code> for the following data-types that are either not available in <code>tf2_geometry_msgs</code> or the messages types are part of <code>autoware_auto_msgs</code> and are therefore custom and not inherently supported by any of the tf2 libraries. The following APIs are provided for the following data types:</p> <ul> <li><code>Point32</code></li> </ul> <pre><code>inline void doTransform(\nconst geometry_msgs::msg::Point32 &amp; t_in,\ngeometry_msgs::msg::Point32 &amp; t_out,\nconst geometry_msgs::msg::TransformStamped &amp; transform)\n</code></pre> <ul> <li><code>Quaternion32</code> (<code>autoware_auto_msgs</code>)</li> </ul> <pre><code>inline void doTransform(\nconst autoware_auto_geometry_msgs::msg::Quaternion32 &amp; t_in,\nautoware_auto_geometry_msgs::msg::Quaternion32 &amp; t_out,\nconst geometry_msgs::msg::TransformStamped &amp; transform)\n</code></pre> <ul> <li><code>BoundingBox</code> (<code>autoware_auto_msgs</code>)</li> </ul> <pre><code>inline void doTransform(\nconst BoundingBox &amp; t_in, BoundingBox &amp; t_out,\nconst geometry_msgs::msg::TransformStamped &amp; transform)\n</code></pre> <ul> <li><code>BoundingBoxArray</code></li> </ul> <pre><code>inline void doTransform(\nconst BoundingBoxArray &amp; t_in,\nBoundingBoxArray &amp; t_out,\nconst geometry_msgs::msg::TransformStamped &amp; transform)\n</code></pre> <p>In addition, the following helper methods are also added:</p> <ul> <li><code>BoundingBoxArray</code></li> </ul> <pre><code>inline tf2::TimePoint getTimestamp(const BoundingBoxArray &amp; t)\n\ninline std::string getFrameId(const BoundingBoxArray &amp; t)\n</code></pre>"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#challenges","title":"Challenges","text":"<ul> <li><code>tf2_geometry_msgs</code> does not implement <code>doTransform</code> for any non-stamped data types, but it is   possible with the same function template. It is needed when transforming sub-data, with main data   that does have a stamp and can call doTransform on the sub-data with the same transform. Is this a useful upstream contribution?</li> <li><code>tf2_geometry_msgs</code> does not have <code>Point</code>, <code>Point32</code>, does not seem it needs one, also the   implementation of non-standard <code>toMsg</code> would not help the convert.</li> <li><code>BoundingBox</code> uses 32-bit float like <code>Quaternion32</code> and <code>Point32</code> to save space, as they are used   repeatedly in <code>BoundingBoxArray</code>. While transforming is it better to convert to 64-bit <code>Quaternion</code>,   <code>Point</code>, or <code>PoseStamped</code>, to re-use existing implementation of <code>doTransform</code>, or does it need to be   implemented? It may not be simple to template.</li> </ul>"},{"location":"common/autoware_testing/design/autoware_testing-design/","title":"autoware_testing","text":""},{"location":"common/autoware_testing/design/autoware_testing-design/#autoware_testing","title":"autoware_testing","text":"<p>This is the design document for the <code>autoware_testing</code> package.</p>"},{"location":"common/autoware_testing/design/autoware_testing-design/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>The package aims to provide a unified way to add standard testing functionality to the package, currently supporting:</p> <ul> <li>Smoke testing (<code>add_smoke_test</code>): launch a node with default configuration and ensure that it starts up and does not crash.</li> </ul>"},{"location":"common/autoware_testing/design/autoware_testing-design/#design","title":"Design","text":"<p>Uses <code>ros_testing</code> (which is an extension of <code>launch_testing</code>) and provides some parametrized, reusable standard tests to run.</p>"},{"location":"common/autoware_testing/design/autoware_testing-design/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Parametrization is limited to package, executable names, parameters filename and executable arguments. Test namespace is set as 'test'. Parameters file for the package is expected to be in <code>param</code> directory inside package.</p>"},{"location":"common/autoware_testing/design/autoware_testing-design/#inputs-outputs-api","title":"Inputs / Outputs / API","text":"<p>To add a smoke test to your package tests, add test dependency on <code>autoware_testing</code> to <code>package.xml</code></p> <pre><code>&lt;test_depend&gt;autoware_testing&lt;/test_depend&gt;\n</code></pre> <p>and add the following two lines to <code>CMakeLists.txt</code> in the <code>IF (BUILD_TESTING)</code> section:</p> <pre><code>find_package(autoware_testing REQUIRED)\nadd_smoke_test(&lt;package_name&gt; &lt;executable_name&gt; [PARAM_FILENAME &lt;param_filename&gt;] [EXECUTABLE_ARGUMENTS &lt;arguments&gt;])\n</code></pre> <p>Where</p> <p><code>&lt;package_name&gt;</code> - [required] tested node package name.</p> <p><code>&lt;executable_name&gt;</code> - [required] tested node executable name.</p> <p><code>&lt;param_filename&gt;</code> - [optional] param filename. Default value is <code>test.param.yaml</code>. Required mostly in situation where there are multiple smoke tests in a package and each requires different parameters set</p> <p><code>&lt;arguments&gt;</code> - [optional] arguments passed to executable. By default no arguments are passed.</p> <p>which adds <code>&lt;executable_name&gt;_smoke_test</code> test to suite.</p> <p>Example test result:</p> <pre><code>build/&lt;package_name&gt;/test_results/&lt;package_name&gt;/&lt;executable_name&gt;_smoke_test.xunit.xml: 1 test, 0 errors, 0 failures, 0 skipped\n</code></pre>"},{"location":"common/autoware_testing/design/autoware_testing-design/#references-external-links","title":"References / External links","text":"<ul> <li>https://en.wikipedia.org/wiki/Smoke_testing_(software)</li> <li>https://github.com/ros2/ros_testing</li> <li>https://github.com/ros2/launch/blob/master/launch_testing</li> </ul>"},{"location":"common/autoware_testing/design/autoware_testing-design/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":"<ul> <li>Adding more types of standard tests.</li> </ul>"},{"location":"common/autoware_testing/design/autoware_testing-design/#related-issues","title":"Related issues","text":"<ul> <li>Issue #700: add smoke test</li> <li>Issue #1224: Port other packages with smoke tests to use <code>autoware_testing</code></li> </ul>"},{"location":"common/bag_time_manager_rviz_plugin/","title":"bag_time_manager_rviz_plugin","text":""},{"location":"common/bag_time_manager_rviz_plugin/#bag_time_manager_rviz_plugin","title":"bag_time_manager_rviz_plugin","text":""},{"location":"common/bag_time_manager_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin allows publishing and controlling the ros bag time.</p>"},{"location":"common/bag_time_manager_rviz_plugin/#output","title":"Output","text":"<p>tbd.</p>"},{"location":"common/bag_time_manager_rviz_plugin/#howtouse","title":"HowToUse","text":"<ol> <li> <p>Start rviz and select panels/Add new panel.</p> <p></p> </li> <li> <p>Select BagTimeManagerPanel and press OK.</p> <p></p> </li> <li> <p>See bag_time_manager_rviz_plugin/BagTimeManagerPanel is added.</p> <p></p> </li> </ol> <ul> <li>Pause/Resume: pause/resume the clock.</li> <li>ApplyRate: apply rate of the clock.</li> </ul>"},{"location":"common/component_interface_specs/","title":"component_interface_specs","text":""},{"location":"common/component_interface_specs/#component_interface_specs","title":"component_interface_specs","text":"<p>This package is a specification of component interfaces.</p>"},{"location":"common/component_interface_tools/","title":"component_interface_tools","text":""},{"location":"common/component_interface_tools/#component_interface_tools","title":"component_interface_tools","text":"<p>This package provides the following tools for component interface.</p>"},{"location":"common/component_interface_tools/#service_log_checker","title":"service_log_checker","text":"<p>Monitor the service log of component_interface_utils and display if the response status is an error.</p>"},{"location":"common/component_interface_utils/","title":"component_interface_utils","text":""},{"location":"common/component_interface_utils/#component_interface_utils","title":"component_interface_utils","text":""},{"location":"common/component_interface_utils/#features","title":"Features","text":"<p>This is a utility package that provides the following features:</p> <ul> <li>Instantiation of the wrapper class</li> <li>Logging for service and client</li> <li>Service exception for response</li> <li>Relays for topic and service</li> </ul>"},{"location":"common/component_interface_utils/#design","title":"Design","text":"<p>This package provides the wrappers for the interface classes of rclcpp. The wrappers limit the usage of the original class to enforce the processing recommended by the component interface. Do not inherit the class of rclcpp, and forward or wrap the member function that is allowed to be used.</p>"},{"location":"common/component_interface_utils/#instantiation-of-the-wrapper-class","title":"Instantiation of the wrapper class","text":"<p>The wrapper class requires interface information in this format.</p> <pre><code>struct SampleService\n{\nusing Service = sample_msgs::srv::ServiceType;\nstatic constexpr char name[] = \"/sample/service\";\n};\n\nstruct SampleMessage\n{\nusing Message = sample_msgs::msg::MessageType;\nstatic constexpr char name[] = \"/sample/message\";\nstatic constexpr size_t depth = 1;\nstatic constexpr auto reliability = RMW_QOS_POLICY_RELIABILITY_RELIABLE;\nstatic constexpr auto durability = RMW_QOS_POLICY_DURABILITY_TRANSIENT_LOCAL;\n};\n</code></pre> <p>Create the wrapper using the above definition as follows.</p> <pre><code>// header file\ncomponent_interface_utils::Service&lt;SampleService&gt;::SharedPtr srv_;\ncomponent_interface_utils::Client&lt;SampleService&gt;::SharedPtr cli_;\ncomponent_interface_utils::Publisher&lt;SampleMessage&gt;::SharedPtr pub_;\ncomponent_interface_utils::Subscription&lt;SampleMessage&gt;::SharedPtr sub_;\n\n// source file\nconst auto node = component_interface_utils::NodeAdaptor(this);\nnode.init_srv(srv_, callback);\nnode.init_cli(cli_);\nnode.init_pub(pub_);\nnode.init_sub(sub_, callback);\n</code></pre>"},{"location":"common/component_interface_utils/#logging-for-service-and-client","title":"Logging for service and client","text":"<p>If the wrapper class is used, logging is automatically enabled. The log level is <code>RCLCPP_INFO</code>.</p>"},{"location":"common/component_interface_utils/#service-exception-for-response","title":"Service exception for response","text":"<p>If the wrapper class is used and the service response has status, throwing <code>ServiceException</code> will automatically catch and set it to status. This is useful when returning an error from a function called from the service callback.</p> <pre><code>void service_callback(Request req, Response res)\n{\nfunction();\nres-&gt;status.success = true;\n}\n\nvoid function()\n{\nthrow ServiceException(ERROR_CODE, \"message\");\n}\n</code></pre> <p>If the wrapper class is not used or the service response has no status, manually catch the <code>ServiceException</code> as follows.</p> <pre><code>void service_callback(Request req, Response res)\n{\ntry {\nfunction();\nres-&gt;status.success = true;\n} catch (const ServiceException &amp; error) {\nres-&gt;status = error.status();\n}\n}\n</code></pre>"},{"location":"common/component_interface_utils/#relays-for-topic-and-service","title":"Relays for topic and service","text":"<p>There are utilities for relaying services and messages of the same type.</p> <pre><code>const auto node = component_interface_utils::NodeAdaptor(this);\nservice_callback_group_ = create_callback_group(rclcpp::CallbackGroupType::MutuallyExclusive);\nnode.relay_message(pub_, sub_);\nnode.relay_service(cli_, srv_, service_callback_group_);  // group is for avoiding deadlocks\n</code></pre>"},{"location":"common/cuda_utils/","title":"cuda_utils","text":""},{"location":"common/cuda_utils/#cuda_utils","title":"cuda_utils","text":""},{"location":"common/cuda_utils/#purpose","title":"Purpose","text":"<p>This package contains a library of common functions related to CUDA.</p>"},{"location":"common/fake_test_node/design/fake_test_node-design/","title":"Fake Test Node","text":""},{"location":"common/fake_test_node/design/fake_test_node-design/#fake-test-node","title":"Fake Test Node","text":""},{"location":"common/fake_test_node/design/fake_test_node-design/#what-this-package-provides","title":"What this package provides","text":"<p>When writing an integration test for a node in C++ using GTest, there is quite some boilerplate code that needs to be written to set up a fake node that would publish expected messages on an expected topic and subscribes to messages on some other topic. This is usually implemented as a custom GTest fixture.</p> <p>This package contains a library that introduces two utility classes that can be used in place of custom fixtures described above to write integration tests for a node:</p> <ul> <li><code>autoware::tools::testing::FakeTestNode</code> - to use as a custom test fixture with <code>TEST_F</code> tests</li> <li><code>autoware::tools::testing::FakeTestNodeParametrized</code> - to use a custom test fixture with the   parametrized <code>TEST_P</code> tests (accepts a template parameter that gets forwarded to   <code>testing::TestWithParam&lt;T&gt;</code>)</li> </ul> <p>These fixtures take care of initializing and re-initializing rclcpp as well as of checking that all subscribers and publishers have a match, thus reducing the amount of boilerplate code that the user needs to write.</p>"},{"location":"common/fake_test_node/design/fake_test_node-design/#how-to-use-this-library","title":"How to use this library","text":"<p>After including the relevant header the user can use a typedef to use a custom fixture name and use the provided classes as fixtures in <code>TEST_F</code> and <code>TEST_P</code> tests directly.</p>"},{"location":"common/fake_test_node/design/fake_test_node-design/#example-usage","title":"Example usage","text":"<p>Let's say there is a node <code>NodeUnderTest</code> that requires testing. It just subscribes to <code>std_msgs::msg::Int32</code> messages and publishes a <code>std_msgs::msg::Bool</code> to indicate that the input is positive. To test such a node the following code can be used utilizing the <code>autoware::tools::testing::FakeTestNode</code>:</p> <pre><code>using FakeNodeFixture = autoware::tools::testing::FakeTestNode;\n\n/// @test Test that we can use a non-parametrized test.\nTEST_F(FakeNodeFixture, Test) {\nInt32 msg{};\nmsg.data = 15;\nconst auto node = std::make_shared&lt;NodeUnderTest&gt;();\n\nBool::SharedPtr last_received_msg{};\nauto fake_odom_publisher = create_publisher&lt;Int32&gt;(\"/input_topic\");\nauto result_odom_subscription = create_subscription&lt;Bool&gt;(\"/output_topic\", *node,\n[&amp;last_received_msg](const Bool::SharedPtr msg) {last_received_msg = msg;});\n\nconst auto dt{std::chrono::milliseconds{100LL}};\nconst auto max_wait_time{std::chrono::seconds{10LL}};\nauto time_passed{std::chrono::milliseconds{0LL}};\nwhile (!last_received_msg) {\nfake_odom_publisher-&gt;publish(msg);\nrclcpp::spin_some(node);\nrclcpp::spin_some(get_fake_node());\nstd::this_thread::sleep_for(dt);\ntime_passed += dt;\nif (time_passed &gt; max_wait_time) {\nFAIL() &lt;&lt; \"Did not receive a message soon enough.\";\n}\n}\nEXPECT_TRUE(last_received_msg-&gt;data);\nSUCCEED();\n}\n</code></pre> <p>Here only the <code>TEST_F</code> example is shown but a <code>TEST_P</code> usage is very similar with a little bit more boilerplate to set up all the parameter values, see <code>test_fake_test_node.cpp</code> for an example usage.</p>"},{"location":"common/geography_utils/","title":"geography_utils","text":""},{"location":"common/geography_utils/#geography_utils","title":"geography_utils","text":""},{"location":"common/geography_utils/#purpose","title":"Purpose","text":"<p>This package contains geography-related functions used by other packages, so please refer to them as needed.</p>"},{"location":"common/global_parameter_loader/Readme/","title":"Autoware Global Parameter Loader","text":""},{"location":"common/global_parameter_loader/Readme/#autoware-global-parameter-loader","title":"Autoware Global Parameter Loader","text":"<p>This package is to set common ROS parameters to each node.</p>"},{"location":"common/global_parameter_loader/Readme/#usage","title":"Usage","text":"<p>Add the following lines to the launch file of the node in which you want to get global parameters.</p> <pre><code>&lt;!-- Global parameters --&gt;\n&lt;include file=\"$(find-pkg-share global_parameter_loader)/launch/global_params.launch.py\"&gt;\n&lt;arg name=\"vehicle_model\" value=\"$(var vehicle_model)\"/&gt;\n&lt;/include&gt;\n</code></pre> <p>The vehicle model parameter is read from <code>config/vehicle_info.param.yaml</code> in <code>vehicle_model</code>_description package.</p>"},{"location":"common/global_parameter_loader/Readme/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Currently only vehicle_info is loaded by this launcher.</p>"},{"location":"common/glog_component/","title":"glog_component","text":""},{"location":"common/glog_component/#glog_component","title":"glog_component","text":"<p>This package provides the glog (google logging library) feature as a ros2 component library. This is used to dynamically load the glog feature with container.</p> <p>See the glog github for the details of its features.</p>"},{"location":"common/glog_component/#example","title":"Example","text":"<p>When you load the <code>glog_component</code> in container, the launch file can be like below:</p> <pre><code>glog_component = ComposableNode(\n    package=\"glog_component\",\n    plugin=\"GlogComponent\",\n    name=\"glog_component\",\n)\n\ncontainer = ComposableNodeContainer(\n    name=\"my_container\",\n    namespace=\"\",\n    package=\"rclcpp_components\",\n    executable=LaunchConfiguration(\"container_executable\"),\n    composable_node_descriptions=[\n        component1,\n        component2,\n        glog_component,\n    ],\n)\n</code></pre>"},{"location":"common/goal_distance_calculator/Readme/","title":"goal_distance_calculator","text":""},{"location":"common/goal_distance_calculator/Readme/#goal_distance_calculator","title":"goal_distance_calculator","text":""},{"location":"common/goal_distance_calculator/Readme/#purpose","title":"Purpose","text":"<p>This node publishes deviation of self-pose from goal pose.</p>"},{"location":"common/goal_distance_calculator/Readme/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"common/goal_distance_calculator/Readme/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/goal_distance_calculator/Readme/#input","title":"Input","text":"Name Type Description <code>/planning/mission_planning/route</code> <code>autoware_auto_planning_msgs::msg::Route</code> Used to get goal pose <code>/tf</code> <code>tf2_msgs/TFMessage</code> TF (self-pose)"},{"location":"common/goal_distance_calculator/Readme/#output","title":"Output","text":"Name Type Description <code>deviation/lateral</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> publish lateral deviation of self-pose from goal pose[m] <code>deviation/longitudinal</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> publish longitudinal deviation of self-pose from goal pose[m] <code>deviation/yaw</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> publish yaw deviation of self-pose from goal pose[rad] <code>deviation/yaw_deg</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> publish yaw deviation of self-pose from goal pose[deg]"},{"location":"common/goal_distance_calculator/Readme/#parameters","title":"Parameters","text":""},{"location":"common/goal_distance_calculator/Readme/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Explanation <code>update_rate</code> double 10.0 Timer callback period. [Hz]"},{"location":"common/goal_distance_calculator/Readme/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Explanation <code>oneshot</code> bool true publish deviations just once or repeatedly"},{"location":"common/goal_distance_calculator/Readme/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"common/grid_map_utils/","title":"Grid Map Utils","text":""},{"location":"common/grid_map_utils/#grid-map-utils","title":"Grid Map Utils","text":""},{"location":"common/grid_map_utils/#overview","title":"Overview","text":"<p>This packages contains a re-implementation of the <code>grid_map::PolygonIterator</code> used to iterate over all cells of a grid map contained inside some polygon.</p>"},{"location":"common/grid_map_utils/#algorithm","title":"Algorithm","text":"<p>This implementation uses the scan line algorithm, a common algorithm used to draw polygons on a rasterized image. The main idea of the algorithm adapted to a grid map is as follow:</p> <ul> <li>calculate intersections between rows of the grid map and the edges of the polygon edges;</li> <li>calculate for each row the column between each pair of intersections;</li> <li>the resulting <code>(row, column)</code> indexes are inside of the polygon.</li> </ul> <p>More details on the scan line algorithm can be found in the References.</p>"},{"location":"common/grid_map_utils/#api","title":"API","text":"<p>The <code>grid_map_utils::PolygonIterator</code> follows the same API as the original <code>grid_map::PolygonIterator</code>.</p>"},{"location":"common/grid_map_utils/#assumptions","title":"Assumptions","text":"<p>The behavior of the <code>grid_map_utils::PolygonIterator</code> is only guaranteed to match the <code>grid_map::PolygonIterator</code> if edges of the polygon do not exactly cross any cell center. In such a case, whether the crossed cell is considered inside or outside of the polygon can vary due to floating precision error.</p>"},{"location":"common/grid_map_utils/#performances","title":"Performances","text":"<p>Benchmarking code is implemented in <code>test/benchmarking.cpp</code> and is also used to validate that the <code>grid_map_utils::PolygonIterator</code> behaves exactly like the <code>grid_map::PolygonIterator</code>.</p> <p>The following figure shows a comparison of the runtime between the implementation of this package (<code>grid_map_utils</code>) and the original implementation (<code>grid_map</code>). The time measured includes the construction of the iterator and the iteration over all indexes and is shown using a logarithmic scale. Results were obtained varying the side size of a square grid map with <code>100 &lt;= n &lt;= 1000</code> (size=<code>n</code> means a grid of <code>n x n</code> cells), random polygons with a number of vertices <code>3 &lt;= m &lt;= 100</code> and with each parameter <code>(n,m)</code> repeated 10 times.</p> <p></p>"},{"location":"common/grid_map_utils/#future-improvements","title":"Future improvements","text":"<p>There exists variations of the scan line algorithm for multiple polygons. These can be implemented if we want to iterate over the cells contained in at least one of multiple polygons.</p> <p>The current implementation imitate the behavior of the original <code>grid_map::PolygonIterator</code> where a cell is selected if its center position is inside the polygon. This behavior could be changed for example to only return all cells overlapped by the polygon.</p>"},{"location":"common/grid_map_utils/#references","title":"References","text":"<ul> <li>https://en.wikipedia.org/wiki/Scanline_rendering</li> <li>https://web.cs.ucdavis.edu/~ma/ECS175_S00/Notes/0411_b.pdf</li> </ul>"},{"location":"common/interpolation/","title":"Interpolation package","text":""},{"location":"common/interpolation/#interpolation-package","title":"Interpolation package","text":"<p>This package supplies linear and spline interpolation functions.</p>"},{"location":"common/interpolation/#linear-interpolation","title":"Linear Interpolation","text":"<p><code>lerp(src_val, dst_val, ratio)</code> (for scalar interpolation) interpolates <code>src_val</code> and <code>dst_val</code> with <code>ratio</code>. This will be replaced with <code>std::lerp(src_val, dst_val, ratio)</code> in <code>C++20</code>.</p> <p><code>lerp(base_keys, base_values, query_keys)</code> (for vector interpolation) applies linear regression to each two continuous points whose x values are<code>base_keys</code> and whose y values are <code>base_values</code>. Then it calculates interpolated values on y-axis for <code>query_keys</code> on x-axis.</p>"},{"location":"common/interpolation/#spline-interpolation","title":"Spline Interpolation","text":"<p><code>spline(base_keys, base_values, query_keys)</code> (for vector interpolation) applies spline regression to each two continuous points whose x values are<code>base_keys</code> and whose y values are <code>base_values</code>. Then it calculates interpolated values on y-axis for <code>query_keys</code> on x-axis.</p>"},{"location":"common/interpolation/#evaluation-of-calculation-cost","title":"Evaluation of calculation cost","text":"<p>We evaluated calculation cost of spline interpolation for 100 points, and adopted the best one which is tridiagonal matrix algorithm. Methods except for tridiagonal matrix algorithm exists in <code>spline_interpolation</code> package, which has been removed from Autoware.</p> Method Calculation time Tridiagonal Matrix Algorithm 0.007 [ms] Preconditioned Conjugate Gradient 0.024 [ms] Successive Over-Relaxation 0.074 [ms]"},{"location":"common/interpolation/#spline-interpolation-algorithm","title":"Spline Interpolation Algorithm","text":"<p>Assuming that the size of <code>base_keys</code> (\\(x_i\\)) and <code>base_values</code> (\\(y_i\\)) are \\(N + 1\\), we aim to calculate spline interpolation with the following equation to interpolate between \\(y_i\\) and \\(y_{i+1}\\).</p> \\[ Y_i(x) = a_i (x - x_i)^3 + b_i (x - x_i)^2 + c_i (x - x_i) + d_i \\ \\ \\ (i = 0, \\dots, N-1) \\] <p>Constraints on spline interpolation are as follows. The number of constraints is \\(4N\\), which is equal to the number of variables of spline interpolation.</p> \\[ \\begin{align} Y_i (x_i) &amp; = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y_i (x_{i+1}) &amp; = y_{i+1} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y'_i (x_{i+1}) &amp; = Y'_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_i (x_{i+1}) &amp; = Y''_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_0 (x_0) &amp; = 0 \\\\ Y''_{N-1} (x_N) &amp; = 0 \\end{align} \\] <p>According to this article, spline interpolation is formulated as the following linear equation.</p> \\[ \\begin{align}  \\begin{pmatrix}     2(h_0 + h_1) &amp; h_1 \\\\     h_0 &amp; 2 (h_1 + h_2) &amp; h_2 &amp; &amp; O \\\\         &amp;     &amp;     &amp; \\ddots \\\\     O &amp;     &amp;     &amp;       &amp; h_{N-2} &amp; 2 (h_{N-2} + h_{N-1})  \\end{pmatrix}  \\begin{pmatrix}     v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_{N-1}  \\end{pmatrix}=  \\begin{pmatrix}     w_1 \\\\ w_2 \\\\ w_3 \\\\ \\vdots \\\\ w_{N-1}  \\end{pmatrix} \\end{align} \\] <p>where</p> \\[ \\begin{align} h_i &amp; = x_{i+1} - x_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ w_i &amp; = 6 \\left(\\frac{y_{i+1} - y_{i+1}}{h_i} - \\frac{y_i - y_{i-1}}{h_{i-1}}\\right) \\ \\ \\ (i = 1, \\dots, N-1) \\end{align} \\] <p>The coefficient matrix of this linear equation is tridiagonal matrix. Therefore, it can be solve with tridiagonal matrix algorithm, which can solve linear equations without gradient descent methods.</p> <p>Solving this linear equation with tridiagonal matrix algorithm, we can calculate coefficients of spline interpolation as follows.</p> \\[ \\begin{align} a_i &amp; = \\frac{v_{i+1} - v_i}{6 (x_{i+1} - x_i)} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ b_i &amp; = \\frac{v_i}{2} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ c_i &amp; = \\frac{y_{i+1} - y_i}{x_{i+1} - x_i} - \\frac{1}{6}(x_{i+1} - x_i)(2 v_i + v_{i+1}) \\ \\ \\ (i = 0, \\dots, N-1) \\\\ d_i &amp; = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\end{align} \\]"},{"location":"common/interpolation/#tridiagonal-matrix-algorithm","title":"Tridiagonal Matrix Algorithm","text":"<p>We solve tridiagonal linear equation according to this article where variables of linear equation are expressed as follows in the implementation.</p> \\[ \\begin{align}  \\begin{pmatrix}     b_0 &amp; c_0 &amp;     &amp; \\\\     a_0 &amp; b_1 &amp; c_2 &amp; O \\\\         &amp;     &amp; \\ddots \\\\     O &amp;     &amp; a_{N-2} &amp;  b_{N-1}  \\end{pmatrix} x = \\begin{pmatrix}     d_0 \\\\ d_2 \\\\ d_3 \\\\ \\vdots \\\\ d_{N-1}  \\end{pmatrix} \\end{align} \\]"},{"location":"common/kalman_filter/","title":"kalman_filter","text":""},{"location":"common/kalman_filter/#kalman_filter","title":"kalman_filter","text":""},{"location":"common/kalman_filter/#purpose","title":"Purpose","text":"<p>This common package contains the kalman filter with time delay and the calculation of the kalman filter.</p>"},{"location":"common/kalman_filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"common/motion_utils/","title":"Motion Utils package","text":""},{"location":"common/motion_utils/#motion-utils-package","title":"Motion Utils package","text":""},{"location":"common/motion_utils/#definition-of-terms","title":"Definition of terms","text":""},{"location":"common/motion_utils/#segment","title":"Segment","text":"<p><code>Segment</code> in Autoware is the line segment between two successive points as follows.</p> <p></p> <p>The nearest segment index and nearest point index to a certain position is not always th same. Therefore, we prepare two different utility functions to calculate a nearest index for points and segments.</p>"},{"location":"common/motion_utils/#nearest-index-search","title":"Nearest index search","text":"<p>In this section, the nearest index and nearest segment index search is explained.</p> <p>We have the same functions for the nearest index search and nearest segment index search. Taking for the example the nearest index search, we have two types of functions.</p> <p>The first function finds the nearest index with distance and yaw thresholds.</p> <pre><code>template &lt;class T&gt;\nsize_t findFirstNearestIndexWithSoftConstraints(\nconst T &amp; points, const geometry_msgs::msg::Pose &amp; pose,\nconst double dist_threshold = std::numeric_limits&lt;double&gt;::max(),\nconst double yaw_threshold = std::numeric_limits&lt;double&gt;::max());\n</code></pre> <p>This function finds the first local solution within thresholds. The reason to find the first local one is to deal with some edge cases explained in the next subsection.</p> <p>There are default parameters for thresholds arguments so that you can decide which thresholds to pass to the function.</p> <ol> <li>When both the distance and yaw thresholds are given.<ul> <li>First, try to find the nearest index with both the distance and yaw thresholds.</li> <li>If not found, try to find again with only the distance threshold.</li> <li>If not found, find without any thresholds.</li> </ul> </li> <li>When only distance are given.<ul> <li>First, try to find the nearest index the distance threshold.</li> <li>If not found, find without any thresholds.</li> </ul> </li> <li>When no thresholds are given.<ul> <li>Find the nearest index.</li> </ul> </li> </ol> <p>The second function finds the nearest index in the lane whose id is <code>lane_id</code>.</p> <pre><code>size_t findNearestIndexFromLaneId(\nconst autoware_auto_planning_msgs::msg::PathWithLaneId &amp; path,\nconst geometry_msgs::msg::Point &amp; pos, const int64_t lane_id);\n</code></pre>"},{"location":"common/motion_utils/#application-to-various-object","title":"Application to various object","text":"<p>Many node packages often calculate the nearest index of objects. We will explain the recommended method to calculate it.</p>"},{"location":"common/motion_utils/#nearest-index-for-the-ego","title":"Nearest index for the ego","text":"<p>Assuming that the path length before the ego is short enough, we expect to find the correct nearest index in the following edge cases by <code>findFirstNearestIndexWithSoftConstraints</code> with both distance and yaw thresholds. Blue circles describes the distance threshold from the base link position and two blue lines describe the yaw threshold against the base link orientation. Among points in these cases, the correct nearest point which is red can be found.</p> <p></p> <p>Therefore, the implementation is as follows.</p> <pre><code>const size_t ego_nearest_idx = findFirstNearestIndexWithSoftConstraints(points, ego_pose, ego_nearest_dist_threshold, ego_nearest_yaw_threshold);\nconst size_t ego_nearest_seg_idx = findFirstNearestIndexWithSoftConstraints(points, ego_pose, ego_nearest_dist_threshold, ego_nearest_yaw_threshold);\n</code></pre>"},{"location":"common/motion_utils/#nearest-index-for-dynamic-objects","title":"Nearest index for dynamic objects","text":"<p>For the ego nearest index, the orientation is considered in addition to the position since the ego is supposed to follow the points. However, for the dynamic objects (e.g., predicted object), sometimes its orientation may be different from the points order, e.g. the dynamic object driving backward although the ego is driving forward.</p> <p>Therefore, the yaw threshold should not be considered for the dynamic object. The implementation is as follows.</p> <pre><code>const size_t dynamic_obj_nearest_idx = findFirstNearestIndexWithSoftConstraints(points, dynamic_obj_pose, dynamic_obj_nearest_dist_threshold);\nconst size_t dynamic_obj_nearest_seg_idx = findFirstNearestIndexWithSoftConstraints(points, dynamic_obj_pose, dynamic_obj_nearest_dist_threshold);\n</code></pre>"},{"location":"common/motion_utils/#nearest-index-for-traffic-objects","title":"Nearest index for traffic objects","text":"<p>In lanelet maps, traffic objects belong to the specific lane. With this specific lane's id, the correct nearest index can be found.</p> <p>The implementation is as follows.</p> <pre><code>// first extract `lane_id` which the traffic object belong to.\nconst size_t traffic_obj_nearest_idx = findNearestIndexFromLaneId(path_with_lane_id, traffic_obj_pos, lane_id);\nconst size_t traffic_obj_nearest_seg_idx = findNearestSegmentIndexFromLaneId(path_with_lane_id, traffic_obj_pos, lane_id);\n</code></pre>"},{"location":"common/motion_utils/#pathtrajectory-length-calculation-between-designated-points","title":"Path/Trajectory length calculation between designated points","text":"<p>Based on the discussion so far, the nearest index search algorithm is different depending on the object type. Therefore, we recommended using the wrapper utility functions which require the nearest index search (e.g., calculating the path length) with each nearest index search.</p> <p>For example, when we want to calculate the path length between the ego and the dynamic object, the implementation is as follows.</p> <pre><code>const size_t ego_nearest_seg_idx = findFirstNearestSegmentIndex(points, ego_pose, ego_nearest_dist_threshold, ego_nearest_yaw_threshold);\nconst size_t dyn_obj_nearest_seg_idx = findFirstNearestSegmentIndex(points, dyn_obj_pose, dyn_obj_nearest_dist_threshold);\nconst double length_from_ego_to_obj = calcSignedArcLength(points, ego_pose, ego_nearest_seg_idx, dyn_obj_pose, dyn_obj_nearest_seg_idx);\n</code></pre>"},{"location":"common/motion_utils/#for-developers","title":"For developers","text":"<p>Some of the template functions in <code>trajectory.hpp</code> are mostly used for specific types (<code>autoware_auto_planning_msgs::msg::PathPoint</code>, <code>autoware_auto_planning_msgs::msg::PathPoint</code>, <code>autoware_auto_planning_msgs::msg::TrajectoryPoint</code>), so they are exported as <code>extern template</code> functions to speed-up compilation time.</p> <p><code>motion_utils.hpp</code> header file was removed because the source files that directly/indirectly include this file took a long time for preprocessing.</p>"},{"location":"common/motion_utils/docs/vehicle/vehicle/","title":"vehicle utils","text":""},{"location":"common/motion_utils/docs/vehicle/vehicle/#vehicle-utils","title":"vehicle utils","text":"<p>Vehicle utils provides a convenient library used to check vehicle status.</p>"},{"location":"common/motion_utils/docs/vehicle/vehicle/#feature","title":"Feature","text":"<p>The library contains following classes.</p>"},{"location":"common/motion_utils/docs/vehicle/vehicle/#vehicle_stop_checker","title":"vehicle_stop_checker","text":"<p>This class check whether the vehicle is stopped or not based on localization result.</p>"},{"location":"common/motion_utils/docs/vehicle/vehicle/#subscribed-topics","title":"Subscribed Topics","text":"Name Type Description <code>/localization/kinematic_state</code> <code>nav_msgs::msg::Odometry</code> vehicle odometry"},{"location":"common/motion_utils/docs/vehicle/vehicle/#parameters","title":"Parameters","text":"Name Type Default Value Explanation <code>velocity_buffer_time_sec</code> double 10.0 odometry buffering time [s]"},{"location":"common/motion_utils/docs/vehicle/vehicle/#member-functions","title":"Member functions","text":"<pre><code>bool isVehicleStopped(const double stop_duration)\n</code></pre> <ul> <li>Check simply whether the vehicle is stopped based on the localization result.</li> <li>Returns <code>true</code> if the vehicle is stopped, even if system outputs a non-zero target velocity.</li> </ul>"},{"location":"common/motion_utils/docs/vehicle/vehicle/#example-usage","title":"Example Usage","text":"<p>Necessary includes:</p> <pre><code>#include &lt;tier4_autoware_utils/vehicle/vehicle_state_checker.hpp&gt;\n</code></pre> <p>1.Create a checker instance.</p> <pre><code>class SampleNode : public rclcpp::Node\n{\npublic:\nSampleNode() : Node(\"sample_node\")\n{\nvehicle_stop_checker_ = std::make_unique&lt;VehicleStopChecker&gt;(this);\n}\n\nstd::unique_ptr&lt;VehicleStopChecker&gt; vehicle_stop_checker_;\n\nbool sampleFunc();\n\n...\n}\n</code></pre> <p>2.Check the vehicle state.</p> <pre><code>bool SampleNode::sampleFunc()\n{\n...\n\nconst auto result_1 = vehicle_stop_checker_-&gt;isVehicleStopped();\n\n...\n\nconst auto result_2 = vehicle_stop_checker_-&gt;isVehicleStopped(3.0);\n\n...\n}\n</code></pre>"},{"location":"common/motion_utils/docs/vehicle/vehicle/#vehicle_arrival_checker","title":"vehicle_arrival_checker","text":"<p>This class check whether the vehicle arrive at stop point based on localization and planning result.</p>"},{"location":"common/motion_utils/docs/vehicle/vehicle/#subscribed-topics_1","title":"Subscribed Topics","text":"Name Type Description <code>/localization/kinematic_state</code> <code>nav_msgs::msg::Odometry</code> vehicle odometry <code>/planning/scenario_planning/trajectory</code> <code>autoware_auto_planning_msgs::msg::Trajectory</code> trajectory"},{"location":"common/motion_utils/docs/vehicle/vehicle/#parameters_1","title":"Parameters","text":"Name Type Default Value Explanation <code>velocity_buffer_time_sec</code> double 10.0 odometry buffering time [s] <code>th_arrived_distance_m</code> double 1.0 threshold distance to check if vehicle has arrived at target point [m]"},{"location":"common/motion_utils/docs/vehicle/vehicle/#member-functions_1","title":"Member functions","text":"<pre><code>bool isVehicleStopped(const double stop_duration)\n</code></pre> <ul> <li>Check simply whether the vehicle is stopped based on the localization result.</li> <li>Returns <code>true</code> if the vehicle is stopped, even if system outputs a non-zero target velocity.</li> </ul> <pre><code>bool isVehicleStoppedAtStopPoint(const double stop_duration)\n</code></pre> <ul> <li>Check whether the vehicle is stopped at stop point based on the localization and planning result.</li> <li>Returns <code>true</code> if the vehicle is not only stopped but also arrived at stop point.</li> </ul>"},{"location":"common/motion_utils/docs/vehicle/vehicle/#example-usage_1","title":"Example Usage","text":"<p>Necessary includes:</p> <pre><code>#include &lt;tier4_autoware_utils/vehicle/vehicle_state_checker.hpp&gt;\n</code></pre> <p>1.Create a checker instance.</p> <pre><code>class SampleNode : public rclcpp::Node\n{\npublic:\nSampleNode() : Node(\"sample_node\")\n{\nvehicle_arrival_checker_ = std::make_unique&lt;VehicleArrivalChecker&gt;(this);\n}\n\nstd::unique_ptr&lt;VehicleArrivalChecker&gt; vehicle_arrival_checker_;\n\nbool sampleFunc();\n\n...\n}\n</code></pre> <p>2.Check the vehicle state.</p> <pre><code>bool SampleNode::sampleFunc()\n{\n...\n\nconst auto result_1 = vehicle_arrival_checker_-&gt;isVehicleStopped();\n\n...\n\nconst auto result_2 = vehicle_arrival_checker_-&gt;isVehicleStopped(3.0);\n\n...\n\nconst auto result_3 = vehicle_arrival_checker_-&gt;isVehicleStoppedAtStopPoint();\n\n...\n\nconst auto result_4 = vehicle_arrival_checker_-&gt;isVehicleStoppedAtStopPoint(3.0);\n\n...\n}\n</code></pre>"},{"location":"common/motion_utils/docs/vehicle/vehicle/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p><code>vehicle_stop_checker</code> and <code>vehicle_arrival_checker</code> cannot check whether the vehicle is stopped more than <code>velocity_buffer_time_sec</code> second.</p>"},{"location":"common/object_recognition_utils/","title":"object_recognition_utils","text":""},{"location":"common/object_recognition_utils/#object_recognition_utils","title":"object_recognition_utils","text":""},{"location":"common/object_recognition_utils/#purpose","title":"Purpose","text":"<p>This package contains a library of common functions that are useful across the object recognition module. This package may include functions for converting between different data types, msg types, and performing common operations on them.</p>"},{"location":"common/osqp_interface/design/osqp_interface-design/","title":"Interface for the OSQP library","text":""},{"location":"common/osqp_interface/design/osqp_interface-design/#interface-for-the-osqp-library","title":"Interface for the OSQP library","text":"<p>This is the design document for the <code>osqp_interface</code> package.</p>"},{"location":"common/osqp_interface/design/osqp_interface-design/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>This packages provides a C++ interface for the OSQP library.</p>"},{"location":"common/osqp_interface/design/osqp_interface-design/#design","title":"Design","text":"<p>The class <code>OSQPInterface</code> takes a problem formulation as Eigen matrices and vectors, converts these objects into C-style Compressed-Column-Sparse matrices and dynamic arrays, loads the data into the OSQP workspace dataholder, and runs the optimizer.</p>"},{"location":"common/osqp_interface/design/osqp_interface-design/#inputs-outputs-api","title":"Inputs / Outputs / API","text":"<p>The interface can be used in several ways:</p> <ol> <li> <p>Initialize the interface WITHOUT data. Load the problem formulation at the optimization call.</p> <pre><code>    osqp_interface = OSQPInterface();\nosqp_interface.optimize(P, A, q, l, u);\n</code></pre> </li> <li> <p>Initialize the interface WITH data.</p> <pre><code>    osqp_interface = OSQPInterface(P, A, q, l, u);\nosqp_interface.optimize();\n</code></pre> </li> <li> <p>WARM START OPTIMIZATION by modifying the problem formulation between optimization runs.</p> <pre><code>    osqp_interface = OSQPInterface(P, A, q, l, u);\nosqp_interface.optimize();\nosqp.initializeProblem(P_new, A_new, q_new, l_new, u_new);\nosqp_interface.optimize();\n</code></pre> <p>The optimization results are returned as a vector by the optimization function.</p> <pre><code>std::tuple&lt;std::vector&lt;double&gt;, std::vector&lt;double&gt;&gt; result = osqp_interface.optimize();\nstd::vector&lt;double&gt; param = std::get&lt;0&gt;(result);\ndouble x_0 = param[0];\ndouble x_1 = param[1];\n</code></pre> </li> </ol>"},{"location":"common/osqp_interface/design/osqp_interface-design/#references-external-links","title":"References / External links","text":"<ul> <li>OSQP library: https://osqp.org/</li> </ul>"},{"location":"common/osqp_interface/design/osqp_interface-design/#related-issues","title":"Related issues","text":""},{"location":"common/path_distance_calculator/Readme/","title":"path_distance_calculator","text":""},{"location":"common/path_distance_calculator/Readme/#path_distance_calculator","title":"path_distance_calculator","text":""},{"location":"common/path_distance_calculator/Readme/#purpose","title":"Purpose","text":"<p>This node publishes a distance from the closest path point from the self-position to the end point of the path. Note that the distance means the arc-length along the path, not the Euclidean distance between the two points.</p>"},{"location":"common/path_distance_calculator/Readme/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"common/path_distance_calculator/Readme/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/path_distance_calculator/Readme/#input","title":"Input","text":"Name Type Description <code>/planning/scenario_planning/lane_driving/behavior_planning/path</code> <code>autoware_auto_planning_msgs::msg::Path</code> Reference path <code>/tf</code> <code>tf2_msgs/TFMessage</code> TF (self-pose)"},{"location":"common/path_distance_calculator/Readme/#output","title":"Output","text":"Name Type Description <code>~/distance</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> Publish a distance from the closest path point from the self-position to the end point of the path[m]"},{"location":"common/path_distance_calculator/Readme/#parameters","title":"Parameters","text":""},{"location":"common/path_distance_calculator/Readme/#node-parameters","title":"Node Parameters","text":"<p>None.</p>"},{"location":"common/path_distance_calculator/Readme/#core-parameters","title":"Core Parameters","text":"<p>None.</p>"},{"location":"common/path_distance_calculator/Readme/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"common/perception_utils/","title":"perception_utils","text":""},{"location":"common/perception_utils/#perception_utils","title":"perception_utils","text":""},{"location":"common/perception_utils/#purpose","title":"Purpose","text":"<p>This package contains a library of common functions that are useful across the perception module.</p>"},{"location":"common/polar_grid/Readme/","title":"Polar Grid","text":""},{"location":"common/polar_grid/Readme/#polar-grid","title":"Polar Grid","text":""},{"location":"common/polar_grid/Readme/#purpose","title":"Purpose","text":"<p>This plugin displays polar grid around ego vehicle in Rviz.</p>"},{"location":"common/polar_grid/Readme/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Explanation <code>Max Range</code> float 200.0f max range for polar grid. [m] <code>Wave Velocity</code> float 100.0f wave ring velocity. [m/s] <code>Delta Range</code> float 10.0f wave ring distance for polar grid. [m]"},{"location":"common/polar_grid/Readme/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"common/qp_interface/design/qp_interface-design/","title":"Interface for QP solvers","text":""},{"location":"common/qp_interface/design/qp_interface-design/#interface-for-qp-solvers","title":"Interface for QP solvers","text":"<p>This is the design document for the <code>qp_interface</code> package.</p>"},{"location":"common/qp_interface/design/qp_interface-design/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>This packages provides a C++ interface for QP solvers. Currently, supported QP solvers are</p> <ul> <li>OSQP library</li> </ul>"},{"location":"common/qp_interface/design/qp_interface-design/#design","title":"Design","text":"<p>The class <code>QPInterface</code> takes a problem formulation as Eigen matrices and vectors, converts these objects into C-style Compressed-Column-Sparse matrices and dynamic arrays, loads the data into the QP workspace dataholder, and runs the optimizer.</p>"},{"location":"common/qp_interface/design/qp_interface-design/#inputs-outputs-api","title":"Inputs / Outputs / API","text":"<p>The interface can be used in several ways:</p> <ol> <li> <p>Initialize the interface, and load the problem formulation at the optimization call.</p> <pre><code>    QPInterface qp_interface;\nqp_interface.optimize(P, A, q, l, u);\n</code></pre> </li> <li> <p>WARM START OPTIMIZATION by modifying the problem formulation between optimization runs.</p> <pre><code>    QPInterface qp_interface(true);\nqp_interface.optimize(P, A, q, l, u);\nqp_interface.optimize(P_new, A_new, q_new, l_new, u_new);\n</code></pre> <p>The optimization results are returned as a vector by the optimization function.</p> <pre><code>const auto solution = qp_interface.optimize();\ndouble x_0 = solution[0];\ndouble x_1 = solution[1];\n</code></pre> </li> </ol>"},{"location":"common/qp_interface/design/qp_interface-design/#references-external-links","title":"References / External links","text":"<ul> <li>OSQP library: https://osqp.org/</li> </ul>"},{"location":"common/qp_interface/design/qp_interface-design/#related-issues","title":"Related issues","text":""},{"location":"common/rtc_manager_rviz_plugin/","title":"rtc_manager_rviz_plugin","text":""},{"location":"common/rtc_manager_rviz_plugin/#rtc_manager_rviz_plugin","title":"rtc_manager_rviz_plugin","text":""},{"location":"common/rtc_manager_rviz_plugin/#purpose","title":"Purpose","text":"<p>The purpose of this Rviz plugin is</p> <ol> <li> <p>To display each content of RTC status.</p> </li> <li> <p>To switch each module of RTC auto mode.</p> </li> <li> <p>To change RTC cooperate commands by button.</p> </li> </ol> <p></p>"},{"location":"common/rtc_manager_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/rtc_manager_rviz_plugin/#input","title":"Input","text":"Name Type Description <code>/api/external/get/rtc_status</code> <code>tier4_rtc_msgs::msg::CooperateStatusArray</code> The statuses of each Cooperate Commands"},{"location":"common/rtc_manager_rviz_plugin/#output","title":"Output","text":"Name Type Description <code>/api/external/set/rtc_commands</code> <code>tier4_rtc_msgs::src::CooperateCommands</code> The Cooperate Commands for each planning <code>/planning/enable_auto_mode/*</code> <code>tier4_rtc_msgs::src::AutoMode</code> The Cooperate Commands mode for each planning module"},{"location":"common/rtc_manager_rviz_plugin/#howtouse","title":"HowToUse","text":"<ol> <li> <p>Start rviz and select panels/Add new panel.    </p> </li> <li> <p>tier4_state_rviz_plugin/RTCManagerPanel and press OK.    </p> </li> </ol>"},{"location":"common/signal_processing/","title":"Signal Processing Methods","text":""},{"location":"common/signal_processing/#signal-processing-methods","title":"Signal Processing Methods","text":"<p>In this package, we present signal processing related methods for the Autoware applications. The following functionalities are available in the current version.</p> <ul> <li>an 1-D Low-pass filter,</li> <li>Butterworth low-pass filter tools.</li> </ul> <p>low-pass filter currently supports only the 1-D low pass filtering.</p>"},{"location":"common/signal_processing/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"common/signal_processing/documentation/ButterworthFilter/","title":"ButterworthFilter","text":""},{"location":"common/signal_processing/documentation/ButterworthFilter/#butterworth-low-pass-filter-design-tool-class","title":"Butterworth Low-pass Filter Design Tool Class","text":"<p>This Butterworth low-pass filter design tool can be used to design a Butterworth filter in continuous and discrete-time from the given specifications of the filter performance. The Butterworth filter is a class implementation. A default constructor creates the object without any argument.</p> <p>The filter can be prepared in three ways. If the filter specifications are known, such as the pass-band, and stop-band frequencies (Wp and Ws) together with the pass-band and stop-band ripple magnitudes (Ap and As), one can call the filter's buttord method with these arguments to obtain the recommended filter order (N) and cut-off frequency (Wc_rad_sec [rad/s]).</p> <p> Figure 1. Butterworth Low-pass filter specification from [1].</p> <p>An example call is demonstrated below;</p> <pre><code>ButterworthFilter bf();\n\nWp = 2.0; // pass-band frequency [rad/sec]\nWs = 3.0; // stop-band frequency [rad/sec]\nAp = 6.0; // pass-band ripple mag or loss [dB]\nAs = 20.0; // stop band ripple attenuation [dB]\n\n// Computing filter coefficients from the specs\nbf.Buttord(Wp, Ws, Ap, As);\n\n// Get the computed order and Cut-Off frequency\nsOrderCutOff NWc = bf.getOrderCutOff();]\n\ncout &lt;&lt; \" The computed order is ;\" &lt;&lt; NWc.N &lt;&lt; endl;\ncout &lt;&lt; \" The computed cut-off frequency is ;\" &lt;&lt; NWc.Wc_rad_sec &lt;&lt; endl;\n</code></pre> <p>The filter order and cut-off frequency can be obtained in a struct using bf.getOrderCutoff() method. These specs can be printed on the screen by calling PrintFilterSpecs() method. If the user would like to define the order and cut-off frequency manually, the setter methods for these variables can be called to set the filter order (N) and the desired cut-off frequency (Wc_rad_sec [rad/sec]) for the filter.</p>"},{"location":"common/signal_processing/documentation/ButterworthFilter/#obtaining-filter-transfer-functions","title":"Obtaining Filter Transfer Functions","text":"<p>The discrete transfer function of the filter requires the roots and gain of the continuous-time transfer function. Therefore, it is a must to call the first computeContinuousTimeTF() to create the continuous-time transfer function of the filter using;</p> <pre><code>bf.computeContinuousTimeTF();\n</code></pre> <p>The computed continuous-time transfer function roots can be printed on the screen using the methods;</p> <pre><code>bf.PrintFilter_ContinuousTimeRoots();\nbf.PrintContinuousTimeTF();\n</code></pre> <p>The resulting screen output for a 5th order filter is demonstrated below.</p> <pre><code> Roots of Continuous Time Filter Transfer Function Denominator are :\n-0.585518 + j 1.80204\n-1.53291 + j 1.11372\n-1.89478 + j 2.32043e-16\n-1.53291 + j -1.11372\n-0.585518 + j -1.80204\n\n\nThe Continuous-Time Transfer Function of the Filter is ;\n\n                                   24.422\n-------------------------------------------------------------------------------\n1.000 *s[5] + 6.132 *s[4] + 18.798 *s[3] + 35.619 *s[2] + 41.711 *s[1] + 24.422\n</code></pre>"},{"location":"common/signal_processing/documentation/ButterworthFilter/#discrete-time-transfer-function-difference-equations","title":"Discrete Time Transfer Function (Difference Equations)","text":"<p>The digital filter equivalent of the continuous-time definitions is produced by using the bi-linear transformation. When creating the discrete-time function of the ButterworthFilter object, its Numerator (Bn) and Denominator (An ) coefficients are stored in a vector of filter order size N.</p> <p>The discrete transfer function method is called using ;</p> <pre><code>bf.computeDiscreteTimeTF();\nbf.PrintDiscreteTimeTF();\n</code></pre> <p>The results are printed on the screen like; The Discrete-Time Transfer Function of the Filter is ;</p> <pre><code>0.191 *z[-5] + 0.956 *z[-4] + 1.913 *z[-3] + 1.913 *z[-2] + 0.956 *z[-1] + 0.191\n--------------------------------------------------------------------------------\n1.000 *z[-5] + 1.885 *z[-4] + 1.888 *z[-3] + 1.014 *z[-2] + 0.298 *z[-1] + 0.037\n</code></pre> <p>and the associated difference coefficients An and Bn by withing a struct ;</p> <pre><code>sDifferenceAnBn AnBn = bf.getAnBn();\n</code></pre> <p>The difference coefficients appear in the filtering equation in the form of.</p> <pre><code>An * Y_filtered = Bn * Y_unfiltered\n</code></pre> <p>To filter a signal given in a vector form ;</p>"},{"location":"common/signal_processing/documentation/ButterworthFilter/#calling-filter-by-a-specified-cut-off-and-sampling-frequencies-in-hz","title":"Calling Filter by a specified cut-off and sampling frequencies [in Hz]","text":"<p>The Butterworth filter can also be created by defining the desired order (N), a cut-off frequency (fc in [Hz]), and a sampling frequency (fs in [Hz]). In this method, the cut-off frequency is pre-warped with respect to the sampling frequency [1, 2] to match the continuous and digital filter frequencies.</p> <p>The filter is prepared by the following calling options;</p> <pre><code> // 3rd METHOD defining a sampling frequency together with the cut-off fc, fs\n bf.setOrder(2);\n bf.setCutOffFrequency(10, 100);\n</code></pre> <p>At this step, we define a boolean variable whether to use the pre-warping option or not.</p> <pre><code>// Compute Continuous Time TF\nbool use_sampling_frequency = true;\nbf.computeContinuousTimeTF(use_sampling_frequency);\nbf.PrintFilter_ContinuousTimeRoots();\nbf.PrintContinuousTimeTF();\n\n// Compute Discrete Time TF\nbf.computeDiscreteTimeTF(use_sampling_frequency);\nbf.PrintDiscreteTimeTF();\n</code></pre> <p>References:</p> <ol> <li> <p>Manolakis, Dimitris G., and Vinay K. Ingle. Applied digital signal processing: theory and practice. Cambridge    University Press, 2011.</p> </li> <li> <p>https://en.wikibooks.org/wiki/Digital_Signal_Processing/Bilinear_Transform</p> </li> </ol>"},{"location":"common/tensorrt_common/","title":"tensorrt_common","text":""},{"location":"common/tensorrt_common/#tensorrt_common","title":"tensorrt_common","text":""},{"location":"common/tensorrt_common/#purpose","title":"Purpose","text":"<p>This package contains a library of common functions related to TensorRT. This package may include functions for handling TensorRT engine and calibration algorithm used for quantization</p>"},{"location":"common/tier4_adapi_rviz_plugin/","title":"tier4_adapi_rviz_plugin","text":""},{"location":"common/tier4_adapi_rviz_plugin/#tier4_adapi_rviz_plugin","title":"tier4_adapi_rviz_plugin","text":""},{"location":"common/tier4_adapi_rviz_plugin/#routepanel","title":"RoutePanel","text":"<p>To use the panel, set the topic name from 2D Goal Pose Tool to <code>/rviz/routing/pose</code>. By default, when a tool publish a pose, the panel immediately sets a route with that as the goal. Enable or disable of allow_goal_modification option can be set with the check box.</p> <p>Push the mode button in the waypoint to enter waypoint mode. In this mode, the pose is added to waypoints. Press the apply button to set the route using the saved waypoints (the last one is a goal). Reset the saved waypoints with the reset button.</p>"},{"location":"common/tier4_api_utils/","title":"tier4_api_utils","text":""},{"location":"common/tier4_api_utils/#tier4_api_utils","title":"tier4_api_utils","text":"<p>This is an old implementation of a class that logs when calling a service. Please use component_interface_utils instead.</p>"},{"location":"common/tier4_automatic_goal_rviz_plugin/","title":"tier4_automatic_goal_rviz_plugin","text":""},{"location":"common/tier4_automatic_goal_rviz_plugin/#tier4_automatic_goal_rviz_plugin","title":"tier4_automatic_goal_rviz_plugin","text":""},{"location":"common/tier4_automatic_goal_rviz_plugin/#purpose","title":"Purpose","text":"<ol> <li> <p>Defining a <code>GoalsList</code> by adding goals using <code>RvizTool</code> (Pose on the map).</p> </li> <li> <p>Automatic execution of the created <code>GoalsList</code> from the selected goal - it can be stopped and restarted.</p> </li> <li> <p>Looping the current <code>GoalsList</code>.</p> </li> <li> <p>Saving achieved goals to a file.</p> </li> <li> <p>Plan the route to one (single) selected goal and starting that route - it can be stopped and restarted.</p> </li> <li> <p>Remove any goal from the list or clear the current route.</p> </li> <li> <p>Save the current <code>GoalsList</code> to a file and load the list from the file.</p> </li> <li> <p>The application enables/disables access to options depending on the current state.</p> </li> <li> <p>The saved <code>GoalsList</code> can be executed without using a plugin - using a node <code>automatic_goal_sender</code>.</p> </li> </ol>"},{"location":"common/tier4_automatic_goal_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/tier4_automatic_goal_rviz_plugin/#input","title":"Input","text":"Name Type Description <code>/api/operation_mode/state</code> <code>autoware_adapi_v1_msgs::msg::OperationModeState</code> The topic represents the state of operation mode <code>/api/routing/state</code> <code>autoware_adapi_v1_msgs::msg::RouteState</code> The topic represents the state of route <code>/rviz2/automatic_goal/goal</code> <code>geometry_msgs::msgs::PoseStamped</code> The topic for adding goals to GoalsList"},{"location":"common/tier4_automatic_goal_rviz_plugin/#output","title":"Output","text":"Name Type Description <code>/api/operation_mode/change_to_autonomous</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to change operation mode to autonomous <code>/api/operation_mode/change_to_stop</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to change operation mode to stop <code>/api/routing/set_route_points</code> <code>autoware_adapi_v1_msgs::srv::SetRoutePoints</code> The service to set route <code>/api/routing/clear_route</code> <code>autoware_adapi_v1_msgs::srv::ClearRoute</code> The service to clear route state <code>/rviz2/automatic_goal/markers</code> <code>visualization_msgs::msg::MarkerArray</code> The topic to visualize goals as rviz markers"},{"location":"common/tier4_automatic_goal_rviz_plugin/#howtouse","title":"HowToUse","text":"<ol> <li> <p>Start rviz and select panels/Add new panel.</p> <p></p> </li> <li> <p>Select <code>tier4_automatic_goal_rviz_plugin/AutowareAutomaticGoalPanel</code> and press OK.</p> </li> <li> <p>Select Add a new tool.</p> <p></p> </li> <li> <p>Select <code>tier4_automatic_goal_rviz_plugin/AutowareAutomaticGoalTool</code> and press OK.</p> </li> <li> <p>Add goals visualization as markers to <code>Displays</code>.</p> <p></p> </li> <li> <p>Append goals to the <code>GoalsList</code> to be achieved using <code>2D Append Goal</code> - in such a way that routes can be planned.</p> </li> <li> <p>Start sequential planning and goal achievement by clicking <code>Send goals automatically</code></p> <p></p> </li> <li> <p>You can save <code>GoalsList</code> by clicking <code>Save to file</code>.</p> </li> <li> <p>After saving, you can run the <code>GoalsList</code> without using a plugin also:</p> <ul> <li>example: <code>ros2 launch tier4_automatic_goal_rviz_plugin automatic_goal_sender.launch.xml goals_list_file_path:=\"/tmp/goals_list.yaml\" goals_achieved_dir_path:=\"/tmp/\"</code><ul> <li><code>goals_list_file_path</code> - is the path to the saved <code>GoalsList</code> file to be loaded</li> <li><code>goals_achieved_dir_path</code> - is the path to the directory where the file <code>goals_achieved.log</code> will be created and the achieved goals will be written to it</li> </ul> </li> </ul> </li> </ol>"},{"location":"common/tier4_automatic_goal_rviz_plugin/#hints","title":"Hints","text":"<p>If the application (Engagement) goes into <code>ERROR</code> mode (usually returns to <code>EDITING</code> later), it means that one of the services returned a calling error (<code>code!=0</code>). In this situation, check the terminal output for more information.</p> <ul> <li>Often it is enough to try again.</li> <li>Sometimes a clearing of the current route is required before retrying.</li> </ul>"},{"location":"common/tier4_autoware_utils/","title":"tier4_autoware_utils","text":""},{"location":"common/tier4_autoware_utils/#tier4_autoware_utils","title":"tier4_autoware_utils","text":""},{"location":"common/tier4_autoware_utils/#purpose","title":"Purpose","text":"<p>This package contains many common functions used by other packages, so please refer to them as needed.</p>"},{"location":"common/tier4_autoware_utils/#for-developers","title":"For developers","text":"<p><code>tier4_autoware_utils.hpp</code> header file was removed because the source files that directly/indirectly include this file took a long time for preprocessing.</p>"},{"location":"common/tier4_camera_view_rviz_plugin/","title":"tier4_camera_view_rviz_plugin","text":""},{"location":"common/tier4_camera_view_rviz_plugin/#tier4_camera_view_rviz_plugin","title":"tier4_camera_view_rviz_plugin","text":""},{"location":"common/tier4_camera_view_rviz_plugin/#thirdpersonview-tool","title":"ThirdPersonView Tool","text":"<p>Add the <code>tier4_camera_view_rviz_plugin/ThirdPersonViewTool</code> tool to the RViz. Push the button, the camera will focus on the vehicle and set the target frame to <code>base_link</code>. Short cut key 'o'.</p>"},{"location":"common/tier4_camera_view_rviz_plugin/#birdeyeview-tool","title":"BirdEyeView Tool","text":"<p>Add the <code>tier4_camera_view_rviz_plugin/BirdEyeViewTool</code> tool to the RViz. Push the button, the camera will turn to the BEV view, the target frame is consistent with the latest frame. Short cut key 'r'.</p>"},{"location":"common/tier4_control_rviz_plugin/","title":"tier4_control_rviz_plugin","text":""},{"location":"common/tier4_control_rviz_plugin/#tier4_control_rviz_plugin","title":"tier4_control_rviz_plugin","text":"<p>This package is to mimic external control for simulation.</p>"},{"location":"common/tier4_control_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/tier4_control_rviz_plugin/#input","title":"Input","text":"Name Type Description <code>/control/current_gate_mode</code> <code>tier4_control_msgs::msg::GateMode</code> Current GATE mode <code>/vehicle/status/velocity_status</code> <code>autoware_auto_vehicle_msgs::msg::VelocityReport</code> Current velocity status <code>/api/autoware/get/engage</code> <code>tier4_external_api_msgs::srv::Engage</code> Getting Engage <code>/vehicle/status/gear_status</code> <code>autoware_auto_vehicle_msgs::msg::GearReport</code> The state of GEAR"},{"location":"common/tier4_control_rviz_plugin/#output","title":"Output","text":"Name Type Description <code>/control/gate_mode_cmd</code> <code>tier4_control_msgs::msg::GateMode</code> GATE mode <code>/external/selected/control_cmd</code> <code>autoware_auto_control_msgs::msg::AckermannControlCommand</code> AckermannControlCommand <code>/external/selected/gear_cmd</code> <code>autoware_auto_vehicle_msgs::msg::GearCommand</code> GEAR"},{"location":"common/tier4_control_rviz_plugin/#usage","title":"Usage","text":"<ol> <li> <p>Start rviz and select Panels.</p> <p></p> </li> <li> <p>Select tier4_control_rviz_plugin/ManualController and press OK.</p> <p></p> </li> <li> <p>Enter velocity in \"Set Cruise Velocity\" and Press the button to confirm. You can notice that GEAR shows D (DRIVE).</p> <p></p> </li> <li> <p>Press \"Enable Manual Control\" and you can notice that \"GATE\" and \"Engage\" turn \"Ready\" and the vehicle starts!</p> <p></p> </li> </ol>"},{"location":"common/tier4_datetime_rviz_plugin/","title":"tier4_datetime_rviz_plugin","text":""},{"location":"common/tier4_datetime_rviz_plugin/#tier4_datetime_rviz_plugin","title":"tier4_datetime_rviz_plugin","text":""},{"location":"common/tier4_datetime_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin displays the ROS Time and Wall Time in rviz.</p>"},{"location":"common/tier4_datetime_rviz_plugin/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"common/tier4_datetime_rviz_plugin/#usage","title":"Usage","text":"<ol> <li>Start rviz and select panels/Add new panel.    </li> <li>Select tier4_datetime_rviz_plugin/AutowareDateTimePanel and press OK.    </li> </ol>"},{"location":"common/tier4_debug_rviz_plugin/","title":"tier4_debug_rviz_plugin","text":""},{"location":"common/tier4_debug_rviz_plugin/#tier4_debug_rviz_plugin","title":"tier4_debug_rviz_plugin","text":"<p>This package is including jsk code. Note that jsk_overlay_utils.cpp and jsk_overlay_utils.hpp are BSD license.</p>"},{"location":"common/tier4_debug_rviz_plugin/#plugins","title":"Plugins","text":""},{"location":"common/tier4_debug_rviz_plugin/#float32multiarraystampedpiechart","title":"Float32MultiArrayStampedPieChart","text":"<p>Pie chart from <code>tier4_debug_msgs::msg::Float32MultiArrayStamped</code>.</p> <p></p>"},{"location":"common/tier4_debug_tools/","title":"tier4_debug_tools","text":""},{"location":"common/tier4_debug_tools/#tier4_debug_tools","title":"tier4_debug_tools","text":"<p>This package provides useful features for debugging Autoware.</p>"},{"location":"common/tier4_debug_tools/#usage","title":"Usage","text":""},{"location":"common/tier4_debug_tools/#tf2pose","title":"tf2pose","text":"<p>This tool converts any <code>tf</code> to <code>pose</code> topic. With this tool, for example, you can plot <code>x</code> values of <code>tf</code> in <code>rqt_multiplot</code>.</p> <pre><code>ros2 run tier4_debug_tools tf2pose {tf_from} {tf_to} {hz}\n</code></pre> <p>Example:</p> <pre><code>$ ros2 run tier4_debug_tools tf2pose base_link ndt_base_link 100\n\n$ ros2 topic echo /tf2pose/pose -n1\nheader:\n  seq: 13\nstamp:\n    secs: 1605168366\nnsecs: 549174070\nframe_id: \"base_link\"\npose:\n  position:\n    x: 0.0387684271191\n    y: -0.00320360406477\n    z: 0.000276674520819\n  orientation:\n    x: 0.000335221893885\n    y: 0.000122020672186\n    z: -0.00539673212896\n    w: 0.999985368502\n---\n</code></pre>"},{"location":"common/tier4_debug_tools/#pose2tf","title":"pose2tf","text":"<p>This tool converts any <code>pose</code> topic to <code>tf</code>.</p> <pre><code>ros2 run tier4_debug_tools pose2tf {pose_topic_name} {tf_name}\n</code></pre> <p>Example:</p> <pre><code>$ ros2 run tier4_debug_tools pose2tf /localization/pose_estimator/pose ndt_pose\n\n$ ros2 run tf tf_echo ndt_pose ndt_base_link 100\nAt time 1605168365.449\n- Translation: [0.000, 0.000, 0.000]\n- Rotation: in Quaternion [0.000, 0.000, 0.000, 1.000]\nin RPY (radian) [0.000, -0.000, 0.000]\nin RPY (degree) [0.000, -0.000, 0.000]\n</code></pre>"},{"location":"common/tier4_debug_tools/#stop_reason2pose","title":"stop_reason2pose","text":"<p>This tool extracts <code>pose</code> from <code>stop_reasons</code>. Topics without numbers such as <code>/stop_reason2pose/pose/detection_area</code> are the nearest stop_reasons, and topics with numbers are individual stop_reasons that are roughly matched with previous ones.</p> <pre><code>ros2 run tier4_debug_tools stop_reason2pose {stop_reason_topic_name}\n</code></pre> <p>Example:</p> <pre><code>$ ros2 run tier4_debug_tools stop_reason2pose /planning/scenario_planning/status/stop_reasons\n\n$ ros2 topic list | ag stop_reason2pose\n/stop_reason2pose/pose/detection_area\n/stop_reason2pose/pose/detection_area_1\n/stop_reason2pose/pose/obstacle_stop\n/stop_reason2pose/pose/obstacle_stop_1\n\n$ ros2 topic echo /stop_reason2pose/pose/detection_area -n1\nheader:\n  seq: 1\nstamp:\n    secs: 1605168355\nnsecs:    821713\nframe_id: \"map\"\npose:\n  position:\n    x: 60608.8433457\n    y: 43886.2410876\n    z: 44.9078212441\n  orientation:\n    x: 0.0\n    y: 0.0\n    z: -0.190261378408\n    w: 0.981733470901\n---\n</code></pre>"},{"location":"common/tier4_debug_tools/#stop_reason2tf","title":"stop_reason2tf","text":"<p>This is an all-in-one script that uses <code>tf2pose</code>, <code>pose2tf</code>, and <code>stop_reason2pose</code>. With this tool, you can view the relative position from base_link to the nearest stop_reason.</p> <pre><code>ros2 run tier4_debug_tools stop_reason2tf {stop_reason_name}\n</code></pre> <p>Example:</p> <pre><code>$ ros2 run tier4_debug_tools stop_reason2tf obstacle_stop\nAt time 1605168359.501\n- Translation: [0.291, -0.095, 0.266]\n- Rotation: in Quaternion [0.007, 0.011, -0.005, 1.000]\nin RPY (radian) [0.014, 0.023, -0.010]\nin RPY (degree) [0.825, 1.305, -0.573]\n</code></pre>"},{"location":"common/tier4_debug_tools/#lateral_error_publisher","title":"lateral_error_publisher","text":"<p>This node calculate the control error and localization error in the trajectory normal direction as shown in the figure below.</p> <p></p> <p>Set the reference trajectory, vehicle pose and ground truth pose in the launch file.</p> <pre><code>ros2 launch tier4_debug_tools lateral_error_publisher.launch.xml\n</code></pre>"},{"location":"common/tier4_localization_rviz_plugin/","title":"tier4_localization_rviz_plugin","text":""},{"location":"common/tier4_localization_rviz_plugin/#tier4_localization_rviz_plugin","title":"tier4_localization_rviz_plugin","text":""},{"location":"common/tier4_localization_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin can display the history of the localization obtained by ekf_localizer or ndt_scan_matching.</p>"},{"location":"common/tier4_localization_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/tier4_localization_rviz_plugin/#input","title":"Input","text":"Name Type Description <code>input/pose</code> <code>geometry_msgs::msg::PoseStamped</code> In input/pose, put the result of localization calculated by ekf_localizer or ndt_scan_matching"},{"location":"common/tier4_localization_rviz_plugin/#parameters","title":"Parameters","text":""},{"location":"common/tier4_localization_rviz_plugin/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>property_buffer_size_</code> int 100 Buffer size of topic <code>property_line_view_</code> bool true Use Line property or not <code>property_line_width_</code> float 0.1 Width of Line property [m] <code>property_line_alpha_</code> float 1.0 Alpha of Line property <code>property_line_color_</code> QColor Qt::white Color of Line property"},{"location":"common/tier4_localization_rviz_plugin/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"common/tier4_localization_rviz_plugin/#usage","title":"Usage","text":"<ol> <li>Start rviz and select Add under the Displays panel.    </li> <li>Select tier4_localization_rviz_plugin/PoseHistory and press OK.    </li> <li>Enter the name of the topic where you want to view the trajectory.    </li> </ol>"},{"location":"common/tier4_logging_level_configure_rviz_plugin/","title":"tier4_logging_level_configure_rviz_plugin","text":""},{"location":"common/tier4_logging_level_configure_rviz_plugin/#tier4_logging_level_configure_rviz_plugin","title":"tier4_logging_level_configure_rviz_plugin","text":"<p>This package provides an rviz_plugin that can easily change the logger level of each node</p> <p></p> <p>This plugin dispatches services to the \"logger name\" associated with \"nodes\" specified in YAML, adjusting the logger level.</p> <p>As of November 2023, in ROS 2 Humble, users are required to initiate a service server in the node to use this feature. (This might be integrated into ROS standards in the future.) For easy service server generation, you can use the LoggerLevelConfigure utility.</p>"},{"location":"common/tier4_perception_rviz_plugin/","title":"tier4_perception_rviz_plugin","text":""},{"location":"common/tier4_perception_rviz_plugin/#tier4_perception_rviz_plugin","title":"tier4_perception_rviz_plugin","text":""},{"location":"common/tier4_perception_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin is used to generate dummy pedestrians, cars, and obstacles in planning simulator.</p>"},{"location":"common/tier4_perception_rviz_plugin/#overview","title":"Overview","text":"<p>The CarInitialPoseTool sends a topic for generating a dummy car. The PedestrianInitialPoseTool sends a topic for generating a dummy pedestrian. The UnknownInitialPoseTool sends a topic for generating a dummy obstacle. The DeleteAllObjectsTool deletes the dummy cars, pedestrians, and obstacles displayed by the above three tools.</p>"},{"location":"common/tier4_perception_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/tier4_perception_rviz_plugin/#output","title":"Output","text":"Name Type Description <code>/simulation/dummy_perception_publisher/object_info</code> <code>dummy_perception_publisher::msg::Object</code> The topic on which to publish dummy object info"},{"location":"common/tier4_perception_rviz_plugin/#parameter","title":"Parameter","text":""},{"location":"common/tier4_perception_rviz_plugin/#core-parameters","title":"Core Parameters","text":""},{"location":"common/tier4_perception_rviz_plugin/#carpose","title":"CarPose","text":"Name Type Default Value Description <code>topic_property_</code> string <code>/simulation/dummy_perception_publisher/object_info</code> The topic on which to publish dummy object info <code>std_dev_x_</code> float 0.03 X standard deviation for initial pose [m] <code>std_dev_y_</code> float 0.03 Y standard deviation for initial pose [m] <code>std_dev_z_</code> float 0.03 Z standard deviation for initial pose [m] <code>std_dev_theta_</code> float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] <code>length_</code> float 4.0 X standard deviation for initial pose [m] <code>width_</code> float 1.8 Y standard deviation for initial pose [m] <code>height_</code> float 2.0 Z standard deviation for initial pose [m] <code>position_z_</code> float 0.0 Z position for initial pose [m] <code>velocity_</code> float 0.0 Velocity [m/s]"},{"location":"common/tier4_perception_rviz_plugin/#buspose","title":"BusPose","text":"Name Type Default Value Description <code>topic_property_</code> string <code>/simulation/dummy_perception_publisher/object_info</code> The topic on which to publish dummy object info <code>std_dev_x_</code> float 0.03 X standard deviation for initial pose [m] <code>std_dev_y_</code> float 0.03 Y standard deviation for initial pose [m] <code>std_dev_z_</code> float 0.03 Z standard deviation for initial pose [m] <code>std_dev_theta_</code> float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] <code>length_</code> float 10.5 X standard deviation for initial pose [m] <code>width_</code> float 2.5 Y standard deviation for initial pose [m] <code>height_</code> float 3.5 Z standard deviation for initial pose [m] <code>position_z_</code> float 0.0 Z position for initial pose [m] <code>velocity_</code> float 0.0 Velocity [m/s]"},{"location":"common/tier4_perception_rviz_plugin/#pedestrianpose","title":"PedestrianPose","text":"Name Type Default Value Description <code>topic_property_</code> string <code>/simulation/dummy_perception_publisher/object_info</code> The topic on which to publish dummy object info <code>std_dev_x_</code> float 0.03 X standard deviation for initial pose [m] <code>std_dev_y_</code> float 0.03 Y standard deviation for initial pose [m] <code>std_dev_z_</code> float 0.03 Z standard deviation for initial pose [m] <code>std_dev_theta_</code> float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] <code>position_z_</code> float 0.0 Z position for initial pose [m] <code>velocity_</code> float 0.0 Velocity [m/s]"},{"location":"common/tier4_perception_rviz_plugin/#unknownpose","title":"UnknownPose","text":"Name Type Default Value Description <code>topic_property_</code> string <code>/simulation/dummy_perception_publisher/object_info</code> The topic on which to publish dummy object info <code>std_dev_x_</code> float 0.03 X standard deviation for initial pose [m] <code>std_dev_y_</code> float 0.03 Y standard deviation for initial pose [m] <code>std_dev_z_</code> float 0.03 Z standard deviation for initial pose [m] <code>std_dev_theta_</code> float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] <code>position_z_</code> float 0.0 Z position for initial pose [m] <code>velocity_</code> float 0.0 Velocity [m/s]"},{"location":"common/tier4_perception_rviz_plugin/#deleteallobjects","title":"DeleteAllObjects","text":"Name Type Default Value Description <code>topic_property_</code> string <code>/simulation/dummy_perception_publisher/object_info</code> The topic on which to publish dummy object info"},{"location":"common/tier4_perception_rviz_plugin/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Using a planning simulator</p>"},{"location":"common/tier4_perception_rviz_plugin/#usage","title":"Usage","text":"<ol> <li>Start rviz and select + on the tool tab.    </li> <li>Select one of the following: tier4_perception_rviz_plugin and press OK.    </li> <li>Select the new item in the tool tab (2D Dummy Car in the example) and click on it in rviz.    </li> </ol>"},{"location":"common/tier4_perception_rviz_plugin/#interactive-manipulation","title":"Interactive manipulation","text":"<p>You can interactively manipulate the object.</p> <ol> <li>Select \"Tool Properties\" in rviz.</li> <li>Select the corresponding object tab in the Tool Properties.</li> <li>Turn the \"Interactive\" checkbox on.    </li> <li>Select the item in the tool tab in you haven't chosen yet.</li> <li>Key commands are as follows.</li> </ol> action key command ADD Shift + Click Right Button MOVE Hold down Right Button + Drug and Drop DELETE Alt + Click Right Button"},{"location":"common/tier4_planning_rviz_plugin/","title":"tier4_planning_rviz_plugin","text":""},{"location":"common/tier4_planning_rviz_plugin/#tier4_planning_rviz_plugin","title":"tier4_planning_rviz_plugin","text":"<p>This package is including jsk code. Note that jsk_overlay_utils.cpp and jsk_overlay_utils.hpp are BSD license.</p>"},{"location":"common/tier4_planning_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin displays the path, trajectory, and maximum speed.</p>"},{"location":"common/tier4_planning_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/tier4_planning_rviz_plugin/#input","title":"Input","text":"Name Type Description <code>/input/path</code> <code>autoware_auto_planning_msgs::msg::Path</code> The topic on which to subscribe path <code>/input/trajectory</code> <code>autoware_auto_planning_msgs::msg::Trajectory</code> The topic on which to subscribe trajectory <code>/planning/scenario_planning/current_max_velocity</code> <code>tier4_planning_msgs/msg/VelocityLimit</code> The topic on which to publish max velocity"},{"location":"common/tier4_planning_rviz_plugin/#output","title":"Output","text":"Name Type Description <code>/planning/mission_planning/checkpoint</code> <code>geometry_msgs/msg/PoseStamped</code> The topic on which to publish checkpoint"},{"location":"common/tier4_planning_rviz_plugin/#parameter","title":"Parameter","text":""},{"location":"common/tier4_planning_rviz_plugin/#core-parameters","title":"Core Parameters","text":""},{"location":"common/tier4_planning_rviz_plugin/#missioncheckpoint","title":"MissionCheckpoint","text":"Name Type Default Value Description <code>pose_topic_property_</code> string <code>mission_checkpoint</code> The topic on which to publish checkpoint <code>std_dev_x_</code> float 0.5 X standard deviation for checkpoint pose [m] <code>std_dev_y_</code> float 0.5 Y standard deviation for checkpoint pose [m] <code>std_dev_theta_</code> float M_PI / 12.0 Theta standard deviation for checkpoint pose [rad] <code>position_z_</code> float 0.0 Z position for checkpoint pose [m]"},{"location":"common/tier4_planning_rviz_plugin/#path","title":"Path","text":"Name Type Default Value Description <code>property_path_view_</code> bool true Use Path property or not <code>property_path_width_view_</code> bool false Use Constant Width or not <code>property_path_width_</code> float 2.0 Width of Path property [m] <code>property_path_alpha_</code> float 1.0 Alpha of Path property <code>property_path_color_view_</code> bool false Use Constant Color or not <code>property_path_color_</code> QColor Qt::black Color of Path property <code>property_velocity_view_</code> bool true Use Velocity property or not <code>property_velocity_alpha_</code> float 1.0 Alpha of Velocity property <code>property_velocity_scale_</code> float 0.3 Scale of Velocity property <code>property_velocity_color_view_</code> bool false Use Constant Color or not <code>property_velocity_color_</code> QColor Qt::black Color of Velocity property <code>property_vel_max_</code> float 3.0 Max velocity [m/s]"},{"location":"common/tier4_planning_rviz_plugin/#drivablearea","title":"DrivableArea","text":"Name Type Default Value Description <code>color_scheme_property_</code> int 0 Color scheme of DrivableArea property <code>alpha_property_</code> float 0.2 Alpha of DrivableArea property <code>draw_under_property_</code> bool false Draw as background or not"},{"location":"common/tier4_planning_rviz_plugin/#pathfootprint","title":"PathFootprint","text":"Name Type Default Value Description <code>property_path_footprint_view_</code> bool true Use Path Footprint property or not <code>property_path_footprint_alpha_</code> float 1.0 Alpha of Path Footprint property <code>property_path_footprint_color_</code> QColor Qt::black Color of Path Footprint property <code>property_vehicle_length_</code> float 4.77 Vehicle length [m] <code>property_vehicle_width_</code> float 1.83 Vehicle width [m] <code>property_rear_overhang_</code> float 1.03 Rear overhang [m]"},{"location":"common/tier4_planning_rviz_plugin/#trajectory","title":"Trajectory","text":"Name Type Default Value Description <code>property_path_view_</code> bool true Use Path property or not <code>property_path_width_</code> float 2.0 Width of Path property [m] <code>property_path_alpha_</code> float 1.0 Alpha of Path property <code>property_path_color_view_</code> bool false Use Constant Color or not <code>property_path_color_</code> QColor Qt::black Color of Path property <code>property_velocity_view_</code> bool true Use Velocity property or not <code>property_velocity_alpha_</code> float 1.0 Alpha of Velocity property <code>property_velocity_scale_</code> float 0.3 Scale of Velocity property <code>property_velocity_color_view_</code> bool false Use Constant Color or not <code>property_velocity_color_</code> QColor Qt::black Color of Velocity property <code>property_velocity_text_view_</code> bool false View text Velocity <code>property_velocity_text_scale_</code> float 0.3 Scale of Velocity property <code>property_vel_max_</code> float 3.0 Max velocity [m/s]"},{"location":"common/tier4_planning_rviz_plugin/#trajectoryfootprint","title":"TrajectoryFootprint","text":"Name Type Default Value Description <code>property_trajectory_footprint_view_</code> bool true Use Trajectory Footprint property or not <code>property_trajectory_footprint_alpha_</code> float 1.0 Alpha of Trajectory Footprint property <code>property_trajectory_footprint_color_</code> QColor QColor(230, 230, 50) Color of Trajectory Footprint property <code>property_vehicle_length_</code> float 4.77 Vehicle length [m] <code>property_vehicle_width_</code> float 1.83 Vehicle width [m] <code>property_rear_overhang_</code> float 1.03 Rear overhang [m] <code>property_trajectory_point_view_</code> bool false Use Trajectory Point property or not <code>property_trajectory_point_alpha_</code> float 1.0 Alpha of Trajectory Point property <code>property_trajectory_point_color_</code> QColor QColor(0, 60, 255) Color of Trajectory Point property <code>property_trajectory_point_radius_</code> float 0.1 Radius of Trajectory Point property"},{"location":"common/tier4_planning_rviz_plugin/#maxvelocity","title":"MaxVelocity","text":"Name Type Default Value Description <code>property_topic_name_</code> string <code>/planning/scenario_planning/current_max_velocity</code> The topic on which to subscribe max velocity <code>property_text_color_</code> QColor QColor(255, 255, 255) Text color <code>property_left_</code> int 128 Left of the plotter window [px] <code>property_top_</code> int 128 Top of the plotter window [px] <code>property_length_</code> int 96 Length of the plotter window [px] <code>property_value_scale_</code> float 1.0 / 4.0 Value scale"},{"location":"common/tier4_planning_rviz_plugin/#usage","title":"Usage","text":"<ol> <li>Start rviz and select Add under the Displays panel.    </li> <li>Select any one of the tier4_planning_rviz_plugin and press OK.    </li> <li>Enter the name of the topic where you want to view the path or trajectory.    </li> </ol>"},{"location":"common/tier4_screen_capture_rviz_plugin/","title":"tier4_screen_capture_rviz_plugin","text":""},{"location":"common/tier4_screen_capture_rviz_plugin/#tier4_screen_capture_rviz_plugin","title":"tier4_screen_capture_rviz_plugin","text":""},{"location":"common/tier4_screen_capture_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin captures the screen of rviz.</p>"},{"location":"common/tier4_screen_capture_rviz_plugin/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>This is only for debug or analyze. The <code>capture screen</code> button is still beta version which can slow frame rate. set lower frame rate according to PC spec.</p>"},{"location":"common/tier4_screen_capture_rviz_plugin/#usage","title":"Usage","text":"<ol> <li>Start rviz and select panels/Add new panel.    </li> </ol>"},{"location":"common/tier4_simulated_clock_rviz_plugin/","title":"tier4_simulated_clock_rviz_plugin","text":""},{"location":"common/tier4_simulated_clock_rviz_plugin/#tier4_simulated_clock_rviz_plugin","title":"tier4_simulated_clock_rviz_plugin","text":""},{"location":"common/tier4_simulated_clock_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin allows publishing and controlling the simulated ROS time.</p>"},{"location":"common/tier4_simulated_clock_rviz_plugin/#output","title":"Output","text":"Name Type Description <code>/clock</code> <code>rosgraph_msgs::msg::Clock</code> the current simulated time"},{"location":"common/tier4_simulated_clock_rviz_plugin/#howtouse","title":"HowToUse","text":"<ol> <li>Start rviz and select panels/Add new panel.    </li> <li>Select tier4_clock_rviz_plugin/SimulatedClock and press OK.    </li> <li> <p>Use the added panel to control how the simulated clock is published.    </p> <ul> <li>Pause button: pause/resume the clock.</li> <li>Speed: speed of the clock relative to the system clock.</li> <li>Rate: publishing rate of the clock.</li> <li>Step button: advance the clock by the specified time step.</li> <li>Time step: value used to advance the clock when pressing the step button d).</li> <li>Time unit: time unit associated with the value from e).</li> </ul> </li> </ol>"},{"location":"common/tier4_state_rviz_plugin/","title":"tier4_state_rviz_plugin","text":""},{"location":"common/tier4_state_rviz_plugin/#tier4_state_rviz_plugin","title":"tier4_state_rviz_plugin","text":""},{"location":"common/tier4_state_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin displays the current status of autoware. This plugin also can engage from the panel.</p>"},{"location":"common/tier4_state_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/tier4_state_rviz_plugin/#input","title":"Input","text":"Name Type Description <code>/api/operation_mode/state</code> <code>autoware_adapi_v1_msgs::msg::OperationModeState</code> The topic represents the state of operation mode <code>/api/routing/state</code> <code>autoware_adapi_v1_msgs::msg::RouteState</code> The topic represents the state of route <code>/api/localization/initialization_state</code> <code>autoware_adapi_v1_msgs::msg::LocalizationInitializationState</code> The topic represents the state of localization initialization <code>/api/motion/state</code> <code>autoware_adapi_v1_msgs::msg::MotionState</code> The topic represents the state of motion <code>/api/autoware/get/emergency</code> <code>tier4_external_api_msgs::msg::Emergency</code> The topic represents the state of external emergency <code>/vehicle/status/gear_status</code> <code>autoware_auto_vehicle_msgs::msg::GearReport</code> The topic represents the state of gear"},{"location":"common/tier4_state_rviz_plugin/#output","title":"Output","text":"Name Type Description <code>/api/operation_mode/change_to_autonomous</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to change operation mode to autonomous <code>/api/operation_mode/change_to_stop</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to change operation mode to stop <code>/api/operation_mode/change_to_local</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to change operation mode to local <code>/api/operation_mode/change_to_remote</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to change operation mode to remote <code>/api/operation_mode/enable_autoware_control</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to enable vehicle control by Autoware <code>/api/operation_mode/disable_autoware_control</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to disable vehicle control by Autoware <code>/api/routing/clear_route</code> <code>autoware_adapi_v1_msgs::srv::ClearRoute</code> The service to clear route state <code>/api/motion/accept_start</code> <code>autoware_adapi_v1_msgs::srv::AcceptStart</code> The service to accept the vehicle to start <code>/api/autoware/set/emergency</code> <code>tier4_external_api_msgs::srv::SetEmergency</code> The service to set external emergency <code>/planning/scenario_planning/max_velocity_default</code> <code>tier4_planning_msgs::msg::VelocityLimit</code> The topic to set maximum speed of the vehicle"},{"location":"common/tier4_state_rviz_plugin/#howtouse","title":"HowToUse","text":"<ol> <li> <p>Start rviz and select panels/Add new panel.</p> <p></p> </li> <li> <p>Select tier4_state_rviz_plugin/AutowareStatePanel and press OK.</p> <p></p> </li> <li> <p>If the auto button is activated, can engage by clicking it.</p> <p></p> </li> </ol>"},{"location":"common/tier4_system_rviz_plugin/","title":"tier4_system_rviz_plugin","text":""},{"location":"common/tier4_system_rviz_plugin/#tier4_system_rviz_plugin","title":"tier4_system_rviz_plugin","text":""},{"location":"common/tier4_system_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin display the Hazard information from Autoware; and output notices when emergencies are from initial localization and route setting.</p>"},{"location":"common/tier4_system_rviz_plugin/#input","title":"Input","text":"Name Type Description <code>/system/emergency/hazard_status</code> <code>autoware_auto_system_msgs::msg::HazardStatusStamped</code> The topic represents the emergency information from Autoware"},{"location":"common/tier4_target_object_type_rviz_plugin/","title":"tier4_target_object_type_rviz_plugin","text":""},{"location":"common/tier4_target_object_type_rviz_plugin/#tier4_target_object_type_rviz_plugin","title":"tier4_target_object_type_rviz_plugin","text":"<p>This plugin allows you to check which types of the dynamic object is being used by each planner.</p> <p></p>"},{"location":"common/tier4_target_object_type_rviz_plugin/#limitations","title":"Limitations","text":"<p>Currently, which parameters of which module to check are hardcoded. In the future, this will be parameterized using YAML.</p>"},{"location":"common/tier4_traffic_light_rviz_plugin/","title":"tier4_traffic_light_rviz_plugin","text":""},{"location":"common/tier4_traffic_light_rviz_plugin/#tier4_traffic_light_rviz_plugin","title":"tier4_traffic_light_rviz_plugin","text":""},{"location":"common/tier4_traffic_light_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin panel publishes dummy traffic light signals.</p>"},{"location":"common/tier4_traffic_light_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/tier4_traffic_light_rviz_plugin/#output","title":"Output","text":"Name Type Description <code>/perception/traffic_light_recognition/traffic_signals</code> <code>autoware_perception_msgs::msg::TrafficSignalArray</code> Publish traffic light signals"},{"location":"common/tier4_traffic_light_rviz_plugin/#howtouse","title":"HowToUse","text":"<ol> <li>Start rviz and select panels/Add new panel.</li> <li>Select TrafficLightPublishPanel and press OK.</li> <li>Set <code>Traffic Light ID</code> &amp; <code>Traffic Light Status</code> and press <code>SET</code> button.</li> <li>Traffic light signals are published, while <code>PUBLISH</code> button is pushed.</li> </ol>"},{"location":"common/tier4_vehicle_rviz_plugin/","title":"tier4_vehicle_rviz_plugin","text":""},{"location":"common/tier4_vehicle_rviz_plugin/#tier4_vehicle_rviz_plugin","title":"tier4_vehicle_rviz_plugin","text":"<p>This package is including jsk code. Note that jsk_overlay_utils.cpp and jsk_overlay_utils.hpp are BSD license.</p>"},{"location":"common/tier4_vehicle_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin provides a visual and easy-to-understand display of vehicle speed, turn signal, steering status and acceleration.</p>"},{"location":"common/tier4_vehicle_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/tier4_vehicle_rviz_plugin/#input","title":"Input","text":"Name Type Description <code>/vehicle/status/velocity_status</code> <code>autoware_auto_vehicle_msgs::msg::VelocityReport</code> The topic is vehicle twist <code>/control/turn_signal_cmd</code> <code>autoware_auto_vehicle_msgs::msg::TurnIndicatorsReport</code> The topic is status of turn signal <code>/vehicle/status/steering_status</code> <code>autoware_auto_vehicle_msgs::msg::SteeringReport</code> The topic is status of steering <code>/localization/acceleration</code> <code>geometry_msgs::msg::AccelWithCovarianceStamped</code> The topic is the acceleration"},{"location":"common/tier4_vehicle_rviz_plugin/#parameter","title":"Parameter","text":""},{"location":"common/tier4_vehicle_rviz_plugin/#core-parameters","title":"Core Parameters","text":""},{"location":"common/tier4_vehicle_rviz_plugin/#consolemeter","title":"ConsoleMeter","text":"Name Type Default Value Description <code>property_text_color_</code> QColor QColor(25, 255, 240) Text color <code>property_left_</code> int 128 Left of the plotter window [px] <code>property_top_</code> int 128 Top of the plotter window [px] <code>property_length_</code> int 256 Height of the plotter window [px] <code>property_value_height_offset_</code> int 0 Height offset of the plotter window [px] <code>property_value_scale_</code> float 1.0 / 6.667 Value scale"},{"location":"common/tier4_vehicle_rviz_plugin/#steeringangle","title":"SteeringAngle","text":"Name Type Default Value Description <code>property_text_color_</code> QColor QColor(25, 255, 240) Text color <code>property_left_</code> int 128 Left of the plotter window [px] <code>property_top_</code> int 128 Top of the plotter window [px] <code>property_length_</code> int 256 Height of the plotter window [px] <code>property_value_height_offset_</code> int 0 Height offset of the plotter window [px] <code>property_value_scale_</code> float 1.0 / 6.667 Value scale <code>property_handle_angle_scale_</code> float 3.0 Scale is steering angle to handle angle"},{"location":"common/tier4_vehicle_rviz_plugin/#turnsignal","title":"TurnSignal","text":"Name Type Default Value Description <code>property_left_</code> int 128 Left of the plotter window [px] <code>property_top_</code> int 128 Top of the plotter window [px] <code>property_width_</code> int 256 Left of the plotter window [px] <code>property_height_</code> int 256 Width of the plotter window [px]"},{"location":"common/tier4_vehicle_rviz_plugin/#velocityhistory","title":"VelocityHistory","text":"Name Type Default Value Description <code>property_velocity_timeout_</code> float 10.0 Timeout of velocity [s] <code>property_velocity_alpha_</code> float 1.0 Alpha of velocity <code>property_velocity_scale_</code> float 0.3 Scale of velocity <code>property_velocity_color_view_</code> bool false Use Constant Color or not <code>property_velocity_color_</code> QColor Qt::black Color of velocity history <code>property_vel_max_</code> float 3.0 Color Border Vel Max [m/s]"},{"location":"common/tier4_vehicle_rviz_plugin/#accelerationmeter","title":"AccelerationMeter","text":"Name Type Default Value Description <code>property_normal_text_color_</code> QColor QColor(25, 255, 240) Normal text color <code>property_emergency_text_color_</code> QColor QColor(255, 80, 80) Emergency acceleration color <code>property_left_</code> int 896 Left of the plotter window [px] <code>property_top_</code> int 128 Top of the plotter window [px] <code>property_length_</code> int 256 Height of the plotter window [px] <code>property_value_height_offset_</code> int 0 Height offset of the plotter window [px] <code>property_value_scale_</code> float 1 / 6.667 Value text scale <code>property_emergency_threshold_max_</code> float 1.0 Max acceleration threshold for emergency [m/s^2] <code>property_emergency_threshold_min_</code> float -2.5 Min acceleration threshold for emergency [m/s^2]"},{"location":"common/tier4_vehicle_rviz_plugin/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"common/tier4_vehicle_rviz_plugin/#usage","title":"Usage","text":"<ol> <li>Start rviz and select Add under the Displays panel.    </li> <li>Select any one of the tier4_vehicle_rviz_plugin and press OK.    </li> <li>Enter the name of the topic where you want to view the status.    </li> </ol>"},{"location":"common/traffic_light_recognition_marker_publisher/Readme/","title":"path_distance_calculator","text":""},{"location":"common/traffic_light_recognition_marker_publisher/Readme/#path_distance_calculator","title":"path_distance_calculator","text":""},{"location":"common/traffic_light_recognition_marker_publisher/Readme/#purpose","title":"Purpose","text":"<p>This node publishes a marker array for visualizing traffic signal recognition results on Rviz.</p> <p></p>"},{"location":"common/traffic_light_recognition_marker_publisher/Readme/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"common/traffic_light_recognition_marker_publisher/Readme/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/traffic_light_recognition_marker_publisher/Readme/#input","title":"Input","text":"Name Type Description <code>/map/vector_map</code> <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> Vector map for getting traffic signal information <code>/perception/traffic_light_recognition/traffic_signals</code> <code>autoware_auto_perception_msgs::msg::TrafficSignalArray</code> The result of traffic signal recognition"},{"location":"common/traffic_light_recognition_marker_publisher/Readme/#output","title":"Output","text":"Name Type Description <code>/perception/traffic_light_recognition/traffic_signals_marker</code> <code>visualization_msgs::msg::MarkerArray</code> Publish a marker array for visualization of traffic signal recognition results"},{"location":"common/traffic_light_recognition_marker_publisher/Readme/#parameters","title":"Parameters","text":"<p>None.</p>"},{"location":"common/traffic_light_recognition_marker_publisher/Readme/#node-parameters","title":"Node Parameters","text":"<p>None.</p>"},{"location":"common/traffic_light_recognition_marker_publisher/Readme/#core-parameters","title":"Core Parameters","text":"<p>None.</p>"},{"location":"common/traffic_light_recognition_marker_publisher/Readme/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"common/traffic_light_utils/","title":"traffic_light_utils","text":""},{"location":"common/traffic_light_utils/#traffic_light_utils","title":"traffic_light_utils","text":""},{"location":"common/traffic_light_utils/#purpose","title":"Purpose","text":"<p>This package contains a library of common functions that are useful across the traffic light recognition module. This package may include functions for handling ROI types, converting between different data types and message types, as well as common functions related to them.</p>"},{"location":"common/tvm_utility/","title":"TVM Utility","text":""},{"location":"common/tvm_utility/#tvm-utility","title":"TVM Utility","text":"<p>This is the design document for the <code>tvm_utility</code> package. For instructions on how to build the tests for YOLOv2 Tiny, see the YOLOv2 Tiny Example Pipeline. For information about where to store test artifacts see the TVM Utility Artifacts.</p>"},{"location":"common/tvm_utility/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>A set of c++ utilities to help build a TVM based machine learning inference pipeline. The library contains a pipeline class which helps building the pipeline and a number of utility functions that are common in machine learning.</p>"},{"location":"common/tvm_utility/#design","title":"Design","text":"<p>The Pipeline Class is a standardized way to write an inference pipeline. The pipeline class contains 3 different stages: the pre-processor, the inference engine and the post-processor. The TVM implementation of an inference engine stage is provided.</p>"},{"location":"common/tvm_utility/#api","title":"API","text":"<p>The pre-processor and post-processor need to be implemented by the user before instantiating the pipeline. You can see example usage in the example pipeline at <code>test/yolo_v2_tiny</code>.</p> <p>Each stage in the pipeline has a <code>schedule</code> function which takes input data as a parameter and return the output data. Once the pipeline object is created, <code>pipeline.schedule</code> is called to run the pipeline.</p> <pre><code>int main() {\n   create_subscription&lt;sensor_msgs::msg::PointCloud2&gt;(\"points_raw\",\n   rclcpp::QoS{1}, [this](const sensor_msgs::msg::PointCloud2::SharedPtr msg)\n   {pipeline.schedule(msg);});\n}\n</code></pre>"},{"location":"common/tvm_utility/#version-checking","title":"Version checking","text":"<p>The <code>InferenceEngineTVM::version_check</code> function can be used to check the version of the neural network in use against the range of earliest to latest supported versions.</p> <p>The <code>InferenceEngineTVM</code> class holds the latest supported version, which needs to be updated when the targeted version changes; after having tested the effect of the version change on the packages dependent on this one.</p> <p>The earliest supported version depends on each package making use of the inference, and so should be defined (and maintained) in those packages.</p>"},{"location":"common/tvm_utility/#models","title":"Models","text":"<p>Dependent packages are expected to use the <code>get_neural_network</code> cmake function from this package in order to build proper external dependency.</p>"},{"location":"common/tvm_utility/#error-detection-and-handling","title":"Error detection and handling","text":"<p><code>std::runtime_error</code> should be thrown whenever an error is encountered. It should be populated with an appropriate text error description.</p>"},{"location":"common/tvm_utility/#neural-networks-provider","title":"Neural Networks Provider","text":"<p>The neural networks are compiled as part of the Model Zoo CI pipeline and saved to an S3 bucket.</p> <p>The <code>get_neural_network</code> function creates an abstraction for the artifact management. Users should check if model configuration header file is under \"data/user/${MODEL_NAME}/\". Otherwise, nothing happens and compilation of the package will be skipped.</p> <p>The structure inside of the source directory of the package making use of the function is as follow:</p> <pre><code>.\n\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 models\n\u2502       \u251c\u2500\u2500 ${MODEL 1}\n\u2502       \u2502   \u2514\u2500\u2500 inference_engine_tvm_config.hpp\n\u2502       \u251c\u2500\u2500 ...\n\u2502       \u2514\u2500\u2500 ${MODEL ...}\n\u2502           \u2514\u2500\u2500 ...\n</code></pre> <p>The <code>inference_engine_tvm_config.hpp</code> file needed for compilation by dependent packages should be available under \"data/models/${MODEL_NAME}/inference_engine_tvm_config.hpp\". Dependent packages can use the cmake <code>add_dependencies</code> function with the name provided in the <code>DEPENDENCY</code> output parameter of <code>get_neural_network</code> to ensure this file is created before it gets used.</p> <p>The other <code>deploy_*</code> files are installed to \"models/${MODEL_NAME}/\" under the <code>share</code> directory of the package.</p> <p>The other model files should be stored in autoware_data folder under package folder with the structure:</p> <pre><code>$HOME/autoware_data\n|     \u2514\u2500\u2500${package}\n|        \u2514\u2500\u2500models\n|           \u251c\u2500\u2500 ${MODEL 1}\n|           |    \u251c\u2500\u2500 deploy_graph.json\n|           |    \u251c\u2500\u2500 deploy_lib.so\n|           |    \u2514\u2500\u2500 deploy_param.params\n|           \u251c\u2500\u2500 ...\n|           \u2514\u2500\u2500 ${MODEL ...}\n|                \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"common/tvm_utility/#inputs-outputs","title":"Inputs / Outputs","text":"<p>Outputs:</p> <ul> <li><code>get_neural_network</code> cmake function; create proper external dependency for a package with use of the model provided by the user.</li> </ul> <p>In/Out:</p> <ul> <li>The <code>DEPENDENCY</code> argument of <code>get_neural_network</code> can be checked for the outcome of the function.   It is an empty string when the neural network wasn't provided by the user.</li> </ul>"},{"location":"common/tvm_utility/#security-considerations","title":"Security considerations","text":""},{"location":"common/tvm_utility/#pipeline","title":"Pipeline","text":"<p>Both the input and output are controlled by the same actor, so the following security concerns are out-of-scope:</p> <ul> <li>Spoofing</li> <li>Tampering</li> </ul> <p>Leaking data to another actor would require a flaw in TVM or the host operating system that allows arbitrary memory to be read, a significant security flaw in itself. This is also true for an external actor operating the pipeline early: only the object that initiated the pipeline can run the methods to receive its output.</p> <p>A Denial-of-Service attack could make the target hardware unusable for other pipelines but would require being able to run code on the CPU, which would already allow a more severe Denial-of-Service attack.</p> <p>No elevation of privilege is required for this package.</p>"},{"location":"common/tvm_utility/#network-provider","title":"Network provider","text":"<p>The pre-compiled networks are downloaded from an S3 bucket and are under threat of spoofing, tampering and denial of service. Spoofing is mitigated by using an https connection. Mitigations for tampering and denial of service are left to AWS.</p> <p>The user-provided networks are installed as they are on the host system. The user is in charge of securing the files they provide with regard to information disclosure.</p>"},{"location":"common/tvm_utility/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":"<p>Future packages will use tvm_utility as part of the perception stack to run machine learning workloads.</p>"},{"location":"common/tvm_utility/#related-issues","title":"Related issues","text":"<p>https://github.com/autowarefoundation/autoware/discussions/2557</p>"},{"location":"common/tvm_utility/tvm-utility-yolo-v2-tiny-tests/","title":"YOLOv2 Tiny Example Pipeline","text":""},{"location":"common/tvm_utility/tvm-utility-yolo-v2-tiny-tests/#yolov2-tiny-example-pipeline","title":"YOLOv2 Tiny Example Pipeline","text":"<p>This is an example implementation of an inference pipeline using the pipeline framework. This example pipeline executes the YOLO V2 Tiny model and decodes its output.</p>"},{"location":"common/tvm_utility/tvm-utility-yolo-v2-tiny-tests/#compiling-the-example","title":"Compiling the Example","text":"<ol> <li> <p>Check if model was downloaded during the env preparation step by ansible and    models files exist in the folder $HOME/autoware_data/tvm_utility/models/yolo_v2_tiny.</p> <p>If not you can download them manually, see Manual Artifacts Downloading.</p> </li> <li> <p>Download an example image to be used as test input. This image needs to be    saved in the <code>artifacts/yolo_v2_tiny/</code> folder.</p> <pre><code>curl https://raw.githubusercontent.com/pjreddie/darknet/master/data/dog.jpg \\\n&gt; artifacts/yolo_v2_tiny/test_image_0.jpg\n</code></pre> </li> <li> <p>Build.</p> <pre><code>colcon build --packages-up-to tvm_utility --cmake-args -DBUILD_EXAMPLE=ON\n</code></pre> </li> <li> <p>Run.</p> <pre><code>ros2 launch tvm_utility yolo_v2_tiny_example.launch.xml\n</code></pre> </li> </ol>"},{"location":"common/tvm_utility/tvm-utility-yolo-v2-tiny-tests/#parameters","title":"Parameters","text":"Name Type Default Value Description <code>image_filename</code> string <code>$(find-pkg-share tvm_utility)/artifacts/yolo_v2_tiny/test_image_0.jpg</code> Filename of the image on which to run the inference. <code>label_filename</code> string <code>$(find-pkg-share tvm_utility)/artifacts/yolo_v2_tiny/labels.txt</code> Name of file containing the human readable names of the classes. One class on each line. <code>anchor_filename</code> string <code>$(find-pkg-share tvm_utility)/artifacts/yolo_v2_tiny/anchors.csv</code> Name of file containing the anchor values for the network. Each line is one anchor. each anchor has 2 comma separated floating point values. <code>data_path</code> string <code>$(env HOME)/autoware_data</code> Packages data and artifacts directory path."},{"location":"common/tvm_utility/tvm-utility-yolo-v2-tiny-tests/#gpu-backend","title":"GPU backend","text":"<p>Vulkan is supported by default by the tvm_vendor package. It can be selected by setting the <code>tvm_utility_BACKEND</code> variable:</p> <pre><code>colcon build --packages-up-to tvm_utility -Dtvm_utility_BACKEND=vulkan\n</code></pre>"},{"location":"common/tvm_utility/artifacts/","title":"TVM Utility Artifacts","text":""},{"location":"common/tvm_utility/artifacts/#tvm-utility-artifacts","title":"TVM Utility Artifacts","text":"<p>Place any test artifacts in subdirectories within this directory.</p> <p>e.g.: ./artifacts/yolo_v2_tiny</p>"},{"location":"control/autonomous_emergency_braking/","title":"Autonomous Emergency Braking (AEB)","text":""},{"location":"control/autonomous_emergency_braking/#autonomous-emergency-braking-aeb","title":"Autonomous Emergency Braking (AEB)","text":""},{"location":"control/autonomous_emergency_braking/#purpose-role","title":"Purpose / Role","text":"<p><code>autonomous_emergency_braking</code> is a module that prevents collisions with obstacles on the predicted path created by a control module or sensor values estimated from the control module.</p>"},{"location":"control/autonomous_emergency_braking/#assumptions","title":"Assumptions","text":"<p>This module has following assumptions.</p> <ul> <li>It is used when driving at low speeds (about 15 km/h).</li> </ul> <ul> <li>The predicted path of the ego vehicle can be made from either the path created from sensors or the path created from a control module, or both.</li> </ul> <ul> <li>The current speed and angular velocity can be obtained from the sensors of the ego vehicle, and it uses point cloud as obstacles.</li> </ul> <p></p>"},{"location":"control/autonomous_emergency_braking/#limitations","title":"Limitations","text":"<ul> <li>AEB might not be able to react with obstacles that are close to the ground. It depends on the performance of the pre-processing methods applied to the point cloud.</li> </ul> <ul> <li>Longitudinal acceleration information obtained from sensors is not used due to the high amount of noise.</li> </ul> <ul> <li>The accuracy of the predicted path created from sensor data depends on the accuracy of sensors attached to the ego vehicle.</li> </ul>"},{"location":"control/autonomous_emergency_braking/#parameters","title":"Parameters","text":"Name Unit Type Description Default value publish_debug_pointcloud [-] bool flag to publish the point cloud used for debugging false use_predicted_trajectory [-] bool flag to use the predicted path from the control module true use_imu_path [-] bool flag to use the predicted path generated by sensor data true voxel_grid_x [m] double down sampling parameters of x-axis for voxel grid filter 0.05 voxel_grid_y [m] double down sampling parameters of y-axis for voxel grid filter 0.05 voxel_grid_z [m] double down sampling parameters of z-axis for voxel grid filter 100000.0 min_generated_path_length [m] double minimum distance for a predicted path generated by sensors 0.5 expand_width [m] double expansion width of the ego vehicle for the collision check 0.1 longitudinal_offset [m] double longitudinal offset distance for collision check 2.0 t_response [s] double response time for the ego to detect the front vehicle starting deceleration 1.0 a_ego_min [m/ss] double maximum deceleration value of the ego vehicle -3.0 a_obj_min [m/ss] double maximum deceleration value of objects -3.0 imu_prediction_time_horizon [s] double time horizon of the predicted path generated by sensors 1.5 imu_prediction_time_interval [s] double time interval of the predicted path generated by sensors 0.1 mpc_prediction_time_horizon [s] double time horizon of the predicted path generated by mpc 1.5 mpc_prediction_time_interval [s] double time interval of the predicted path generated by mpc 0.1 aeb_hz [-] double frequency at which AEB operates per second 10"},{"location":"control/autonomous_emergency_braking/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>AEB has the following steps before it outputs the emergency stop signal.</p> <ol> <li> <p>Activate AEB if necessary.</p> </li> <li> <p>Generate a predicted path of the ego vehicle.</p> </li> <li> <p>Get target obstacles from the input point cloud.</p> </li> <li> <p>Collision check with target obstacles.</p> </li> <li> <p>Send emergency stop signals to <code>/diagnostics</code>.</p> </li> </ol> <p>We give more details of each section below.</p>"},{"location":"control/autonomous_emergency_braking/#1-activate-aeb-if-necessary","title":"1. Activate AEB if necessary","text":"<p>We do not activate AEB module if it satisfies the following conditions.</p> <ul> <li>Ego vehicle is not in autonomous driving state</li> </ul> <ul> <li>When the ego vehicle is not moving (Current Velocity is very low)</li> </ul>"},{"location":"control/autonomous_emergency_braking/#2-generate-a-predicted-path-of-the-ego-vehicle","title":"2. Generate a predicted path of the ego vehicle","text":"<p>AEB generates a predicted path based on current velocity and current angular velocity obtained from attached sensors. Note that if <code>use_imu_path</code> is <code>false</code>, it skips this step. This predicted path is generated as:</p> \\[ x_{k+1} = x_k + v cos(\\theta_k) dt \\\\ y_{k+1} = y_k + v sin(\\theta_k) dt \\\\ \\theta_{k+1} = \\theta_k + \\omega dt \\] <p>where \\(v\\) and \\(\\omega\\) are current longitudinal velocity and angular velocity respectively. \\(dt\\) is time interval that users can define in advance.</p>"},{"location":"control/autonomous_emergency_braking/#3-get-target-obstacles-from-the-input-point-cloud","title":"3. Get target obstacles from the input point cloud","text":"<p>After generating the ego predicted path, we select target obstacles from the input point cloud. This obstacle filtering has two major steps, which are rough filtering and rigorous filtering.</p>"},{"location":"control/autonomous_emergency_braking/#rough-filtering","title":"Rough filtering","text":"<p>In rough filtering step, we select target obstacle with simple filter. Create a search area up to a certain distance (default 5[m]) away from the predicted path of the ego vehicle and ignore the point cloud (obstacles) that are not within it. The image of the rough filtering is illustrated below.</p> <p></p>"},{"location":"control/autonomous_emergency_braking/#rigorous-filtering","title":"Rigorous filtering","text":"<p>After rough filtering, it performs a geometric collision check to determine whether the filtered obstacles actually have possibility to collide with the ego vehicle. In this check, the ego vehicle is represented as a rectangle, and the point cloud obstacles are represented as points.</p> <p></p>"},{"location":"control/autonomous_emergency_braking/#4-collision-check-with-target-obstacles","title":"4. Collision check with target obstacles","text":"<p>In the fourth step, it checks the collision with filtered obstacles using RSS distance. RSS is formulated as:</p> \\[ d = v_{ego}*t_{response} + v_{ego}^2/(2*a_{min}) - v_{obj}^2/(2*a_{obj_{min}}) + offset \\] <p>where \\(v_{ego}\\) and \\(v_{obj}\\) is current ego and obstacle velocity, \\(a_{min}\\) and \\(a_{obj_{min}}\\) is ego and object minimum acceleration (maximum deceleration), \\(t_{response}\\) is response time of the ego vehicle to start deceleration. Therefore the distance from the ego vehicle to the obstacle is smaller than this RSS distance \\(d\\), the ego vehicle send emergency stop signals. This is illustrated in the following picture.</p> <p></p>"},{"location":"control/autonomous_emergency_braking/#5-send-emergency-stop-signals-to-diagnostics","title":"5. Send emergency stop signals to <code>/diagnostics</code>","text":"<p>If AEB detects collision with point cloud obstacles in the previous step, it sends emergency signal to <code>/diagnostics</code> in this step. Note that in order to enable emergency stop, it has to send ERROR level emergency. Moreover, AEB user should modify the setting file to keep the emergency level, otherwise Autoware does not hold the emergency state.</p>"},{"location":"control/control_performance_analysis/","title":"control_performance_analysis","text":""},{"location":"control/control_performance_analysis/#control_performance_analysis","title":"control_performance_analysis","text":""},{"location":"control/control_performance_analysis/#purpose","title":"Purpose","text":"<p><code>control_performance_analysis</code> is the package to analyze the tracking performance of a control module and monitor the driving status of the vehicle.</p> <p>This package is used as a tool to quantify the results of the control module. That's why it doesn't interfere with the core logic of autonomous driving.</p> <p>Based on the various input from planning, control, and vehicle, it publishes the result of analysis as <code>control_performance_analysis::msg::ErrorStamped</code> defined in this package.</p> <p>All results in <code>ErrorStamped</code> message are calculated in Frenet Frame of curve. Errors and velocity errors are calculated by using paper below.</p> <p><code>Werling, Moritz &amp; Groell, Lutz &amp; Bretthauer, Georg. (2010). Invariant Trajectory Tracking With a Full-Size Autonomous Road Vehicle. IEEE Transactions on Robotics. 26. 758 - 765. 10.1109/TRO.2010.2052325.</code></p> <p>If you are interested in calculations, you can see the error and error velocity calculations in section <code>C. Asymptotical Trajectory Tracking With Orientation Control</code>.</p> <p>Error acceleration calculations are made based on the velocity calculations above. You can see below the calculation of error acceleration.</p> <p></p>"},{"location":"control/control_performance_analysis/#input-output","title":"Input / Output","text":""},{"location":"control/control_performance_analysis/#input-topics","title":"Input topics","text":"Name Type Description <code>/planning/scenario_planning/trajectory</code> autoware_auto_planning_msgs::msg::Trajectory Output trajectory from planning module. <code>/control/command/control_cmd</code> autoware_auto_control_msgs::msg::AckermannControlCommand Output control command from control module. <code>/vehicle/status/steering_status</code> autoware_auto_vehicle_msgs::msg::SteeringReport Steering information from vehicle. <code>/localization/kinematic_state</code> nav_msgs::msg::Odometry Use twist from odometry. <code>/tf</code> tf2_msgs::msg::TFMessage Extract ego pose from tf."},{"location":"control/control_performance_analysis/#output-topics","title":"Output topics","text":"Name Type Description <code>/control_performance/performance_vars</code> control_performance_analysis::msg::ErrorStamped The result of the performance analysis. <code>/control_performance/driving_status</code> control_performance_analysis::msg::DrivingMonitorStamped Driving status (acceleration, jerk etc.) monitoring"},{"location":"control/control_performance_analysis/#outputs","title":"Outputs","text":""},{"location":"control/control_performance_analysis/#control_performance_analysismsgdrivingmonitorstamped","title":"control_performance_analysis::msg::DrivingMonitorStamped","text":"Name Type Description <code>longitudinal_acceleration</code> float [m / s^2] <code>longitudinal_jerk</code> float [m / s^3] <code>lateral_acceleration</code> float [m / s^2] <code>lateral_jerk</code> float [m / s^3] <code>desired_steering_angle</code> float [rad] <code>controller_processing_time</code> float Timestamp between last two control command messages [ms]"},{"location":"control/control_performance_analysis/#control_performance_analysismsgerrorstamped","title":"control_performance_analysis::msg::ErrorStamped","text":"Name Type Description <code>lateral_error</code> float [m] <code>lateral_error_velocity</code> float [m / s] <code>lateral_error_acceleration</code> float [m / s^2] <code>longitudinal_error</code> float [m] <code>longitudinal_error_velocity</code> float [m / s] <code>longitudinal_error_acceleration</code> float [m / s^2] <code>heading_error</code> float [rad] <code>heading_error_velocity</code> float [rad / s] <code>control_effort_energy</code> float [u * R * u^T] <code>error_energy</code> float lateral_error^2 + heading_error^2 <code>value_approximation</code> float V = xPx' ; Value function from DARE Lyap matrix P <code>curvature_estimate</code> float [1 / m] <code>curvature_estimate_pp</code> float [1 / m] <code>vehicle_velocity_error</code> float [m / s] <code>tracking_curvature_discontinuity_ability</code> float Measures the ability to tracking the curvature changes [<code>abs(delta(curvature)) / (1 + abs(delta(lateral_error))</code>]"},{"location":"control/control_performance_analysis/#parameters","title":"Parameters","text":"Name Type Description <code>curvature_interval_length</code> double Used for estimating current curvature <code>prevent_zero_division_value</code> double Value to avoid zero division. Default is <code>0.001</code> <code>odom_interval</code> unsigned integer Interval between odom messages, increase it for smoother curve. <code>acceptable_max_distance_to_waypoint</code> double Maximum distance between trajectory point and vehicle [m] <code>acceptable_max_yaw_difference_rad</code> double Maximum yaw difference between trajectory point and vehicle [rad] <code>low_pass_filter_gain</code> double Low pass filter gain"},{"location":"control/control_performance_analysis/#usage","title":"Usage","text":"<ul> <li>After launched simulation and control module, launch the <code>control_performance_analysis.launch.xml</code>.</li> <li>You should be able to see the driving monitor and error variables in topics.</li> <li>If you want to visualize the results, you can use <code>Plotjuggler</code> and use <code>config/controller_monitor.xml</code> as layout.</li> <li>After import the layout, please specify the topics that are listed below.</li> </ul> <ul> <li>/localization/kinematic_state</li> <li>/vehicle/status/steering_status</li> <li>/control_performance/driving_status</li> <li>/control_performance/performance_vars</li> </ul> <ul> <li>In <code>Plotjuggler</code> you can export the statistic (max, min, average) values as csv file. Use that statistics to compare the control modules.</li> </ul>"},{"location":"control/control_performance_analysis/#future-improvements","title":"Future Improvements","text":"<ul> <li>Implement a LPF by cut-off frequency, differential equation and discrete state space update.</li> </ul>"},{"location":"control/control_validator/","title":"Control Validator","text":""},{"location":"control/control_validator/#control-validator","title":"Control Validator","text":"<p>The <code>control_validator</code> is a module that checks the validity of the output of the control component. The status of the validation can be viewed in the <code>/diagnostics</code> topic.</p> <p></p>"},{"location":"control/control_validator/#supported-features","title":"Supported features","text":"<p>The following features are supported for the validation and can have thresholds set by parameters:</p> <ul> <li>Deviation check between reference trajectory and predicted trajectory : invalid when the largest deviation between the predicted trajectory and reference trajectory is greater than the given threshold.</li> </ul> <p></p> <p>Other features are to be implemented.</p>"},{"location":"control/control_validator/#inputsoutputs","title":"Inputs/Outputs","text":""},{"location":"control/control_validator/#inputs","title":"Inputs","text":"<p>The <code>control_validator</code> takes in the following inputs:</p> Name Type Description <code>~/input/kinematics</code> nav_msgs/Odometry ego pose and twist <code>~/input/reference_trajectory</code> autoware_auto_control_msgs/Trajectory reference trajectory which is outputted from planning module to to be followed <code>~/input/predicted_trajectory</code> autoware_auto_control_msgs/Trajectory predicted trajectory which is outputted from control module"},{"location":"control/control_validator/#outputs","title":"Outputs","text":"<p>It outputs the following:</p> Name Type Description <code>~/output/validation_status</code> control_validator/ControlValidatorStatus validator status to inform the reason why the trajectory is valid/invalid <code>/diagnostics</code> diagnostic_msgs/DiagnosticStatus diagnostics to report errors"},{"location":"control/control_validator/#parameters","title":"Parameters","text":"<p>The following parameters can be set for the <code>control_validator</code>:</p>"},{"location":"control/control_validator/#system-parameters","title":"System parameters","text":"Name Type Description Default value <code>publish_diag</code> bool if true, diagnostics msg is published. true <code>diag_error_count_threshold</code> int the Diag will be set to ERROR when the number of consecutive invalid trajectory exceeds this threshold. (For example, threshold = 1 means, even if the trajectory is invalid, the Diag will not be ERROR if the next trajectory is valid.) true <code>display_on_terminal</code> bool show error msg on terminal true"},{"location":"control/control_validator/#algorithm-parameters","title":"Algorithm parameters","text":""},{"location":"control/control_validator/#thresholds","title":"Thresholds","text":"<p>The input trajectory is detected as invalid if the index exceeds the following thresholds.</p> Name Type Description Default value <code>thresholds.max_distance_deviation</code> double invalid threshold of the max distance deviation between the predicted path and the reference trajectory [m] 1.0"},{"location":"control/external_cmd_selector/","title":"external_cmd_selector","text":""},{"location":"control/external_cmd_selector/#external_cmd_selector","title":"external_cmd_selector","text":""},{"location":"control/external_cmd_selector/#purpose","title":"Purpose","text":"<p><code>external_cmd_selector</code> is the package to publish <code>external_control_cmd</code>, <code>gear_cmd</code>, <code>hazard_lights_cmd</code>, <code>heartbeat</code> and <code>turn_indicators_cmd</code>, according to the current mode, which is <code>remote</code> or <code>local</code>.</p> <p>The current mode is set via service, <code>remote</code> is remotely operated, <code>local</code> is to use the values calculated by Autoware.</p>"},{"location":"control/external_cmd_selector/#input-output","title":"Input / Output","text":""},{"location":"control/external_cmd_selector/#input-topics","title":"Input topics","text":"Name Type Description <code>/api/external/set/command/local/control</code> TBD Local. Calculated control value. <code>/api/external/set/command/local/heartbeat</code> TBD Local. Heartbeat. <code>/api/external/set/command/local/shift</code> TBD Local. Gear shift like drive, rear and etc. <code>/api/external/set/command/local/turn_signal</code> TBD Local. Turn signal like left turn, right turn and etc. <code>/api/external/set/command/remote/control</code> TBD Remote. Calculated control value. <code>/api/external/set/command/remote/heartbeat</code> TBD Remote. Heartbeat. <code>/api/external/set/command/remote/shift</code> TBD Remote. Gear shift like drive, rear and etc. <code>/api/external/set/command/remote/turn_signal</code> TBD Remote. Turn signal like left turn, right turn and etc."},{"location":"control/external_cmd_selector/#output-topics","title":"Output topics","text":"Name Type Description <code>/control/external_cmd_selector/current_selector_mode</code> TBD Current selected mode, remote or local. <code>/diagnostics</code> diagnostic_msgs::msg::DiagnosticArray Check if node is active or not. <code>/external/selected/external_control_cmd</code> TBD Pass through control command with current mode. <code>/external/selected/gear_cmd</code> autoware_auto_vehicle_msgs::msg::GearCommand Pass through gear command with current mode. <code>/external/selected/hazard_lights_cmd</code> autoware_auto_vehicle_msgs::msg::HazardLightsCommand Pass through hazard light with current mode. <code>/external/selected/heartbeat</code> TBD Pass through heartbeat with current mode. <code>/external/selected/turn_indicators_cmd</code> autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand Pass through turn indicator with current mode."},{"location":"control/joy_controller/","title":"joy_controller","text":""},{"location":"control/joy_controller/#joy_controller","title":"joy_controller","text":""},{"location":"control/joy_controller/#role","title":"Role","text":"<p><code>joy_controller</code> is the package to convert a joy msg to autoware commands (e.g. steering wheel, shift, turn signal, engage) for a vehicle.</p>"},{"location":"control/joy_controller/#input-output","title":"Input / Output","text":""},{"location":"control/joy_controller/#input-topics","title":"Input topics","text":"Name Type Description <code>~/input/joy</code> sensor_msgs::msg::Joy joy controller command <code>~/input/odometry</code> nav_msgs::msg::Odometry ego vehicle odometry to get twist"},{"location":"control/joy_controller/#output-topics","title":"Output topics","text":"Name Type Description <code>~/output/control_command</code> autoware_auto_control_msgs::msg::AckermannControlCommand lateral and longitudinal control command <code>~/output/external_control_command</code> tier4_external_api_msgs::msg::ControlCommandStamped lateral and longitudinal control command <code>~/output/shift</code> tier4_external_api_msgs::msg::GearShiftStamped gear command <code>~/output/turn_signal</code> tier4_external_api_msgs::msg::TurnSignalStamped turn signal command <code>~/output/gate_mode</code> tier4_control_msgs::msg::GateMode gate mode (Auto or External) <code>~/output/heartbeat</code> tier4_external_api_msgs::msg::Heartbeat heartbeat <code>~/output/vehicle_engage</code> autoware_auto_vehicle_msgs::msg::Engage vehicle engage"},{"location":"control/joy_controller/#parameters","title":"Parameters","text":"Parameter Type Description <code>joy_type</code> string joy controller type (default: DS4) <code>update_rate</code> double update rate to publish control commands <code>accel_ratio</code> double ratio to calculate acceleration (commanded acceleration is ratio * operation) <code>brake_ratio</code> double ratio to calculate deceleration (commanded acceleration is -ratio * operation) <code>steer_ratio</code> double ratio to calculate deceleration (commanded steer is ratio * operation) <code>steering_angle_velocity</code> double steering angle velocity for operation <code>accel_sensitivity</code> double sensitivity to calculate acceleration for external API (commanded acceleration is pow(operation, 1 / sensitivity)) <code>brake_sensitivity</code> double sensitivity to calculate deceleration for external API (commanded acceleration is pow(operation, 1 / sensitivity)) <code>velocity_gain</code> double ratio to calculate velocity by acceleration <code>max_forward_velocity</code> double absolute max velocity to go forward <code>max_backward_velocity</code> double absolute max velocity to go backward <code>backward_accel_ratio</code> double ratio to calculate deceleration (commanded acceleration is -ratio * operation)"},{"location":"control/joy_controller/#p65-joystick-key-map","title":"P65 Joystick Key Map","text":"Action Button Acceleration R2 Brake L2 Steering Left Stick Left Right Shift up Cursor Up Shift down Cursor Down Shift Drive Cursor Left Shift Reverse Cursor Right Turn Signal Left L1 Turn Signal Right R1 Clear Turn Signal A Gate Mode B Emergency Stop Select Clear Emergency Stop Start Autoware Engage X Autoware Disengage Y Vehicle Engage PS Vehicle Disengage Right Trigger"},{"location":"control/joy_controller/#ds4-joystick-key-map","title":"DS4 Joystick Key Map","text":"Action Button Acceleration R2, \u00d7, or Right Stick Up Brake L2, \u25a1, or Right Stick Down Steering Left Stick Left Right Shift up Cursor Up Shift down Cursor Down Shift Drive Cursor Left Shift Reverse Cursor Right Turn Signal Left L1 Turn Signal Right R1 Clear Turn Signal SHARE Gate Mode OPTIONS Emergency Stop PS Clear Emergency Stop PS Autoware Engage \u25cb Autoware Disengage \u25cb Vehicle Engage \u25b3 Vehicle Disengage \u25b3"},{"location":"control/lane_departure_checker/","title":"Lane Departure Checker","text":""},{"location":"control/lane_departure_checker/#lane-departure-checker","title":"Lane Departure Checker","text":"<p>The Lane Departure Checker checks if vehicle follows a trajectory. If it does not follow the trajectory, it reports its status via <code>diagnostic_updater</code>.</p>"},{"location":"control/lane_departure_checker/#features","title":"Features","text":"<p>This package includes the following features:</p> <ul> <li>Lane Departure: Check if ego vehicle is going to be out of lane boundaries based on output from control module (predicted trajectory).</li> <li>Trajectory Deviation: Check if ego vehicle's pose does not deviate from the trajectory. Checking lateral, longitudinal and yaw deviation.</li> <li>Road Border Departure: Check if ego vehicle's footprint, generated from the control's output, extends beyond the road border.</li> </ul>"},{"location":"control/lane_departure_checker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"control/lane_departure_checker/#how-to-extend-footprint-by-covariance","title":"How to extend footprint by covariance","text":"<ol> <li> <p>Calculate the standard deviation of error ellipse(covariance) in vehicle coordinate.</p> <p>1.Transform covariance into vehicle coordinate.</p> <p> </p> <p>Calculate covariance in vehicle coordinate.</p> <p> </p> <p>2.The longitudinal length we want to expand is correspond to marginal distribution of \\(x_{vehicle}\\), which is represented in \\(Cov_{vehicle}(0,0)\\). In the same way, the lateral length is represented in \\(Cov_{vehicle}(1,1)\\). Wikipedia reference here.</p> </li> <li> <p>Expand footprint based on the standard deviation multiplied with <code>footprint_margin_scale</code>.</p> </li> </ol>"},{"location":"control/lane_departure_checker/#interface","title":"Interface","text":""},{"location":"control/lane_departure_checker/#input","title":"Input","text":"<ul> <li>/localization/kinematic_state [<code>nav_msgs::msg::Odometry</code>]</li> <li>/map/vector_map [<code>autoware_auto_mapping_msgs::msg::HADMapBin</code>]</li> <li>/planning/mission_planning/route [<code>autoware_planning_msgs::msg::LaneletRoute</code>]</li> <li>/planning/scenario_planning/trajectory [<code>autoware_auto_planning_msgs::msg::Trajectory</code>]</li> <li>/control/trajectory_follower/predicted_trajectory [<code>autoware_auto_planning_msgs::msg::Trajectory</code>]</li> </ul>"},{"location":"control/lane_departure_checker/#output","title":"Output","text":"<ul> <li>[<code>diagnostic_updater</code>] lane_departure : Update diagnostic level when ego vehicle is out of lane.</li> <li>[<code>diagnostic_updater</code>] trajectory_deviation : Update diagnostic level when ego vehicle deviates from trajectory.</li> </ul>"},{"location":"control/lane_departure_checker/#parameters","title":"Parameters","text":""},{"location":"control/lane_departure_checker/#node-parameters","title":"Node Parameters","text":""},{"location":"control/lane_departure_checker/#general-parameters","title":"General Parameters","text":"Name Type Description Default value will_out_of_lane_checker bool Enable checker whether ego vehicle footprint will depart from lane True out_of_lane_checker bool Enable checker whether ego vehicle footprint is out of lane True boundary_departure_checker bool Enable checker whether ego vehicle footprint wil depart from boundary specified by boundary_types_to_detect False update_rate double Frequency for publishing [Hz] 10.0 visualize_lanelet bool Flag for visualizing lanelet False"},{"location":"control/lane_departure_checker/#parameters-for-lane-departure","title":"Parameters For Lane Departure","text":"Name Type Description Default value include_right_lanes bool Flag for including right lanelet in borders False include_left_lanes bool Flag for including left lanelet in borders False include_opposite_lanes bool Flag for including opposite lanelet in borders False include_conflicting_lanes bool Flag for including conflicting lanelet in borders False"},{"location":"control/lane_departure_checker/#parameters-for-road-border-departure","title":"Parameters For Road Border Departure","text":"Name Type Description Default value boundary_types_to_detect std::vector\\&lt;std::string&gt; line_string types to detect with boundary_departure_checker [road_border]"},{"location":"control/lane_departure_checker/#core-parameters","title":"Core Parameters","text":"Name Type Description Default value footprint_margin_scale double Coefficient for expanding footprint margin. Multiplied by 1 standard deviation. 1.0 resample_interval double Minimum Euclidean distance between points when resample trajectory.[m] 0.3 max_deceleration double Maximum deceleration when calculating braking distance. 2.8 delay_time double Delay time which took to actuate brake when calculating braking distance. [second] 1.3 max_lateral_deviation double Maximum lateral deviation in vehicle coordinate. [m] 2.0 max_longitudinal_deviation double Maximum longitudinal deviation in vehicle coordinate. [m] 2.0 max_yaw_deviation_deg double Maximum ego yaw deviation from trajectory. [deg] 60.0"},{"location":"control/mpc_lateral_controller/","title":"MPC Lateral Controller","text":""},{"location":"control/mpc_lateral_controller/#mpc-lateral-controller","title":"MPC Lateral Controller","text":"<p>This is the design document for the lateral controller node in the <code>trajectory_follower_node</code> package.</p>"},{"location":"control/mpc_lateral_controller/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>This node is used to general lateral control commands (steering angle and steering rate) when following a path.</p>"},{"location":"control/mpc_lateral_controller/#design","title":"Design","text":"<p>The node uses an implementation of linear model predictive control (MPC) for accurate path tracking. The MPC uses a model of the vehicle to simulate the trajectory resulting from the control command. The optimization of the control command is formulated as a Quadratic Program (QP).</p> <p>Different vehicle models are implemented:</p> <ul> <li>kinematics : bicycle kinematics model with steering 1st-order delay.</li> <li>kinematics_no_delay : bicycle kinematics model without steering delay.</li> <li>dynamics : bicycle dynamics model considering slip angle.   The kinematics model is being used by default. Please see the reference [1] for more details.</li> </ul> <p>For the optimization, a Quadratic Programming (QP) solver is used and two options are currently implemented:</p> <ul> <li>unconstraint_fast : use least square method to solve unconstraint QP with eigen.</li> <li>osqp: run the following ADMM   algorithm (for more details see the related papers at   the Citing OSQP section):</li> </ul>"},{"location":"control/mpc_lateral_controller/#filtering","title":"Filtering","text":"<p>Filtering is required for good noise reduction. A Butterworth filter is employed for processing the yaw and lateral errors, which are used as inputs for the MPC, as well as for refining the output steering angle. Other filtering methods can be considered as long as the noise reduction performances are good enough. The moving average filter for example is not suited and can yield worse results than without any filtering.</p>"},{"location":"control/mpc_lateral_controller/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The tracking is not accurate if the first point of the reference trajectory is at or in front of the current ego pose.</p>"},{"location":"control/mpc_lateral_controller/#inputs-outputs-api","title":"Inputs / Outputs / API","text":""},{"location":"control/mpc_lateral_controller/#inputs","title":"Inputs","text":"<p>Set the following from the controller_node</p> <ul> <li><code>autoware_auto_planning_msgs/Trajectory</code> : reference trajectory to follow.</li> <li><code>nav_msgs/Odometry</code>: current odometry</li> <li><code>autoware_auto_vehicle_msgs/SteeringReport</code>: current steering</li> </ul>"},{"location":"control/mpc_lateral_controller/#outputs","title":"Outputs","text":"<p>Return LateralOutput which contains the following to the controller node</p> <ul> <li><code>autoware_auto_control_msgs/AckermannLateralCommand</code></li> <li>LateralSyncData<ul> <li>steer angle convergence</li> </ul> </li> </ul>"},{"location":"control/mpc_lateral_controller/#mpc-class","title":"MPC class","text":"<p>The <code>MPC</code> class (defined in <code>mpc.hpp</code>) provides the interface with the MPC algorithm. Once a vehicle model, a QP solver, and the reference trajectory to follow have been set (using <code>setVehicleModel()</code>, <code>setQPSolver()</code>, <code>setReferenceTrajectory()</code>), a lateral control command can be calculated by providing the current steer, velocity, and pose to function <code>calculateMPC()</code>.</p>"},{"location":"control/mpc_lateral_controller/#parameter-description","title":"Parameter description","text":"<p>The default parameters defined in <code>param/lateral_controller_defaults.param.yaml</code> are adjusted to the AutonomouStuff Lexus RX 450h for under 40 km/h driving.</p>"},{"location":"control/mpc_lateral_controller/#system","title":"System","text":"Name Type Description Default value traj_resample_dist double distance of waypoints in resampling [m] 0.1 use_steer_prediction boolean flag for using steer prediction (do not use steer measurement) false admissible_position_error double stop vehicle when following position error is larger than this value [m] 5.0 admissible_yaw_error_rad double stop vehicle when following yaw angle error is larger than this value [rad] 1.57"},{"location":"control/mpc_lateral_controller/#path-smoothing","title":"Path Smoothing","text":"Name Type Description Default value enable_path_smoothing boolean path smoothing flag. This should be true when uses path resampling to reduce resampling noise. false path_filter_moving_ave_num int number of data points moving average filter for path smoothing 25 curvature_smoothing_num_traj int index distance of points used in curvature calculation for trajectory: p(i-num), p(i), p(i+num). larger num makes less noisy values. 15 curvature_smoothing_num_ref_steer int index distance of points used in curvature calculation for reference steering command: p(i-num), p(i), p(i+num). larger num makes less noisy values. 15"},{"location":"control/mpc_lateral_controller/#trajectory-extending","title":"Trajectory Extending","text":"Name Type Description Default value extend_trajectory_for_end_yaw_control boolean trajectory extending flag for end yaw control true"},{"location":"control/mpc_lateral_controller/#mpc-optimization","title":"MPC Optimization","text":"Name Type Description Default value qp_solver_type string QP solver option. described below in detail. \"osqp\" mpc_prediction_horizon int total prediction step for MPC 50 mpc_prediction_dt double prediction period for one step [s] 0.1 mpc_weight_lat_error double weight for lateral error 1.0 mpc_weight_heading_error double weight for heading error 0.0 mpc_weight_heading_error_squared_vel double weight for heading error * velocity 0.3 mpc_weight_steering_input double weight for steering error (steer command - reference steer) 1.0 mpc_weight_steering_input_squared_vel double weight for steering error (steer command - reference steer) * velocity 0.25 mpc_weight_lat_jerk double weight for lateral jerk (steer(i) - steer(i-1)) * velocity 0.1 mpc_weight_steer_rate double weight for steering rate [rad/s] 0.0 mpc_weight_steer_acc double weight for derivatives of the steering rate [rad/ss] 0.000001 mpc_low_curvature_weight_lat_error double [used in a low curvature trajectory] weight for lateral error 0.1 mpc_low_curvature_weight_heading_error double [used in a low curvature trajectory] weight for heading error 0.0 mpc_low_curvature_weight_heading_error_squared_vel double [used in a low curvature trajectory] weight for heading error * velocity 0.3 mpc_low_curvature_weight_steering_input double [used in a low curvature trajectory] weight for steering error (steer command - reference steer) 1.0 mpc_low_curvature_weight_steering_input_squared_vel double [used in a low curvature trajectory] weight for steering error (steer command - reference steer) * velocity 0.25 mpc_low_curvature_weight_lat_jerk double [used in a low curvature trajectory] weight for lateral jerk (steer(i) - steer(i-1)) * velocity 0.0 mpc_low_curvature_weight_steer_rate double [used in a low curvature trajectory] weight for steering rate [rad/s] 0.0 mpc_low_curvature_weight_steer_acc double [used in a low curvature trajectory] weight for derivatives of the steering rate [rad/ss] 0.000001 mpc_low_curvature_thresh_curvature double threshold of curvature to use \"low_curvature\" parameter 0.0 mpc_weight_terminal_lat_error double terminal lateral error weight in matrix Q to improve mpc stability 1.0 mpc_weight_terminal_heading_error double terminal heading error weight in matrix Q to improve mpc stability 0.1 mpc_zero_ff_steer_deg double threshold that feed-forward angle becomes zero 0.5 mpc_acceleration_limit double limit on the vehicle's acceleration 2.0 mpc_velocity_time_constant double time constant used for velocity smoothing 0.3 mpc_min_prediction_length double minimum prediction length 5.0"},{"location":"control/mpc_lateral_controller/#vehicle-model","title":"Vehicle Model","text":"Name Type Description Default value vehicle_model_type string vehicle model type for mpc prediction \"kinematics\" input_delay double steering input delay time for delay compensation 0.24 vehicle_model_steer_tau double steering dynamics time constant (1d approximation) [s] 0.3 steer_rate_lim_dps_list_by_curvature [double] steering angle rate limit list depending on curvature [deg/s] [40.0, 50.0, 60.0] curvature_list_for_steer_rate_lim [double] curvature list for steering angle rate limit interpolation in ascending order [/m] [0.001, 0.002, 0.01] steer_rate_lim_dps_list_by_velocity [double] steering angle rate limit list depending on velocity [deg/s] [60.0, 50.0, 40.0] velocity_list_for_steer_rate_lim [double] velocity list for steering angle rate limit interpolation in ascending order [m/s] [10.0, 15.0, 20.0] acceleration_limit double acceleration limit for trajectory velocity modification [m/ss] 2.0 velocity_time_constant double velocity dynamics time constant for trajectory velocity modification [s] 0.3"},{"location":"control/mpc_lateral_controller/#lowpass-filter-for-noise-reduction","title":"Lowpass Filter for Noise Reduction","text":"Name Type Description Default value steering_lpf_cutoff_hz double cutoff frequency of lowpass filter for steering output command [hz] 3.0 error_deriv_lpf_cutoff_hz double cutoff frequency of lowpass filter for error derivative [Hz] 5.0"},{"location":"control/mpc_lateral_controller/#stop-state","title":"Stop State","text":"Name Type Description Default value stop_state_entry_ego_speed *1 double threshold value of the ego vehicle speed used to the stop state entry condition 0.001 stop_state_entry_target_speed *1 double threshold value of the target speed used to the stop state entry condition 0.001 converged_steer_rad double threshold value of the steer convergence 0.1 keep_steer_control_until_converged boolean keep steer control until steer is converged true new_traj_duration_time double threshold value of the time to be considered as new trajectory 1.0 new_traj_end_dist double threshold value of the distance between trajectory ends to be considered as new trajectory 0.3 mpc_converged_threshold_rps double threshold value to be sure output of the optimization is converged, it is used in stopped state 0.01 <p>(*1) To prevent unnecessary steering movement, the steering command is fixed to the previous value in the stop state.</p>"},{"location":"control/mpc_lateral_controller/#steer-offset","title":"Steer Offset","text":"<p>Defined in the <code>steering_offset</code> namespace. This logic is designed as simple as possible, with minimum design parameters.</p> Name Type Description Default value enable_auto_steering_offset_removal boolean Estimate the steering offset and apply compensation true update_vel_threshold double If the velocity is smaller than this value, the data is not used for the offset estimation 5.56 update_steer_threshold double If the steering angle is larger than this value, the data is not used for the offset estimation. 0.035 average_num int The average of this number of data is used as a steering offset. 1000 steering_offset_limit double The angle limit to be applied to the offset compensation. 0.02"},{"location":"control/mpc_lateral_controller/#for-dynamics-model-wip","title":"For dynamics model (WIP)","text":"Name Type Description Default value cg_to_front_m double distance from baselink to the front axle[m] 1.228 cg_to_rear_m double distance from baselink to the rear axle [m] 1.5618 mass_fl double mass applied to front left tire [kg] 600 mass_fr double mass applied to front right tire [kg] 600 mass_rl double mass applied to rear left tire [kg] 600 mass_rr double mass applied to rear right tire [kg] 600 cf double front cornering power [N/rad] 155494.663 cr double rear cornering power [N/rad] 155494.663"},{"location":"control/mpc_lateral_controller/#how-to-tune-mpc-parameters","title":"How to tune MPC parameters","text":""},{"location":"control/mpc_lateral_controller/#set-kinematics-information","title":"Set kinematics information","text":"<p>First, it's important to set the appropriate parameters for vehicle kinematics. This includes parameters like <code>wheelbase</code>, which represents the distance between the front and rear wheels, and <code>max_steering_angle</code>, which indicates the maximum tire steering angle. These parameters should be set in the <code>vehicle_info.param.yaml</code>.</p>"},{"location":"control/mpc_lateral_controller/#set-dynamics-information","title":"Set dynamics information","text":"<p>Next, you need to set the proper parameters for the dynamics model. These include the time constant <code>steering_tau</code> and time delay <code>steering_delay</code> for steering dynamics, and the maximum acceleration <code>mpc_acceleration_limit</code> and the time constant <code>mpc_velocity_time_constant</code> for velocity dynamics.</p>"},{"location":"control/mpc_lateral_controller/#confirmation-of-the-input-information","title":"Confirmation of the input information","text":"<p>It's also important to make sure the input information is accurate. Information such as the velocity of the center of the rear wheel [m/s] and the steering angle of the tire [rad] is required. Please note that there have been frequent reports of performance degradation due to errors in input information. For instance, there are cases where the velocity of the vehicle is offset due to an unexpected difference in tire radius, or the tire angle cannot be accurately measured due to a deviation in the steering gear ratio or midpoint. It is suggested to compare information from multiple sensors (e.g., integrated vehicle speed and GNSS position, steering angle and IMU angular velocity), and ensure the input information for MPC is appropriate.</p>"},{"location":"control/mpc_lateral_controller/#mpc-weight-tuning","title":"MPC weight tuning","text":"<p>Then, tune the weights of the MPC. One simple approach of tuning is to keep the weight for the lateral deviation (<code>weight_lat_error</code>) constant, and vary the input weight (<code>weight_steering_input</code>) while observing the trade-off between steering oscillation and control accuracy.</p> <p>Here, <code>weight_lat_error</code> acts to suppress the lateral error in path following, while <code>weight_steering_input</code> works to adjust the steering angle to a standard value determined by the path's curvature. When <code>weight_lat_error</code> is large, the steering moves significantly to improve accuracy, which can cause oscillations. On the other hand, when <code>weight_steering_input</code> is large, the steering doesn't respond much to tracking errors, providing stable driving but potentially reducing tracking accuracy.</p> <p>The steps are as follows:</p> <ol> <li>Set <code>weight_lat_error</code> = 0.1, <code>weight_steering_input</code> = 1.0 and other weights to 0.</li> <li>If the vehicle oscillates when driving, set <code>weight_steering_input</code> larger.</li> <li>If the tracking accuracy is low, set <code>weight_steering_input</code> smaller.</li> </ol> <p>If you want to adjust the effect only in the high-speed range, you can use <code>weight_steering_input_squared_vel</code>. This parameter corresponds to the steering weight in the high-speed range.</p>"},{"location":"control/mpc_lateral_controller/#descriptions-for-weights","title":"Descriptions for weights","text":"<ul> <li><code>weight_lat_error</code>: Reduce lateral tracking error. This acts like P gain in PID.</li> <li><code>weight_heading_error</code>: Make a drive straight. This acts like D gain in PID.</li> <li><code>weight_heading_error_squared_vel_coeff</code> : Make a drive straight in high speed range.</li> <li><code>weight_steering_input</code>: Reduce oscillation of tracking.</li> <li><code>weight_steering_input_squared_vel_coeff</code>: Reduce oscillation of tracking in high speed range.</li> <li><code>weight_lat_jerk</code>: Reduce lateral jerk.</li> <li><code>weight_terminal_lat_error</code>: Preferable to set a higher value than normal lateral weight <code>weight_lat_error</code> for stability.</li> <li><code>weight_terminal_heading_error</code>: Preferable to set a higher value than normal heading weight <code>weight_heading_error</code> for stability.</li> </ul>"},{"location":"control/mpc_lateral_controller/#other-tips-for-tuning","title":"Other tips for tuning","text":"<p>Here are some tips for adjusting other parameters:</p> <ul> <li>In theory, increasing terminal weights, <code>weight_terminal_lat_error</code> and <code>weight_terminal_heading_error</code>, can enhance the tracking stability. This method sometimes proves effective.</li> <li>A larger <code>prediction_horizon</code> and a smaller <code>prediction_sampling_time</code> are efficient for tracking performance. However, these come at the cost of higher computational costs.</li> <li>If you want to modify the weight according to the trajectory curvature (for instance, when you're driving on a sharp curve and want a larger weight), use <code>mpc_low_curvature_thresh_curvature</code> and adjust <code>mpc_low_curvature_weight_**</code> weights.</li> <li>If you want to adjust the steering rate limit based on the vehicle speed and trajectory curvature, you can modify the values of <code>steer_rate_lim_dps_list_by_curvature</code>, <code>curvature_list_for_steer_rate_lim</code>, <code>steer_rate_lim_dps_list_by_velocity</code>, <code>velocity_list_for_steer_rate_lim</code>. By doing this, you can enforce the steering rate limit during high-speed driving or relax it while curving.</li> <li>In case your target curvature appears jagged, adjusting <code>curvature_smoothing</code> becomes critically important for accurate curvature calculations. A larger value yields a smooth curvature calculation which reduces noise but can cause delay in feedforward computation and potentially degrade performance.</li> <li>Adjusting the <code>steering_lpf_cutoff_hz</code> value can also be effective to forcefully reduce computational noise. This refers to the cutoff frequency in the second order Butterworth filter installed in the final layer. The smaller the cutoff frequency, the stronger the noise reduction, but it also induce operation delay.</li> <li>If the vehicle consistently deviates laterally from the trajectory, it's most often due to the offset of the steering sensor or self-position estimation. It's preferable to eliminate these biases before inputting into MPC, but it's also possible to remove this bias within MPC. To utilize this, set <code>enable_auto_steering_offset_removal</code> to true and activate the steering offset remover. The steering offset estimation logic works when driving at high speeds with the steering close to the center, applying offset removal.</li> <li>If the onset of steering in curves is late, it's often due to incorrect delay time and time constant in the steering model. Please recheck the values of <code>input_delay</code> and <code>vehicle_model_steer_tau</code>. Additionally, as a part of its debug information, MPC outputs the current steering angle assumed by the MPC model, so please check if that steering angle matches the actual one.</li> </ul>"},{"location":"control/mpc_lateral_controller/#references-external-links","title":"References / External links","text":"<ul> <li>[1] Jarrod M. Snider, \"Automatic Steering Methods for Autonomous Automobile Path Tracking\",   Robotics Institute, Carnegie Mellon University, February 2009.</li> </ul>"},{"location":"control/mpc_lateral_controller/#related-issues","title":"Related issues","text":""},{"location":"control/mpc_lateral_controller/model_predictive_control_algorithm/","title":"MPC Algorithm","text":""},{"location":"control/mpc_lateral_controller/model_predictive_control_algorithm/#mpc-algorithm","title":"MPC Algorithm","text":""},{"location":"control/mpc_lateral_controller/model_predictive_control_algorithm/#introduction","title":"Introduction","text":"<p>Model Predictive Control (MPC) is a control method that solves an optimization problem during each control cycle to determine an optimal control sequence based on a given vehicle model. The calculated sequence of control inputs is used to control the system.</p> <p>In simpler terms, an MPC controller calculates a series of control inputs that optimize the state and output trajectories to achieve the desired behavior. The key characteristics of an MPC control system can be summarized as follows:</p> <ol> <li>Prediction of Future Trajectories: MPC computes a control sequence by predicting the future state and output trajectories. The first control input is applied to the system, and this process repeats in a receding horizon manner at each control cycle.</li> <li>Handling of Constraints: MPC is capable of handling constraints on the state and input variables during the optimization phase. This ensures that the system operates within specified limits.</li> <li>Handling of Complex Dynamics: MPC algorithms can handle complex dynamics, whether they are linear or nonlinear in nature.</li> </ol> <p>The choice between a linear or nonlinear model or constraint equation depends on the specific formulation of the MPC problem. If any nonlinear expressions are present in the motion equation or constraints, the optimization problem becomes nonlinear. In the following sections, we provide a step-by-step explanation of how linear and nonlinear optimization problems are solved within the MPC framework. Note that in this documentation, we utilize the linearization method to accommodate the nonlinear model.</p>"},{"location":"control/mpc_lateral_controller/model_predictive_control_algorithm/#linear-mpc-formulation","title":"Linear MPC formulation","text":""},{"location":"control/mpc_lateral_controller/model_predictive_control_algorithm/#formulate-as-an-optimization-problem","title":"Formulate as an optimization problem","text":"<p>This section provides an explanation of MPC specifically for linear systems. In the following section, it also demonstrates the formulation of a vehicle path following problem as an application.</p> <p>In the linear MPC formulation, all motion and constraint expressions are linear. For the path following problem, let's assume that the system's motion can be described by a set of equations, denoted as (1). The state evolution and measurements are presented in a discrete state space format, where matrices \\(A\\), \\(B\\), and \\(C\\) represent the state transition, control, and measurement matrices, respectively.</p> \\[ \\begin{gather} x_{k+1}=Ax_{k}+Bu_{k}+w_{k}, y_{k}=Cx_{k} \\tag{1} \\\\ x_{k}\\in R^{n},u_{k}\\in R^{m},w_{k}\\in R^{n}, y_{k}\\in R^{l}, A\\in R^{n\\times n}, B\\in R^{n\\times m}, C\\in R^{l \\times n} \\end{gather} \\] <p>Equation (1) represents the state-space equation, where \\(x_k\\) represents the internal states, \\(u_k\\) denotes the input, and \\(w_k\\) represents a known disturbance caused by linearization or problem structure. The measurements are indicated by the variable \\(y_k\\).</p> <p>It's worth noting that another advantage of MPC is its ability to effectively handle the disturbance term \\(w\\). While it is referred to as a disturbance here, it can take various forms as long as it adheres to the equation's structure.</p> <p>The state transition and measurement equations in (1) are iterative, moving from time \\(k\\) to time \\(k+1\\). By propagating the equation starting from an initial state and control pair \\((x_0, u_0)\\) along with a specified horizon of \\(N\\) steps, one can predict the trajectories of states and measurements.</p> <p>For simplicity, let's assume the initial state is \\(x_0\\) with \\(k=0\\).</p> <p>To begin, we can compute the state \\(x_1\\) at \\(k=1\\) using equation (1) by substituting the initial state into the equation. Since we are seeking a solution for the input sequence, we represent the inputs as decision variables in the symbolic expressions.</p> \\[ \\begin{align} x_{1} = Ax_{0} + Bu_{0} + w_{0} \\tag{2} \\end{align} \\] <p>Then, when \\(k=2\\), using also equation (2), we get</p> \\[ \\begin{align} x_{2} &amp; = Ax_{1} + Bu_{1} + w_{1} \\\\ &amp; = A(Ax_{0} + Bu_{0} + w_{0}) + Bu_{1} + w_{1} \\\\ &amp; = A^{2}x_{0} + ABu_{0} + Aw_{0} + Bu_{1} + w_{1} \\\\ &amp; = A^{2}x_{0} + \\begin{bmatrix}AB &amp; B \\end{bmatrix}\\begin{bmatrix}u_{0}\\\\ u_{1} \\end{bmatrix} + \\begin{bmatrix}A &amp; I \\end{bmatrix}\\begin{bmatrix}w_{0}\\\\ w_{1} \\end{bmatrix} \\tag{3} \\end{align} \\] <p>When \\(k=3\\) , from equation (3)</p> \\[ \\begin{align} x_{3} &amp; = Ax_{2} + Bu_{2} + w_{2} \\\\ &amp; = A(A^{2}x_{0} + ABu_{0} + Bu_{1} + Aw_{0} + w_{1} ) + Bu_{2} + w_{2} \\\\ &amp; = A^{3}x_{0} + A^{2}Bu_{0} + ABu_{1} + A^{2}w_{0} + Aw_{1} + Bu_{2} + w_{2} \\\\ &amp; = A^{3}x_{0} + \\begin{bmatrix}A^{2}B &amp; AB &amp; B  \\end{bmatrix}\\begin{bmatrix}u_{0}\\\\ u_{1} \\\\ u_{2} \\end{bmatrix} + \\begin{bmatrix} A^{2} &amp; A &amp; I \\end{bmatrix}\\begin{bmatrix}w_{0}\\\\ w_{1} \\\\ w_{2} \\end{bmatrix} \\tag{4} \\end{align} \\] <p>If \\(k=n\\) , then</p> \\[ \\begin{align} x_{n} = A^{n}x_{0} + \\begin{bmatrix}A^{n-1}B &amp; A^{n-2}B &amp; \\dots  &amp; B  \\end{bmatrix}\\begin{bmatrix}u_{0}\\\\ u_{1} \\\\ \\vdots  \\\\ u_{n-1} \\end{bmatrix} + \\begin{bmatrix} A^{n-1} &amp; A^{n-2} &amp; \\dots &amp; I \\end{bmatrix}\\begin{bmatrix}w_{0}\\\\ w_{1} \\\\ \\vdots \\\\ w_{n-1} \\end{bmatrix} \\tag{5} \\end{align} \\] <p>Putting all of them together with (2) to (5) yields the following matrix equation;</p> \\[ \\begin{align} \\begin{bmatrix}x_{1}\\\\ x_{2} \\\\ x_{3} \\\\ \\vdots  \\\\ x_{n} \\end{bmatrix} = \\begin{bmatrix}A^{1}\\\\ A^{2} \\\\ A^{3} \\\\ \\vdots  \\\\ A^{n} \\end{bmatrix}x_{0} + \\begin{bmatrix}B &amp; 0 &amp; \\dots  &amp; &amp; 0 \\\\ AB &amp; B &amp; 0 &amp; \\dots &amp; 0  \\\\ A^{2}B &amp; AB &amp; B &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; &amp; 0 \\\\ A^{n-1}B &amp; A^{n-2}B &amp; \\dots &amp; AB &amp; B \\end{bmatrix}\\begin{bmatrix}u_{0}\\\\ u_{1} \\\\ u_{2} \\\\ \\vdots  \\\\ u_{n-1} \\end{bmatrix} \\\\ + \\begin{bmatrix}I &amp; 0 &amp; \\dots  &amp; &amp; 0 \\\\ A &amp; I &amp; 0 &amp; \\dots &amp; 0  \\\\ A^{2} &amp; A &amp; I &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; &amp; 0 \\\\ A^{n-1} &amp; A^{n-2} &amp; \\dots &amp; A &amp; I \\end{bmatrix}\\begin{bmatrix}w_{0}\\\\ w_{1} \\\\ w_{2} \\\\ \\vdots  \\\\ w_{n-1} \\end{bmatrix} \\tag{6} \\end{align} \\] <p>In this case, the measurements (outputs) become; \\(y_{k}=Cx_{k}\\), so</p> \\[ \\begin{align} \\begin{bmatrix}y_{1}\\\\ y_{2} \\\\ y_{3} \\\\ \\vdots  \\\\ y_{n} \\end{bmatrix} = \\begin{bmatrix}C &amp; 0 &amp; \\dots  &amp; &amp; 0 \\\\ 0 &amp; C &amp; 0 &amp; \\dots &amp; 0  \\\\ 0 &amp; 0 &amp; C &amp; \\dots &amp; 0 \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; 0 \\\\ 0 &amp; \\dots &amp; 0 &amp; 0 &amp; C \\end{bmatrix}\\begin{bmatrix}x_{1}\\\\ x_{2} \\\\ x_{3} \\\\ \\vdots  \\\\ x_{n} \\end{bmatrix} \\tag{7} \\end{align} \\] <p>We can combine equations (6) and (7) into the following form:</p> \\[ \\begin{align} X = Fx_{0} + GU +SW, Y=HX \\tag{8} \\end{align} \\] <p>This form is similar to the original state-space equations (1), but it introduces new matrices: the state transition matrix \\(F\\), control matrix \\(G\\), disturbance matrix \\(W\\), and measurement matrix \\(H\\). In these equations, \\(X\\) represents the predicted states, given by \\(\\begin{bmatrix}x_{1} &amp; x_{2} &amp; \\dots &amp; x_{n} \\end{bmatrix}^{T}\\).</p> <p>Now that \\(G\\), \\(S\\), \\(W\\), and \\(H\\) are known, we can express the output behavior \\(Y\\) for the next \\(n\\) steps as a function of the input \\(U\\). This allows us to calculate the control input \\(U\\) so that \\(Y(U)\\) follows the target trajectory \\(Y_{ref}\\).</p> <p>The next step is to define a cost function. The cost function generally uses the following quadratic form;</p> \\[ \\begin{align} J = (Y - Y_{ref})^{T}Q(Y - Y_{ref}) + (U - U_{ref})^{T}R(U - U_{ref}) \\tag{9} \\end{align} \\] <p>where \\(U_{ref}\\) is the target or steady-state input around which the system is linearized for \\(U\\).</p> <p>This cost function is the same as that of the LQR controller. The first term of \\(J\\) penalizes the deviation from the reference trajectory. The second term penalizes the deviation from the reference (or steady-state) control trajectory. The \\(Q\\) and \\(R\\) are the cost weights Positive and Positive semi-semidefinite matrices.</p> <p>Note: in some cases, \\(U_{ref}=0\\) is used, but this can mean the steering angle should be set to \\(0\\) even if the vehicle is turning a curve. Thus \\(U_{ref}\\) is used for the explanation here. This \\(U_{ref}\\) can be pre-calculated from the curvature of the target trajectory or the steady-state analyses.</p> <p>As the resulting trajectory output is now \\(Y=Y(x_{0}, U)\\), the cost function depends only on U and the initial state conditions which yields the cost \\(J=J(x_{0}, U)\\). Let\u2019s find the \\(U\\) that minimizes this.</p> <p>Substituting equation (8) into equation (9) and tidying up the equation for \\(U\\).</p> \\[ \\begin{align} J(U) &amp;= (H(Fx_{0}+GU+SW)-Y_{ref})^{T}Q(H(Fx_{0}+GU+SW)-Y_{ref})+(U-U_{ref})^{T}R(U-U_{ref}) \\\\ &amp; =U^{T}(G^{T}H^{T}QHG+R)U+2\\left\\{(H(Fx_{0}+SW)-Y_{ref})^{T}QHG-U_{ref}^{T}R\\right\\}U +(\\rm{constant}) \\tag{10} \\end{align} \\] <p>This equation is a quadratic form of \\(U\\) (i.e. \\(U^{T}AU+B^{T}U\\))</p> <p>The coefficient matrix of the quadratic term of \\(U\\), \\(G^{T}C^{T}QCG+R\\) , is positive definite due to the positive and semi-positive definiteness requirement for \\(Q\\) and \\(R\\). Therefore, the cost function is a convex quadratic function in U, which can efficiently be solved by convex optimization.</p>"},{"location":"control/mpc_lateral_controller/model_predictive_control_algorithm/#apply-to-vehicle-path-following-problem-nonlinear-problem","title":"Apply to vehicle path-following problem (nonlinear problem)","text":"<p>Because the path-following problem with a kinematic vehicle model is nonlinear, we cannot directly use the linear MPC methods described in the preceding section. There are several ways to deal with a nonlinearity such as using the nonlinear optimization solver. Here, the linearization is applied to the nonlinear vehicle model along the reference trajectory, and consequently, the nonlinear model is converted into a linear time-varying model.</p> <p>For a nonlinear kinematic vehicle model, the discrete-time update equations are as follows:</p> \\[ \\begin{align} x_{k+1} &amp;= x_{k} + v\\cos\\theta_{k} \\text{d}t \\\\ y_{k+1} &amp;= y_{k} + v\\sin\\theta_{k} \\text{d}t \\\\ \\theta_{k+1} &amp;= \\theta_{k} + \\frac{v\\tan\\delta_{k}}{L} \\text{d}t \\tag{11} \\\\ \\delta_{k+1} &amp;= \\delta_{k} - \\tau^{-1}\\left(\\delta_{k}-\\delta_{des}\\right)\\text{d}t \\end{align} \\] <p></p> <p>The vehicle reference is the center of the rear axle and all states are measured at this point. The states, parameters, and control variables are shown in the following table.</p> Symbol Represent \\(v\\) Vehicle speed measured at the center of rear axle \\(\\theta\\) Yaw (heading angle) in global coordinate system \\(\\delta\\) Vehicle steering angle \\(\\delta_{des}\\) Vehicle target steering angle \\(L\\) Vehicle wheelbase (distance between the rear and front axles) \\(\\tau\\) Time constant for the first order steering dynamics <p>We assume in this example that the MPC only generates the steering control, and the trajectory generator gives the vehicle speed along the trajectory.</p> <p>The kinematic vehicle model discrete update equations contain trigonometric functions; sin and cos, and the vehicle coordinates \\(x\\), \\(y\\), and yaw angles are global coordinates. In path tracking applications, it is common to reformulate the model in error dynamics to convert the control into a regulator problem in which the targets become zero (zero error).</p> <p></p> <p>We make small angle assumptions for the following derivations of linear equations. Given the nonlinear dynamics and omitting the longitudinal coordinate \\(x\\), the resulting set of equations become;</p> \\[ \\begin{align} y_{k+1} &amp;= y_{k} + v\\sin\\theta_{k} \\text{d}t \\\\ \\theta_{k+1} &amp;= \\theta_{k} + \\frac{v\\tan\\delta_{k}}{L} \\text{d}t - \\kappa_{r}v\\cos\\theta_{k}\\text{d}t \\tag{12} \\\\ \\delta_{k+1} &amp;= \\delta_{k} - \\tau^{-1}\\left(\\delta_{k}-\\delta_{des}\\right)\\text{d}t \\end{align} \\] <p>Where \\(\\kappa_{r}\\left(s\\right)\\) is the curvature along the trajectory parametrized by the arc length.</p> <p>There are three expressions in the update equations that are subject to linear approximation: the lateral deviation (or lateral coordinate) \\(y\\), the heading angle (or the heading angle error) \\(\\theta\\), and the steering \\(\\delta\\). We can make a small angle assumption on the heading angle \\(\\theta\\).</p> <p>In the path tracking problem, the curvature of the trajectory \\(\\kappa_{r}\\) is known in advance. At the lower speeds, the Ackermann formula approximates the reference steering angle \\(\\theta_{r}\\)(this value corresponds to the \\(U_{ref}\\) mentioned above). The Ackermann steering expression can be written as;</p> \\[ \\begin{align} \\delta_{r} = \\arctan\\left(L\\kappa_{r}\\right) \\end{align} \\] <p>When the vehicle is turning a path, its steer angle \\(\\delta\\) should be close to the value \\(\\delta_{r}\\). Therefore, \\(\\delta\\) can be expressed,</p> \\[ \\begin{align} \\delta = \\delta_{r} + \\Delta \\delta, \\Delta\\delta \\ll 1 \\end{align} \\] <p>Substituting this equation into equation (12), and approximate \\(\\Delta\\delta\\) to be small.</p> \\[ \\begin{align} \\tan\\delta &amp;\\simeq \\tan\\delta_{r} + \\frac{\\text{d}\\tan\\delta}{\\text{d}\\delta} \\Biggm|_{\\delta=\\delta_{r}}\\Delta\\delta \\\\ &amp;= \\tan \\delta_{r} + \\frac{1}{\\cos^{2}\\delta_{r}}\\Delta\\delta \\\\ &amp;= \\tan \\delta_{r} + \\frac{1}{\\cos^{2}\\delta_{r}}\\left(\\delta-\\delta_{r}\\right) \\\\ &amp;= \\tan \\delta_{r} - \\frac{\\delta_{r}}{\\cos^{2}\\delta_{r}} + \\frac{1}{\\cos^{2}\\delta_{r}}\\delta \\end{align} \\] <p>Using this, \\(\\theta_{k+1}\\) can be expressed</p> \\[ \\begin{align} \\theta_{k+1} &amp;= \\theta_{k} + \\frac{v\\tan\\delta_{k}}{L}\\text{d}t - \\kappa_{r}v\\cos\\delta_{k}\\text{d}t \\\\ &amp;\\simeq \\theta_{k} + \\frac{v}{L}\\text{d}t\\left(\\tan\\delta_{r} - \\frac{\\delta_{r}}{\\cos^{2}\\delta_{r}} + \\frac{1}{\\cos^{2}\\delta_{r}}\\delta_{k} \\right) - \\kappa_{r}v\\text{d}t \\\\ &amp;= \\theta_{k} + \\frac{v}{L}\\text{d}t\\left(L\\kappa_{r} - \\frac{\\delta_{r}}{\\cos^{2}\\delta_{r}} + \\frac{1}{\\cos^{2}\\delta_{r}}\\delta_{k} \\right) - \\kappa_{r}v\\text{d}t \\\\ &amp;= \\theta_{k} + \\frac{v}{L}\\frac{\\text{d}t}{\\cos^{2}\\delta_{r}}\\delta_{k} - \\frac{v}{L}\\frac{\\delta_{r}\\text{d}t}{\\cos^{2}\\delta_{r}} \\end{align} \\] <p>Finally, the linearized time-varying model equation becomes;</p> \\[ \\begin{align} \\begin{bmatrix} y_{k+1} \\\\ \\theta_{k+1} \\\\ \\delta_{k+1} \\end{bmatrix} = \\begin{bmatrix} 1 &amp; v\\text{d}t &amp; 0 \\\\ 0 &amp; 1 &amp; \\frac{v}{L}\\frac{\\text{d}t}{\\cos^{2}\\delta_{r}} \\\\ 0 &amp; 0 &amp; 1 - \\tau^{-1}\\text{d}t \\end{bmatrix} \\begin{bmatrix} y_{k} \\\\ \\theta_{k} \\\\ \\delta_{k} \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\\\ \\tau^{-1}\\text{d}t \\end{bmatrix}\\delta_{des} + \\begin{bmatrix} 0 \\\\ -\\frac{v}{L}\\frac{\\delta_{r}\\text{d}t}{\\cos^{2}\\delta_{r}} \\\\ 0 \\end{bmatrix} \\end{align} \\] <p>This equation has the same form as equation (1) of the linear MPC assumption, but the matrices \\(A\\), \\(B\\), and \\(w\\) change depending on the coordinate transformation. To make this explicit, the entire equation is written as follows</p> \\[ \\begin{align} x_{k+1} = A_{k}x_{k} + B_{k}u_{k}+w_{k} \\end{align} \\] <p>Comparing equation (1), \\(A \\rightarrow A_{k}\\). This means that the \\(A\\) matrix is a linear approximation in the vicinity of the trajectory after \\(k\\) steps (i.e., \\(k* \\text{d}t\\) seconds), and it can be obtained if the trajectory is known in advance.</p> <p>Using this equation, write down the update equation likewise (2) ~ (6)</p> \\[ \\begin{align} \\begin{bmatrix}  x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix} = \\begin{bmatrix}  A_{1} \\\\ A_{1}A_{0} \\\\ A_{2}A_{1}A_{0} \\\\ \\vdots \\\\ \\prod_{i=0}^{n-1} A_{k} \\end{bmatrix} x_{0} + \\begin{bmatrix}  B_{0} &amp; 0 &amp; \\dots &amp; &amp; 0 \\\\ A_{1}B_{0} &amp; B_{1} &amp; 0 &amp; \\dots &amp; 0 \\\\ A_{2}A_{1}B_{0} &amp; A_{2}B_{1} &amp; B_{2} &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp;\\ddots &amp; 0 \\\\ \\prod_{i=1}^{n-1} A_{k}B_{0} &amp; \\prod_{i=2}^{n-1} A_{k}B_{1} &amp; \\dots &amp; A_{n-1}B_{n-1} &amp; B_{n-1} \\end{bmatrix} \\begin{bmatrix}  u_{0} \\\\ u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n-1} \\end{bmatrix} + \\begin{bmatrix} I &amp; 0 &amp; \\dots &amp; &amp; 0 \\\\ A_{1} &amp; I &amp; 0 &amp; \\dots &amp; 0 \\\\ A_{2}A_{1} &amp; A_{2} &amp; I &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp;\\ddots &amp; 0 \\\\ \\prod_{i=1}^{n-1} A_{k} &amp; \\prod_{i=2}^{n-1} A_{k} &amp; \\dots &amp; A_{n-1} &amp; I \\end{bmatrix} \\begin{bmatrix}  w_{0} \\\\ w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{n-1} \\end{bmatrix} \\end{align} \\] <p>As it has the same form as equation (6), convex optimization is applicable for as much as the model in the former section.</p>"},{"location":"control/mpc_lateral_controller/model_predictive_control_algorithm/#the-cost-functions-and-constraints","title":"The cost functions and constraints","text":"<p>In this section, we give the details on how to set up the cost function and constraint conditions.</p>"},{"location":"control/mpc_lateral_controller/model_predictive_control_algorithm/#the-cost-function","title":"The cost function","text":""},{"location":"control/mpc_lateral_controller/model_predictive_control_algorithm/#weight-for-error-and-input","title":"Weight for error and input","text":"<p>MPC states and control weights appear in the cost function in a similar way as LQR (9). In the vehicle path following the problem described above, if C is the unit matrix, the output \\(y = x = \\left[y, \\theta, \\delta\\right]\\). (To avoid confusion with the y-directional deviation, here \\(e\\) is used for the lateral deviation.)</p> <p>As an example, let's determine the weight matrix \\(Q_{1}\\) of the evaluation function for the number of prediction steps \\(n=2\\) system as follows.</p> \\[ \\begin{align} Q_{1} = \\begin{bmatrix} q_{e} &amp; 0 &amp; 0 &amp; 0 &amp; 0&amp; 0 \\\\ 0 &amp; q_{\\theta} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; q_{e} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; q_{\\theta} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\end{align} \\] <p>The first term in the cost function (9) with \\(n=2\\), is shown as follow (\\(Y_{ref}\\) is set to \\(0\\))</p> \\[ \\begin{align} q_{e}\\left(e_{0}^{2} + e_{1}^{2} \\right) + q_{\\theta}\\left(\\theta_{0}^{2} + \\theta_{1}^{2} \\right) \\end{align} \\] <p>This shows that \\(q_{e}\\) is the weight for the lateral error and \\(q\\) is for the angular error. In this example, \\(q_{e}\\) acts as the proportional - P gain and \\(q_{\\theta}\\) as the derivative - D gain for the lateral tracking error. The balance of these factors (including R) will be determined through actual experiments.</p>"},{"location":"control/mpc_lateral_controller/model_predictive_control_algorithm/#weight-for-non-diagonal-term","title":"Weight for non-diagonal term","text":"<p>MPC can handle the non-diagonal term in its calculation (as long as the resulting matrix is positive definite).</p> <p>For instance, write \\(Q_{2}\\) as follows for the \\(n=2\\) system.</p> \\[ \\begin{align} Q_{2} = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; q_{d} &amp; 0 &amp; 0 &amp; -q_{d} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -q_{d} &amp; 0 &amp; 0 &amp; q_{d} \\end{bmatrix} \\end{align} \\] <p>Expanding the first term of the evaluation function using \\(Q_{2}\\)</p> \\[ \\begin{align} q_{d}\\left(\\delta_{0}^{2} -2\\delta_{0}\\delta_{1} + \\delta_{1}^{2} \\right) = q_{d}\\left( \\delta_{0} - \\delta_{1}\\right)^{2} \\end{align} \\] <p>The value of \\(q_{d}\\) is weighted by the amount of change in \\(\\delta\\), which will prevent the tire from moving quickly. By adding this section, the system can evaluate the balance between tracking accuracy and change of steering wheel angle.</p> <p>Since the weight matrix can be added linearly, the final weight can be set as \\(Q = Q_{1} + Q_{2}\\).</p> <p>Furthermore, MPC optimizes over a period of time, the time-varying weight can be considered in the optimization.</p>"},{"location":"control/mpc_lateral_controller/model_predictive_control_algorithm/#constraints","title":"Constraints","text":""},{"location":"control/mpc_lateral_controller/model_predictive_control_algorithm/#input-constraint","title":"Input constraint","text":"<p>The main advantage of MPC controllers is the capability to deal with any state or input constraints. The constraints can be expressed as box constraints, such as \"the tire angle must be within \u00b130 degrees\", and can be put in the following form;</p> \\[ \\begin{align} u_{min} &lt; u &lt; u_{max} \\end{align} \\] <p>The constraints must be linear and convex in the linear MPC applications.</p>"},{"location":"control/mpc_lateral_controller/model_predictive_control_algorithm/#constraints-on-the-derivative-of-the-input","title":"Constraints on the derivative of the input","text":"<p>We can also put constraints on the input deviations. As the derivative of steering angle is \\(\\dot{u}\\), its box constraint is</p> \\[ \\begin{align} \\dot{u}_{min} &lt; \\dot{u} &lt; \\dot{u}_{max} \\end{align} \\] <p>We discretize \\(\\dot{u}\\) as \\(\\left(u_{k} - u_{k-1}\\right)/\\text{d}t\\) and multiply both sides by dt, and the resulting constraint become linear and convex</p> \\[ \\begin{align} \\dot{u}_{min}\\text{d}t &lt; u_{k} - u_{k-1} &lt; \\dot{u}_{max}\\text{d}t \\end{align} \\] <p>Along the prediction or control horizon, i.e for setting \\(n=3\\)</p> \\[ \\begin{align} \\dot{u}_{min}\\text{d}t &lt; u_{1} - u_{0} &lt; \\dot{u}_{max}\\text{d}t \\\\ \\dot{u}_{min}\\text{d}t &lt; u_{2} - u_{1} &lt; \\dot{u}_{max}\\text{d}t \\end{align} \\] <p>and aligning the inequality signs</p> \\[ \\begin{align} u_{1} - u_{0} &amp;&lt; \\dot{u}_{max}\\text{d}t \\\\ + u_{1} + u_{0} &amp;&lt; -\\dot{u}_{min}\\text{d}t \\\\ u_{2} - u_{1} &amp;&lt; \\dot{u}_{max}\\text{d}t \\\\ + u_{2} + u_{1} &amp;&lt; - \\dot{u}_{min}\\text{d}t \\end{align} \\] <p>We can obtain a matrix expression for the resulting constraint equation in the form of</p> \\[ \\begin{align} Ax \\leq b \\end{align} \\] <p>Thus, putting this inequality to fit the form above, the constraints against \\(\\dot{u}\\) can be included at the first-order approximation level.</p> \\[ \\begin{align} \\begin{bmatrix} -1 &amp; 1 &amp; 0 \\\\ 1 &amp; -1 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 \\end{bmatrix}\\begin{bmatrix} u_{0} \\\\ u_{1} \\\\ u_{2} \\end{bmatrix} \\leq \\begin{bmatrix} \\dot{u}_{max}\\text{d}t \\\\ -\\dot{u}_{min}\\text{d}t \\\\ \\dot{u}_{max}\\text{d}t \\\\ -\\dot{u}_{min}\\text{d}t \\end{bmatrix} \\end{align} \\]"},{"location":"control/obstacle_collision_checker/","title":"obstacle_collision_checker","text":""},{"location":"control/obstacle_collision_checker/#obstacle_collision_checker","title":"obstacle_collision_checker","text":""},{"location":"control/obstacle_collision_checker/#purpose","title":"Purpose","text":"<p><code>obstacle_collision_checker</code> is a module to check obstacle collision for predicted trajectory and publish diagnostic errors if collision is found.</p>"},{"location":"control/obstacle_collision_checker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"control/obstacle_collision_checker/#flow-chart","title":"Flow chart","text":""},{"location":"control/obstacle_collision_checker/#algorithms","title":"Algorithms","text":""},{"location":"control/obstacle_collision_checker/#check-data","title":"Check data","text":"<p>Check that <code>obstacle_collision_checker</code> receives no ground pointcloud, predicted_trajectory, reference trajectory, and current velocity data.</p>"},{"location":"control/obstacle_collision_checker/#diagnostic-update","title":"Diagnostic update","text":"<p>If any collision is found on predicted path, this module sets <code>ERROR</code> level as diagnostic status else sets <code>OK</code>.</p>"},{"location":"control/obstacle_collision_checker/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"control/obstacle_collision_checker/#input","title":"Input","text":"Name Type Description <code>~/input/trajectory</code> <code>autoware_auto_planning_msgs::msg::Trajectory</code> Reference trajectory <code>~/input/trajectory</code> <code>autoware_auto_planning_msgs::msg::Trajectory</code> Predicted trajectory <code>/perception/obstacle_segmentation/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Pointcloud of obstacles which the ego-vehicle should stop or avoid <code>/tf</code> <code>tf2_msgs::msg::TFMessage</code> TF <code>/tf_static</code> <code>tf2_msgs::msg::TFMessage</code> TF static"},{"location":"control/obstacle_collision_checker/#output","title":"Output","text":"Name Type Description <code>~/debug/marker</code> <code>visualization_msgs::msg::MarkerArray</code> Marker for visualization"},{"location":"control/obstacle_collision_checker/#parameters","title":"Parameters","text":"Name Type Description Default value <code>delay_time</code> <code>double</code> Delay time of vehicle [s] 0.3 <code>footprint_margin</code> <code>double</code> Foot print margin [m] 0.0 <code>max_deceleration</code> <code>double</code> Max deceleration for ego vehicle to stop [m/s^2] 2.0 <code>resample_interval</code> <code>double</code> Interval for resampling trajectory [m] 0.3 <code>search_radius</code> <code>double</code> Search distance from trajectory to point cloud [m] 5.0"},{"location":"control/obstacle_collision_checker/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>To perform proper collision check, it is necessary to get probably predicted trajectory and obstacle pointclouds without noise.</p>"},{"location":"control/operation_mode_transition_manager/","title":"operation_mode_transition_manager","text":""},{"location":"control/operation_mode_transition_manager/#operation_mode_transition_manager","title":"operation_mode_transition_manager","text":""},{"location":"control/operation_mode_transition_manager/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>This module is responsible for managing the different modes of operation for the Autoware system. The possible modes are:</p> <ul> <li><code>Autonomous</code>: the vehicle is fully controlled by the autonomous driving system</li> <li><code>Local</code>: the vehicle is controlled by a physically connected control system such as a joy stick</li> <li><code>Remote</code>: the vehicle is controlled by a remote controller</li> <li><code>Stop</code>: the vehicle is stopped and there is no active control system.</li> </ul> <p>There is also an <code>In Transition</code> state that occurs during each mode transitions. During this state, the transition to the new operator is not yet complete, and the previous operator is still responsible for controlling the system until the transition is complete. Some actions may be restricted during the <code>In Transition</code> state, such as sudden braking or steering. (This is restricted by the <code>vehicle_cmd_gate</code>).</p>"},{"location":"control/operation_mode_transition_manager/#features","title":"Features","text":"<ul> <li>Transit mode between <code>Autonomous</code>, <code>Local</code>, <code>Remote</code> and <code>Stop</code> based on the indication command.</li> <li>Check whether the each transition is available (safe or not).</li> <li>Limit some sudden motion control in <code>In Transition</code> mode (this is done with <code>vehicle_cmd_gate</code> feature).</li> <li>Check whether the transition is completed.</li> </ul> <ul> <li>Transition between the <code>Autonomous</code>, <code>Local</code>, <code>Remote</code>, and <code>Stop</code> modes based on the indicated command.</li> <li>Determine whether each transition is safe to execute.</li> <li>Restrict certain sudden motion controls during the <code>In Transition</code> mode (using the <code>vehicle_cmd_gate</code> feature).</li> <li>Verify that the transition is complete.</li> </ul>"},{"location":"control/operation_mode_transition_manager/#design","title":"Design","text":"<p>A rough design of the relationship between `operation_mode_transition_manager`` and the other nodes is shown below.</p> <p></p> <p>A more detailed structure is below.</p> <p></p> <p>Here we see that <code>operation_mode_transition_manager</code> has multiple state transitions as follows</p> <ul> <li>AUTOWARE ENABLED &lt;---&gt; DISABLED<ul> <li>ENABLED: the vehicle is controlled by Autoware.</li> <li>DISABLED: the vehicle is out of Autoware control, expecting the e.g. manual driving.</li> </ul> </li> <li>AUTOWARE ENABLED &lt;---&gt; AUTO/LOCAL/REMOTE/NONE<ul> <li>AUTO: the vehicle is controlled by Autoware, with the autonomous control command calculated by the planning/control component.</li> <li>LOCAL: the vehicle is controlled by Autoware, with the locally connected operator, e.g. joystick controller.</li> <li>REMOTE: the vehicle is controlled by Autoware, with the remotely connected operator.</li> <li>NONE: the vehicle is not controlled by any operator.</li> </ul> </li> <li>IN TRANSITION &lt;---&gt; COMPLETED<ul> <li>IN TRANSITION: the mode listed above is in the transition process, expecting the former operator to have a responsibility to confirm the transition is completed.</li> <li>COMPLETED: the mode transition is completed.</li> </ul> </li> </ul>"},{"location":"control/operation_mode_transition_manager/#inputs-outputs-api","title":"Inputs / Outputs / API","text":""},{"location":"control/operation_mode_transition_manager/#inputs","title":"Inputs","text":"<p>For the mode transition:</p> <ul> <li>/system/operation_mode/change_autoware_control [<code>tier4_system_msgs/srv/ChangeAutowareControl</code>]: change operation mode to Autonomous</li> <li>/system/operation_mode/change_operation_mode [<code>tier4_system_msgs/srv/ChangeOperationMode</code>]: change operation mode</li> </ul> <p>For the transition availability/completion check:</p> <ul> <li>/control/command/control_cmd [<code>autoware_auto_control_msgs/msg/AckermannControlCommand</code>]: vehicle control signal</li> <li>/localization/kinematic_state [<code>nav_msgs/msg/Odometry</code>]: ego vehicle state</li> <li>/planning/scenario_planning/trajectory [<code>autoware_auto_planning_msgs/msg/Trajectory</code>]: planning trajectory</li> <li>/vehicle/status/control_mode [<code>autoware_auto_vehicle_msgs/msg/ControlModeReport</code>]: vehicle control mode (autonomous/manual)</li> <li>/control/vehicle_cmd_gate/operation_mode [<code>autoware_adapi_v1_msgs/msg/OperationModeState</code>]: the operation mode in the <code>vehicle_cmd_gate</code>. (To be removed)</li> </ul> <p>For the backward compatibility (to be removed):</p> <ul> <li>/api/autoware/get/engage [<code>autoware_auto_vehicle_msgs/msg/Engage</code>]</li> <li>/control/current_gate_mode [<code>tier4_control_msgs/msg/GateMode</code>]</li> <li>/control/external_cmd_selector/current_selector_mode [<code>tier4_control_msgs/msg/ExternalCommandSelectorMode</code>]</li> </ul>"},{"location":"control/operation_mode_transition_manager/#outputs","title":"Outputs","text":"<ul> <li>/system/operation_mode/state [<code>autoware_adapi_v1_msgs/msg/OperationModeState</code>]: to inform the current operation mode</li> <li>/control/operation_mode_transition_manager/debug_info [<code>operation_mode_transition_manager/msg/OperationModeTransitionManagerDebug</code>]: detailed information about the operation mode transition</li> </ul> <ul> <li>/control/gate_mode_cmd [<code>tier4_control_msgs/msg/GateMode</code>]: to change the <code>vehicle_cmd_gate</code> state to use its features (to be removed)</li> <li>/autoware/engage [<code>autoware_auto_vehicle_msgs/msg/Engage</code>]:</li> </ul> <ul> <li>/control/control_mode_request [<code>autoware_auto_vehicle_msgs/srv/ControlModeCommand</code>]: to change the vehicle control mode (autonomous/manual)</li> <li>/control/external_cmd_selector/select_external_command [<code>tier4_control_msgs/srv/ExternalCommandSelect</code>]:</li> </ul>"},{"location":"control/operation_mode_transition_manager/#parameters","title":"Parameters","text":"Name Type Description Default Range transition_timeout float If the state transition is not completed within this time, it is considered a transition failure. 10.0 \u22650.0 frequency_hz float running hz 10.0 \u22650.0 enable_engage_on_driving boolean Set true if you want to engage the autonomous driving mode while the vehicle is driving. If set to false, it will deny Engage in any situation where the vehicle speed is not zero. Note that if you use this feature without adjusting the parameters, it may cause issues like sudden deceleration. Before using, please ensure the engage condition and the vehicle_cmd_gate transition filter are appropriately adjusted. false N/A check_engage_condition boolean If false, autonomous transition is always available. false N/A nearest_dist_deviation_threshold float distance threshold used to find nearest trajectory point [m] 3.0 \u22650.0 nearest_yaw_deviation_threshold float angle threshold used to find nearest trajectory point [rad] 1.57 \u2265-3.142 engage_acceptable_limits.allow_autonomous_in_stopped boolean If true, autonomous transition is available when the vehicle is stopped even if other checks fail. true N/A engage_acceptable_limits.dist_threshold float The distance between the trajectory and ego vehicle must be within this distance for Autonomous transition. 1.5 \u22650.0 engage_acceptable_limits.yaw_threshold float The yaw angle between trajectory and ego vehicle must be within this threshold for Autonomous transition. 0.524 \u2265-3.142 engage_acceptable_limits.speed_upper_threshold float The velocity deviation between control command and ego vehicle must be within this threshold for Autonomous transition. 10.0 N/A engage_acceptable_limits.speed_lower_threshold float The velocity deviation between control command and ego vehicle must be within this threshold for Autonomous transition. -10.0 N/A engage_acceptable_limits.acc_threshold float The control command acceleration must be less than this threshold for Autonomous transition. 1.5 \u22650.0 engage_acceptable_limits.lateral_acc_threshold float The control command lateral acceleration must be less than this threshold for Autonomous transition. 1.0 \u22650.0 engage_acceptable_limits.lateral_acc_diff_threshold float The lateral acceleration deviation between the control command must be less than this threshold for Autonomous transition. 0.5 \u22650.0 stable_check.duration float The stable condition must be satisfied for this duration to complete the transition. 0.1 \u22650.0 stable_check.dist_threshold float The distance between the trajectory and ego vehicle must be within this distance to complete Autonomous transition. 1.5 \u22650.0 stable_check.speed_upper_threshold float The velocity deviation between control command and ego vehicle must be within this threshold to complete Autonomous transition. 2.0 N/A stable_check.speed_lower_threshold float The velocity deviation between control command and ego vehicle must be within this threshold to complete Autonomous transition. -2.0 N/A stable_check.yaw_threshold float The yaw angle between trajectory and ego vehicle must be within this threshold to complete Autonomous transition. 0,262 \u2265-3.142 Name Type Description Default value <code>transition_timeout</code> <code>double</code> If the state transition is not completed within this time, it is considered a transition failure. 10.0 <code>frequency_hz</code> <code>double</code> running hz 10.0 <code>enable_engage_on_driving</code> <code>bool</code> Set true if you want to engage the autonomous driving mode while the vehicle is driving. If set to false, it will deny Engage in any situation where the vehicle speed is not zero. Note that if you use this feature without adjusting the parameters, it may cause issues like sudden deceleration. Before using, please ensure the engage condition and the vehicle_cmd_gate transition filter are appropriately adjusted. 0.1 <code>check_engage_condition</code> <code>bool</code> If false, autonomous transition is always available 0.1 <code>nearest_dist_deviation_threshold</code> <code>double</code> distance threshold used to find nearest trajectory point 3.0 <code>nearest_yaw_deviation_threshold</code> <code>double</code> angle threshold used to find nearest trajectory point 1.57 <p>For <code>engage_acceptable_limits</code> related parameters:</p> Name Type Description Default value <code>allow_autonomous_in_stopped</code> <code>bool</code> If true, autonomous transition is available when the vehicle is stopped even if other checks fail. true <code>dist_threshold</code> <code>double</code> the distance between the trajectory and ego vehicle must be within this distance for <code>Autonomous</code> transition. 1.5 <code>yaw_threshold</code> <code>double</code> the yaw angle between trajectory and ego vehicle must be within this threshold for <code>Autonomous</code> transition. 0.524 <code>speed_upper_threshold</code> <code>double</code> the velocity deviation between control command and ego vehicle must be within this threshold for <code>Autonomous</code> transition. 10.0 <code>speed_lower_threshold</code> <code>double</code> the velocity deviation between the control command and ego vehicle must be within this threshold for <code>Autonomous</code> transition. -10.0 <code>acc_threshold</code> <code>double</code> the control command acceleration must be less than this threshold for <code>Autonomous</code> transition. 1.5 <code>lateral_acc_threshold</code> <code>double</code> the control command lateral acceleration must be less than this threshold for <code>Autonomous</code> transition. 1.0 <code>lateral_acc_diff_threshold</code> <code>double</code> the lateral acceleration deviation between the control command must be less than this threshold for <code>Autonomous</code> transition. 0.5 <p>For <code>stable_check</code> related parameters:</p> Name Type Description Default value <code>duration</code> <code>double</code> the stable condition must be satisfied for this duration to complete the transition. 0.1 <code>dist_threshold</code> <code>double</code> the distance between the trajectory and ego vehicle must be within this distance to complete <code>Autonomous</code> transition. 1.5 <code>yaw_threshold</code> <code>double</code> the yaw angle between trajectory and ego vehicle must be within this threshold to complete <code>Autonomous</code> transition. 0.262 <code>speed_upper_threshold</code> <code>double</code> the velocity deviation between control command and ego vehicle must be within this threshold to complete <code>Autonomous</code> transition. 2.0 <code>speed_lower_threshold</code> <code>double</code> the velocity deviation between control command and ego vehicle must be within this threshold to complete <code>Autonomous</code> transition. 2.0"},{"location":"control/operation_mode_transition_manager/#engage-check-behavior-on-each-parameter-setting","title":"Engage check behavior on each parameter setting","text":"<p>This matrix describes the scenarios in which the vehicle can be engaged based on the combinations of parameter settings:</p> <code>enable_engage_on_driving</code> <code>check_engage_condition</code> <code>allow_autonomous_in_stopped</code> Scenarios where engage is permitted x x x Only when the vehicle is stationary. x x o Only when the vehicle is stationary. x o x When the vehicle is stationary and all engage conditions are met. x o o Only when the vehicle is stationary. o x x At any time (Caution: Not recommended). o x o At any time (Caution: Not recommended). o o x When all engage conditions are met, regardless of vehicle status. o o o When all engage conditions are met or the vehicle is stationary."},{"location":"control/operation_mode_transition_manager/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":"<ul> <li>Need to remove backward compatibility interfaces.</li> <li>This node should be merged to the <code>vehicle_cmd_gate</code> due to its strong connection.</li> </ul>"},{"location":"control/pid_longitudinal_controller/","title":"PID Longitudinal Controller","text":""},{"location":"control/pid_longitudinal_controller/#pid-longitudinal-controller","title":"PID Longitudinal Controller","text":""},{"location":"control/pid_longitudinal_controller/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>The longitudinal_controller computes the target acceleration to achieve the target velocity set at each point of the target trajectory using a feed-forward/back control.</p> <p>It also contains a slope force correction that takes into account road slope information, and a delay compensation function. It is assumed that the target acceleration calculated here will be properly realized by the vehicle interface.</p> <p>Note that the use of this module is not mandatory for Autoware if the vehicle supports the \"target speed\" interface.</p>"},{"location":"control/pid_longitudinal_controller/#design-inner-workings-algorithms","title":"Design / Inner-workings / Algorithms","text":""},{"location":"control/pid_longitudinal_controller/#states","title":"States","text":"<p>This module has four state transitions as shown below in order to handle special processing in a specific situation.</p> <ul> <li>DRIVE<ul> <li>Executes target velocity tracking by PID control.</li> <li>It also applies the delay compensation and slope compensation.</li> </ul> </li> <li>STOPPING<ul> <li>Controls the motion just before stopping.</li> <li>Special sequence is performed to achieve accurate and smooth stopping.</li> </ul> </li> <li>STOPPED<ul> <li>Performs operations in the stopped state (e.g. brake hold)</li> </ul> </li> <li>EMERGENCY.<ul> <li>Enters an emergency state when certain conditions are met (e.g., when the vehicle has crossed a certain distance of a stop line).</li> <li>The recovery condition (whether or not to keep emergency state until the vehicle completely stops) or the deceleration in the emergency state are defined by parameters.</li> </ul> </li> </ul> <p>The state transition diagram is shown below.</p> <p></p>"},{"location":"control/pid_longitudinal_controller/#logics","title":"Logics","text":""},{"location":"control/pid_longitudinal_controller/#control-block-diagram","title":"Control Block Diagram","text":""},{"location":"control/pid_longitudinal_controller/#feedforward-ff","title":"FeedForward (FF)","text":"<p>The reference acceleration set in the trajectory and slope compensation terms are output as a feedforward. Under ideal conditions with no modeling error, this FF term alone should be sufficient for velocity tracking.</p> <p>Tracking errors causing modeling or discretization errors are removed by the feedback control (now using PID).</p>"},{"location":"control/pid_longitudinal_controller/#brake-keeping","title":"Brake keeping","text":"<p>From the viewpoint of ride comfort, stopping with 0 acceleration is important because it reduces the impact of braking. However, if the target acceleration when stopping is 0, the vehicle may cross over the stop line or accelerate a little in front of the stop line due to vehicle model error or gradient estimation error.</p> <p>For reliable stopping, the target acceleration calculated by the FeedForward system is limited to a negative acceleration when stopping.</p> <p></p>"},{"location":"control/pid_longitudinal_controller/#slope-compensation","title":"Slope compensation","text":"<p>Based on the slope information, a compensation term is added to the target acceleration.</p> <p>There are two sources of the slope information, which can be switched by a parameter.</p> <ul> <li>Pitch of the estimated ego-pose (default)<ul> <li>Calculates the current slope from the pitch angle of the estimated ego-pose</li> <li>Pros: Easily available</li> <li>Cons: Cannot extract accurate slope information due to the influence of vehicle vibration.</li> </ul> </li> <li>Z coordinate on the trajectory<ul> <li>Calculates the road slope from the difference of z-coordinates between the front and rear wheel positions in the target trajectory</li> <li>Pros: More accurate than pitch information, if the z-coordinates of the route are properly maintained</li> <li>Pros: Can be used in combination with delay compensation (not yet implemented)</li> <li>Cons: z-coordinates of high-precision map is needed.</li> <li>Cons: Does not support free space planning (for now)</li> </ul> </li> </ul> <p>Notation: This function works correctly only in a vehicle system that does not have acceleration feedback in the low-level control system.</p> <p>This compensation adds gravity correction to the target acceleration, resulting in an output value that is no longer equal to the target acceleration that the autonomous driving system desires. Therefore, it conflicts with the role of the acceleration feedback in the low-level controller. For instance, if the vehicle is attempting to start with an acceleration of <code>1.0 m/s^2</code> and a gravity correction of <code>-1.0 m/s^2</code> is applied, the output value will be <code>0</code>. If this output value is mistakenly treated as the target acceleration, the vehicle will not start.</p> <p>A suitable example of a vehicle system for the slope compensation function is one in which the output acceleration from the longitudinal_controller is converted into target accel/brake pedal input without any feedbacks. In this case, the output acceleration is just used as a feedforward term to calculate the target pedal, and hence the issue mentioned above does not arise.</p> <p>Note: The angle of the slope is defined as positive for an uphill slope, while the pitch angle of the ego pose is defined as negative when facing upward. They have an opposite definition.</p> <p></p>"},{"location":"control/pid_longitudinal_controller/#pid-control","title":"PID control","text":"<p>For deviations that cannot be handled by FeedForward control, such as model errors, PID control is used to construct a feedback system.</p> <p>This PID control calculates the target acceleration from the deviation between the current ego-velocity and the target velocity.</p> <p>This PID logic has a maximum value for the output of each term. This is to prevent the following:</p> <ul> <li>Large integral terms may cause unintended behavior by users.</li> <li>Unintended noise may cause the output of the derivative term to be very large.</li> </ul> <p>Note: by default, the integral term in the control system is not accumulated when the vehicle is stationary. This precautionary measure aims to prevent unintended accumulation of the integral term in scenarios where Autoware assumes the vehicle is engaged, but an external system has immobilized the vehicle to initiate startup procedures.</p> <p>However, certain situations may arise, such as when the vehicle encounters a depression in the road surface during startup or if the slope compensation is inaccurately estimated (lower than necessary), leading to a failure to initiate motion. To address these scenarios, it is possible to activate error integration even when the vehicle is at rest by setting the <code>enable_integration_at_low_speed</code> parameter to true.</p> <p>When <code>enable_integration_at_low_speed</code> is set to true, the PID controller will initiate integration of the acceleration error after a specified duration defined by the <code>time_threshold_before_pid_integration</code> parameter has elapsed without the vehicle surpassing a minimum velocity set by the <code>current_vel_threshold_pid_integration</code> parameter.</p> <p>The presence of the <code>time_threshold_before_pid_integration</code> parameter is important for practical PID tuning. Integrating the error when the vehicle is stationary or at low speed can complicate PID tuning. This parameter effectively introduces a delay before the integral part becomes active, preventing it from kicking in immediately. This delay allows for more controlled and effective tuning of the PID controller.</p> <p>At present, PID control is implemented from the viewpoint of trade-off between development/maintenance cost and performance. This may be replaced by a higher performance controller (adaptive control or robust control) in future development.</p>"},{"location":"control/pid_longitudinal_controller/#time-delay-compensation","title":"Time delay compensation","text":"<p>At high speeds, the delay of actuator systems such as gas pedals and brakes has a significant impact on driving accuracy. Depending on the actuating principle of the vehicle, the mechanism that physically controls the gas pedal and brake typically has a delay of about a hundred millisecond.</p> <p>In this controller, the predicted ego-velocity and the target velocity after the delay time are calculated and used for the feedback to address the time delay problem.</p>"},{"location":"control/pid_longitudinal_controller/#slope-compensation_1","title":"Slope compensation","text":"<p>Based on the slope information, a compensation term is added to the target acceleration.</p> <p>There are two sources of the slope information, which can be switched by a parameter.</p> <ul> <li>Pitch of the estimated ego-pose (default)<ul> <li>Calculates the current slope from the pitch angle of the estimated ego-pose</li> <li>Pros: Easily available</li> <li>Cons: Cannot extract accurate slope information due to the influence of vehicle vibration.</li> </ul> </li> <li>Z coordinate on the trajectory<ul> <li>Calculates the road slope from the difference of z-coordinates between the front and rear wheel positions in the target trajectory</li> <li>Pros: More accurate than pitch information, if the z-coordinates of the route are properly maintained</li> <li>Pros: Can be used in combination with delay compensation (not yet implemented)</li> <li>Cons: z-coordinates of high-precision map is needed.</li> <li>Cons: Does not support free space planning (for now)</li> </ul> </li> </ul>"},{"location":"control/pid_longitudinal_controller/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ol> <li>Smoothed target velocity and its acceleration shall be set in the trajectory<ol> <li>The velocity command is not smoothed inside the controller (only noise may be removed).</li> <li>For step-like target signal, tracking is performed as fast as possible.</li> </ol> </li> <li>The vehicle velocity must be an appropriate value<ol> <li>The ego-velocity must be a signed-value corresponding to the forward/backward direction</li> <li>The ego-velocity should be given with appropriate noise processing.</li> <li>If there is a large amount of noise in the ego-velocity, the tracking performance will be significantly reduced.</li> </ol> </li> <li>The output of this controller must be achieved by later modules (e.g. vehicle interface).<ol> <li>If the vehicle interface does not have the target velocity or acceleration interface (e.g., the vehicle only has a gas pedal and brake interface), an appropriate conversion must be done after this controller.</li> </ol> </li> </ol>"},{"location":"control/pid_longitudinal_controller/#inputs-outputs-api","title":"Inputs / Outputs / API","text":""},{"location":"control/pid_longitudinal_controller/#input","title":"Input","text":"<p>Set the following from the controller_node</p> <ul> <li><code>autoware_auto_planning_msgs/Trajectory</code> : reference trajectory to follow.</li> <li><code>nav_msgs/Odometry</code>: current odometry</li> </ul>"},{"location":"control/pid_longitudinal_controller/#output","title":"Output","text":"<p>Return LongitudinalOutput which contains the following to the controller node</p> <ul> <li><code>autoware_auto_control_msgs/LongitudinalCommand</code>: command to control the longitudinal motion of the vehicle. It contains the target velocity and target acceleration.</li> <li>LongitudinalSyncData<ul> <li>velocity convergence(currently not used)</li> </ul> </li> </ul>"},{"location":"control/pid_longitudinal_controller/#pidcontroller-class","title":"PIDController class","text":"<p>The <code>PIDController</code> class is straightforward to use. First, gains and limits must be set (using <code>setGains()</code> and <code>setLimits()</code>) for the proportional (P), integral (I), and derivative (D) components. Then, the velocity can be calculated by providing the current error and time step duration to the <code>calculate()</code> function.</p>"},{"location":"control/pid_longitudinal_controller/#parameter-description","title":"Parameter description","text":"<p>The default parameters defined in <code>param/lateral_controller_defaults.param.yaml</code> are adjusted to the AutonomouStuff Lexus RX 450h for under 40 km/h driving.</p> Name Type Description Default value delay_compensation_time double delay for longitudinal control [s] 0.17 enable_smooth_stop bool flag to enable transition to STOPPING true enable_overshoot_emergency bool flag to enable transition to EMERGENCY when the ego is over the stop line with a certain distance. See <code>emergency_state_overshoot_stop_dist</code>. true enable_large_tracking_error_emergency bool flag to enable transition to EMERGENCY when the closest trajectory point search is failed due to a large deviation between trajectory and ego pose. true enable_slope_compensation bool flag to modify output acceleration for slope compensation. The source of the slope angle can be selected from ego-pose or trajectory angle. See <code>use_trajectory_for_pitch_calculation</code>. true enable_brake_keeping_before_stop bool flag to keep a certain acceleration during DRIVE state before the ego stops. See Brake keeping. false enable_keep_stopped_until_steer_convergence bool flag to keep stopped condition until until the steer converges. true max_acc double max value of output acceleration [m/s^2] 3.0 min_acc double min value of output acceleration [m/s^2] -5.0 max_jerk double max value of jerk of output acceleration [m/s^3] 2.0 min_jerk double min value of jerk of output acceleration [m/s^3] -5.0 use_trajectory_for_pitch_calculation bool If true, the slope is estimated from trajectory z-level. Otherwise the pitch angle of the ego pose is used. false lpf_pitch_gain double gain of low-pass filter for pitch estimation 0.95 max_pitch_rad double max value of estimated pitch [rad] 0.1 min_pitch_rad double min value of estimated pitch [rad] -0.1"},{"location":"control/pid_longitudinal_controller/#state-transition","title":"State transition","text":"Name Type Description Default value drive_state_stop_dist double The state will transit to DRIVE when the distance to the stop point is longer than <code>drive_state_stop_dist</code> + <code>drive_state_offset_stop_dist</code> [m] 0.5 drive_state_offset_stop_dist double The state will transit to DRIVE when the distance to the stop point is longer than <code>drive_state_stop_dist</code> + <code>drive_state_offset_stop_dist</code> [m] 1.0 stopping_state_stop_dist double The state will transit to STOPPING when the distance to the stop point is shorter than <code>stopping_state_stop_dist</code> [m] 0.5 stopped_state_entry_vel double threshold of the ego velocity in transition to the STOPPED state [m/s] 0.01 stopped_state_entry_acc double threshold of the ego acceleration in transition to the STOPPED state [m/s^2] 0.1 emergency_state_overshoot_stop_dist double If <code>enable_overshoot_emergency</code> is true and the ego is <code>emergency_state_overshoot_stop_dist</code>-meter ahead of the stop point, the state will transit to EMERGENCY. [m] 1.5 emergency_state_traj_trans_dev double If the ego's position is <code>emergency_state_traj_tran_dev</code> meter away from the nearest trajectory point, the state will transit to EMERGENCY. [m] 3.0 emergency_state_traj_rot_dev double If the ego's orientation is <code>emergency_state_traj_rot_dev</code> rad away from the nearest trajectory point orientation, the state will transit to EMERGENCY. [rad] 0.784"},{"location":"control/pid_longitudinal_controller/#drive-parameter","title":"DRIVE Parameter","text":"Name Type Description Default value kp double p gain for longitudinal control 1.0 ki double i gain for longitudinal control 0.1 kd double d gain for longitudinal control 0.0 max_out double max value of PID's output acceleration during DRIVE state [m/s^2] 1.0 min_out double min value of PID's output acceleration during DRIVE state [m/s^2] -1.0 max_p_effort double max value of acceleration with p gain 1.0 min_p_effort double min value of acceleration with p gain -1.0 max_i_effort double max value of acceleration with i gain 0.3 min_i_effort double min value of acceleration with i gain -0.3 max_d_effort double max value of acceleration with d gain 0.0 min_d_effort double min value of acceleration with d gain 0.0 lpf_vel_error_gain double gain of low-pass filter for velocity error 0.9 enable_integration_at_low_speed bool Whether to enable integration of acceleration errors when the vehicle speed is lower than <code>current_vel_threshold_pid_integration</code> or not. current_vel_threshold_pid_integration double Velocity error is integrated for I-term only when the absolute value of current velocity is larger than this parameter. [m/s] time_threshold_before_pid_integration double How much time without the vehicle moving must past to enable PID error integration. [s] 5.0 brake_keeping_acc double If <code>enable_brake_keeping_before_stop</code> is true, a certain acceleration is kept during DRIVE state before the ego stops [m/s^2] See Brake keeping. 0.2"},{"location":"control/pid_longitudinal_controller/#stopping-parameter-smooth-stop","title":"STOPPING Parameter (smooth stop)","text":"<p>Smooth stop is enabled if <code>enable_smooth_stop</code> is true. In smooth stop, strong acceleration (<code>strong_acc</code>) will be output first to decrease the ego velocity. Then weak acceleration (<code>weak_acc</code>) will be output to stop smoothly by decreasing the ego jerk. If the ego does not stop in a certain time or some-meter over the stop point, weak acceleration to stop right (<code>weak_stop_acc</code>) now will be output. If the ego is still running, strong acceleration (<code>strong_stop_acc</code>) to stop right now will be output.</p> Name Type Description Default value smooth_stop_max_strong_acc double max strong acceleration [m/s^2] -0.5 smooth_stop_min_strong_acc double min strong acceleration [m/s^2] -0.8 smooth_stop_weak_acc double weak acceleration [m/s^2] -0.3 smooth_stop_weak_stop_acc double weak acceleration to stop right now [m/s^2] -0.8 smooth_stop_strong_stop_acc double strong acceleration to be output when the ego is <code>smooth_stop_strong_stop_dist</code>-meter over the stop point. [m/s^2] -3.4 smooth_stop_max_fast_vel double max fast vel to judge the ego is running fast [m/s]. If the ego is running fast, strong acceleration will be output. 0.5 smooth_stop_min_running_vel double min ego velocity to judge if the ego is running or not [m/s] 0.01 smooth_stop_min_running_acc double min ego acceleration to judge if the ego is running or not [m/s^2] 0.01 smooth_stop_weak_stop_time double max time to output weak acceleration [s]. After this, strong acceleration will be output. 0.8 smooth_stop_weak_stop_dist double Weak acceleration will be output when the ego is <code>smooth_stop_weak_stop_dist</code>-meter before the stop point. [m] -0.3 smooth_stop_strong_stop_dist double Strong acceleration will be output when the ego is <code>smooth_stop_strong_stop_dist</code>-meter over the stop point. [m] -0.5"},{"location":"control/pid_longitudinal_controller/#stopped-parameter","title":"STOPPED Parameter","text":"Name Type Description Default value stopped_vel double target velocity in STOPPED state [m/s] 0.0 stopped_acc double target acceleration in STOPPED state [m/s^2] -3.4 stopped_jerk double target jerk in STOPPED state [m/s^3] -5.0"},{"location":"control/pid_longitudinal_controller/#emergency-parameter","title":"EMERGENCY Parameter","text":"Name Type Description Default value emergency_vel double target velocity in EMERGENCY state [m/s] 0.0 emergency_acc double target acceleration in an EMERGENCY state [m/s^2] -5.0 emergency_jerk double target jerk in an EMERGENCY state [m/s^3] -3.0"},{"location":"control/pid_longitudinal_controller/#references-external-links","title":"References / External links","text":""},{"location":"control/pid_longitudinal_controller/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"control/pid_longitudinal_controller/#related-issues","title":"Related issues","text":""},{"location":"control/predicted_path_checker/","title":"Predicted Path Checker","text":""},{"location":"control/predicted_path_checker/#predicted-path-checker","title":"Predicted Path Checker","text":""},{"location":"control/predicted_path_checker/#purpose","title":"Purpose","text":"<p>The Predicted Path Checker package is designed for autonomous vehicles to check the predicted path generated by control modules. It handles potential collisions that the planning module might not be able to handle and that in the brake distance. In case of collision in brake distance, the package will send a diagnostic message labeled \"ERROR\" to alert the system to send emergency and in the case of collisions in outside reference trajectory, it sends pause request to pause interface to make the vehicle stop.</p> <p></p>"},{"location":"control/predicted_path_checker/#algorithm","title":"Algorithm","text":"<p>The package algorithm evaluates the predicted trajectory against the reference trajectory and the predicted objects in the environment. It checks for potential collisions and, if necessary, generates an appropriate response to avoid them ( emergency or pause request).</p>"},{"location":"control/predicted_path_checker/#inner-algorithm","title":"Inner Algorithm","text":"<p>cutTrajectory() -&gt; It cuts the predicted trajectory with input length. Length is calculated by multiplying the velocity of ego vehicle with \"trajectory_check_time\" parameter and \"min_trajectory_length\".</p> <p>filterObstacles() -&gt; It filters the predicted objects in the environment. It filters the objects which are not in front of the vehicle and far away from predicted trajectory.</p> <p>checkTrajectoryForCollision() -&gt; It checks the predicted trajectory for collision with the predicted objects. It calculates both polygon of trajectory points and predicted objects and checks intersection of both polygons. If there is an intersection, it calculates the nearest collision point. It returns the nearest collision point of polygon and the predicted object. It also checks predicted objects history which are intersect with the footprint before to avoid unexpected behaviors. Predicted objects history stores the objects if it was detected below the \"chattering_threshold\" seconds ago.</p> <p>If the \"enable_z_axis_obstacle_filtering\" parameter is set to true, it filters the predicted objects in the Z-axis by using \"z_axis_filtering_buffer\". If the object does not intersect with the Z-axis, it is filtered out.</p> <p></p> <p>calculateProjectedVelAndAcc() -&gt; It calculates the projected velocity and acceleration of the predicted object on predicted trajectory's collision point's axes.</p> <p>isInBrakeDistance() -&gt; It checks if the stop point is in brake distance. It gets relative velocity and acceleration of ego vehicle with respect to the predicted object. It calculates the brake distance, if the point in brake distance, it returns true.</p> <p>isItDiscretePoint() -&gt; It checks if the stop point on predicted trajectory is discrete point or not. If it is not discrete point, planning should handle the stop.</p> <p>isThereStopPointOnRefTrajectory() -&gt; It checks if there is a stop point on reference trajectory. If there is a stop point before the stop index, it returns true. Otherwise, it returns false, and node is going to call pause interface to make the vehicle stop.</p>"},{"location":"control/predicted_path_checker/#inputs","title":"Inputs","text":"Name Type Description <code>~/input/reference_trajectory</code> <code>autoware_auto_planning_msgs::msg::Trajectory</code> Reference trajectory <code>~/input/predicted_trajectory</code> <code>autoware_auto_planning_msgs::msg::Trajectory</code> Predicted trajectory <code>~/input/objects</code> <code>autoware_auto_perception_msgs::msg::PredictedObject</code> Dynamic objects in the environment <code>~/input/odometry</code> <code>nav_msgs::msg::Odometry</code> Odometry message of vehicle to get current velocity <code>~/input/current_accel</code> <code>geometry_msgs::msg::AccelWithCovarianceStamped</code> Current acceleration <code>/control/vehicle_cmd_gate/is_paused</code> <code>tier4_control_msgs::msg::IsPaused</code> Current pause state of the vehicle"},{"location":"control/predicted_path_checker/#outputs","title":"Outputs","text":"Name Type Description <code>~/debug/marker</code> <code>visualization_msgs::msg::MarkerArray</code> Marker for visualization <code>~/debug/virtual_wall</code> <code>visualization_msgs::msg::MarkerArray</code> Virtual wall marker for visualization <code>/control/vehicle_cmd_gate/set_pause</code> <code>tier4_control_msgs::srv::SetPause</code> Pause service to make the vehicle stop <code>/diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticStatus</code> Diagnostic status of vehicle"},{"location":"control/predicted_path_checker/#parameters","title":"Parameters","text":""},{"location":"control/predicted_path_checker/#node-parameters","title":"Node Parameters","text":"Name Type Description Default value <code>update_rate</code> <code>double</code> The update rate [Hz] 10.0 <code>delay_time</code> <code>double</code> he time delay considered for the emergency response [s] 0.17 <code>max_deceleration</code> <code>double</code> Max deceleration for ego vehicle to stop [m/s^2] 1.5 <code>resample_interval</code> <code>double</code> Interval for resampling trajectory [m] 0.5 <code>stop_margin</code> <code>double</code> The stopping margin [m] 0.5 <code>ego_nearest_dist_threshold</code> <code>double</code> The nearest distance threshold for ego vehicle [m] 3.0 <code>ego_nearest_yaw_threshold</code> <code>double</code> The nearest yaw threshold for ego vehicle [rad] 1.046 <code>min_trajectory_check_length</code> <code>double</code> The minimum trajectory check length in meters [m] 1.5 <code>trajectory_check_time</code> <code>double</code> The trajectory check time in seconds. [s] 3.0 <code>distinct_point_distance_threshold</code> <code>double</code> The distinct point distance threshold [m] 0.3 <code>distinct_point_yaw_threshold</code> <code>double</code> The distinct point yaw threshold [deg] 5.0 <code>filtering_distance_threshold</code> <code>double</code> It ignores the objects if distance is higher than this [m] 1.5 <code>use_object_prediction</code> <code>bool</code> If true, node predicts current pose of the objects wrt delta time [-] true"},{"location":"control/predicted_path_checker/#collision-checker-parameters","title":"Collision Checker Parameters","text":"Name Type Description Default value <code>width_margin</code> <code>double</code> The width margin for collision checking [Hz] 0.2 <code>chattering_threshold</code> <code>double</code> The chattering threshold for collision detection [s] 0.2 <code>z_axis_filtering_buffer</code> <code>double</code> The Z-axis filtering buffer [m] 0.3 <code>enable_z_axis_obstacle_filtering</code> <code>bool</code> A boolean flag indicating if Z-axis obstacle filtering is enabled false"},{"location":"control/pure_pursuit/","title":"Pure Pursuit Controller","text":""},{"location":"control/pure_pursuit/#pure-pursuit-controller","title":"Pure Pursuit Controller","text":"<p>The Pure Pursuit Controller module calculates the steering angle for tracking a desired trajectory using the pure pursuit algorithm. This is used as a lateral controller plugin in the <code>trajectory_follower_node</code>.</p>"},{"location":"control/pure_pursuit/#inputs","title":"Inputs","text":"<p>Set the following from the controller_node</p> <ul> <li><code>autoware_auto_planning_msgs/Trajectory</code> : reference trajectory to follow.</li> <li><code>nav_msgs/Odometry</code>: current ego pose and velocity information</li> </ul>"},{"location":"control/pure_pursuit/#outputs","title":"Outputs","text":"<p>Return LateralOutput which contains the following to the controller node</p> <ul> <li><code>autoware_auto_control_msgs/AckermannLateralCommand</code>: target steering angle</li> <li>LateralSyncData<ul> <li>steer angle convergence</li> </ul> </li> <li><code>autoware_auto_planning_msgs/Trajectory</code>: predicted path for ego vehicle</li> </ul>"},{"location":"control/shift_decider/","title":"Shift Decider","text":""},{"location":"control/shift_decider/#shift-decider","title":"Shift Decider","text":""},{"location":"control/shift_decider/#purpose","title":"Purpose","text":"<p><code>shift_decider</code> is a module to decide shift from ackermann control command.</p>"},{"location":"control/shift_decider/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"control/shift_decider/#flow-chart","title":"Flow chart","text":""},{"location":"control/shift_decider/#algorithms","title":"Algorithms","text":""},{"location":"control/shift_decider/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"control/shift_decider/#input","title":"Input","text":"Name Type Description <code>~/input/control_cmd</code> <code>autoware_auto_control_msgs::msg::AckermannControlCommand</code> Control command for vehicle."},{"location":"control/shift_decider/#output","title":"Output","text":"Name Type Description <code>~output/gear_cmd</code> <code>autoware_auto_vehicle_msgs::msg::GearCommand</code> Gear for drive forward / backward."},{"location":"control/shift_decider/#parameters","title":"Parameters","text":"<p>none.</p>"},{"location":"control/shift_decider/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"control/trajectory_follower_base/","title":"Trajectory Follower","text":""},{"location":"control/trajectory_follower_base/#trajectory-follower","title":"Trajectory Follower","text":"<p>This is the design document for the <code>trajectory_follower</code> package.</p>"},{"location":"control/trajectory_follower_base/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>This package provides the interface of longitudinal and lateral controllers used by the node of the <code>trajectory_follower_node</code> package. We can implement a detailed controller by deriving the longitudinal and lateral base interfaces.</p>"},{"location":"control/trajectory_follower_base/#design","title":"Design","text":"<p>There are lateral and longitudinal base interface classes and each algorithm inherits from this class to implement. The interface class has the following base functions.</p> <ul> <li><code>isReady()</code>: Check if the control is ready to compute.</li> <li><code>run()</code>: Compute control commands and return to Trajectory Follower Nodes. This must be implemented by inherited algorithms.</li> <li><code>sync()</code>: Input the result of running the other controller.<ul> <li>steer angle convergence<ul> <li>allow keeping stopped until steer is converged.</li> </ul> </li> <li>velocity convergence(currently not used)</li> </ul> </li> </ul> <p>See the Design of Trajectory Follower Nodes for how these functions work in the node.</p>"},{"location":"control/trajectory_follower_base/#separated-lateral-steering-and-longitudinal-velocity-controls","title":"Separated lateral (steering) and longitudinal (velocity) controls","text":"<p>This longitudinal controller assumes that the roles of lateral and longitudinal control are separated as follows.</p> <ul> <li>Lateral control computes a target steering to keep the vehicle on the trajectory, assuming perfect velocity tracking.</li> <li>Longitudinal control computes a target velocity/acceleration to keep the vehicle velocity on the trajectory speed, assuming perfect trajectory tracking.</li> </ul> <p>Ideally, dealing with the lateral and longitudinal control as a single mixed problem can achieve high performance. In contrast, there are two reasons to provide velocity controller as a stand-alone function, described below.</p>"},{"location":"control/trajectory_follower_base/#complex-requirements-for-longitudinal-motion","title":"Complex requirements for longitudinal motion","text":"<p>The longitudinal vehicle behavior that humans expect is difficult to express in a single logic. For example, the expected behavior just before stopping differs depending on whether the ego-position is ahead/behind of the stop line, or whether the current speed is higher/lower than the target speed to achieve a human-like movement.</p> <p>In addition, some vehicles have difficulty measuring the ego-speed at extremely low speeds. In such cases, a configuration that can improve the functionality of the longitudinal control without affecting the lateral control is important.</p> <p>There are many characteristics and needs that are unique to longitudinal control. Designing them separately from the lateral control keeps the modules less coupled and improves maintainability.</p>"},{"location":"control/trajectory_follower_base/#nonlinear-coupling-of-lateral-and-longitudinal-motion","title":"Nonlinear coupling of lateral and longitudinal motion","text":"<p>The lat-lon mixed control problem is very complex and uses nonlinear optimization to achieve high performance. Since it is difficult to guarantee the convergence of the nonlinear optimization, a simple control logic is also necessary for development.</p> <p>Also, the benefits of simultaneous longitudinal and lateral control are small if the vehicle doesn't move at high speed.</p>"},{"location":"control/trajectory_follower_base/#related-issues","title":"Related issues","text":""},{"location":"control/trajectory_follower_node/","title":"Trajectory Follower Nodes","text":""},{"location":"control/trajectory_follower_node/#trajectory-follower-nodes","title":"Trajectory Follower Nodes","text":""},{"location":"control/trajectory_follower_node/#purpose","title":"Purpose","text":"<p>Generate control commands to follow a given Trajectory.</p>"},{"location":"control/trajectory_follower_node/#design","title":"Design","text":"<p>This is a node of the functionalities implemented in the controller class derived from trajectory_follower_base package. It has instances of those functionalities, gives them input data to perform calculations, and publishes control commands.</p> <p>By default, the controller instance with the <code>Controller</code> class as follows is used.</p> <p></p> <p>The process flow of <code>Controller</code> class is as follows.</p> <pre><code>// 1. create input data\nconst auto input_data = createInputData(*get_clock());\nif (!input_data) {\nreturn;\n}\n\n// 2. check if controllers are ready\nconst bool is_lat_ready = lateral_controller_-&gt;isReady(*input_data);\nconst bool is_lon_ready = longitudinal_controller_-&gt;isReady(*input_data);\nif (!is_lat_ready || !is_lon_ready) {\nreturn;\n}\n\n// 3. run controllers\nconst auto lat_out = lateral_controller_-&gt;run(*input_data);\nconst auto lon_out = longitudinal_controller_-&gt;run(*input_data);\n\n// 4. sync with each other controllers\nlongitudinal_controller_-&gt;sync(lat_out.sync_data);\nlateral_controller_-&gt;sync(lon_out.sync_data);\n\n// 5. publish control command\ncontrol_cmd_pub_-&gt;publish(out);\n</code></pre> <p>Giving the longitudinal controller information about steer convergence allows it to control steer when stopped if following parameters are <code>true</code></p> <ul> <li>lateral controller<ul> <li><code>keep_steer_control_until_converged</code></li> </ul> </li> <li>longitudinal controller<ul> <li><code>enable_keep_stopped_until_steer_convergence</code></li> </ul> </li> </ul>"},{"location":"control/trajectory_follower_node/#inputs-outputs-api","title":"Inputs / Outputs / API","text":""},{"location":"control/trajectory_follower_node/#inputs","title":"Inputs","text":"<ul> <li><code>autoware_auto_planning_msgs/Trajectory</code> : reference trajectory to follow.</li> <li><code>nav_msgs/Odometry</code>: current odometry</li> <li><code>autoware_auto_vehicle_msgs/SteeringReport</code> current steering</li> </ul>"},{"location":"control/trajectory_follower_node/#outputs","title":"Outputs","text":"<ul> <li><code>autoware_auto_control_msgs/AckermannControlCommand</code>: message containing both lateral and longitudinal commands.</li> </ul>"},{"location":"control/trajectory_follower_node/#parameter","title":"Parameter","text":"<ul> <li><code>ctrl_period</code>: control commands publishing period</li> <li><code>timeout_thr_sec</code>: duration in second after which input messages are discarded.<ul> <li>Each time the node receives lateral and longitudinal commands from each controller, it publishes an <code>AckermannControlCommand</code> if the following two conditions are met.<ol> <li>Both commands have been received.</li> <li>The last received commands are not older than defined by <code>timeout_thr_sec</code>.</li> </ol> </li> </ul> </li> <li><code>lateral_controller_mode</code>: <code>mpc</code> or <code>pure_pursuit</code><ul> <li>(currently there is only <code>PID</code> for longitudinal controller)</li> </ul> </li> </ul>"},{"location":"control/trajectory_follower_node/#debugging","title":"Debugging","text":"<p>Debug information are published by the lateral and longitudinal controller using <code>tier4_debug_msgs/Float32MultiArrayStamped</code> messages.</p> <p>A configuration file for PlotJuggler is provided in the <code>config</code> folder which, when loaded, allow to automatically subscribe and visualize information useful for debugging.</p> <p>In addition, the predicted MPC trajectory is published on topic <code>output/lateral/predicted_trajectory</code> and can be visualized in Rviz.</p>"},{"location":"control/trajectory_follower_node/design/simple_trajectory_follower-design/","title":"Simple Trajectory Follower","text":""},{"location":"control/trajectory_follower_node/design/simple_trajectory_follower-design/#simple-trajectory-follower","title":"Simple Trajectory Follower","text":""},{"location":"control/trajectory_follower_node/design/simple_trajectory_follower-design/#purpose","title":"Purpose","text":"<p>Provide a base trajectory follower code that is simple and flexible to use. This node calculates control command based on a reference trajectory and an ego vehicle kinematics.</p>"},{"location":"control/trajectory_follower_node/design/simple_trajectory_follower-design/#design","title":"Design","text":""},{"location":"control/trajectory_follower_node/design/simple_trajectory_follower-design/#inputs-outputs","title":"Inputs / Outputs","text":"<p>Inputs</p> <ul> <li><code>input/reference_trajectory</code> [autoware_auto_planning_msgs::msg::Trajectory] : reference trajectory to follow.</li> <li><code>input/current_kinematic_state</code> [nav_msgs::msg::Odometry] : current state of the vehicle (position, velocity, etc).</li> <li>Output</li> <li><code>output/control_cmd</code> [autoware_auto_control_msgs::msg::AckermannControlCommand] : generated control command.</li> </ul>"},{"location":"control/trajectory_follower_node/design/simple_trajectory_follower-design/#parameters","title":"Parameters","text":"Name Type Description Default value use_external_target_vel bool use external target velocity defined by parameter when true, else follow the velocity on target trajectory points. false external_target_vel float target velocity used when <code>use_external_target_vel</code> is true. 0.0 lateral_deviation float target lateral deviation when following. 0.0"},{"location":"control/vehicle_cmd_gate/","title":"vehicle_cmd_gate","text":""},{"location":"control/vehicle_cmd_gate/#vehicle_cmd_gate","title":"vehicle_cmd_gate","text":""},{"location":"control/vehicle_cmd_gate/#purpose","title":"Purpose","text":"<p><code>vehicle_cmd_gate</code> is the package to get information from emergency handler, planning module, external controller, and send a msg to vehicle.</p>"},{"location":"control/vehicle_cmd_gate/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"control/vehicle_cmd_gate/#input","title":"Input","text":"Name Type Description <code>~/input/steering</code> <code>autoware_auto_vehicle_msgs::msg::SteeringReport</code> steering status <code>~/input/auto/control_cmd</code> <code>autoware_auto_control_msgs::msg::AckermannControlCommand</code> command for lateral and longitudinal velocity from planning module <code>~/input/auto/turn_indicators_cmd</code> <code>autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand</code> turn indicators command from planning module <code>~/input/auto/hazard_lights_cmd</code> <code>autoware_auto_vehicle_msgs::msg::HazardLightsCommand</code> hazard lights command from planning module <code>~/input/auto/gear_cmd</code> <code>autoware_auto_vehicle_msgs::msg::GearCommand</code> gear command from planning module <code>~/input/external/control_cmd</code> <code>autoware_auto_control_msgs::msg::AckermannControlCommand</code> command for lateral and longitudinal velocity from external <code>~/input/external/turn_indicators_cmd</code> <code>autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand</code> turn indicators command from external <code>~/input/external/hazard_lights_cmd</code> <code>autoware_auto_vehicle_msgs::msg::HazardLightsCommand</code> hazard lights command from external <code>~/input/external/gear_cmd</code> <code>autoware_auto_vehicle_msgs::msg::GearCommand</code> gear command from external <code>~/input/external_emergency_stop_heartbeat</code> <code>tier4_external_api_msgs::msg::Heartbeat</code> heartbeat <code>~/input/gate_mode</code> <code>tier4_control_msgs::msg::GateMode</code> gate mode (AUTO or EXTERNAL) <code>~/input/emergency/control_cmd</code> <code>autoware_auto_control_msgs::msg::AckermannControlCommand</code> command for lateral and longitudinal velocity from emergency handler <code>~/input/emergency/hazard_lights_cmd</code> <code>autoware_auto_vehicle_msgs::msg::HazardLightsCommand</code> hazard lights command from emergency handler <code>~/input/emergency/gear_cmd</code> <code>autoware_auto_vehicle_msgs::msg::GearCommand</code> gear command from emergency handler <code>~/input/engage</code> <code>autoware_auto_vehicle_msgs::msg::Engage</code> engage signal <code>~/input/operation_mode</code> <code>autoware_adapi_v1_msgs::msg::OperationModeState</code> operation mode of Autoware"},{"location":"control/vehicle_cmd_gate/#output","title":"Output","text":"Name Type Description <code>~/output/vehicle_cmd_emergency</code> <code>autoware_auto_system_msgs::msg::EmergencyState</code> emergency state which was originally in vehicle command <code>~/output/command/control_cmd</code> <code>autoware_auto_control_msgs::msg::AckermannControlCommand</code> command for lateral and longitudinal velocity to vehicle <code>~/output/command/turn_indicators_cmd</code> <code>autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand</code> turn indicators command to vehicle <code>~/output/command/hazard_lights_cmd</code> <code>autoware_auto_vehicle_msgs::msg::HazardLightsCommand</code> hazard lights command to vehicle <code>~/output/command/gear_cmd</code> <code>autoware_auto_vehicle_msgs::msg::GearCommand</code> gear command to vehicle <code>~/output/gate_mode</code> <code>tier4_control_msgs::msg::GateMode</code> gate mode (AUTO or EXTERNAL) <code>~/output/engage</code> <code>autoware_auto_vehicle_msgs::msg::Engage</code> engage signal <code>~/output/external_emergency</code> <code>tier4_external_api_msgs::msg::Emergency</code> external emergency signal <code>~/output/operation_mode</code> <code>tier4_system_msgs::msg::OperationMode</code> current operation mode of the vehicle_cmd_gate"},{"location":"control/vehicle_cmd_gate/#parameters","title":"Parameters","text":"Parameter Type Description <code>update_period</code> double update period <code>use_emergency_handling</code> bool true when emergency handler is used <code>check_external_emergency_heartbeat</code> bool true when checking heartbeat for emergency stop <code>system_emergency_heartbeat_timeout</code> double timeout for system emergency <code>external_emergency_stop_heartbeat_timeout</code> double timeout for external emergency <code>filter_activated_count_threshold</code> int threshold for filter activation <code>filter_activated_velocity_threshold</code> double velocity threshold for filter activation <code>stop_hold_acceleration</code> double longitudinal acceleration cmd when vehicle should stop <code>emergency_acceleration</code> double longitudinal acceleration cmd when vehicle stop with emergency <code>moderate_stop_service_acceleration</code> double longitudinal acceleration cmd when vehicle stop with moderate stop service <code>nominal.vel_lim</code> double limit of longitudinal velocity (activated in AUTONOMOUS operation mode) <code>nominal.reference_speed_point</code> velocity point used as a reference when calculate control command limit (activated in AUTONOMOUS operation mode). The size of this array must be equivalent to the size of the limit array. <code>nominal.lon_acc_lim</code> array of limits of longitudinal acceleration (activated in AUTONOMOUS operation mode) <code>nominal.lon_jerk_lim</code> array of limits of longitudinal jerk (activated in AUTONOMOUS operation mode) <code>nominal.lat_acc_lim</code> array of limits of lateral acceleration (activated in AUTONOMOUS operation mode) <code>nominal.lat_jerk_lim</code> array of limits of lateral jerk (activated in AUTONOMOUS operation mode) <code>on_transition.vel_lim</code> double limit of longitudinal velocity (activated in TRANSITION operation mode) <code>on_transition.reference_speed_point</code> velocity point used as a reference when calculate control command limit (activated in TRANSITION operation mode). The size of this array must be equivalent to the size of the limit array. <code>on_transition.lon_acc_lim</code> array of limits of longitudinal acceleration (activated in TRANSITION operation mode) <code>on_transition.lon_jerk_lim</code> array of limits of longitudinal jerk (activated in TRANSITION operation mode) <code>on_transition.lat_acc_lim</code> array of limits of lateral acceleration (activated in TRANSITION operation mode) <code>on_transition.lat_jerk_lim</code> array of limits of lateral jerk (activated in TRANSITION operation mode)"},{"location":"control/vehicle_cmd_gate/#filter-function","title":"Filter function","text":"<p>This module incorporates a limitation filter to the control command right before its published. Primarily for safety, this filter restricts the output range of all control commands published through Autoware.</p> <p>The limitation values are calculated based on the 1D interpolation of the limitation array parameters. Here is an example for the longitudinal jerk limit.</p> <p></p> <p>Notation: this filter is not designed to enhance ride comfort. Its main purpose is to detect and remove abnormal values in the control outputs during the final stages of Autoware. If this filter is frequently active, it implies the control module may need tuning. If you're aiming to smoothen the signal via a low-pass filter or similar techniques, that should be handled in the control module. When the filter is activated, the topic <code>~/is_filter_activated</code> is published.</p>"},{"location":"control/vehicle_cmd_gate/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The parameter <code>check_external_emergency_heartbeat</code> (true by default) enables an emergency stop request from external modules. This feature requires a <code>~/input/external_emergency_stop_heartbeat</code> topic for health monitoring of the external module, and the vehicle_cmd_gate module will not start without the topic. The <code>check_external_emergency_heartbeat</code> parameter must be false when the \"external emergency stop\" function is not used.</p>"},{"location":"evaluator/diagnostic_converter/","title":"Planning Evaluator","text":""},{"location":"evaluator/diagnostic_converter/#planning-evaluator","title":"Planning Evaluator","text":""},{"location":"evaluator/diagnostic_converter/#purpose","title":"Purpose","text":"<p>This package provides a node to convert <code>diagnostic_msgs::msg::DiagnosticArray</code> messages into <code>tier4_simulation_msgs::msg::UserDefinedValue</code> messages.</p>"},{"location":"evaluator/diagnostic_converter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The node subscribes to all topics listed in the parameters and assumes they publish <code>DiagnosticArray</code> messages. Each time such message is received, it is converted into as many <code>UserDefinedValue</code> messages as the number of <code>KeyValue</code> objects. The format of the output topic is detailed in the output section.</p>"},{"location":"evaluator/diagnostic_converter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"evaluator/diagnostic_converter/#inputs","title":"Inputs","text":"<p>The node listens to <code>DiagnosticArray</code> messages on the topics specified in the parameters.</p>"},{"location":"evaluator/diagnostic_converter/#outputs","title":"Outputs","text":"<p>The node outputs <code>UserDefinedValue</code> messages that are converted from the received <code>DiagnosticArray</code>.</p> <p>The name of the output topics are generated from the corresponding input topic, the name of the diagnostic status, and the key of the diagnostic. For example, we might listen to topic <code>/diagnostic_topic</code> and receive a <code>DiagnosticArray</code> with 2 status:</p> <ul> <li>Status with <code>name: \"x\"</code>.<ul> <li>Key: <code>a</code>.</li> <li>Key: <code>b</code>.</li> </ul> </li> <li>Status with <code>name: \"y\"</code>.<ul> <li>Key: <code>a</code>.</li> <li>Key: <code>c</code>.</li> </ul> </li> </ul> <p>The resulting topics to publish the <code>UserDefinedValue</code> are as follows:</p> <ul> <li><code>/metrics_x_a</code>.</li> <li><code>/metrics_x_b</code>.</li> <li><code>/metrics_y_a</code>.</li> <li><code>/metrics_y_c</code>.</li> </ul>"},{"location":"evaluator/diagnostic_converter/#parameters","title":"Parameters","text":"Name Type Description <code>diagnostic_topics</code> list of <code>string</code> list of DiagnosticArray topics to convert to UserDefinedValue"},{"location":"evaluator/diagnostic_converter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Values in the <code>KeyValue</code> objects of a <code>DiagnosticStatus</code> are assumed to be of type <code>double</code>.</p>"},{"location":"evaluator/diagnostic_converter/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"evaluator/kinematic_evaluator/","title":"Kinematic Evaluator","text":""},{"location":"evaluator/kinematic_evaluator/#kinematic-evaluator","title":"Kinematic Evaluator","text":"<p>TBD</p>"},{"location":"evaluator/localization_evaluator/","title":"Localization Evaluator","text":""},{"location":"evaluator/localization_evaluator/#localization-evaluator","title":"Localization Evaluator","text":"<p>TBD</p>"},{"location":"evaluator/planning_evaluator/","title":"Planning Evaluator","text":""},{"location":"evaluator/planning_evaluator/#planning-evaluator","title":"Planning Evaluator","text":""},{"location":"evaluator/planning_evaluator/#purpose","title":"Purpose","text":"<p>This package provides nodes that generate metrics to evaluate the quality of planning and control.</p>"},{"location":"evaluator/planning_evaluator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The evaluation node calculates metrics each time it receives a trajectory <code>T(0)</code>. Metrics are calculated using the following information:</p> <ul> <li>the trajectory <code>T(0)</code> itself.</li> <li>the previous trajectory <code>T(-1)</code>.</li> <li>the reference trajectory assumed to be used as the reference to plan <code>T(0)</code>.</li> <li>the current ego pose.</li> <li>the set of objects in the environment.</li> </ul> <p>These information are maintained by an instance of class <code>MetricsCalculator</code> which is also responsible for calculating metrics.</p>"},{"location":"evaluator/planning_evaluator/#stat","title":"Stat","text":"<p>Each metric is calculated using a <code>Stat</code> instance which contains the minimum, maximum, and mean values calculated for the metric as well as the number of values measured.</p>"},{"location":"evaluator/planning_evaluator/#metric-calculation-and-adding-more-metrics","title":"Metric calculation and adding more metrics","text":"<p>All possible metrics are defined in the <code>Metric</code> enumeration defined <code>include/planning_evaluator/metrics/metric.hpp</code>. This file also defines conversions from/to string as well as human readable descriptions to be used as header of the output file.</p> <p>The <code>MetricsCalculator</code> is responsible for calculating metric statistics through calls to function:</p> <pre><code>Stat&lt;double&gt; MetricsCalculator::calculate(const Metric metric, const Trajectory &amp; traj) const;\n</code></pre> <p>Adding a new metric <code>M</code> requires the following steps:</p> <ul> <li><code>metrics/metric.hpp</code>: add <code>M</code> to the <code>enum</code>, to the from/to string conversion maps, and to the description map.</li> <li><code>metrics_calculator.cpp</code>: add <code>M</code> to the <code>switch/case</code> statement of the <code>calculate</code> function.</li> <li>Add <code>M</code> to the <code>selected_metrics</code> parameters.</li> </ul>"},{"location":"evaluator/planning_evaluator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"evaluator/planning_evaluator/#inputs","title":"Inputs","text":"Name Type Description <code>~/input/trajectory</code> <code>autoware_auto_planning_msgs::msg::Trajectory</code> Main trajectory to evaluate <code>~/input/reference_trajectory</code> <code>autoware_auto_planning_msgs::msg::Trajectory</code> Reference trajectory to use for deviation metrics <code>~/input/objects</code> <code>autoware_auto_perception_msgs::msg::PredictedObjects</code> Obstacles"},{"location":"evaluator/planning_evaluator/#outputs","title":"Outputs","text":"<p>Each metric is published on a topic named after the metric name.</p> Name Type Description <code>~/metrics</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> DiagnosticArray with a DiagnosticStatus for each metric <p>When shut down, the evaluation node writes the values of the metrics measured during its lifetime to a file as specified by the <code>output_file</code> parameter.</p>"},{"location":"evaluator/planning_evaluator/#parameters","title":"Parameters","text":"Name Type Description <code>output_file</code> <code>string</code> file used to write metrics <code>ego_frame</code> <code>string</code> frame used for the ego pose <code>selected_metrics</code> List metrics to measure and publish <code>trajectory.min_point_dist_m</code> <code>double</code> minimum distance between two successive points to use for angle calculation <code>trajectory.lookahead.max_dist_m</code> <code>double</code> maximum distance from ego along the trajectory to use for calculation <code>trajectory.lookahead.max_time_m</code> <code>double</code> maximum time ahead of ego along the trajectory to use for calculation <code>obstacle.dist_thr_m</code> <code>double</code> distance between ego and the obstacle below which a collision is considered"},{"location":"evaluator/planning_evaluator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>There is a strong assumption that when receiving a trajectory <code>T(0)</code>, it has been generated using the last received reference trajectory and objects. This can be wrong if a new reference trajectory or objects are published while <code>T(0)</code> is being calculated.</p> <p>Precision is currently limited by the resolution of the trajectories. It is possible to interpolate the trajectory and reference trajectory to increase precision but would make computation significantly more expensive.</p>"},{"location":"evaluator/planning_evaluator/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":"<ul> <li>Use <code>Route</code> or <code>Path</code> messages as reference trajectory.</li> <li>RSS metrics (done in another node https://tier4.atlassian.net/browse/AJD-263).</li> <li>Add option to publish the <code>min</code> and <code>max</code> metric values. For now only the <code>mean</code> value is published.</li> <li><code>motion_evaluator_node</code>.<ul> <li>Node which constructs a trajectory over time from the real motion of ego.</li> <li>Only a proof of concept is currently implemented.</li> </ul> </li> </ul>"},{"location":"launch/tier4_autoware_api_launch/","title":"tier4_autoware_api_launch","text":""},{"location":"launch/tier4_autoware_api_launch/#tier4_autoware_api_launch","title":"tier4_autoware_api_launch","text":""},{"location":"launch/tier4_autoware_api_launch/#description","title":"Description","text":"<p>This package contains launch files that run nodes to convert Autoware internal topics into consistent API used by external software (e.g., fleet management system, simulator).</p>"},{"location":"launch/tier4_autoware_api_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_autoware_api_launch/#usage","title":"Usage","text":"<p>You can include as follows in <code>*.launch.xml</code> to use <code>autoware_api.launch.xml</code>.</p> <pre><code>  &lt;include file=\"$(find-pkg-share tier4_autoware_api_launch)/launch/autoware_api.launch.xml\"/&gt;\n</code></pre>"},{"location":"launch/tier4_autoware_api_launch/#notes","title":"Notes","text":"<p>For reducing processing load, we use the Component feature in ROS 2 (similar to Nodelet in ROS 1 )</p>"},{"location":"launch/tier4_control_launch/","title":"tier4_control_launch","text":""},{"location":"launch/tier4_control_launch/#tier4_control_launch","title":"tier4_control_launch","text":""},{"location":"launch/tier4_control_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_control_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_control_launch/#usage","title":"Usage","text":"<p>You can include as follows in <code>*.launch.xml</code> to use <code>control.launch.py</code>.</p> <p>Note that you should provide parameter paths as <code>PACKAGE_param_path</code>. The list of parameter paths you should provide is written at the top of <code>planning.launch.xml</code>.</p> <pre><code>&lt;include file=\"$(find-pkg-share tier4_control_launch)/launch/control.launch.py\"&gt;\n&lt;!-- options for lateral_controller_mode: mpc_follower, pure_pursuit --&gt;\n&lt;!-- Parameter files --&gt;\n&lt;arg name=\"FOO_NODE_param_path\" value=\"...\"/&gt;\n&lt;arg name=\"BAR_NODE_param_path\" value=\"...\"/&gt;\n...\n  &lt;arg name=\"lateral_controller_mode\" value=\"mpc_follower\" /&gt;\n&lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_control_launch/#notes","title":"Notes","text":"<p>For reducing processing load, we use the Component feature in ROS 2 (similar to Nodelet in ROS 1 )</p>"},{"location":"launch/tier4_localization_launch/","title":"tier4_localization_launch","text":""},{"location":"launch/tier4_localization_launch/#tier4_localization_launch","title":"tier4_localization_launch","text":""},{"location":"launch/tier4_localization_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_localization_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_localization_launch/#usage","title":"Usage","text":"<p>Include <code>localization.launch.xml</code> in other launch files as follows.</p> <p>You can select which methods in localization to launch as <code>pose_estimator</code> or <code>twist_estimator</code> by specifying <code>pose_source</code> and <code>twist_source</code>.</p> <p>In addition, you should provide parameter paths as <code>PACKAGE_param_path</code>. The list of parameter paths you should provide is written at the top of <code>localization.launch.xml</code>.</p> <pre><code>  &lt;include file=\"$(find-pkg-share tier4_localization_launch)/launch/localization.launch.xml\"&gt;\n&lt;!-- Localization methods --&gt;\n&lt;arg name=\"pose_source\" value=\"...\"/&gt;\n&lt;arg name=\"twist_source\" value=\"...\"/&gt;\n\n&lt;!-- Parameter files --&gt;\n&lt;arg name=\"FOO_param_path\" value=\"...\"/&gt;\n&lt;arg name=\"BAR_param_path\" value=\"...\"/&gt;\n...\n  &lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_map_launch/","title":"tier4_map_launch","text":""},{"location":"launch/tier4_map_launch/#tier4_map_launch","title":"tier4_map_launch","text":""},{"location":"launch/tier4_map_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_map_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_map_launch/#usage","title":"Usage","text":"<p>You can include as follows in <code>*.launch.xml</code> to use <code>map.launch.py</code>.</p> <p>Note that you should provide parameter paths as <code>PACKAGE_param_path</code>. The list of parameter paths you should provide is written at the top of <code>map.launch.xml</code>.</p> <pre><code>&lt;arg name=\"map_path\" description=\"point cloud and lanelet2 map directory path\"/&gt;\n&lt;arg name=\"lanelet2_map_file\" default=\"lanelet2_map.osm\" description=\"lanelet2 map file name\"/&gt;\n&lt;arg name=\"pointcloud_map_file\" default=\"pointcloud_map.pcd\" description=\"pointcloud map file name\"/&gt;\n\n&lt;include file=\"$(find-pkg-share tier4_map_launch)/launch/map.launch.py\"&gt;\n&lt;arg name=\"lanelet2_map_path\" value=\"$(var map_path)/$(var lanelet2_map_file)\" /&gt;\n&lt;arg name=\"pointcloud_map_path\" value=\"$(var map_path)/$(var pointcloud_map_file)\"/&gt;\n\n&lt;!-- Parameter files --&gt;\n&lt;arg name=\"FOO_param_path\" value=\"...\"/&gt;\n&lt;arg name=\"BAR_param_path\" value=\"...\"/&gt;\n...\n&lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_map_launch/#notes","title":"Notes","text":"<p>For reducing processing load, we use the Component feature in ROS 2 (similar to Nodelet in ROS 1 )</p>"},{"location":"launch/tier4_perception_launch/","title":"tier4_perception_launch","text":""},{"location":"launch/tier4_perception_launch/#tier4_perception_launch","title":"tier4_perception_launch","text":""},{"location":"launch/tier4_perception_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_perception_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_perception_launch/#usage","title":"Usage","text":"<p>You can include as follows in <code>*.launch.xml</code> to use <code>perception.launch.xml</code>.</p> <p>Note that you should provide parameter paths as <code>PACKAGE_param_path</code>. The list of parameter paths you should provide is written at the top of <code>perception.launch.xml</code>.</p> <pre><code>  &lt;include file=\"$(find-pkg-share tier4_perception_launch)/launch/perception.launch.xml\"&gt;\n&lt;!-- options for mode: camera_lidar_fusion, lidar, camera --&gt;\n&lt;arg name=\"mode\" value=\"lidar\" /&gt;\n\n&lt;!-- Parameter files --&gt;\n&lt;arg name=\"FOO_param_path\" value=\"...\"/&gt;\n&lt;arg name=\"BAR_param_path\" value=\"...\"/&gt;\n...\n  &lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_planning_launch/","title":"tier4_planning_launch","text":""},{"location":"launch/tier4_planning_launch/#tier4_planning_launch","title":"tier4_planning_launch","text":""},{"location":"launch/tier4_planning_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_planning_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_planning_launch/#usage","title":"Usage","text":"<p>Note that you should provide parameter paths as <code>PACKAGE_param_path</code>. The list of parameter paths you should provide is written at the top of <code>planning.launch.xml</code>.</p> <pre><code>&lt;include file=\"$(find-pkg-share tier4_planning_launch)/launch/planning.launch.xml\"&gt;\n&lt;!-- Parameter files --&gt;\n&lt;arg name=\"FOO_NODE_param_path\" value=\"...\"/&gt;\n&lt;arg name=\"BAR_NODE_param_path\" value=\"...\"/&gt;\n...\n&lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_sensing_launch/","title":"tier4_sensing_launch","text":""},{"location":"launch/tier4_sensing_launch/#tier4_sensing_launch","title":"tier4_sensing_launch","text":""},{"location":"launch/tier4_sensing_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_sensing_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_sensing_launch/#usage","title":"Usage","text":"<p>You can include as follows in <code>*.launch.xml</code> to use <code>sensing.launch.xml</code>.</p> <pre><code>  &lt;include file=\"$(find-pkg-share tier4_sensing_launch)/launch/sensing.launch.xml\"&gt;\n&lt;arg name=\"launch_driver\" value=\"true\"/&gt;\n&lt;arg name=\"sensor_model\" value=\"$(var sensor_model)\"/&gt;\n&lt;arg name=\"vehicle_param_file\" value=\"$(find-pkg-share $(var vehicle_model)_description)/config/vehicle_info.param.yaml\"/&gt;\n&lt;arg name=\"vehicle_mirror_param_file\" value=\"$(find-pkg-share $(var vehicle_model)_description)/config/mirror.param.yaml\"/&gt;\n&lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_sensing_launch/#launch-directory-structure","title":"Launch Directory Structure","text":"<p>This package finds sensor settings of specified sensor model in <code>launch</code>.</p> <pre><code>launch/\n\u251c\u2500\u2500 aip_x1 # Sensor model name\n\u2502   \u251c\u2500\u2500 camera.launch.xml # Camera\n\u2502   \u251c\u2500\u2500 gnss.launch.xml # GNSS\n\u2502   \u251c\u2500\u2500 imu.launch.xml # IMU\n\u2502   \u251c\u2500\u2500 lidar.launch.xml # LiDAR\n\u2502   \u2514\u2500\u2500 pointcloud_preprocessor.launch.py # for preprocessing pointcloud\n...\n</code></pre>"},{"location":"launch/tier4_sensing_launch/#notes","title":"Notes","text":"<p>This package finds settings with variables.</p> <p>ex.)</p> <pre><code>&lt;include file=\"$(find-pkg-share tier4_sensing_launch)/launch/$(var sensor_model)/lidar.launch.xml\"&gt;\n</code></pre>"},{"location":"launch/tier4_simulator_launch/","title":"tier4_simulator_launch","text":""},{"location":"launch/tier4_simulator_launch/#tier4_simulator_launch","title":"tier4_simulator_launch","text":""},{"location":"launch/tier4_simulator_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_simulator_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_simulator_launch/#usage","title":"Usage","text":"<pre><code>  &lt;include file=\"$(find-pkg-share tier4_simulator_launch)/launch/simulator.launch.xml\"&gt;\n&lt;arg name=\"vehicle_info_param_file\" value=\"VEHICLE_INFO_PARAM_FILE\" /&gt;\n&lt;arg name=\"vehicle_model\" value=\"VEHICLE_MODEL\"/&gt;\n&lt;/include&gt;\n</code></pre> <p>The simulator model used in simple_planning_simulator is loaded from \"config/simulator_model.param.yaml\" in the \"<code>VEHICLE_MODEL</code>_description\" package.</p>"},{"location":"launch/tier4_system_launch/","title":"tier4_system_launch","text":""},{"location":"launch/tier4_system_launch/#tier4_system_launch","title":"tier4_system_launch","text":""},{"location":"launch/tier4_system_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_system_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_system_launch/#usage","title":"Usage","text":"<p>Note that you should provide parameter paths as <code>PACKAGE_param_path</code>. The list of parameter paths you should provide is written at the top of <code>system.launch.xml</code>.</p> <pre><code>  &lt;include file=\"$(find-pkg-share tier4_system_launch)/launch/system.launch.xml\"&gt;\n&lt;arg name=\"run_mode\" value=\"online\"/&gt;\n&lt;arg name=\"sensor_model\" value=\"SENSOR_MODEL\"/&gt;\n\n&lt;!-- Parameter files --&gt;\n&lt;arg name=\"FOO_param_path\" value=\"...\"/&gt;\n&lt;arg name=\"BAR_param_path\" value=\"...\"/&gt;\n...\n  &lt;/include&gt;\n</code></pre> <p>The sensing configuration parameters used in system_error_monitor are loaded from \"config/diagnostic_aggregator/sensor_kit.param.yaml\" in the \"<code>SENSOR_MODEL</code>_description\" package.</p>"},{"location":"launch/tier4_vehicle_launch/","title":"tier4_vehicle_launch","text":""},{"location":"launch/tier4_vehicle_launch/#tier4_vehicle_launch","title":"tier4_vehicle_launch","text":""},{"location":"launch/tier4_vehicle_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_vehicle_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_vehicle_launch/#usage","title":"Usage","text":"<p>You can include as follows in <code>*.launch.xml</code> to use <code>vehicle.launch.xml</code>.</p> <pre><code>  &lt;arg name=\"vehicle_model\" default=\"sample_vehicle\" description=\"vehicle model name\"/&gt;\n&lt;arg name=\"sensor_model\" default=\"sample_sensor_kit\" description=\"sensor model name\"/&gt;\n\n&lt;include file=\"$(find-pkg-share tier4_vehicle_launch)/launch/vehicle.launch.xml\"&gt;\n&lt;arg name=\"vehicle_model\" value=\"$(var vehicle_model)\"/&gt;\n&lt;arg name=\"sensor_model\" value=\"$(var sensor_model)\"/&gt;\n&lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_vehicle_launch/#notes","title":"Notes","text":"<p>This package finds some external packages and settings with variables and package names.</p> <p>ex.)</p> <pre><code>&lt;let name=\"vehicle_model_pkg\" value=\"$(find-pkg-share $(var vehicle_model)_description)\"/&gt;\n</code></pre> <pre><code>&lt;arg name=\"config_dir\" default=\"$(find-pkg-share individual_params)/config/$(var vehicle_id)/$(var sensor_model)\"/&gt;\n</code></pre>"},{"location":"launch/tier4_vehicle_launch/#vehiclexacro","title":"vehicle.xacro","text":""},{"location":"launch/tier4_vehicle_launch/#arguments","title":"Arguments","text":"Name Type Description Default sensor_model String sensor model name \"\" vehicle_model String vehicle model name \"\""},{"location":"launch/tier4_vehicle_launch/#usage_1","title":"Usage","text":"<p>You can write as follows in <code>*.launch.xml</code>.</p> <pre><code>  &lt;arg name=\"vehicle_model\" default=\"sample_vehicle\" description=\"vehicle model name\"/&gt;\n&lt;arg name=\"sensor_model\" default=\"sample_sensor_kit\" description=\"sensor model name\"/&gt;\n&lt;arg name=\"model\" default=\"$(find-pkg-share tier4_vehicle_launch)/urdf/vehicle.xacro\"/&gt;\n\n&lt;node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" exec=\"robot_state_publisher\"&gt;\n&lt;param name=\"robot_description\" value=\"$(command 'xacro $(var model) vehicle_model:=$(var vehicle_model) sensor_model:=$(var sensor_model)')\"/&gt;\n&lt;/node&gt;\n</code></pre>"},{"location":"localization/ekf_localizer/","title":"Overview","text":""},{"location":"localization/ekf_localizer/#overview","title":"Overview","text":"<p>The Extend Kalman Filter Localizer estimates robust and less noisy robot pose and twist by integrating the 2D vehicle dynamics model with input ego-pose and ego-twist messages. The algorithm is designed especially for fast-moving robots such as autonomous driving systems.</p>"},{"location":"localization/ekf_localizer/#flowchart","title":"Flowchart","text":"<p>The overall flowchart of the ekf_localizer is described below.</p> <p> </p>"},{"location":"localization/ekf_localizer/#features","title":"Features","text":"<p>This package includes the following features:</p> <ul> <li>Time delay compensation for input messages, which enables proper integration of input information with varying time delays. This is important especially for high-speed moving robots, such as autonomous driving vehicles. (see the following figure).</li> <li>Automatic estimation of yaw bias prevents modeling errors caused by sensor mounting angle errors, which can improve estimation accuracy.</li> <li>Mahalanobis distance gate enables probabilistic outlier detection to determine which inputs should be used or ignored.</li> <li>Smooth update, the Kalman Filter measurement update is typically performed when a measurement is obtained, but it can cause large changes in the estimated value, especially for low-frequency measurements. Since the algorithm can consider the measurement time, the measurement data can be divided into multiple pieces and integrated smoothly while maintaining consistency (see the following figure).</li> <li>Calculation of vertical correction amount from pitch mitigates localization instability on slopes. For example, when going uphill, it behaves as if it is buried in the ground (see the left side of the \"Calculate delta from pitch\" figure) because EKF only considers 3DoF(x,y,yaw). Therefore, EKF corrects the z-coordinate according to the formula (see the right side of the \"Calculate delta from pitch\" figure).</li> </ul> <p> </p> <p> </p> <p> </p>"},{"location":"localization/ekf_localizer/#launch","title":"Launch","text":"<p>The <code>ekf_localizer</code> starts with the default parameters with the following command.</p> <pre><code>roslaunch ekf_localizer ekf_localizer.launch\n</code></pre> <p>The parameters and input topic names can be set in the <code>ekf_localizer.launch</code> file.</p>"},{"location":"localization/ekf_localizer/#node","title":"Node","text":""},{"location":"localization/ekf_localizer/#subscribed-topics","title":"Subscribed Topics","text":"<ul> <li> <p>measured_pose_with_covariance (geometry_msgs/PoseWithCovarianceStamped)</p> <p>Input pose source with the measurement covariance matrix.</p> </li> </ul> <ul> <li> <p>measured_twist_with_covariance (geometry_msgs/TwistWithCovarianceStamped)</p> <p>Input twist source with the measurement covariance matrix.</p> </li> </ul> <ul> <li> <p>initialpose (geometry_msgs/PoseWithCovarianceStamped)</p> <p>Initial pose for EKF. The estimated pose is initialized with zeros at the start. It is initialized with this message whenever published.</p> </li> </ul>"},{"location":"localization/ekf_localizer/#published-topics","title":"Published Topics","text":"<ul> <li> <p>ekf_odom (nav_msgs/Odometry)</p> <p>Estimated odometry.</p> </li> </ul> <ul> <li> <p>ekf_pose (geometry_msgs/PoseStamped)</p> <p>Estimated pose.</p> </li> </ul> <ul> <li> <p>ekf_pose_with_covariance (geometry_msgs/PoseWithCovarianceStamped)</p> <p>Estimated pose with covariance.</p> </li> </ul> <ul> <li> <p>ekf_biased_pose (geometry_msgs/PoseStamped)</p> <p>Estimated pose including the yaw bias</p> </li> </ul> <ul> <li> <p>ekf_biased_pose_with_covariance (geometry_msgs/PoseWithCovarianceStamped)</p> <p>Estimated pose with covariance including the yaw bias</p> </li> </ul> <ul> <li> <p>ekf_twist (geometry_msgs/TwistStamped)</p> <p>Estimated twist.</p> </li> </ul> <ul> <li> <p>ekf_twist_with_covariance (geometry_msgs/TwistWithCovarianceStamped)</p> <p>The estimated twist with covariance.</p> </li> </ul> <ul> <li> <p>diagnostics (diagnostic_msgs/DiagnosticArray)</p> <p>The diagnostic information.</p> </li> </ul>"},{"location":"localization/ekf_localizer/#published-tf","title":"Published TF","text":"<ul> <li> <p>base_link</p> <p>TF from \"map\" coordinate to estimated pose.</p> </li> </ul>"},{"location":"localization/ekf_localizer/#functions","title":"Functions","text":""},{"location":"localization/ekf_localizer/#predict","title":"Predict","text":"<p>The current robot state is predicted from previously estimated data using a given prediction model. This calculation is called at a constant interval (<code>predict_frequency [Hz]</code>). The prediction equation is described at the end of this page.</p>"},{"location":"localization/ekf_localizer/#measurement-update","title":"Measurement Update","text":"<p>Before the update, the Mahalanobis distance is calculated between the measured input and the predicted state, the measurement update is not performed for inputs where the Mahalanobis distance exceeds the given threshold.</p> <p>The predicted state is updated with the latest measured inputs, measured_pose, and measured_twist. The updates are performed with the same frequency as prediction, usually at a high frequency, in order to enable smooth state estimation.</p>"},{"location":"localization/ekf_localizer/#parameter-description","title":"Parameter description","text":"<p>The parameters are set in <code>launch/ekf_localizer.launch</code> .</p>"},{"location":"localization/ekf_localizer/#for-node","title":"For Node","text":"Name Type Description Default value show_debug_info bool Flag to display debug info false predict_frequency double Frequency for filtering and publishing [Hz] 50.0 tf_rate double Frequency for tf broadcasting [Hz] 10.0 extend_state_step int Max delay step which can be dealt with in EKF. Large number increases computational cost. 50 enable_yaw_bias_estimation bool Flag to enable yaw bias estimation true"},{"location":"localization/ekf_localizer/#for-pose-measurement","title":"For pose measurement","text":"Name Type Description Default value pose_additional_delay double Additional delay time for pose measurement [s] 0.0 pose_measure_uncertainty_time double Measured time uncertainty used for covariance calculation [s] 0.01 pose_smoothing_steps int A value for smoothing steps 5 pose_gate_dist double Limit of Mahalanobis distance used for outliers detection 10000.0"},{"location":"localization/ekf_localizer/#for-twist-measurement","title":"For twist measurement","text":"Name Type Description Default value twist_additional_delay double Additional delay time for twist [s] 0.0 twist_smoothing_steps int A value for smoothing steps 2 twist_gate_dist double Limit of Mahalanobis distance used for outliers detection 10000.0"},{"location":"localization/ekf_localizer/#for-process-noise","title":"For process noise","text":"Name Type Description Default value proc_stddev_vx_c double Standard deviation of process noise in time differentiation expression of linear velocity x, noise for d_vx = 0 2.0 proc_stddev_wz_c double Standard deviation of process noise in time differentiation expression of angular velocity z, noise for d_wz = 0 0.2 proc_stddev_yaw_c double Standard deviation of process noise in time differentiation expression of yaw, noise for d_yaw = omega 0.005 proc_stddev_yaw_bias_c double Standard deviation of process noise in time differentiation expression of yaw_bias, noise for d_yaw_bias = 0 0.001 <p>note: process noise for positions x &amp; y are calculated automatically from nonlinear dynamics.</p>"},{"location":"localization/ekf_localizer/#for-diagnostics","title":"For diagnostics","text":"Name Type Description Default value pose_no_update_count_threshold_warn size_t The threshold at which a WARN state is triggered due to the Pose Topic update not happening continuously for a certain number of times. 50 pose_no_update_count_threshold_error size_t The threshold at which an ERROR state is triggered due to the Pose Topic update not happening continuously for a certain number of times. 250 twist_no_update_count_threshold_warn size_t The threshold at which a WARN state is triggered due to the Twist Topic update not happening continuously for a certain number of times. 50 twist_no_update_count_threshold_error size_t The threshold at which an ERROR state is triggered due to the Twist Topic update not happening continuously for a certain number of times. 250"},{"location":"localization/ekf_localizer/#misc","title":"Misc","text":"Name Type Description Default value threshold_observable_velocity_mps double Minimum value for velocity that will be used for EKF. Mainly used for dead zone in velocity sensor 0.0 (disabled)"},{"location":"localization/ekf_localizer/#how-to-tune-ekf-parameters","title":"How to tune EKF parameters","text":""},{"location":"localization/ekf_localizer/#0-preliminaries","title":"0. Preliminaries","text":"<ul> <li>Check header time in pose and twist message is set to sensor time appropriately, because time delay is calculated from this value. If it is difficult to set an appropriate time due to the timer synchronization problem, use <code>twist_additional_delay</code> and <code>pose_additional_delay</code> to correct the time.</li> <li>Check if the relation between measurement pose and twist is appropriate (whether the derivative of the pose has a similar value to twist). This discrepancy is caused mainly by unit error (such as confusing radian/degree) or bias noise, and it causes large estimation errors.</li> </ul>"},{"location":"localization/ekf_localizer/#1-tune-sensor-parameters","title":"1. Tune sensor parameters","text":"<p>Set standard deviation for each sensor. The <code>pose_measure_uncertainty_time</code> is for the uncertainty of the header timestamp data. You can also tune a number of steps for smoothing for each observed sensor data by tuning <code>*_smoothing_steps</code>. Increasing the number will improve the smoothness of the estimation, but may have an adverse effect on the estimation performance.</p> <ul> <li><code>pose_measure_uncertainty_time</code></li> <li><code>pose_smoothing_steps</code></li> <li><code>twist_smoothing_steps</code></li> </ul>"},{"location":"localization/ekf_localizer/#2-tune-process-model-parameters","title":"2. Tune process model parameters","text":"<ul> <li><code>proc_stddev_vx_c</code> : set to maximum linear acceleration</li> <li><code>proc_stddev_wz_c</code> : set to maximum angular acceleration</li> <li><code>proc_stddev_yaw_c</code> : This parameter describes the correlation between the yaw and yaw rate. A large value means the change in yaw does not correlate to the estimated yaw rate. If this is set to 0, it means the change in estimated yaw is equal to yaw rate. Usually, this should be set to 0.</li> <li><code>proc_stddev_yaw_bias_c</code> : This parameter is the standard deviation for the rate of change in yaw bias. In most cases, yaw bias is constant, so it can be very small, but must be non-zero.</li> </ul>"},{"location":"localization/ekf_localizer/#kalman-filter-model","title":"Kalman Filter Model","text":""},{"location":"localization/ekf_localizer/#kinematics-model-in-update-function","title":"kinematics model in update function","text":"<p>where <code>b_k</code> is the yawbias.</p>"},{"location":"localization/ekf_localizer/#time-delay-model","title":"time delay model","text":"<p>The measurement time delay is handled by an augmented state [1] (See, Section 7.3 FIXED-LAG SMOOTHING).</p> <p></p> <p>Note that, although the dimension gets larger since the analytical expansion can be applied based on the specific structures of the augmented states, the computational complexity does not significantly change.</p>"},{"location":"localization/ekf_localizer/#test-result-with-autoware-ndt","title":"Test Result with Autoware NDT","text":""},{"location":"localization/ekf_localizer/#diagnostics","title":"Diagnostics","text":""},{"location":"localization/ekf_localizer/#the-conditions-that-result-in-a-warn-state","title":"The conditions that result in a WARN state","text":"<ul> <li>The node is not in the activate state.</li> <li>The number of consecutive no measurement update via the Pose/Twist topic exceeds the <code>pose_no_update_count_threshold_warn</code>/<code>twist_no_update_count_threshold_warn</code>.</li> <li>The timestamp of the Pose/Twist topic is beyond the delay compensation range.</li> <li>The Pose/Twist topic is beyond the range of Mahalanobis distance for covariance estimation.</li> </ul>"},{"location":"localization/ekf_localizer/#the-conditions-that-result-in-an-error-state","title":"The conditions that result in an ERROR state","text":"<ul> <li>The number of consecutive no measurement update via the Pose/Twist topic exceeds the <code>pose_no_update_count_threshold_error</code>/<code>twist_no_update_count_threshold_error</code>.</li> </ul>"},{"location":"localization/ekf_localizer/#known-issues","title":"Known issues","text":"<ul> <li>In the presence of multiple inputs with yaw estimation, yaw bias <code>b_k</code> in the current EKF state would not make any sense, since it is intended to capture the extrinsic parameter's calibration error of a sensor. Thus, future work includes introducing yaw bias for each sensor with yaw estimation.</li> </ul>"},{"location":"localization/ekf_localizer/#reference","title":"reference","text":"<p>[1] Anderson, B. D. O., &amp; Moore, J. B. (1979). Optimal filtering. Englewood Cliffs, NJ: Prentice-Hall.</p>"},{"location":"localization/geo_pose_projector/","title":"geo_pose_projector","text":""},{"location":"localization/geo_pose_projector/#geo_pose_projector","title":"geo_pose_projector","text":""},{"location":"localization/geo_pose_projector/#overview","title":"Overview","text":"<p>This node is a simple node that subscribes to the geo-referenced pose topic and publishes the pose in the map frame.</p>"},{"location":"localization/geo_pose_projector/#subscribed-topics","title":"Subscribed Topics","text":"Name Type Description <code>input_geo_pose</code> <code>geographic_msgs::msg::GeoPoseWithCovarianceStamped</code> geo-referenced pose <code>/map/map_projector_info</code> <code>tier4_map_msgs::msg::MapProjectedObjectInfo</code> map projector info"},{"location":"localization/geo_pose_projector/#published-topics","title":"Published Topics","text":"Name Type Description <code>output_pose</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> pose in map frame <code>/tf</code> <code>tf2_msgs::msg::TFMessage</code> tf from parent link to the child link"},{"location":"localization/geo_pose_projector/#parameters","title":"Parameters","text":"Name Type Description Default Range publish_tf boolean whether to publish tf True N/A parent_frame string parent frame for published tf map N/A child_frame string child frame for published tf pose_estimator_base_link N/A"},{"location":"localization/geo_pose_projector/#limitations","title":"Limitations","text":"<p>The covariance conversion may be incorrect depending on the projection type you are using. The covariance of input topic is expressed in (Latitude, Longitude, Altitude) as a diagonal matrix. Currently, we assume that the x axis is the east direction and the y axis is the north direction. Thus, the conversion may be incorrect when this assumption breaks, especially when the covariance of latitude and longitude is different.</p>"},{"location":"localization/gyro_odometer/","title":"gyro_odometer","text":""},{"location":"localization/gyro_odometer/#gyro_odometer","title":"gyro_odometer","text":""},{"location":"localization/gyro_odometer/#purpose","title":"Purpose","text":"<p><code>gyro_odometer</code> is the package to estimate twist by combining imu and vehicle speed.</p>"},{"location":"localization/gyro_odometer/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/gyro_odometer/#input","title":"Input","text":"Name Type Description <code>vehicle/twist_with_covariance</code> <code>geometry_msgs::msg::TwistWithCovarianceStamped</code> twist with covariance from vehicle <code>imu</code> <code>sensor_msgs::msg::Imu</code> imu from sensor"},{"location":"localization/gyro_odometer/#output","title":"Output","text":"Name Type Description <code>twist_with_covariance</code> <code>geometry_msgs::msg::TwistWithCovarianceStamped</code> estimated twist with covariance"},{"location":"localization/gyro_odometer/#parameters","title":"Parameters","text":"Name Type Description Default Range output_frame string output's frame id base_link N/A message_timeout_sec float delay tolerance time for message 0.2 N/A"},{"location":"localization/gyro_odometer/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>[Assumption] The frame_id of input twist message must be set to base_link.</li> </ul> <ul> <li>[Assumption] The covariance in the input messages must be properly assigned.</li> </ul> <ul> <li>[Assumption] The angular velocity is set to zero if both the longitudinal vehicle velocity and the angular velocity around the yaw axis are sufficiently small. This is for suppression of the IMU angular velocity bias. Without this process, we misestimate the vehicle status when stationary.</li> </ul> <ul> <li>[Limitation] The frequency of the output messages depends on the frequency of the input IMU message.</li> </ul> <ul> <li>[Limitation] We cannot produce reliable values for the lateral and vertical velocities. Therefore we assign large values to the corresponding elements in the output covariance matrix.</li> </ul>"},{"location":"localization/landmark_based_localizer/","title":"Landmark Based Localizer","text":""},{"location":"localization/landmark_based_localizer/#landmark-based-localizer","title":"Landmark Based Localizer","text":"<p>This directory contains packages for landmark-based localization.</p> <p>Landmarks are, for example</p> <ul> <li>AR tags detected by camera</li> <li>Boards characterized by intensity detected by LiDAR</li> </ul> <p>etc.</p> <p>Since these landmarks are easy to detect and estimate pose, the ego pose can be calculated from the pose of the detected landmark if the pose of the landmark is written on the map in advance.</p> <p>Currently, landmarks are assumed to be flat.</p> <p>The following figure shows the principle of localization in the case of <code>ar_tag_based_localizer</code>.</p> <p></p> <p>This calculated ego pose is passed to the EKF, where it is fused with the twist information and used to estimate a more accurate ego pose.</p>"},{"location":"localization/landmark_based_localizer/#node-diagram","title":"Node diagram","text":""},{"location":"localization/landmark_based_localizer/#landmark_manager","title":"<code>landmark_manager</code>","text":"<p>The definitions of the landmarks written to the map are introduced in the next section. See <code>Map Specifications</code>.</p> <p>The <code>landmark_manager</code> is a utility package to load landmarks from the map.</p> <ul> <li>Translation : The center of the four vertices of the landmark</li> <li>Rotation : Let the vertex numbers be 1, 2, 3, 4 counterclockwise as shown in the next section. Direction is defined as the cross product of the vector from 1 to 2 and the vector from 2 to 3.</li> </ul> <p>Users can define landmarks as Lanelet2 4-vertex polygons. In this case, it is possible to define an arrangement in which the four vertices cannot be considered to be on the same plane. The direction of the landmark in that case is difficult to calculate. So, if the 4 vertices are considered as forming a tetrahedron and its volume exceeds the <code>volume_threshold</code> parameter, the landmark will not publish tf_static.</p>"},{"location":"localization/landmark_based_localizer/#landmark-based-localizer-packages","title":"Landmark based localizer packages","text":"<ul> <li>ar_tag_based_localizer</li> <li>etc.</li> </ul>"},{"location":"localization/landmark_based_localizer/#map-specifications","title":"Map specifications","text":"<p>For this package to work correctly, the poses of the landmarks must be specified in the Lanelet2 map format that <code>map_loader</code> and <code>landmark_manager</code> can interpret.</p> <p>The four vertices of a landmark are defined counterclockwise.</p> <p>The order of the four vertices is defined as follows. In the coordinate system of a landmark,</p> <ul> <li>the x-axis is parallel to the vector from the first vertex to the second vertex</li> <li>the y-axis is parallel to the vector from the second vertex to the third vertex</li> </ul> <p></p>"},{"location":"localization/landmark_based_localizer/#example-of-lanelet2_maposm","title":"Example of <code>lanelet2_map.osm</code>","text":"<p>The values provided below are placeholders. Ensure to input the correct coordinates corresponding to the actual location where the landmark is placed, such as <code>lat</code>, <code>lon</code>, <code>mgrs_code</code>, <code>local_x</code>, <code>local_y</code>.</p> <p>For example, when using the AR tag, it would look like this.</p> <pre><code>...\n\n  &lt;node id=\"1\" lat=\"35.8xxxxx\" lon=\"139.6xxxxx\"&gt;\n&lt;tag k=\"mgrs_code\" v=\"99XXX000000\"/&gt;\n&lt;tag k=\"local_x\" v=\"22.2356\"/&gt;\n&lt;tag k=\"local_y\" v=\"87.4506\"/&gt;\n&lt;tag k=\"ele\" v=\"2.1725\"/&gt;\n&lt;/node&gt;\n&lt;node id=\"2\" lat=\"35.8xxxxx\" lon=\"139.6xxxxx\"&gt;\n&lt;tag k=\"mgrs_code\" v=\"99XXX000000\"/&gt;\n&lt;tag k=\"local_x\" v=\"22.639\"/&gt;\n&lt;tag k=\"local_y\" v=\"87.5886\"/&gt;\n&lt;tag k=\"ele\" v=\"2.5947\"/&gt;\n&lt;/node&gt;\n&lt;node id=\"3\" lat=\"35.8xxxxx\" lon=\"139.6xxxxx\"&gt;\n&lt;tag k=\"mgrs_code\" v=\"99XXX000000\"/&gt;\n&lt;tag k=\"local_x\" v=\"22.2331\"/&gt;\n&lt;tag k=\"local_y\" v=\"87.4713\"/&gt;\n&lt;tag k=\"ele\" v=\"3.0208\"/&gt;\n&lt;/node&gt;\n&lt;node id=\"4\" lat=\"35.8xxxxx\" lon=\"139.6xxxxx\"&gt;\n&lt;tag k=\"mgrs_code\" v=\"99XXX000000\"/&gt;\n&lt;tag k=\"local_x\" v=\"21.8298\"/&gt;\n&lt;tag k=\"local_y\" v=\"87.3332\"/&gt;\n&lt;tag k=\"ele\" v=\"2.5985\"/&gt;\n&lt;/node&gt;\n\n...\n\n  &lt;way id=\"5\"&gt;\n&lt;nd ref=\"1\"/&gt;\n&lt;nd ref=\"2\"/&gt;\n&lt;nd ref=\"3\"/&gt;\n&lt;nd ref=\"4\"/&gt;\n&lt;tag k=\"type\" v=\"pose_marker\"/&gt;\n&lt;tag k=\"subtype\" v=\"apriltag_16h5\"/&gt;\n&lt;tag k=\"area\" v=\"yes\"/&gt;\n&lt;tag k=\"marker_id\" v=\"0\"/&gt;\n&lt;/way&gt;\n\n...\n</code></pre>"},{"location":"localization/landmark_based_localizer/#about-consider_orientation","title":"About <code>consider_orientation</code>","text":"<p>The <code>calculate_new_self_pose</code> function in the <code>LandmarkManager</code> class includes a boolean argument named <code>consider_orientation</code>. This argument determines the method used to calculate the new self pose based on detected and mapped landmarks. The following image illustrates the difference between the two methods.</p> <p></p>"},{"location":"localization/landmark_based_localizer/#consider_orientation-true","title":"<code>consider_orientation = true</code>","text":"<p>In this mode, the new self pose is calculated so that the relative Pose of the \"landmark detected from the current self pose\" is equal to the relative Pose of the \"landmark mapped from the new self pose\". This method can correct for orientation, but is strongly affected by the orientation error of the landmark detection.</p>"},{"location":"localization/landmark_based_localizer/#consider_orientation-false","title":"<code>consider_orientation = false</code>","text":"<p>In this mode, the new self pose is calculated so that only the relative position is correct for x, y, and z.</p> <p>This method can not correct for orientation, but it is not affected by the orientation error of the landmark detection.</p>"},{"location":"localization/landmark_based_localizer/ar_tag_based_localizer/","title":"AR Tag Based Localizer","text":""},{"location":"localization/landmark_based_localizer/ar_tag_based_localizer/#ar-tag-based-localizer","title":"AR Tag Based Localizer","text":"<p>ArTagBasedLocalizer is a vision-based localization node.</p> <p></p> <p>This node uses the ArUco library to detect AR-Tags from camera images and calculates and publishes the pose of the ego vehicle based on these detections. The positions and orientations of the AR-Tags are assumed to be written in the Lanelet2 format.</p>"},{"location":"localization/landmark_based_localizer/ar_tag_based_localizer/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/landmark_based_localizer/ar_tag_based_localizer/#ar_tag_based_localizer-node","title":"<code>ar_tag_based_localizer</code> node","text":""},{"location":"localization/landmark_based_localizer/ar_tag_based_localizer/#input","title":"Input","text":"Name Type Description <code>~/input/lanelet2_map</code> <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> Data of lanelet2 <code>~/input/image</code> <code>sensor_msgs::msg::Image</code> Camera Image <code>~/input/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> Camera Info <code>~/input/ekf_pose</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> EKF Pose without IMU correction. It is used to validate detected AR tags by filtering out False Positives. Only if the EKF Pose and the AR tag-detected Pose are within a certain temporal and spatial range, the AR tag-detected Pose is considered valid and published."},{"location":"localization/landmark_based_localizer/ar_tag_based_localizer/#output","title":"Output","text":"Name Type Description <code>~/output/pose_with_covariance</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> Estimated Pose <code>~/debug/result</code> <code>sensor_msgs::msg::Image</code> [debug topic] Image in which marker detection results are superimposed on the input image <code>~/debug/marker</code> <code>visualization_msgs::msg::MarkerArray</code> [debug topic] Loaded landmarks to visualize in Rviz as thin boards <code>/tf</code> <code>geometry_msgs::msg::TransformStamped</code> [debug topic] TF from camera to detected tag <code>/diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> Diagnostics outputs"},{"location":"localization/landmark_based_localizer/ar_tag_based_localizer/#how-to-launch","title":"How to launch","text":"<p>When launching Autoware, set <code>artag</code> for <code>pose_source</code>.</p> <pre><code>ros2 launch autoware_launch ... \\\npose_source:=artag \\\n...\n</code></pre>"},{"location":"localization/landmark_based_localizer/ar_tag_based_localizer/#rosbag","title":"Rosbag","text":""},{"location":"localization/landmark_based_localizer/ar_tag_based_localizer/#sample-rosbag-and-map-awsim-data","title":"Sample rosbag and map (AWSIM data)","text":"<p>This data is simulated data created by AWSIM. Essentially, AR tag-based self-localization is not intended for such public road driving, but for driving in a smaller area, so the max driving speed is set at 15 km/h.</p> <p>It is a known problem that the timing of when each AR tag begins to be detected can cause significant changes in estimation.</p> <p></p>"},{"location":"localization/landmark_based_localizer/ar_tag_based_localizer/#sample-rosbag-and-map-real-world-data","title":"Sample rosbag and map (Real world data)","text":"<p>Please remap the topic names and play it.</p> <pre><code>ros2 bag play /path/to/ar_tag_based_localizer_sample_bag/ -r 0.5 -s sqlite3 \\\n--remap /sensing/camera/front/image:=/sensing/camera/traffic_light/image_raw \\\n/sensing/camera/front/image/info:=/sensing/camera/traffic_light/camera_info\n</code></pre> <p>This dataset contains issues such as missing IMU data, and overall the accuracy is low. Even when running AR tag-based self-localization, significant difference from the true trajectory can be observed.</p> <p>The image below shows the trajectory when the sample is executed and plotted.</p> <p></p> <p>The pull request video below should also be helpful.</p> <p>https://github.com/autowarefoundation/autoware.universe/pull/4347#issuecomment-1663155248</p>"},{"location":"localization/landmark_based_localizer/ar_tag_based_localizer/#principle","title":"Principle","text":""},{"location":"localization/localization_error_monitor/","title":"localization_error_monitor","text":""},{"location":"localization/localization_error_monitor/#localization_error_monitor","title":"localization_error_monitor","text":""},{"location":"localization/localization_error_monitor/#purpose","title":"Purpose","text":"<p>localization_error_monitor is a package for diagnosing localization errors by monitoring uncertainty of the localization results. The package monitors the following two values:</p> <ul> <li>size of long radius of confidence ellipse</li> <li>size of confidence ellipse along lateral direction (body-frame)</li> </ul>"},{"location":"localization/localization_error_monitor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/localization_error_monitor/#input","title":"Input","text":"Name Type Description <code>input/pose_with_cov</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> localization result"},{"location":"localization/localization_error_monitor/#output","title":"Output","text":"Name Type Description <code>debug/ellipse_marker</code> <code>visualization_msgs::msg::Marker</code> ellipse marker <code>diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> diagnostics outputs"},{"location":"localization/localization_error_monitor/#parameters","title":"Parameters","text":"Name Type Description <code>scale</code> double scale factor for monitored values (default: 3.0) <code>error_ellipse_size</code> double error threshold for long radius of confidence ellipse [m] (default: 1.0) <code>warn_ellipse_size</code> double warning threshold for long radius of confidence ellipse [m] (default: 0.8) <code>error_ellipse_size_lateral_direction</code> double error threshold for size of confidence ellipse along lateral direction [m] (default: 0.3) <code>warn_ellipse_size_lateral_direction</code> double warning threshold for size of confidence ellipse along lateral direction [m] (default: 0.25)"},{"location":"localization/localization_util/","title":"localization_util","text":""},{"location":"localization/localization_util/#localization_util","title":"localization_util","text":"<p>`localization_util`` is a localization utility package.</p> <p>This package does not have a node, it is just a library.</p>"},{"location":"localization/ndt_scan_matcher/","title":"ndt_scan_matcher","text":""},{"location":"localization/ndt_scan_matcher/#ndt_scan_matcher","title":"ndt_scan_matcher","text":""},{"location":"localization/ndt_scan_matcher/#purpose","title":"Purpose","text":"<p>ndt_scan_matcher is a package for position estimation using the NDT scan matching method.</p> <p>There are two main functions in this package:</p> <ul> <li>estimate position by scan matching</li> <li>estimate initial position via the ROS service using the Monte Carlo method</li> </ul> <p>One optional function is regularization. Please see the regularization chapter in the back for details. It is disabled by default.</p>"},{"location":"localization/ndt_scan_matcher/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/ndt_scan_matcher/#input","title":"Input","text":"Name Type Description <code>ekf_pose_with_covariance</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> initial pose <code>pointcloud_map</code> <code>sensor_msgs::msg::PointCloud2</code> map pointcloud <code>points_raw</code> <code>sensor_msgs::msg::PointCloud2</code> sensor pointcloud <code>sensing/gnss/pose_with_covariance</code> <code>sensor_msgs::msg::PoseWithCovarianceStamped</code> base position for regularization term <p><code>sensing/gnss/pose_with_covariance</code> is required only when regularization is enabled.</p>"},{"location":"localization/ndt_scan_matcher/#output","title":"Output","text":"Name Type Description <code>ndt_pose</code> <code>geometry_msgs::msg::PoseStamped</code> estimated pose <code>ndt_pose_with_covariance</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> estimated pose with covariance <code>/diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> diagnostics <code>points_aligned</code> <code>sensor_msgs::msg::PointCloud2</code> [debug topic] pointcloud aligned by scan matching <code>points_aligned_no_ground</code> <code>sensor_msgs::msg::PointCloud2</code> [debug topic] de-grounded pointcloud aligned by scan matching <code>initial_pose_with_covariance</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> [debug topic] initial pose used in scan matching <code>multi_ndt_pose</code> <code>geometry_msgs::msg::PoseArray</code> [debug topic] estimated poses from multiple initial poses in real-time covariance estimation <code>multi_initial_pose</code> <code>geometry_msgs::msg::PoseArray</code> [debug topic] initial poses for real-time covariance estimation <code>exe_time_ms</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> [debug topic] execution time for scan matching [ms] <code>transform_probability</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> [debug topic] score of scan matching <code>no_ground_transform_probability</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> [debug topic] score of scan matching based on de-grounded LiDAR scan <code>iteration_num</code> <code>tier4_debug_msgs::msg::Int32Stamped</code> [debug topic] number of scan matching iterations <code>initial_to_result_relative_pose</code> <code>geometry_msgs::msg::PoseStamped</code> [debug topic] relative pose between the initial point and the convergence point <code>initial_to_result_distance</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> [debug topic] distance difference between the initial point and the convergence point [m] <code>initial_to_result_distance_old</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> [debug topic] distance difference between the older of the two initial points used in linear interpolation and the convergence point [m] <code>initial_to_result_distance_new</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> [debug topic] distance difference between the newer of the two initial points used in linear interpolation and the convergence point [m] <code>ndt_marker</code> <code>visualization_msgs::msg::MarkerArray</code> [debug topic] markers for debugging <code>monte_carlo_initial_pose_marker</code> <code>visualization_msgs::msg::MarkerArray</code> [debug topic] particles used in initial position estimation"},{"location":"localization/ndt_scan_matcher/#service","title":"Service","text":"Name Type Description <code>ndt_align_srv</code> <code>autoware_localization_srvs::srv::PoseWithCovarianceStamped</code> service to estimate initial pose"},{"location":"localization/ndt_scan_matcher/#parameters","title":"Parameters","text":""},{"location":"localization/ndt_scan_matcher/#core-parameters","title":"Core Parameters","text":"Name Type Description <code>base_frame</code> string Vehicle reference frame <code>ndt_base_frame</code> string NDT reference frame <code>map_frame</code> string map frame <code>input_sensor_points_queue_size</code> int Subscriber queue size <code>trans_epsilon</code> double The max difference between two consecutive transformations to consider convergence <code>step_size</code> double The newton line search maximum step length <code>resolution</code> double The ND voxel grid resolution [m] <code>max_iterations</code> int The number of iterations required to calculate alignment <code>converged_param_type</code> int The type of indicators for scan matching score (0: TP, 1: NVTL) <code>converged_param_transform_probability</code> double TP threshold for deciding whether to trust the estimation result (when converged_param_type = 0) <code>converged_param_nearest_voxel_transformation_likelihood</code> double NVTL threshold for deciding whether to trust the estimation result (when converged_param_type = 1) <code>initial_estimate_particles_num</code> int The number of particles to estimate initial pose <code>n_startup_trials</code> int The number of initial random trials in the TPE (Tree-Structured Parzen Estimator). <code>lidar_topic_timeout_sec</code> double Tolerance of timestamp difference between current time and sensor pointcloud <code>initial_pose_timeout_sec</code> int Tolerance of timestamp difference between initial_pose and sensor pointcloud. [sec] <code>initial_pose_distance_tolerance_m</code> double Tolerance of distance difference between two initial poses used for linear interpolation. [m] <code>num_threads</code> int Number of threads used for parallel computing <code>output_pose_covariance</code> std::array The covariance of output pose <p>(TP: Transform Probability, NVTL: Nearest Voxel Transform Probability)</p>"},{"location":"localization/ndt_scan_matcher/#regularization","title":"Regularization","text":""},{"location":"localization/ndt_scan_matcher/#abstract","title":"Abstract","text":"<p>This is a function that adds the regularization term to the NDT optimization problem as follows.</p> \\[ \\begin{align}     \\min_{\\mathbf{R},\\mathbf{t}}     \\mathrm{NDT}(\\mathbf{R},\\mathbf{t})     +\\mathrm{scale\\ factor}\\cdot \\left|         \\mathbf{R}^\\top         (\\mathbf{t_{base}-\\mathbf{t}})         \\cdot         \\begin{pmatrix}             1\\\\             0\\\\             0         \\end{pmatrix}         \\right|^2 \\end{align} \\] <p>, where t_base is base position measured by GNSS or other means. NDT(R,t) stands for the pure NDT cost function. The regularization term shifts the optimal solution to the base position in the longitudinal direction of the vehicle. Only errors along the longitudinal direction with respect to the base position are considered; errors along Z-axis and lateral-axis error are not considered.</p> <p>Although the regularization term has rotation as a parameter, the gradient and hessian associated with it is not computed to stabilize the optimization. Specifically, the gradients are computed as follows.</p> \\[ \\begin{align}     &amp;g_x=\\nabla_x \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) + 2 \\mathrm{scale\\ factor} \\cos\\theta_z\\cdot e_{\\mathrm{longitudinal}}     \\\\     &amp;g_y=\\nabla_y \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) + 2 \\mathrm{scale\\ factor} \\sin\\theta_z\\cdot e_{\\mathrm{longitudinal}}     \\\\     &amp;g_z=\\nabla_z \\mathrm{NDT}(\\mathbf{R},\\mathbf{t})     \\\\     &amp;g_\\mathbf{R}=\\nabla_\\mathbf{R} \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) \\end{align} \\] <p>Regularization is disabled by default. If you wish to use it, please edit the following parameters to enable it.</p>"},{"location":"localization/ndt_scan_matcher/#where-is-regularization-available","title":"Where is regularization available","text":"<p>This feature is effective on feature-less roads where GNSS is available, such as</p> <ul> <li>bridges</li> <li>highways</li> <li>farm roads</li> </ul> <p>By remapping the base position topic to something other than GNSS, as described below, it can be valid outside of these.</p>"},{"location":"localization/ndt_scan_matcher/#using-other-base-position","title":"Using other base position","text":"<p>Other than GNSS, you can give other global position topics obtained from magnetic markers, visual markers or etc. if they are available in your environment. (Currently Autoware does not provide a node that gives such pose.) To use your topic for regularization, you need to remap the <code>input_regularization_pose_topic</code> with your topic in <code>ndt_scan_matcher.launch.xml</code>. By default, it is remapped with <code>/sensing/gnss/pose_with_covariance</code>.</p>"},{"location":"localization/ndt_scan_matcher/#limitations","title":"Limitations","text":"<p>Since this function determines the base position by linear interpolation from the recently subscribed poses, topics that are published at a low frequency relative to the driving speed cannot be used. Inappropriate linear interpolation may result in bad optimization results.</p> <p>When using GNSS for base location, the regularization can have negative effects in tunnels, indoors, and near skyscrapers. This is because if the base position is far off from the true value, NDT scan matching may converge to inappropriate optimal position.</p>"},{"location":"localization/ndt_scan_matcher/#parameters_1","title":"Parameters","text":"Name Type Description <code>regularization_enabled</code> bool Flag to add regularization term to NDT optimization (FALSE by default) <code>regularization_scale_factor</code> double Coefficient of the regularization term. <p>Regularization is disabled by default because GNSS is not always accurate enough to serve the appropriate base position in any scenes.</p> <p>If the scale_factor is too large, the NDT will be drawn to the base position and scan matching may fail. Conversely, if it is too small, the regularization benefit will be lost.</p> <p>Note that setting scale_factor to 0 is equivalent to disabling regularization.</p>"},{"location":"localization/ndt_scan_matcher/#example","title":"Example","text":"<p>The following figures show tested maps.</p> <ul> <li>The left is a map with enough features that NDT can successfully localize.</li> <li>The right is a map with so few features that the NDT cannot localize well.</li> </ul> <p> </p> <p>The following figures show the trajectories estimated on the feature-less map with standard NDT and regularization-enabled NDT, respectively. The color of the trajectory indicates the error (meter) from the reference trajectory, which is computed with the feature-rich map.</p> <ul> <li>The left figure shows that the pure NDT causes a longitudinal error in the bridge and is not able to recover.</li> <li>The right figure shows that the regularization suppresses the longitudinal error.</li> </ul> <p> </p>"},{"location":"localization/ndt_scan_matcher/#dynamic-map-loading","title":"Dynamic map loading","text":"<p>Autoware supports dynamic map loading feature for <code>ndt_scan_matcher</code>. Using this feature, NDT dynamically requests for the surrounding pointcloud map to <code>pointcloud_map_loader</code>, and then receive and preprocess the map in an online fashion.</p> <p>Using the feature, <code>ndt_scan_matcher</code> can theoretically handle any large size maps in terms of memory usage. (Note that it is still possible that there exists a limitation due to other factors, e.g. floating-point error)</p> <p></p>"},{"location":"localization/ndt_scan_matcher/#additional-interfaces","title":"Additional interfaces","text":""},{"location":"localization/ndt_scan_matcher/#additional-inputs","title":"Additional inputs","text":"Name Type Description <code>input_ekf_odom</code> <code>nav_msgs::msg::Odometry</code> Vehicle localization results (used for map update decision)"},{"location":"localization/ndt_scan_matcher/#additional-outputs","title":"Additional outputs","text":"Name Type Description <code>debug/loaded_pointcloud_map</code> <code>sensor_msgs::msg::PointCloud2</code> pointcloud maps used for localization (for debug)"},{"location":"localization/ndt_scan_matcher/#additional-client","title":"Additional client","text":"Name Type Description <code>client_map_loader</code> <code>autoware_map_msgs::srv::GetDifferentialPointCloudMap</code> map loading client"},{"location":"localization/ndt_scan_matcher/#parameters_2","title":"Parameters","text":"Name Type Description <code>use_dynamic_map_loading</code> bool Flag to enable dynamic map loading feature for NDT (TRUE by default) <code>dynamic_map_loading_update_distance</code> double Distance traveled to load new map(s) <code>dynamic_map_loading_map_radius</code> double Map loading radius for every update <code>lidar_radius</code> double LiDAR radius used for localization (only used for diagnosis)"},{"location":"localization/ndt_scan_matcher/#enabling-the-dynamic-map-loading-feature","title":"Enabling the dynamic map loading feature","text":"<p>To use dynamic map loading feature for <code>ndt_scan_matcher</code>, you also need to appropriately configure some other settings outside of this node. Follow the next two instructions.</p> <ol> <li>enable dynamic map loading interface in <code>pointcloud_map_loader</code> (by setting <code>enable_differential_load</code> to true in the package)</li> <li>split the PCD files into grids (recommended size: 20[m] x 20[m])</li> </ol> <p>Note that the dynamic map loading may FAIL if the map is split into two or more large size map (e.g. 1000[m] x 1000[m]). Please provide either of</p> <ul> <li>one PCD map file</li> <li>multiple PCD map files divided into small size (~20[m])</li> </ul> <p>Here is a split PCD map for <code>sample-map-rosbag</code> from Autoware tutorial: <code>sample-map-rosbag_split.zip</code></p> PCD files <code>use_dynamic_map_loading</code> <code>enable_differential_load</code> How NDT loads map(s) single file true true at once (standard) single file true false does NOT work single file false true/false at once (standard) multiple files true true dynamically multiple files true false does NOT work multiple files false true/false at once (standard)"},{"location":"localization/ndt_scan_matcher/#scan-matching-score-based-on-de-grounded-lidar-scan","title":"Scan matching score based on de-grounded LiDAR scan","text":""},{"location":"localization/ndt_scan_matcher/#abstract_1","title":"Abstract","text":"<p>This is a function that uses de-grounded LiDAR scan to estimate the scan matching score. This score can reflect the current localization performance more accurately. related issue.</p>"},{"location":"localization/ndt_scan_matcher/#parameters_3","title":"Parameters","text":"Name Type Description <code>estimate_scores_for_degrounded_scan</code> bool Flag for using scan matching score based on de-grounded LiDAR scan (FALSE by default) <code>z_margin_for_ground_removal</code> double Z-value margin for removal ground points"},{"location":"localization/ndt_scan_matcher/#2d-real-time-covariance-estimation","title":"2D real-time covariance estimation","text":""},{"location":"localization/ndt_scan_matcher/#abstract_2","title":"Abstract","text":"<p>Calculate 2D covariance (xx, xy, yx, yy) in real time using the NDT convergence from multiple initial poses. The arrangement of multiple initial poses is efficiently limited by the Hessian matrix of the NDT score function. In this implementation, the number of initial positions is fixed to simplify the code. The covariance can be seen as error ellipse from ndt_pose_with_covariance setting on rviz2. original paper.</p> <p>Note that this function may spoil healthy system behavior if it consumes much calculation resources.</p>"},{"location":"localization/ndt_scan_matcher/#parameters_4","title":"Parameters","text":"<p>initial_pose_offset_model is rotated around (x,y) = (0,0) in the direction of the first principal component of the Hessian matrix. initial_pose_offset_model_x &amp; initial_pose_offset_model_y must have the same number of elements.</p> Name Type Description <code>use_covariance_estimation</code> bool Flag for using real-time covariance estimation (FALSE by default) <code>initial_pose_offset_model_x</code> std::vector X-axis offset [m] <code>initial_pose_offset_model_y</code> std::vector Y-axis offset [m]"},{"location":"localization/pose2twist/","title":"pose2twist","text":""},{"location":"localization/pose2twist/#pose2twist","title":"pose2twist","text":""},{"location":"localization/pose2twist/#purpose","title":"Purpose","text":"<p>This <code>pose2twist</code> calculates the velocity from the input pose history. In addition to the computed twist, this node outputs the linear-x and angular-z components as a float message to simplify debugging.</p> <p>The <code>twist.linear.x</code> is calculated as <code>sqrt(dx * dx + dy * dy + dz * dz) / dt</code>, and the values in the <code>y</code> and <code>z</code> fields are zero. The <code>twist.angular</code> is calculated as <code>d_roll / dt</code>, <code>d_pitch / dt</code> and <code>d_yaw / dt</code> for each field.</p>"},{"location":"localization/pose2twist/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/pose2twist/#input","title":"Input","text":"Name Type Description pose geometry_msgs::msg::PoseStamped pose source to used for the velocity calculation."},{"location":"localization/pose2twist/#output","title":"Output","text":"Name Type Description twist geometry_msgs::msg::TwistStamped twist calculated from the input pose history. linear_x tier4_debug_msgs::msg::Float32Stamped linear-x field of the output twist. angular_z tier4_debug_msgs::msg::Float32Stamped angular-z field of the output twist."},{"location":"localization/pose2twist/#parameters","title":"Parameters","text":"<p>none.</p>"},{"location":"localization/pose2twist/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>none.</p>"},{"location":"localization/pose_initializer/","title":"pose_initializer","text":""},{"location":"localization/pose_initializer/#pose_initializer","title":"pose_initializer","text":""},{"location":"localization/pose_initializer/#purpose","title":"Purpose","text":"<p>The <code>pose_initializer</code> is the package to send an initial pose to <code>ekf_localizer</code>. It receives roughly estimated initial pose from GNSS/user. Passing the pose to <code>ndt_scan_matcher</code>, and it gets a calculated ego pose from <code>ndt_scan_matcher</code> via service. Finally, it publishes the initial pose to <code>ekf_localizer</code>. This node depends on the map height fitter library. See here for more details.</p>"},{"location":"localization/pose_initializer/#interfaces","title":"Interfaces","text":""},{"location":"localization/pose_initializer/#parameters","title":"Parameters","text":"Name Type Description <code>ekf_enabled</code> bool If true, EKF localizer is activated. <code>ndt_enabled</code> bool If true, the pose will be estimated by NDT scan matcher, otherwise it is passed through. <code>stop_check_enabled</code> bool If true, initialization is accepted only when the vehicle is stopped. <code>stop_check_duration</code> bool The duration used for the stop check above. <code>gnss_enabled</code> bool If true, use the GNSS pose when no pose is specified. <code>gnss_pose_timeout</code> bool The duration that the GNSS pose is valid."},{"location":"localization/pose_initializer/#services","title":"Services","text":"Name Type Description <code>/localization/initialize</code> autoware_adapi_v1_msgs::srv::InitializeLocalization initial pose from api"},{"location":"localization/pose_initializer/#clients","title":"Clients","text":"Name Type Description <code>/localization/pose_estimator/ndt_align_srv</code> tier4_localization_msgs::srv::PoseWithCovarianceStamped pose estimation service"},{"location":"localization/pose_initializer/#subscriptions","title":"Subscriptions","text":"Name Type Description <code>/sensing/gnss/pose_with_covariance</code> geometry_msgs::msg::PoseWithCovarianceStamped pose from gnss <code>/sensing/vehicle_velocity_converter/twist_with_covariance</code> geometry_msgs::msg::TwistStamped twist for stop check"},{"location":"localization/pose_initializer/#publications","title":"Publications","text":"Name Type Description <code>/localization/initialization_state</code> autoware_adapi_v1_msgs::msg::LocalizationInitializationState pose initialization state <code>/initialpose3d</code> geometry_msgs::msg::PoseWithCovarianceStamped calculated initial ego pose"},{"location":"localization/pose_initializer/#connection-with-default-ad-api","title":"Connection with Default AD API","text":"<p>This <code>pose_initializer</code> is used via default AD API. For detailed description of the API description, please refer to the description of <code>default_ad_api</code>.</p> <p></p>"},{"location":"localization/pose_instability_detector/","title":"pose_instability_detector","text":""},{"location":"localization/pose_instability_detector/#pose_instability_detector","title":"pose_instability_detector","text":"<p>The <code>pose_instability_detector</code> package includes a node designed to monitor the stability of <code>/localization/kinematic_state</code>, which is an output topic of the Extended Kalman Filter (EKF).</p> <p>This node triggers periodic timer callbacks to compare two poses:</p> <ul> <li>The pose obtained by integrating the twist values from the last received message on <code>/localization/kinematic_state</code> over a duration specified by <code>interval_sec</code>.</li> <li>The latest pose from <code>/localization/kinematic_state</code>.</li> </ul> <p>The results of this comparison are then output to the <code>/diagnostics</code> topic.</p> <p>If this node outputs WARN messages to <code>/diagnostics</code>, it means that the EKF output is significantly different from the integrated twist values. This discrepancy suggests that there may be an issue with either the estimated pose or the input twist.</p> <p>The following diagram provides an overview of what the timeline of this process looks like:</p> <p></p>"},{"location":"localization/pose_instability_detector/#parameters","title":"Parameters","text":"Name Type Description Default Range interval_sec float The interval of timer_callback in seconds. 1 &gt;0 threshold_diff_position_x float The threshold of diff_position x (m). 1 \u22650.0 threshold_diff_position_y float The threshold of diff_position y (m). 1 \u22650.0 threshold_diff_position_z float The threshold of diff_position z (m). 1 \u22650.0 threshold_diff_angle_x float The threshold of diff_angle x (rad). 1 \u22650.0 threshold_diff_angle_y float The threshold of diff_angle y (rad). 1 \u22650.0 threshold_diff_angle_z float The threshold of diff_angle z (rad). 1 \u22650.0"},{"location":"localization/pose_instability_detector/#input","title":"Input","text":"Name Type Description <code>~/input/odometry</code> nav_msgs::msg::Odometry Pose estimated by EKF <code>~/input/twist</code> geometry_msgs::msg::TwistWithCovarianceStamped Twist"},{"location":"localization/pose_instability_detector/#output","title":"Output","text":"Name Type Description <code>~/debug/diff_pose</code> geometry_msgs::msg::PoseStamped diff_pose <code>/diagnostics</code> diagnostic_msgs::msg::DiagnosticArray Diagnostics"},{"location":"localization/stop_filter/","title":"stop_filter","text":""},{"location":"localization/stop_filter/#stop_filter","title":"stop_filter","text":""},{"location":"localization/stop_filter/#purpose","title":"Purpose","text":"<p>When this function did not exist, each node used a different criterion to determine whether the vehicle is stopping or not, resulting that some nodes were in operation of stopping the vehicle and some nodes continued running in the drive mode. This node aims to:</p> <ul> <li>apply a uniform stopping decision criterion to several nodes.</li> <li>suppress the control noise by overwriting the velocity and angular velocity with zero.</li> </ul>"},{"location":"localization/stop_filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/stop_filter/#input","title":"Input","text":"Name Type Description <code>input/odom</code> <code>nav_msgs::msg::Odometry</code> localization odometry"},{"location":"localization/stop_filter/#output","title":"Output","text":"Name Type Description <code>output/odom</code> <code>nav_msgs::msg::Odometry</code> odometry with suppressed longitudinal and yaw twist <code>debug/stop_flag</code> <code>tier4_debug_msgs::msg::BoolStamped</code> flag to represent whether the vehicle is stopping or not"},{"location":"localization/stop_filter/#parameters","title":"Parameters","text":"Name Type Description Default Range vx_threshold float Longitudinal velocity threshold to determine if the vehicle is stopping. [m/s] 0.01 \u22650.0 wz_threshold float Yaw velocity threshold to determine if the vehicle is stopping. [rad/s] 0.01 \u22650.0"},{"location":"localization/tree_structured_parzen_estimator/","title":"tree_structured_parzen_estimator","text":""},{"location":"localization/tree_structured_parzen_estimator/#tree_structured_parzen_estimator","title":"tree_structured_parzen_estimator","text":"<p>`tree_structured_parzen_estimator`` is a package for black-box optimization.</p> <p>This package does not have a node, it is just a library.</p>"},{"location":"localization/twist2accel/","title":"twist2accel","text":""},{"location":"localization/twist2accel/#twist2accel","title":"twist2accel","text":""},{"location":"localization/twist2accel/#purpose","title":"Purpose","text":"<p>This package is responsible for estimating acceleration using the output of <code>ekf_localizer</code>. It uses lowpass filter to mitigate the noise.</p>"},{"location":"localization/twist2accel/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/twist2accel/#input","title":"Input","text":"Name Type Description <code>input/odom</code> <code>nav_msgs::msg::Odometry</code> localization odometry <code>input/twist</code> <code>geometry_msgs::msg::TwistWithCovarianceStamped</code> twist"},{"location":"localization/twist2accel/#output","title":"Output","text":"Name Type Description <code>output/accel</code> <code>geometry_msgs::msg::AccelWithCovarianceStamped</code> estimated acceleration"},{"location":"localization/twist2accel/#parameters","title":"Parameters","text":"Name Type Description <code>use_odom</code> bool use odometry if true, else use twist input (default: true) <code>accel_lowpass_gain</code> double lowpass gain for lowpass filter in estimating acceleration (default: 0.9)"},{"location":"localization/twist2accel/#future-work","title":"Future work","text":"<p>Future work includes integrating acceleration into the EKF state.</p>"},{"location":"localization/yabloc/","title":"YabLoc","text":""},{"location":"localization/yabloc/#yabloc","title":"YabLoc","text":"<p>YabLoc is vision-based localization with vector map. https://youtu.be/Eaf6r_BNFfk</p> <p></p> <p>It estimates position by matching road surface markings extracted from images with a vector map. Point cloud maps and LiDAR are not required. YabLoc enables users localize vehicles that are not equipped with LiDAR and in environments where point cloud maps are not available.</p>"},{"location":"localization/yabloc/#packages","title":"Packages","text":"<ul> <li>yabloc_common</li> <li>yabloc_image_processing</li> <li>yabloc_particle_filter</li> <li>yabloc_pose_initializer</li> </ul>"},{"location":"localization/yabloc/#how-to-launch-yabloc-instead-of-ndt","title":"How to launch YabLoc instead of NDT","text":"<p>When launching autoware, if you set <code>pose_source:=yabloc</code> as an argument, YabLoc will be launched instead of NDT. By default, <code>pose_source</code> is <code>ndt</code>.</p> <p>A sample command to run YabLoc is as follows</p> <pre><code>ros2 launch autoware_launch logging_simulator.launch.xml \\\nmap_path:=$HOME/autoware_map/sample-map-rosbag\\\nvehicle_model:=sample_vehicle \\\nsensor_model:=sample_sensor_kit \\\npose_source:=yabloc\n</code></pre>"},{"location":"localization/yabloc/#architecture","title":"Architecture","text":""},{"location":"localization/yabloc/#principle","title":"Principle","text":"<p>The diagram below illustrates the basic principle of YabLoc. It extracts road surface markings by extracting the line segments using the road area obtained from graph-based segmentation. The red line at the center-top of the diagram represents the line segments identified as road surface markings. YabLoc transforms these segments for each particle and determines the particle's weight by comparing them with the cost map generated from Lanelet2.</p> <p></p>"},{"location":"localization/yabloc/#visualization","title":"Visualization","text":""},{"location":"localization/yabloc/#core-visualization-topics","title":"Core visualization topics","text":"<p>These topics are not visualized by default.</p> <p></p> index topic name description 1 <code>/localization/yabloc/pf/predicted_particle_marker</code> particle distribution of particle filter. Red particles are probable candidate. 2 <code>/localization/yabloc/pf/scored_cloud</code> 3D projected line segments. the color indicates how well they match the map. 3 <code>/localization/yabloc/image_processing/lanelet2_overlay_image</code> overlay of lanelet2 (yellow lines) onto image based on estimated pose. If they match well with the actual road markings, it means that the localization performs well."},{"location":"localization/yabloc/#image-topics-for-debug","title":"Image topics for debug","text":"<p>These topics are not visualized by default.</p> <p></p> index topic name description 1 <code>/localization/yabloc/pf/cost_map_image</code> cost map made from lanelet2 2 <code>/localization/yabloc/pf/match_image</code> projected line segments 3 <code>/localization/yabloc/image_processing/image_with_colored_line_segment</code> classified line segments. green line segments are used in particle correction 4 <code>/localization/yabloc/image_processing/lanelet2_overlay_image</code> overlay of lanelet2 5 <code>/localization/yabloc/image_processing/segmented_image</code> graph based segmentation result"},{"location":"localization/yabloc/#limitation","title":"Limitation","text":"<ul> <li>Running YabLoc and NDT simultaneously is not supported.<ul> <li>This is because running both at the same time may be computationally too expensive.</li> <li>Also, in most cases, NDT is superior to YabLoc, so there is less benefit to running them at the same time.</li> </ul> </li> <li>It does not estimate roll and pitch, therefore some of the perception nodes may not work well.</li> <li>It does not support multiple cameras now. But it will in the future.</li> <li>In places where there are few road surface markings, such as intersections, the estimation heavily relies on GNSS, IMU, and vehicle's wheel odometry.</li> <li>If the road boundary or road surface markings are not included in the Lanelet2, the estimation is likely to fail.</li> <li>The sample rosbag provided in the autoware tutorial does not include images, so it is not possible to run YabLoc with it.<ul> <li>If you want to test the functionality of YabLoc, the sample test data provided in this PR is useful.</li> </ul> </li> </ul>"},{"location":"localization/yabloc/yabloc_common/","title":"yabloc_common","text":""},{"location":"localization/yabloc/yabloc_common/#yabloc_common","title":"yabloc_common","text":"<p>This package contains some executable nodes related to map. Also, This provides some yabloc common library.</p> <ul> <li>ground_server</li> <li>ll2_decomposer</li> </ul>"},{"location":"localization/yabloc/yabloc_common/#ground_server","title":"ground_server","text":""},{"location":"localization/yabloc/yabloc_common/#purpose","title":"Purpose","text":"<p>It estimates the height and tilt of the ground from lanelet2.</p>"},{"location":"localization/yabloc/yabloc_common/#input-outputs","title":"Input / Outputs","text":""},{"location":"localization/yabloc/yabloc_common/#input","title":"Input","text":"Name Type Description <code>input/vector_map</code> <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> vector map <code>input/pose</code> <code>geometry_msgs::msg::PoseStamped</code> estimated self pose"},{"location":"localization/yabloc/yabloc_common/#output","title":"Output","text":"Name Type Description <code>output/ground</code> <code>std_msgs::msg::Float32MultiArray</code> estimated ground parameters. it contains x, y, z, normal_x, normal_y, normal_z. <code>output/ground_markers</code> <code>visualization_msgs::msg::Marker</code> visualization of estimated ground plane <code>output/ground_status</code> <code>std_msgs::msg::String</code> status log of ground plane estimation <code>output/height</code> <code>std_msgs::msg::Float32</code> altitude <code>output/near_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> point cloud extracted from lanelet2 and used for ground tilt estimation"},{"location":"localization/yabloc/yabloc_common/#parameters","title":"Parameters","text":"Name Type Description <code>force_zero_tilt</code> bool if true, the tilt is always determined to be horizontal. <code>K</code> int parameter for nearest k search <code>R</code> int parameter for radius search"},{"location":"localization/yabloc/yabloc_common/#ll2_decomposer","title":"ll2_decomposer","text":""},{"location":"localization/yabloc/yabloc_common/#purpose_1","title":"Purpose","text":"<p>This node extracts the elements related to the road surface markings and yabloc from lanelet2.</p>"},{"location":"localization/yabloc/yabloc_common/#input-outputs_1","title":"Input / Outputs","text":""},{"location":"localization/yabloc/yabloc_common/#input_1","title":"Input","text":"Name Type Description <code>input/vector_map</code> <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> vector map"},{"location":"localization/yabloc/yabloc_common/#output_1","title":"Output","text":"Name Type Description <code>output/ll2_bounding_box</code> <code>sensor_msgs::msg::PointCloud2</code> bounding boxes extracted from lanelet2 <code>output/ll2_road_marking</code> <code>sensor_msgs::msg::PointCloud2</code> road surface markings extracted from lanelet2 <code>output/ll2_sign_board</code> <code>sensor_msgs::msg::PointCloud2</code> traffic sign boards extracted from lanelet2 <code>output/sign_board_marker</code> <code>visualization_msgs::msg::MarkerArray</code> visualized traffic sign boards"},{"location":"localization/yabloc/yabloc_common/#parameters_1","title":"Parameters","text":"Name Type Description <code>road_marking_labels</code> vector\\&lt;string&gt; This label is used to extract the road surface markings from lanelet2. <code>sign_board_labels</code> vector\\&lt;string&gt; This label is used to extract the traffic sign boards from lanelet2. <code>bounding_box_labels</code> vector\\&lt;string&gt; This label is used to extract the bounding boxes from lanelet2."},{"location":"localization/yabloc/yabloc_image_processing/","title":"yabloc_image_processing","text":""},{"location":"localization/yabloc/yabloc_image_processing/#yabloc_image_processing","title":"yabloc_image_processing","text":"<p>This package contains some executable nodes related to image processing.</p> <ul> <li>line_segment_detector</li> <li>graph_segmentation</li> <li>segment_filter</li> <li>undistort</li> <li>lanelet2_overlay</li> <li>line_segments_overlay</li> </ul>"},{"location":"localization/yabloc/yabloc_image_processing/#line_segment_detector","title":"line_segment_detector","text":""},{"location":"localization/yabloc/yabloc_image_processing/#purpose","title":"Purpose","text":"<p>This node extract all line segments from gray scale image.</p>"},{"location":"localization/yabloc/yabloc_image_processing/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_image_processing/#input","title":"Input","text":"Name Type Description <code>input/image_raw</code> <code>sensor_msgs::msg::Image</code> undistorted image"},{"location":"localization/yabloc/yabloc_image_processing/#output","title":"Output","text":"Name Type Description <code>output/image_with_line_segments</code> <code>sensor_msgs::msg::Image</code> image with line segments highlighted <code>output/line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> detected line segments as point cloud. each point contains x,y,z, normal_x, normal_y, normal_z and z, and normal_z are always empty."},{"location":"localization/yabloc/yabloc_image_processing/#graph_segmentation","title":"graph_segmentation","text":""},{"location":"localization/yabloc/yabloc_image_processing/#purpose_1","title":"Purpose","text":"<p>This node extract road surface region by graph-based-segmentation.</p>"},{"location":"localization/yabloc/yabloc_image_processing/#inputs-outputs_1","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_image_processing/#input_1","title":"Input","text":"Name Type Description <code>input/image_raw</code> <code>sensor_msgs::msg::Image</code> undistorted image"},{"location":"localization/yabloc/yabloc_image_processing/#output_1","title":"Output","text":"Name Type Description <code>output/mask_image</code> <code>sensor_msgs::msg::Image</code> image with masked segments determined as road surface area <code>output/segmented_image</code> <code>sensor_msgs::msg::Image</code> segmented image for visualization"},{"location":"localization/yabloc/yabloc_image_processing/#parameters","title":"Parameters","text":"Name Type Description <code>target_height_ratio</code> double height on the image to retrieve the candidate road surface <code>target_candidate_box_width</code> int size of the square area to search for candidate road surfaces <code>pickup_additional_graph_segment</code> bool if this is true, additional regions of similar color are retrieved <code>similarity_score_threshold</code> double threshold for picking up additional areas <code>sigma</code> double parameters for cv::ximgproc::segmentation <code>k</code> double parameters for cv::ximgproc::segmentation <code>min_size</code> double parameters for cv::ximgproc::segmentation"},{"location":"localization/yabloc/yabloc_image_processing/#segment_filter","title":"segment_filter","text":""},{"location":"localization/yabloc/yabloc_image_processing/#purpose_2","title":"Purpose","text":"<p>This is a node that integrates the results of graph_segment and lsd to extract road surface markings.</p>"},{"location":"localization/yabloc/yabloc_image_processing/#inputs-outputs_2","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_image_processing/#input_2","title":"Input","text":"Name Type Description <code>input/line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> detected line segment <code>input/mask_image</code> <code>sensor_msgs::msg::Image</code> image with masked segments determined as road surface area <code>input/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> undistorted camera info"},{"location":"localization/yabloc/yabloc_image_processing/#output_2","title":"Output","text":"Name Type Description <code>output/line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> filtered line segments for visualization <code>output/projected_image</code> <code>sensor_msgs::msg::Image</code> projected filtered line segments for visualization <code>output/projected_line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> projected filtered line segments"},{"location":"localization/yabloc/yabloc_image_processing/#parameters_1","title":"Parameters","text":"Name Type Description <code>min_segment_length</code> double min length threshold (if it is negative, it is unlimited) <code>max_segment_distance</code> double max distance threshold (if it is negative, it is unlimited) <code>max_lateral_distance</code> double max lateral distance threshold (if it is negative, it is unlimited) <code>publish_image_with_segment_for_debug</code> bool toggle whether to publish the filtered line segment for debug <code>max_range</code> double range of debug projection visualization <code>image_size</code> int image size of debug projection visualization"},{"location":"localization/yabloc/yabloc_image_processing/#undistort","title":"undistort","text":""},{"location":"localization/yabloc/yabloc_image_processing/#purpose_3","title":"Purpose","text":"<p>This node performs image resizing and undistortion at the same time.</p>"},{"location":"localization/yabloc/yabloc_image_processing/#inputs-outputs_3","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_image_processing/#input_3","title":"Input","text":"Name Type Description <code>input/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> camera info <code>input/image_raw</code> <code>sensor_msgs::msg::Image</code> raw camera image <code>input/image_raw/compressed</code> <code>sensor_msgs::msg::CompressedImage</code> compressed camera image <p>This node subscribes to both compressed image and raw image topics. If raw image is subscribed to even once, compressed image will no longer be subscribed to. This is to avoid redundant decompression within Autoware.</p>"},{"location":"localization/yabloc/yabloc_image_processing/#output_3","title":"Output","text":"Name Type Description <code>output/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> resized camera info <code>output/image_raw</code> <code>sensor_msgs::msg::CompressedImage</code> undistorted and resized image"},{"location":"localization/yabloc/yabloc_image_processing/#parameters_2","title":"Parameters","text":"Name Type Description <code>use_sensor_qos</code> bool where to use sensor qos or not <code>width</code> int resized image width size <code>override_frame_id</code> string value for overriding the camera's frame_id. if blank, frame_id of static_tf is not overwritten"},{"location":"localization/yabloc/yabloc_image_processing/#about-tf_static-overriding","title":"about tf_static overriding","text":"click to open  Some nodes requires `/tf_static` from `/base_link` to the frame_id of `/sensing/camera/traffic_light/image_raw/compressed` (e.g. `/traffic_light_left_camera/camera_optical_link`). You can verify that the tf_static is correct with the following command.  <pre><code>ros2 run tf2_ros tf2_echo base_link traffic_light_left_camera/camera_optical_link\n</code></pre>  If the wrong `/tf_static` are broadcasted due to using a prototype vehicle, not having accurate calibration data, or some other unavoidable reason, it is useful to give the frame_id in `override_camera_frame_id`. If you give it a non-empty string, `/image_processing/undistort_node` will rewrite the frame_id in camera_info. For example, you can give a different tf_static as follows.  <pre><code>ros2 launch yabloc_launch sample_launch.xml override_camera_frame_id:=fake_camera_optical_link\nros2 run tf2_ros static_transform_publisher \\\n--frame-id base_link \\\n--child-frame-id fake_camera_optical_link \\\n--roll -1.57 \\\n--yaw -1.570\n</code></pre>"},{"location":"localization/yabloc/yabloc_image_processing/#lanelet2_overlay","title":"lanelet2_overlay","text":""},{"location":"localization/yabloc/yabloc_image_processing/#purpose_4","title":"Purpose","text":"<p>This node overlays lanelet2 on the camera image based on the estimated self-position.</p>"},{"location":"localization/yabloc/yabloc_image_processing/#inputs-outputs_4","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_image_processing/#input_4","title":"Input","text":"Name Type Description <code>input/pose</code> <code>geometry_msgs::msg::PoseStamped</code> estimated self pose <code>input/projected_line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> projected line segments including non-road markings <code>input/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> undistorted camera info <code>input/image_raw</code> <code>sensor_msgs::msg::Image</code> undistorted camera image <code>input/ground</code> <code>std_msgs::msg::Float32MultiArray</code> ground tilt <code>input/ll2_road_marking</code> <code>sensor_msgs::msg::PointCloud2</code> lanelet2 elements regarding road surface markings <code>input/ll2_sign_board</code> <code>sensor_msgs::msg::PointCloud2</code> lanelet2 elements regarding traffic sign boards"},{"location":"localization/yabloc/yabloc_image_processing/#output_4","title":"Output","text":"Name Type Description <code>output/lanelet2_overlay_image</code> <code>sensor_msgs::msg::Image</code> lanelet2 overlaid image <code>output/projected_marker</code> <code>visualization_msgs::msg::Marker</code> 3d projected line segments including non-road markings"},{"location":"localization/yabloc/yabloc_image_processing/#line_segments_overlay","title":"line_segments_overlay","text":""},{"location":"localization/yabloc/yabloc_image_processing/#purpose_5","title":"Purpose","text":"<p>This node visualize classified line segments on the camera image</p>"},{"location":"localization/yabloc/yabloc_image_processing/#inputs-outputs_5","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_image_processing/#input_5","title":"Input","text":"Name Type Description <code>input/line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> classified line segments <code>input/image_raw</code> <code>sensor_msgs::msg::Image</code> undistorted camera image"},{"location":"localization/yabloc/yabloc_image_processing/#output_5","title":"Output","text":"Name Type Description <code>output/image_with_colored_line_segments</code> <code>sensor_msgs::msg::Image</code> image with highlighted line segments"},{"location":"localization/yabloc/yabloc_monitor/","title":"yabloc_monitor","text":""},{"location":"localization/yabloc/yabloc_monitor/#yabloc_monitor","title":"yabloc_monitor","text":"<p>YabLoc monitor is a node that monitors the status of the YabLoc localization system. It is a wrapper node that monitors the status of the YabLoc localization system and publishes the status as diagnostics.</p>"},{"location":"localization/yabloc/yabloc_monitor/#feature","title":"Feature","text":""},{"location":"localization/yabloc/yabloc_monitor/#availability","title":"Availability","text":"<p>The node monitors the final output pose of YabLoc to verify the availability of YabLoc.</p>"},{"location":"localization/yabloc/yabloc_monitor/#others","title":"Others","text":"<p>To be added,</p>"},{"location":"localization/yabloc/yabloc_monitor/#interfaces","title":"Interfaces","text":""},{"location":"localization/yabloc/yabloc_monitor/#input","title":"Input","text":"Name Type Description <code>~/input/yabloc_pose</code> <code>geometry_msgs/PoseStamped</code> The final output pose of YabLoc"},{"location":"localization/yabloc/yabloc_monitor/#output","title":"Output","text":"Name Type Description <code>/diagnostics</code> <code>diagnostic_msgs/DiagnosticArray</code> Diagnostics outputs"},{"location":"localization/yabloc/yabloc_particle_filter/","title":"yabLoc_particle_filter","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#yabloc_particle_filter","title":"yabLoc_particle_filter","text":"<p>This package contains some executable nodes related to particle filter.</p> <ul> <li>particle_predictor</li> <li>gnss_particle_corrector</li> <li>camera_particle_corrector</li> </ul>"},{"location":"localization/yabloc/yabloc_particle_filter/#particle_predictor","title":"particle_predictor","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#purpose","title":"Purpose","text":"<ul> <li>This node performs predictive updating and resampling of particles.</li> <li>It retroactively reflects the particle weights determined by the corrector node.</li> </ul>"},{"location":"localization/yabloc/yabloc_particle_filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#input","title":"Input","text":"Name Type Description <code>input/initialpose</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> to specify the initial position of particles <code>input/twist_with_covariance</code> <code>geometry_msgs::msg::TwistWithCovarianceStamped</code> linear velocity and angular velocity of prediction update <code>input/height</code> <code>std_msgs::msg::Float32</code> ground height <code>input/weighted_particles</code> <code>yabloc_particle_filter::msg::ParticleArray</code> particles weighted by corrector nodes"},{"location":"localization/yabloc/yabloc_particle_filter/#output","title":"Output","text":"Name Type Description <code>output/pose_with_covariance</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> particle centroid with covariance <code>output/pose</code> <code>geometry_msgs::msg::PoseStamped</code> particle centroid with covariance <code>output/predicted_particles</code> <code>yabloc_particle_filter::msg::ParticleArray</code> particles weighted by predictor nodes <code>debug/init_marker</code> <code>visualization_msgs::msg::Marker</code> debug visualization of initial position <code>debug/particles_marker_array</code> <code>visualization_msgs::msg::MarkerArray</code> particles visualization. published if <code>visualize</code> is true"},{"location":"localization/yabloc/yabloc_particle_filter/#parameters","title":"Parameters","text":"Name Type Description <code>visualize</code> bool whether particles are also published in visualization_msgs or not <code>static_linear_covariance</code> double to override the covariance of <code>/twist_with_covariance</code> <code>static_angular_covariance</code> double to override the covariance of <code>/twist_with_covariance</code> <code>resampling_interval_seconds</code> double the interval of particle resampling <code>num_of_particles</code> int the number of particles <code>prediction_rate</code> double frequency of forecast updates, in Hz <code>cov_xx_yy</code> vector\\&lt;double&gt; the covariance of initial pose"},{"location":"localization/yabloc/yabloc_particle_filter/#gnss_particle_corrector","title":"gnss_particle_corrector","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#purpose_1","title":"Purpose","text":"<ul> <li>This node estimated particles weight using GNSS.</li> <li>It supports two types of input: <code>ublox_msgs::msg::NavPVT</code> and <code>geometry_msgs::msg::PoseWithCovarianceStamped</code>.</li> </ul>"},{"location":"localization/yabloc/yabloc_particle_filter/#inputs-outputs_1","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#input_1","title":"Input","text":"Name Type Description <code>input/height</code> <code>std_msgs::msg::Float32</code> ground height <code>input/predicted_particles</code> <code>yabloc_particle_filter::msg::ParticleArray</code> predicted particles <code>input/pose_with_covariance</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> gnss measurement. used if <code>use_ublox_msg</code> is false <code>input/navpvt</code> <code>ublox_msgs::msg::NavPVT</code> gnss measurement. used if <code>use_ublox_msg</code> is true"},{"location":"localization/yabloc/yabloc_particle_filter/#output_1","title":"Output","text":"Name Type Description <code>output/weighted_particles</code> <code>yabloc_particle_filter::msg::ParticleArray</code> weighted particles <code>debug/gnss_range_marker</code> <code>visualization_msgs::msg::MarkerArray</code> gnss weight distribution <code>debug/particles_marker_array</code> <code>visualization_msgs::msg::MarkerArray</code> particles visualization. published if <code>visualize</code> is true"},{"location":"localization/yabloc/yabloc_particle_filter/#parameters_1","title":"Parameters","text":"Name Type Description <code>acceptable_max_delay</code> double how long to hold the predicted particles <code>visualize</code> double whether publish particles as marker_array or not <code>mahalanobis_distance_threshold</code> double if the Mahalanobis distance to the GNSS for particle exceeds this, the correction skips. <code>for_fixed/max_weight</code> bool parameter for gnss weight distribution <code>for_fixed/flat_radius</code> bool parameter for gnss weight distribution <code>for_fixed/max_radius</code> bool parameter for gnss weight distribution <code>for_fixed/min_weight</code> bool parameter for gnss weight distribution <code>for_not_fixed/flat_radius</code> bool parameter for gnss weight distribution <code>for_not_fixed/max_radius</code> bool parameter for gnss weight distribution <code>for_not_fixed/min_weight</code> bool parameter for gnss weight distribution <code>for_not_fixed/max_weight</code> bool parameter for gnss weight distribution"},{"location":"localization/yabloc/yabloc_particle_filter/#camera_particle_corrector","title":"camera_particle_corrector","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#purpose_2","title":"Purpose","text":"<ul> <li>This node estimated particles weight using GNSS.</li> </ul>"},{"location":"localization/yabloc/yabloc_particle_filter/#inputs-outputs_2","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#input_2","title":"Input","text":"Name Type Description <code>input/predicted_particles</code> <code>yabloc_particle_filter::msg::ParticleArray</code> predicted particles <code>input/ll2_bounding_box</code> <code>sensor_msgs::msg::PointCloud2</code> road surface markings converted to line segments <code>input/ll2_road_marking</code> <code>sensor_msgs::msg::PointCloud2</code> road surface markings converted to line segments <code>input/projected_line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> projected line segments <code>input/pose</code> <code>geometry_msgs::msg::PoseStamped</code> reference to retrieve the area map around the self location"},{"location":"localization/yabloc/yabloc_particle_filter/#output_2","title":"Output","text":"Name Type Description <code>output/weighted_particles</code> <code>yabloc_particle_filter::msg::ParticleArray</code> weighted particles <code>debug/cost_map_image</code> <code>sensor_msgs::msg::Image</code> cost map created from lanelet2 <code>debug/cost_map_range</code> <code>visualization_msgs::msg::MarkerArray</code> cost map boundary <code>debug/match_image</code> <code>sensor_msgs::msg::Image</code> projected line segments image <code>debug/scored_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> weighted 3d line segments <code>debug/scored_post_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> weighted 3d line segments which are iffy <code>debug/state_string</code> <code>std_msgs::msg::String</code> string describing the node state <code>debug/particles_marker_array</code> <code>visualization_msgs::msg::MarkerArray</code> particles visualization. published if <code>visualize</code> is true"},{"location":"localization/yabloc/yabloc_particle_filter/#parameters_2","title":"Parameters","text":"Name Type Description <code>acceptable_max_delay</code> double how long to hold the predicted particles <code>visualize</code> double whether publish particles as marker_array or not <code>image_size</code> int image size of debug/cost_map_image <code>max_range</code> double width of hierarchical cost map <code>gamma</code> double gamma value of the intensity gradient of the cost map <code>min_prob</code> double minimum particle weight the corrector node gives <code>far_weight_gain</code> double <code>exp(-far_weight_gain_ * squared_distance_from_camera)</code> is weight gain. if this is large, the nearby road markings will be more important <code>enabled_at_first</code> bool if it is false, this node is not activated at first. you can activate by service call"},{"location":"localization/yabloc/yabloc_particle_filter/#services","title":"Services","text":"Name Type Description <code>switch_srv</code> <code>std_srvs::srv::SetBool</code> activation and deactivation of correction"},{"location":"localization/yabloc/yabloc_pose_initializer/","title":"yabloc_pose_initializer","text":""},{"location":"localization/yabloc/yabloc_pose_initializer/#yabloc_pose_initializer","title":"yabloc_pose_initializer","text":"<p>This package contains a node related to initial pose estimation.</p> <ul> <li>camera_pose_initializer</li> </ul> <p>This package requires the pre-trained semantic segmentation model for runtime. This model is usually downloaded by <code>ansible</code> during env preparation phase of the installation. It is also possible to download it manually. Even if the model is not downloaded, initialization will still complete, but the accuracy may be compromised.</p> <p>To download and extract the model manually:</p> <pre><code>$ mkdir -p ~/autoware_data/yabloc_pose_initializer/\n$ wget -P ~/autoware_data/yabloc_pose_initializer/ \\\nhttps://s3.ap-northeast-2.wasabisys.com/pinto-model-zoo/136_road-segmentation-adas-0001/resources.tar.gz\n$ tar xzf ~/autoware_data/yabloc_pose_initializer/resources.tar.gz -C ~/autoware_data/yabloc_pose_initializer/\n</code></pre>"},{"location":"localization/yabloc/yabloc_pose_initializer/#note","title":"Note","text":"<p>This package makes use of external code. The trained files are provided by apollo. The trained files are automatically downloaded during env preparation.</p> <p>Original model URL</p> <p>https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/road-segmentation-adas-0001</p> <p>Open Model Zoo is licensed under Apache License Version 2.0.</p> <p>Converted model URL</p> <p>https://github.com/PINTO0309/PINTO_model_zoo/tree/main/136_road-segmentation-adas-0001</p> <p>model conversion scripts are released under the MIT license</p>"},{"location":"localization/yabloc/yabloc_pose_initializer/#special-thanks","title":"Special thanks","text":"<ul> <li>openvinotoolkit/open_model_zoo</li> <li>PINTO0309</li> </ul>"},{"location":"localization/yabloc/yabloc_pose_initializer/#camera_pose_initializer","title":"camera_pose_initializer","text":""},{"location":"localization/yabloc/yabloc_pose_initializer/#purpose","title":"Purpose","text":"<ul> <li>This node estimates the initial position using the camera at the request of ADAPI.</li> </ul>"},{"location":"localization/yabloc/yabloc_pose_initializer/#input","title":"Input","text":"Name Type Description <code>input/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> undistorted camera info <code>input/image_raw</code> <code>sensor_msgs::msg::Image</code> undistorted camera image <code>input/vector_map</code> <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> vector map"},{"location":"localization/yabloc/yabloc_pose_initializer/#output","title":"Output","text":"Name Type Description <code>output/candidates</code> <code>visualization_msgs::msg::MarkerArray</code> initial pose candidates"},{"location":"localization/yabloc/yabloc_pose_initializer/#parameters","title":"Parameters","text":"Name Type Description <code>angle_resolution</code> int how many divisions of 1 sigma angle range"},{"location":"localization/yabloc/yabloc_pose_initializer/#services","title":"Services","text":"Name Type Description <code>yabloc_align_srv</code> <code>tier4_localization_msgs::srv::PoseWithCovarianceStamped</code> initial pose estimation request"},{"location":"map/map_height_fitter/","title":"map_height_fitter","text":""},{"location":"map/map_height_fitter/#map_height_fitter","title":"map_height_fitter","text":"<p>This library fits the given point with the ground of the point cloud map. The map loading operation is switched by the parameter <code>enable_partial_load</code> of the node specified by <code>map_loader_name</code>. The node using this library must use multi thread executor.</p> Interface Local Name Description Parameter map_loader_name The point cloud map loader name. Subscription ~/pointcloud_map The topic name to load the whole map Client ~/partial_map_load The service name to load the partial map"},{"location":"map/map_loader/","title":"map_loader package","text":""},{"location":"map/map_loader/#map_loader-package","title":"map_loader package","text":"<p>This package provides the features of loading various maps.</p>"},{"location":"map/map_loader/#pointcloud_map_loader","title":"pointcloud_map_loader","text":""},{"location":"map/map_loader/#feature","title":"Feature","text":"<p><code>pointcloud_map_loader</code> provides pointcloud maps to the other Autoware nodes in various configurations. Currently, it supports the following two types:</p> <ul> <li>Publish raw pointcloud map</li> <li>Publish downsampled pointcloud map</li> <li>Send partial pointcloud map loading via ROS 2 service</li> <li>Send differential pointcloud map loading via ROS 2 service</li> </ul> <p>NOTE: We strongly recommend to use divided maps when using large pointcloud map to enable the latter two features (partial and differential load). Please go through the prerequisites section for more details, and follow the instruction for dividing the map and preparing the metadata.</p>"},{"location":"map/map_loader/#prerequisites","title":"Prerequisites","text":""},{"location":"map/map_loader/#prerequisites-on-pointcloud-map-files","title":"Prerequisites on pointcloud map file(s)","text":"<p>You may provide either a single .pcd file or multiple .pcd files. If you are using multiple PCD data and either of <code>enable_partial_load</code>, <code>enable_differential_load</code> or <code>enable_selected_load</code> are set true, it MUST obey the following rules:</p> <ol> <li>The pointcloud map should be projected on the same coordinate defined in <code>map_projection_loader</code>, in order to be consistent with the lanelet2 map and other packages that converts between local and geodetic coordinates. For more information, please refer to the readme of <code>map_projection_loader</code>.</li> <li>It must be divided by straight lines parallel to the x-axis and y-axis. The system does not support division by diagonal lines or curved lines.</li> <li>The division size along each axis should be equal.</li> <li>The division size should be about 20m x 20m. Particularly, care should be taken as using too large division size (for example, more than 100m) may have adverse effects on dynamic map loading features in ndt_scan_matcher and compare_map_segmentation.</li> <li>All the split maps should not overlap with each other.</li> <li>Metadata file should also be provided. The metadata structure description is provided below.</li> </ol> <p>Note that these rules are not applicable when <code>enable_partial_load</code>, <code>enable_differential_load</code> and <code>enable_selected_load</code> are all set false. In this case, however, you also need to disable dynamic map loading mode for other nodes as well (ndt_scan_matcher and compare_map_segmentation as of June 2023).</p>"},{"location":"map/map_loader/#metadata-structure","title":"Metadata structure","text":"<p>The metadata should look like this:</p> <pre><code>x_resolution: 20.0\ny_resolution: 20.0\nA.pcd: [1200, 2500] # -&gt; 1200 &lt; x &lt; 1220, 2500 &lt; y &lt; 2520\nB.pcd: [1220, 2500] # -&gt; 1220 &lt; x &lt; 1240, 2500 &lt; y &lt; 2520\nC.pcd: [1200, 2520] # -&gt; 1200 &lt; x &lt; 1220, 2520 &lt; y &lt; 2540\nD.pcd: [1240, 2520] # -&gt; 1240 &lt; x &lt; 1260, 2520 &lt; y &lt; 2540\n</code></pre> <p>where,</p> <ul> <li><code>x_resolution</code> and <code>y_resolution</code></li> <li><code>A.pcd</code>, <code>B.pcd</code>, etc, are the names of PCD files.</li> <li>List such as <code>[1200, 2500]</code> are the values indicate that for this PCD file, x coordinates are between 1200 and 1220 (<code>x_resolution</code> + <code>x_coordinate</code>) and y coordinates are between 2500 and 2520 (<code>y_resolution</code> + <code>y_coordinate</code>).</li> </ul> <p>You may use pointcloud_divider from MAP IV for dividing pointcloud map as well as generating the compatible metadata.yaml.</p>"},{"location":"map/map_loader/#directory-structure-of-these-files","title":"Directory structure of these files","text":"<p>If you only have one pointcloud map, Autoware will assume the following directory structure by default.</p> <pre><code>sample-map-rosbag\n\u251c\u2500\u2500 lanelet2_map.osm\n\u251c\u2500\u2500 pointcloud_map.pcd\n</code></pre> <p>If you have multiple rosbags, an example directory structure would be as follows. Note that you need to have a metadata when you have multiple pointcloud map files.</p> <pre><code>sample-map-rosbag\n\u251c\u2500\u2500 lanelet2_map.osm\n\u251c\u2500\u2500 pointcloud_map.pcd\n\u2502 \u251c\u2500\u2500 A.pcd\n\u2502 \u251c\u2500\u2500 B.pcd\n\u2502 \u251c\u2500\u2500 C.pcd\n\u2502 \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 map_projector_info.yaml\n\u2514\u2500\u2500 pointcloud_map_metadata.yaml\n</code></pre>"},{"location":"map/map_loader/#specific-features","title":"Specific features","text":""},{"location":"map/map_loader/#publish-raw-pointcloud-map-ros-2-topic","title":"Publish raw pointcloud map (ROS 2 topic)","text":"<p>The node publishes the raw pointcloud map loaded from the <code>.pcd</code> file(s).</p>"},{"location":"map/map_loader/#publish-downsampled-pointcloud-map-ros-2-topic","title":"Publish downsampled pointcloud map (ROS 2 topic)","text":"<p>The node publishes the downsampled pointcloud map loaded from the <code>.pcd</code> file(s). You can specify the downsample resolution by changing the <code>leaf_size</code> parameter.</p>"},{"location":"map/map_loader/#publish-metadata-of-pointcloud-map-ros-2-topic","title":"Publish metadata of pointcloud map (ROS 2 topic)","text":"<p>The node publishes the pointcloud metadata attached with an ID. Metadata is loaded from the <code>.yaml</code> file. Please see the description of <code>PointCloudMapMetaData.msg</code> for details.</p>"},{"location":"map/map_loader/#send-partial-pointcloud-map-ros-2-service","title":"Send partial pointcloud map (ROS 2 service)","text":"<p>Here, we assume that the pointcloud maps are divided into grids.</p> <p>Given a query from a client node, the node sends a set of pointcloud maps that overlaps with the queried area. Please see the description of <code>GetPartialPointCloudMap.srv</code> for details.</p>"},{"location":"map/map_loader/#send-differential-pointcloud-map-ros-2-service","title":"Send differential pointcloud map (ROS 2 service)","text":"<p>Here, we assume that the pointcloud maps are divided into grids.</p> <p>Given a query and set of map IDs, the node sends a set of pointcloud maps that overlap with the queried area and are not included in the set of map IDs. Please see the description of <code>GetDifferentialPointCloudMap.srv</code> for details.</p>"},{"location":"map/map_loader/#send-selected-pointcloud-map-ros-2-service","title":"Send selected pointcloud map (ROS 2 service)","text":"<p>Here, we assume that the pointcloud maps are divided into grids.</p> <p>Given IDs query from a client node, the node sends a set of pointcloud maps (each of which attached with unique ID) specified by query. Please see the description of <code>GetSelectedPointCloudMap.srv</code> for details.</p>"},{"location":"map/map_loader/#parameters","title":"Parameters","text":"Name Type Description Default value enable_whole_load bool A flag to enable raw pointcloud map publishing true enable_downsampled_whole_load bool A flag to enable downsampled pointcloud map publishing false enable_partial_load bool A flag to enable partial pointcloud map server false enable_differential_load bool A flag to enable differential pointcloud map server false enable_selected_load bool A flag to enable selected pointcloud map server false leaf_size float Downsampling leaf size (only used when enable_downsampled_whole_load is set true) 3.0 pcd_paths_or_directory std::string Path(s) to pointcloud map file or directory pcd_metadata_path std::string Path to pointcloud metadata file"},{"location":"map/map_loader/#interfaces","title":"Interfaces","text":"<ul> <li><code>output/pointcloud_map</code> (sensor_msgs/msg/PointCloud2) : Raw pointcloud map</li> <li><code>output/pointcloud_map_metadata</code> (autoware_map_msgs/msg/PointCloudMapMetaData) : Metadata of pointcloud map</li> <li><code>output/debug/downsampled_pointcloud_map</code> (sensor_msgs/msg/PointCloud2) : Downsampled pointcloud map</li> <li><code>service/get_partial_pcd_map</code> (autoware_map_msgs/srv/GetPartialPointCloudMap) : Partial pointcloud map</li> <li><code>service/get_differential_pcd_map</code> (autoware_map_msgs/srv/GetDifferentialPointCloudMap) : Differential pointcloud map</li> <li><code>service/get_selected_pcd_map</code> (autoware_map_msgs/srv/GetSelectedPointCloudMap) : Selected pointcloud map</li> <li>pointcloud map file(s) (.pcd)</li> <li>metadata of pointcloud map(s) (.yaml)</li> </ul>"},{"location":"map/map_loader/#lanelet2_map_loader","title":"lanelet2_map_loader","text":""},{"location":"map/map_loader/#feature_1","title":"Feature","text":"<p>lanelet2_map_loader loads Lanelet2 file and publishes the map data as autoware_auto_mapping_msgs/HADMapBin message. The node projects lan/lon coordinates into arbitrary coordinates defined in <code>/map/map_projector_info</code> from <code>map_projection_loader</code>. Please see tier4_autoware_msgs/msg/MapProjectorInfo.msg for supported projector types.</p>"},{"location":"map/map_loader/#how-to-run","title":"How to run","text":"<p><code>ros2 run map_loader lanelet2_map_loader --ros-args -p lanelet2_map_path:=path/to/map.osm</code></p>"},{"location":"map/map_loader/#subscribed-topics","title":"Subscribed Topics","text":"<ul> <li>~input/map_projector_info (tier4_map_msgs/MapProjectorInfo) : Projection type for Autoware</li> </ul>"},{"location":"map/map_loader/#published-topics","title":"Published Topics","text":"<ul> <li>~output/lanelet2_map (autoware_auto_mapping_msgs/HADMapBin) : Binary data of loaded Lanelet2 Map</li> </ul>"},{"location":"map/map_loader/#parameters_1","title":"Parameters","text":"Name Type Description Default value center_line_resolution double Define the resolution of the lanelet center line 5.0 lanelet2_map_path std::string The lanelet2 map path None"},{"location":"map/map_loader/#lanelet2_map_visualization","title":"lanelet2_map_visualization","text":""},{"location":"map/map_loader/#feature_2","title":"Feature","text":"<p>lanelet2_map_visualization visualizes autoware_auto_mapping_msgs/HADMapBin messages into visualization_msgs/MarkerArray.</p>"},{"location":"map/map_loader/#how-to-run_1","title":"How to Run","text":"<p><code>ros2 run map_loader lanelet2_map_visualization</code></p>"},{"location":"map/map_loader/#subscribed-topics_1","title":"Subscribed Topics","text":"<ul> <li>~input/lanelet2_map (autoware_auto_mapping_msgs/HADMapBin) : binary data of Lanelet2 Map</li> </ul>"},{"location":"map/map_loader/#published-topics_1","title":"Published Topics","text":"<ul> <li>~output/lanelet2_map_marker (visualization_msgs/MarkerArray) : visualization messages for RViz</li> </ul>"},{"location":"map/map_projection_loader/","title":"map_projection_loader","text":""},{"location":"map/map_projection_loader/#map_projection_loader","title":"map_projection_loader","text":""},{"location":"map/map_projection_loader/#feature","title":"Feature","text":"<p><code>map_projection_loader</code> is responsible for publishing <code>map_projector_info</code> that defines in which kind of coordinate Autoware is operating. This is necessary information especially when you want to convert from global (geoid) to local coordinate or the other way around.</p> <ul> <li>If <code>map_projector_info_path</code> DOES exist, this node loads it and publishes the map projection information accordingly.</li> <li>If <code>map_projector_info_path</code> does NOT exist, the node assumes that you are using the <code>MGRS</code> projection type, and loads the lanelet2 map instead to extract the MGRS grid.<ul> <li>DEPRECATED WARNING: This interface that uses the lanelet2 map is not recommended. Please prepare the YAML file instead.</li> </ul> </li> </ul>"},{"location":"map/map_projection_loader/#map-projector-info-file-specification","title":"Map projector info file specification","text":"<p>You need to provide a YAML file, namely <code>map_projector_info.yaml</code> under the <code>map_path</code> directory. For <code>pointcloud_map_metadata.yaml</code>, please refer to the Readme of <code>map_loader</code>.</p> <pre><code>sample-map-rosbag\n\u251c\u2500\u2500 lanelet2_map.osm\n\u251c\u2500\u2500 pointcloud_map.pcd\n\u251c\u2500\u2500 map_projector_info.yaml\n\u2514\u2500\u2500 pointcloud_map_metadata.yaml\n</code></pre>"},{"location":"map/map_projection_loader/#using-local-coordinate","title":"Using local coordinate","text":"<pre><code># map_projector_info.yaml\nprojector_type: local\n</code></pre>"},{"location":"map/map_projection_loader/#using-mgrs","title":"Using MGRS","text":"<p>If you want to use MGRS, please specify the MGRS grid as well.</p> <pre><code># map_projector_info.yaml\nprojector_type: MGRS\nvertical_datum: WGS84\nmgrs_grid: 54SUE\n</code></pre>"},{"location":"map/map_projection_loader/#using-localcartesianutm","title":"Using LocalCartesianUTM","text":"<p>If you want to use local cartesian UTM, please specify the map origin as well.</p> <pre><code># map_projector_info.yaml\nprojector_type: LocalCartesianUTM\nvertical_datum: WGS84\nmap_origin:\nlatitude: 35.6762 # [deg]\nlongitude: 139.6503 # [deg]\naltitude: 0.0 # [m]\n</code></pre>"},{"location":"map/map_projection_loader/#using-transversemercator","title":"Using TransverseMercator","text":"<p>If you want to use Transverse Mercator projection, please specify the map origin as well.</p> <pre><code># map_projector_info.yaml\nprojector_type: TransverseMercator\nvertical_datum: WGS84\nmap_origin:\nlatitude: 35.6762 # [deg]\nlongitude: 139.6503 # [deg]\naltitude: 0.0 # [m]\n</code></pre>"},{"location":"map/map_projection_loader/#published-topics","title":"Published Topics","text":"<ul> <li>~/map_projector_info (tier4_map_msgs/MapProjectorInfo) : Topic for defining map projector information</li> </ul>"},{"location":"map/map_projection_loader/#parameters","title":"Parameters","text":"Name Type Description map_projector_info_path std::string A path to map_projector_info.yaml (used by default) lanelet2_map_path std::string A path to lanelet2 map (used only when <code>map_projector_info_path</code> does not exist)"},{"location":"map/map_tf_generator/Readme/","title":"map_tf_generator","text":""},{"location":"map/map_tf_generator/Readme/#map_tf_generator","title":"map_tf_generator","text":""},{"location":"map/map_tf_generator/Readme/#purpose","title":"Purpose","text":"<p>The nodes in this package broadcast the <code>viewer</code> frame for visualization of the map in RViz.</p> <p>Note that there is no module to need the <code>viewer</code> frame and this is used only for visualization.</p> <p>The following are the supported methods to calculate the position of the <code>viewer</code> frame:</p> <ul> <li><code>pcd_map_tf_generator_node</code> outputs the geometric center of all points in the PCD.</li> <li><code>vector_map_tf_generator_node</code> outputs the geometric center of all points in the point layer.</li> </ul>"},{"location":"map/map_tf_generator/Readme/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"map/map_tf_generator/Readme/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"map/map_tf_generator/Readme/#input","title":"Input","text":""},{"location":"map/map_tf_generator/Readme/#pcd_map_tf_generator","title":"pcd_map_tf_generator","text":"Name Type Description <code>/map/pointcloud_map</code> <code>sensor_msgs::msg::PointCloud2</code> Subscribe pointcloud map to calculate position of <code>viewer</code> frames"},{"location":"map/map_tf_generator/Readme/#vector_map_tf_generator","title":"vector_map_tf_generator","text":"Name Type Description <code>/map/vector_map</code> <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> Subscribe vector map to calculate position of <code>viewer</code> frames"},{"location":"map/map_tf_generator/Readme/#output","title":"Output","text":"Name Type Description <code>/tf_static</code> <code>tf2_msgs/msg/TFMessage</code> Broadcast <code>viewer</code> frames"},{"location":"map/map_tf_generator/Readme/#parameters","title":"Parameters","text":""},{"location":"map/map_tf_generator/Readme/#node-parameters","title":"Node Parameters","text":"<p>None</p>"},{"location":"map/map_tf_generator/Readme/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Explanation <code>viewer_frame</code> string viewer Name of <code>viewer</code> frame <code>map_frame</code> string map The parent frame name of viewer frame"},{"location":"map/map_tf_generator/Readme/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"perception/bytetrack/","title":"bytetrack","text":""},{"location":"perception/bytetrack/#bytetrack","title":"bytetrack","text":""},{"location":"perception/bytetrack/#purpose","title":"Purpose","text":"<p>The core algorithm, named <code>ByteTrack</code>, mainly aims to perform multi-object tracking. Because the algorithm associates almost every detection box including ones with low detection scores, the number of false negatives is expected to decrease by using it.</p> <p>demo video</p>"},{"location":"perception/bytetrack/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/bytetrack/#cite","title":"Cite","text":"<ul> <li>Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang,   \"ByteTrack: Multi-Object Tracking by Associating Every Detection Box\", in the proc. of the ECCV   2022, [ref]</li> <li>This package is ported version toward Autoware from this repository   (The C++ implementation by the ByteTrack's authors)</li> </ul>"},{"location":"perception/bytetrack/#2d-tracking-modification-from-original-codes","title":"2d tracking modification from original codes","text":"<p>The paper just says that the 2d tracking algorithm is a simple Kalman filter. Original codes use the <code>top-left-corner</code> and <code>aspect ratio</code> and <code>size</code> as the state vector.</p> <p>This is sometimes unstable because the aspectratio can be changed by the occlusion. So, we use the <code>top-left</code> and <code>size</code> as the state vector.</p> <p>Kalman filter settings can be controlled by the parameters in <code>config/bytetrack_node.param.yaml</code>.</p>"},{"location":"perception/bytetrack/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/bytetrack/#bytetrack_node","title":"bytetrack_node","text":""},{"location":"perception/bytetrack/#input","title":"Input","text":"Name Type Description <code>in/rect</code> <code>tier4_perception_msgs/DetectedObjectsWithFeature</code> The detected objects with 2D bounding boxes"},{"location":"perception/bytetrack/#output","title":"Output","text":"Name Type Description <code>out/objects</code> <code>tier4_perception_msgs/DetectedObjectsWithFeature</code> The detected objects with 2D bounding boxes <code>out/objects/debug/uuid</code> <code>tier4_perception_msgs/DynamicObjectArray</code> The universally unique identifiers (UUID) for each object"},{"location":"perception/bytetrack/#bytetrack_visualizer","title":"bytetrack_visualizer","text":""},{"location":"perception/bytetrack/#input_1","title":"Input","text":"Name Type Description <code>in/image</code> <code>sensor_msgs/Image</code> or <code>sensor_msgs/CompressedImage</code> The input image on which object detection is performed <code>in/rect</code> <code>tier4_perception_msgs/DetectedObjectsWithFeature</code> The detected objects with 2D bounding boxes <code>in/uuid</code> <code>tier4_perception_msgs/DynamicObjectArray</code> The universally unique identifiers (UUID) for each object"},{"location":"perception/bytetrack/#output_1","title":"Output","text":"Name Type Description <code>out/image</code> <code>sensor_msgs/Image</code> The image that detection bounding boxes and their UUIDs are drawn"},{"location":"perception/bytetrack/#parameters","title":"Parameters","text":""},{"location":"perception/bytetrack/#bytetrack_node_1","title":"bytetrack_node","text":"Name Type Default Value Description <code>track_buffer_length</code> int 30 The frame count that a tracklet is considered to be lost"},{"location":"perception/bytetrack/#bytetrack_visualizer_1","title":"bytetrack_visualizer","text":"Name Type Default Value Description <code>use_raw</code> bool false The flag for the node to switch <code>sensor_msgs/Image</code> or <code>sensor_msgs/CompressedImage</code> as input"},{"location":"perception/bytetrack/#assumptionsknown-limits","title":"Assumptions/Known limits","text":""},{"location":"perception/bytetrack/#reference-repositories","title":"Reference repositories","text":"<ul> <li>https://github.com/ifzhang/ByteTrack</li> </ul>"},{"location":"perception/bytetrack/#license","title":"License","text":"<p>The codes under the <code>lib</code> directory are copied from the original codes and modified. The original codes belong to the MIT license stated as follows, while this ported packages are provided with Apache License 2.0:</p> <p>MIT License</p> <p>Copyright (c) 2021 Yifu Zhang</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"perception/cluster_merger/","title":"cluster merger","text":""},{"location":"perception/cluster_merger/#cluster-merger","title":"cluster merger","text":""},{"location":"perception/cluster_merger/#purpose","title":"Purpose","text":"<p>cluster_merger is a package for merging pointcloud clusters as detected objects with feature type.</p>"},{"location":"perception/cluster_merger/#inner-working-algorithms","title":"Inner-working / Algorithms","text":"<p>The clusters of merged topics are simply concatenated from clusters of input topics.</p>"},{"location":"perception/cluster_merger/#input-output","title":"Input / Output","text":""},{"location":"perception/cluster_merger/#input","title":"Input","text":"Name Type Description <code>input/cluster0</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> pointcloud clusters <code>input/cluster1</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> pointcloud clusters"},{"location":"perception/cluster_merger/#output","title":"Output","text":"Name Type Description <code>output/clusters</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> merged clusters"},{"location":"perception/cluster_merger/#parameters","title":"Parameters","text":"Name Type Description Default value <code>output_frame_id</code> string The header frame_id of output topic. base_link"},{"location":"perception/cluster_merger/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/cluster_merger/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/cluster_merger/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/cluster_merger/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/cluster_merger/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/compare_map_segmentation/","title":"compare_map_segmentation","text":""},{"location":"perception/compare_map_segmentation/#compare_map_segmentation","title":"compare_map_segmentation","text":""},{"location":"perception/compare_map_segmentation/#purpose","title":"Purpose","text":"<p>The <code>compare_map_segmentation</code> is a node that filters the ground points from the input pointcloud by using map info (e.g. pcd, elevation map or split map pointcloud from map_loader interface).</p>"},{"location":"perception/compare_map_segmentation/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/compare_map_segmentation/#compare-elevation-map-filter","title":"Compare Elevation Map Filter","text":"<p>Compare the z of the input points with the value of elevation_map. The height difference is calculated by the binary integration of neighboring cells. Remove points whose height difference is below the <code>height_diff_thresh</code>.</p> <p> </p>"},{"location":"perception/compare_map_segmentation/#distance-based-compare-map-filter","title":"Distance Based Compare Map Filter","text":"<p>This filter compares the input pointcloud with the map pointcloud using the <code>nearestKSearch</code> function of <code>kdtree</code> and removes points that are close to the map point cloud. The map pointcloud can be loaded statically at once at the beginning or dynamically as the vehicle moves.</p>"},{"location":"perception/compare_map_segmentation/#voxel-based-approximate-compare-map-filter","title":"Voxel Based Approximate Compare Map Filter","text":"<p>The filter loads the map point cloud, which can be loaded statically at the beginning or dynamically during vehicle movement, and creates a voxel grid of the map point cloud. The filter uses the getCentroidIndexAt function in combination with the getGridCoordinates function from the VoxelGrid class to find input points that are inside the voxel grid and removes them.</p>"},{"location":"perception/compare_map_segmentation/#voxel-based-compare-map-filter","title":"Voxel Based Compare Map Filter","text":"<p>The filter loads the map pointcloud (static loading whole map at once at beginning or dynamic loading during vehicle moving) and utilizes VoxelGrid to downsample map pointcloud.</p> <p>For each point of input pointcloud, the filter use <code>getCentroidIndexAt</code> combine with <code>getGridCoordinates</code> function from VoxelGrid class to check if the downsampled map point existing surrounding input points. Remove the input point which has downsampled map point in voxels containing or being close to the point.</p>"},{"location":"perception/compare_map_segmentation/#voxel-distance-based-compare-map-filter","title":"Voxel Distance based Compare Map Filter","text":"<p>This filter is a combination of the distance_based_compare_map_filter and voxel_based_approximate_compare_map_filter. The filter loads the map point cloud, which can be loaded statically at the beginning or dynamically during vehicle movement, and creates a voxel grid and a k-d tree of the map point cloud. The filter uses the getCentroidIndexAt function in combination with the getGridCoordinates function from the VoxelGrid class to find input points that are inside the voxel grid and removes them. For points that do not belong to any voxel grid, they are compared again with the map point cloud using the radiusSearch function of the k-d tree and are removed if they are close enough to the map.</p>"},{"location":"perception/compare_map_segmentation/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/compare_map_segmentation/#compare-elevation-map-filter_1","title":"Compare Elevation Map Filter","text":""},{"location":"perception/compare_map_segmentation/#input","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points <code>~/input/elevation_map</code> <code>grid_map::msg::GridMap</code> elevation map"},{"location":"perception/compare_map_segmentation/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"perception/compare_map_segmentation/#parameters","title":"Parameters","text":"Name Type Description Default value <code>map_layer_name</code> string elevation map layer name elevation <code>map_frame</code> float frame_id of the map that is temporarily used before elevation_map is subscribed map <code>height_diff_thresh</code> float Remove points whose height difference is below this value [m] 0.15"},{"location":"perception/compare_map_segmentation/#other-filters","title":"Other Filters","text":""},{"location":"perception/compare_map_segmentation/#input_1","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points <code>~/input/map</code> <code>sensor_msgs::msg::PointCloud2</code> map (in case static map loading) <code>/localization/kinematic_state</code> <code>nav_msgs::msg::Odometry</code> current ego-vehicle pose (in case dynamic map loading)"},{"location":"perception/compare_map_segmentation/#output_1","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"perception/compare_map_segmentation/#parameters_1","title":"Parameters","text":"Name Type Description Default value <code>use_dynamic_map_loading</code> bool map loading mode selection, <code>true</code> for dynamic map loading, <code>false</code> for static map loading, recommended for no-split map pointcloud true <code>distance_threshold</code> float Threshold distance to compare input points with map points [m] 0.5 <code>map_update_distance_threshold</code> float Threshold of vehicle movement distance when map update is necessary (in dynamic map loading) [m] 10.0 <code>map_loader_radius</code> float Radius of map need to be loaded (in dynamic map loading) [m] 150.0 <code>timer_interval_ms</code> int Timer interval to check if the map update is necessary (in dynamic map loading) [ms] 100 <code>publish_debug_pcd</code> bool Enable to publish voxelized updated map in <code>debug/downsampled_map/pointcloud</code> for debugging. It might cause additional computation cost false <code>downsize_ratio_z_axis</code> double Positive ratio to reduce voxel_leaf_size and neighbor point distance threshold in z axis 0.5"},{"location":"perception/compare_map_segmentation/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/compare_map_segmentation/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/compare_map_segmentation/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/compare_map_segmentation/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/compare_map_segmentation/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/crosswalk_traffic_light_estimator/","title":"crosswalk_traffic_light_estimator","text":""},{"location":"perception/crosswalk_traffic_light_estimator/#crosswalk_traffic_light_estimator","title":"crosswalk_traffic_light_estimator","text":""},{"location":"perception/crosswalk_traffic_light_estimator/#purpose","title":"Purpose","text":"<p><code>crosswalk_traffic_light_estimator</code> is a module that estimates pedestrian traffic signals from HDMap and detected vehicle traffic signals.</p>"},{"location":"perception/crosswalk_traffic_light_estimator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/crosswalk_traffic_light_estimator/#input","title":"Input","text":"Name Type Description <code>~/input/vector_map</code> <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> vector map <code>~/input/route</code> <code>autoware_planning_msgs::msg::LaneletRoute</code> route <code>~/input/classified/traffic_signals</code> <code>tier4_perception_msgs::msg::TrafficSignalArray</code> classified signals"},{"location":"perception/crosswalk_traffic_light_estimator/#output","title":"Output","text":"Name Type Description <code>~/output/traffic_signals</code> <code>tier4_perception_msgs::msg::TrafficSignalArray</code> output that contains estimated pedestrian traffic signals"},{"location":"perception/crosswalk_traffic_light_estimator/#parameters","title":"Parameters","text":"Name Type Description Default value <code>use_last_detect_color</code> <code>bool</code> If this parameter is <code>true</code>, this module estimates pedestrian's traffic signal as RED not only when vehicle's traffic signal is detected as GREEN/AMBER but also when detection results change GREEN/AMBER to UNKNOWN. (If detection results change RED or AMBER to UNKNOWN, this module estimates pedestrian's traffic signal as UNKNOWN.) If this parameter is <code>false</code>, this module use only latest detection results for estimation. (Only when the detection result is GREEN/AMBER, this module estimates pedestrian's traffic signal as RED.) <code>true</code> <code>last_detect_color_hold_time</code> <code>double</code> The time threshold to hold for last detect color. <code>2.0</code>"},{"location":"perception/crosswalk_traffic_light_estimator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>If traffic between pedestrians and vehicles is controlled by traffic signals, the crosswalk traffic signal maybe RED in order to prevent pedestrian from crossing when the following conditions are satisfied.</p>"},{"location":"perception/crosswalk_traffic_light_estimator/#situation1","title":"Situation1","text":"<ul> <li>crosswalk conflicts STRAIGHT lanelet</li> <li>the lanelet refers GREEN or AMBER traffic signal (The following pictures show only GREEN case)</li> </ul>"},{"location":"perception/crosswalk_traffic_light_estimator/#situation2","title":"Situation2","text":"<ul> <li>crosswalk conflicts different turn direction lanelets (STRAIGHT and LEFT, LEFT and RIGHT, RIGHT and STRAIGHT)</li> <li>the lanelets refer GREEN or AMBER traffic signal (The following pictures show only GREEN case)</li> </ul>"},{"location":"perception/crosswalk_traffic_light_estimator/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/crosswalk_traffic_light_estimator/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"perception/detected_object_feature_remover/","title":"detected_object_feature_remover","text":""},{"location":"perception/detected_object_feature_remover/#detected_object_feature_remover","title":"detected_object_feature_remover","text":""},{"location":"perception/detected_object_feature_remover/#purpose","title":"Purpose","text":"<p>The <code>detected_object_feature_remover</code> is a package to convert topic-type from <code>DetectedObjectWithFeatureArray</code> to <code>DetectedObjects</code>.</p>"},{"location":"perception/detected_object_feature_remover/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/detected_object_feature_remover/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/detected_object_feature_remover/#input","title":"Input","text":"Name Type Description <code>~/input</code> <code>tier4_perception_msgs::msg::DetectedObjectWithFeatureArray</code> detected objects with feature field"},{"location":"perception/detected_object_feature_remover/#output","title":"Output","text":"Name Type Description <code>~/output</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> detected objects"},{"location":"perception/detected_object_feature_remover/#parameters","title":"Parameters","text":"<p>None</p>"},{"location":"perception/detected_object_feature_remover/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/detected_object_validation/","title":"detected_object_validation","text":""},{"location":"perception/detected_object_validation/#detected_object_validation","title":"detected_object_validation","text":""},{"location":"perception/detected_object_validation/#purpose","title":"Purpose","text":"<p>The purpose of this package is to eliminate obvious false positives of DetectedObjects.</p>"},{"location":"perception/detected_object_validation/#referencesexternal-links","title":"References/External links","text":"<ul> <li>Obstacle pointcloud based validator</li> <li>Occupancy grid based validator</li> <li>Object lanelet filter</li> <li>Object position filter</li> </ul>"},{"location":"perception/detected_object_validation/object-lanelet-filter/","title":"object_lanelet_filter","text":""},{"location":"perception/detected_object_validation/object-lanelet-filter/#object_lanelet_filter","title":"object_lanelet_filter","text":""},{"location":"perception/detected_object_validation/object-lanelet-filter/#purpose","title":"Purpose","text":"<p>The <code>object_lanelet_filter</code> is a node that filters detected object by using vector map. The objects only inside of the vector map will be published.</p>"},{"location":"perception/detected_object_validation/object-lanelet-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/detected_object_validation/object-lanelet-filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/detected_object_validation/object-lanelet-filter/#input","title":"Input","text":"Name Type Description <code>input/vector_map</code> <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> vector map <code>input/object</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> input detected objects"},{"location":"perception/detected_object_validation/object-lanelet-filter/#output","title":"Output","text":"Name Type Description <code>output/object</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> filtered detected objects"},{"location":"perception/detected_object_validation/object-lanelet-filter/#parameters","title":"Parameters","text":""},{"location":"perception/detected_object_validation/object-lanelet-filter/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>filter_target_label.UNKNOWN</code> bool false If true, unknown objects are filtered. <code>filter_target_label.CAR</code> bool false If true, car objects are filtered. <code>filter_target_label.TRUCK</code> bool false If true, truck objects are filtered. <code>filter_target_label.BUS</code> bool false If true, bus objects are filtered. <code>filter_target_label.TRAILER</code> bool false If true, trailer objects are filtered. <code>filter_target_label.MOTORCYCLE</code> bool false If true, motorcycle objects are filtered. <code>filter_target_label.BICYCLE</code> bool false If true, bicycle objects are filtered. <code>filter_target_label.PEDESTRIAN</code> bool false If true, pedestrian objects are filtered."},{"location":"perception/detected_object_validation/object-lanelet-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The lanelet filter is performed based on the shape polygon and bounding box of the objects.</p>"},{"location":"perception/detected_object_validation/object-lanelet-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/detected_object_validation/object-lanelet-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/detected_object_validation/object-lanelet-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/detected_object_validation/object-lanelet-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/detected_object_validation/object-position-filter/","title":"object_position_filter","text":""},{"location":"perception/detected_object_validation/object-position-filter/#object_position_filter","title":"object_position_filter","text":""},{"location":"perception/detected_object_validation/object-position-filter/#purpose","title":"Purpose","text":"<p>The <code>object_position_filter</code> is a node that filters detected object based on x,y values. The objects only inside of the x, y bound will be published.</p>"},{"location":"perception/detected_object_validation/object-position-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/detected_object_validation/object-position-filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/detected_object_validation/object-position-filter/#input","title":"Input","text":"Name Type Description <code>input/object</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> input detected objects"},{"location":"perception/detected_object_validation/object-position-filter/#output","title":"Output","text":"Name Type Description <code>output/object</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> filtered detected objects"},{"location":"perception/detected_object_validation/object-position-filter/#parameters","title":"Parameters","text":""},{"location":"perception/detected_object_validation/object-position-filter/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>filter_target_label.UNKNOWN</code> bool false If true, unknown objects are filtered. <code>filter_target_label.CAR</code> bool false If true, car objects are filtered. <code>filter_target_label.TRUCK</code> bool false If true, truck objects are filtered. <code>filter_target_label.BUS</code> bool false If true, bus objects are filtered. <code>filter_target_label.TRAILER</code> bool false If true, trailer objects are filtered. <code>filter_target_label.MOTORCYCLE</code> bool false If true, motorcycle objects are filtered. <code>filter_target_label.BICYCLE</code> bool false If true, bicycle objects are filtered. <code>filter_target_label.PEDESTRIAN</code> bool false If true, pedestrian objects are filtered. <code>upper_bound_x</code> float 100.00 Bound for filtering. Only used if filter_by_xy_position is true <code>lower_bound_x</code> float 0.00 Bound for filtering. Only used if filter_by_xy_position is true <code>upper_bound_y</code> float 50.00 Bound for filtering. Only used if filter_by_xy_position is true <code>lower_bound_y</code> float -50.00 Bound for filtering. Only used if filter_by_xy_position is true"},{"location":"perception/detected_object_validation/object-position-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Filtering is performed based on the center position of the object.</p>"},{"location":"perception/detected_object_validation/object-position-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/detected_object_validation/object-position-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/detected_object_validation/object-position-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/detected_object_validation/object-position-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/","title":"obstacle pointcloud based validator","text":""},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#obstacle-pointcloud-based-validator","title":"obstacle pointcloud based validator","text":""},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>If the number of obstacle point groups in the DetectedObjects is small, it is considered a false positive and removed. The obstacle point cloud can be a point cloud after compare map filtering or a ground filtered point cloud.</p> <p></p> <p>In the debug image above, the red DetectedObject is the validated object. The blue object is the deleted object.</p>"},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#input","title":"Input","text":"Name Type Description <code>~/input/detected_objects</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> DetectedObjects <code>~/input/obstacle_pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Obstacle point cloud of dynamic objects"},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> validated DetectedObjects"},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#parameters","title":"Parameters","text":"Name Type Description <code>using_2d_validator</code> bool The xy-plane projected (2D) obstacle point clouds will be used for validation <code>min_points_num</code> int The minimum number of obstacle point clouds in DetectedObjects <code>max_points_num</code> int The max number of obstacle point clouds in DetectedObjects <code>min_points_and_distance_ratio</code> float Threshold value of the number of point clouds per object when the distance from baselink is 1m, because the number of point clouds varies with the distance from baselink. <code>enable_debugger</code> bool Whether to create debug topics or not?"},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Currently, only represented objects as BoundingBox or Cylinder are supported.</p>"},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/","title":"occupancy grid based validator","text":""},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#occupancy-grid-based-validator","title":"occupancy grid based validator","text":""},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Compare the occupancy grid map with the DetectedObject, and if a larger percentage of obstacles are in freespace, delete them.</p> <p></p> <p>Basically, it takes an occupancy grid map as input and generates a binary image of freespace or other.</p> <p>A mask image is generated for each DetectedObject and the average value (percentage) in the mask image is calculated. If the percentage is low, it is deleted.</p>"},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#input","title":"Input","text":"Name Type Description <code>~/input/detected_objects</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> DetectedObjects <code>~/input/occupancy_grid_map</code> <code>nav_msgs::msg::OccupancyGrid</code> OccupancyGrid with no time series calculation is preferred."},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> validated DetectedObjects"},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#parameters","title":"Parameters","text":"Name Type Description <code>mean_threshold</code> float The percentage threshold of allowed non-freespace. <code>enable_debug</code> bool Whether to display debug images or not?"},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Currently, only vehicle represented as BoundingBox are supported.</p>"},{"location":"perception/detection_by_tracker/","title":"detection_by_tracker","text":""},{"location":"perception/detection_by_tracker/#detection_by_tracker","title":"detection_by_tracker","text":""},{"location":"perception/detection_by_tracker/#purpose","title":"Purpose","text":"<p>This package feeds back the tracked objects to the detection module to keep it stable and keep detecting objects. </p> <p>The detection by tracker takes as input an unknown object containing a cluster of points and a tracker. The unknown object is optimized to fit the size of the tracker so that it can continue to be detected.</p>"},{"location":"perception/detection_by_tracker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The detection by tracker receives an unknown object containing a point cloud and a tracker, where the unknown object is mainly shape-fitted using euclidean clustering. Shape fitting using euclidean clustering and other methods has a problem called under segmentation and over segmentation.</p> <p> Adapted from [3]</p> <p>Simply looking at the overlap between the unknown object and the tracker does not work. We need to take measures for under segmentation and over segmentation.</p>"},{"location":"perception/detection_by_tracker/#policy-for-dealing-with-over-segmentation","title":"Policy for dealing with over segmentation","text":"<ol> <li>Merge the unknown objects in the tracker as a single object.</li> <li>Shape fitting using the tracker information such as angle and size as reference information.</li> </ol>"},{"location":"perception/detection_by_tracker/#policy-for-dealing-with-under-segmentation","title":"Policy for dealing with under segmentation","text":"<ol> <li>Compare the tracker and unknown objects, and determine that those with large recall and small precision are under segmented objects.</li> <li>In order to divide the cluster of under segmented objects, it iterate the parameters to make small clusters.</li> <li>Adjust the parameters several times and adopt the one with the highest IoU.</li> </ol>"},{"location":"perception/detection_by_tracker/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/detection_by_tracker/#input","title":"Input","text":"Name Type Description <code>~/input/initial_objects</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> unknown objects <code>~/input/tracked_objects</code> <code>tier4_perception_msgs::msg::TrackedObjects</code> trackers"},{"location":"perception/detection_by_tracker/#output","title":"Output","text":"Name Type Description <code>~/output</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> objects"},{"location":"perception/detection_by_tracker/#parameters","title":"Parameters","text":"Name Type Description Default value <code>tracker_ignore_label.UNKNOWN</code> <code>bool</code> If true, the node will ignore the tracker if its label is unknown. <code>true</code> <code>tracker_ignore_label.CAR</code> <code>bool</code> If true, the node will ignore the tracker if its label is CAR. <code>false</code> <code>tracker_ignore_label.PEDESTRIAN</code> <code>bool</code> If true, the node will ignore the tracker if its label is pedestrian. <code>false</code> <code>tracker_ignore_label.BICYCLE</code> <code>bool</code> If true, the node will ignore the tracker if its label is bicycle. <code>false</code> <code>tracker_ignore_label.MOTORCYCLE</code> <code>bool</code> If true, the node will ignore the tracker if its label is MOTORCYCLE. <code>false</code> <code>tracker_ignore_label.BUS</code> <code>bool</code> If true, the node will ignore the tracker if its label is bus. <code>false</code> <code>tracker_ignore_label.TRUCK</code> <code>bool</code> If true, the node will ignore the tracker if its label is truck. <code>false</code> <code>tracker_ignore_label.TRAILER</code> <code>bool</code> If true, the node will ignore the tracker if its label is TRAILER. <code>false</code>"},{"location":"perception/detection_by_tracker/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/detection_by_tracker/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/detection_by_tracker/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/detection_by_tracker/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>[1] M. Himmelsbach, et al. \"Tracking and classification of arbitrary objects with bottom-up/top-down detection.\" (2012).</p> <p>[2] Arya Senna Abdul Rachman, Arya. \"3D-LIDAR Multi Object Tracking for Autonomous Driving: Multi-target Detection and Tracking under Urban Road Uncertainties.\" (2017).</p> <p>[3] David Held, et al. \"A Probabilistic Framework for Real-time 3D Segmentation using Spatial, Temporal, and Semantic Cues.\" (2016).</p>"},{"location":"perception/detection_by_tracker/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/elevation_map_loader/","title":"elevation_map_loader","text":""},{"location":"perception/elevation_map_loader/#elevation_map_loader","title":"elevation_map_loader","text":""},{"location":"perception/elevation_map_loader/#purpose","title":"Purpose","text":"<p>This package provides elevation map for compare_map_segmentation.</p>"},{"location":"perception/elevation_map_loader/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Generate elevation_map from subscribed pointcloud_map and vector_map and publish it. Save the generated elevation_map locally and load it from next time.</p> <p>The elevation value of each cell is the average value of z of the points of the lowest cluster. Cells with No elevation value can be inpainted using the values of neighboring cells.</p> <p> </p>"},{"location":"perception/elevation_map_loader/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/elevation_map_loader/#input","title":"Input","text":"Name Type Description <code>input/pointcloud_map</code> <code>sensor_msgs::msg::PointCloud2</code> The point cloud map <code>input/vector_map</code> <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> (Optional) The binary data of lanelet2 map <code>input/pointcloud_map_metadata</code> <code>autoware_map_msgs::msg::PointCloudMapMetaData</code> (Optional) The metadata of point cloud map"},{"location":"perception/elevation_map_loader/#output","title":"Output","text":"Name Type Description <code>output/elevation_map</code> <code>grid_map_msgs::msg::GridMap</code> The elevation map <code>output/elevation_map_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> (Optional) The point cloud generated from the value of elevation map"},{"location":"perception/elevation_map_loader/#service","title":"Service","text":"Name Type Description <code>service/get_selected_pcd_map</code> <code>autoware_map_msgs::srv::GetSelectedPointCloudMap</code> (Optional) service to request point cloud map. If pointcloud_map_loader uses selected pointcloud map loading via ROS 2 service, use this."},{"location":"perception/elevation_map_loader/#parameters","title":"Parameters","text":""},{"location":"perception/elevation_map_loader/#node-parameters","title":"Node parameters","text":"Name Type Description Default value map_layer_name std::string elevation_map layer name elevation param_file_path std::string GridMap parameters config path_default elevation_map_directory std::string elevation_map file (bag2) path_default map_frame std::string map_frame when loading elevation_map file map use_inpaint bool Whether to inpaint empty cells true inpaint_radius float Radius of a circular neighborhood of each point inpainted that is considered by the algorithm [m] 0.3 use_elevation_map_cloud_publisher bool Whether to publish <code>output/elevation_map_cloud</code> false use_lane_filter bool Whether to filter elevation_map with vector_map false lane_margin float Margin distance from the lane polygon of the area to be included in the inpainting mask [m]. Used only when use_lane_filter=True. 0.0 use_sequential_load bool Whether to get point cloud map by service false sequential_map_load_num int The number of point cloud maps to load at once (only used when use_sequential_load is set true). This should not be larger than number of all point cloud map cells. 1"},{"location":"perception/elevation_map_loader/#gridmap-parameters","title":"GridMap parameters","text":"<p>The parameters are described on <code>config/elevation_map_parameters.yaml</code>.</p>"},{"location":"perception/elevation_map_loader/#general-parameters","title":"General parameters","text":"Name Type Description Default value pcl_grid_map_extraction/num_processing_threads int Number of threads for processing grid map cells. Filtering of the raw input point cloud is not parallelized. 12"},{"location":"perception/elevation_map_loader/#grid-map-parameters","title":"Grid map parameters","text":"<p>See: https://github.com/ANYbotics/grid_map/tree/ros2/grid_map_pcl</p> <p>Resulting grid map parameters.</p> Name Type Description Default value pcl_grid_map_extraction/grid_map/min_num_points_per_cell int Minimum number of points in the point cloud that have to fall within any of the grid map cells. Otherwise the cell elevation will be set to NaN. 3 pcl_grid_map_extraction/grid_map/resolution float Resolution of the grid map. Width and length are computed automatically. 0.3 pcl_grid_map_extraction/grid_map/height_type int The parameter that determine the elevation of a cell <code>0: Smallest value among the average values of each cluster</code>, <code>1: Mean value of the cluster with the most points</code> 1 pcl_grid_map_extraction/grid_map/height_thresh float Height range from the smallest cluster (Only for height_type 1) 1.0"},{"location":"perception/elevation_map_loader/#point-cloud-pre-processing-parameters","title":"Point Cloud Pre-processing Parameters","text":""},{"location":"perception/elevation_map_loader/#rigid-body-transform-parameters","title":"Rigid body transform parameters","text":"<p>Rigid body transform that is applied to the point cloud before computing elevation.</p> Name Type Description Default value pcl_grid_map_extraction/cloud_transform/translation float Translation (xyz) that is applied to the input point cloud before computing elevation. 0.0 pcl_grid_map_extraction/cloud_transform/rotation float Rotation (intrinsic rotation, convention X-Y'-Z'') that is applied to the input point cloud before computing elevation. 0.0"},{"location":"perception/elevation_map_loader/#cluster-extraction-parameters","title":"Cluster extraction parameters","text":"<p>Cluster extraction is based on pcl algorithms. See https://pointclouds.org/documentation/tutorials/cluster_extraction.html for more details.</p> Name Type Description Default value pcl_grid_map_extraction/cluster_extraction/cluster_tolerance float Distance between points below which they will still be considered part of one cluster. 0.2 pcl_grid_map_extraction/cluster_extraction/min_num_points int Min number of points that a cluster needs to have (otherwise it will be discarded). 3 pcl_grid_map_extraction/cluster_extraction/max_num_points int Max number of points that a cluster can have (otherwise it will be discarded). 1000000"},{"location":"perception/elevation_map_loader/#outlier-removal-parameters","title":"Outlier removal parameters","text":"<p>See https://pointclouds.org/documentation/tutorials/statistical_outlier.html for more explanation on outlier removal.</p> Name Type Description Default value pcl_grid_map_extraction/outlier_removal/is_remove_outliers float Whether to perform statistical outlier removal. false pcl_grid_map_extraction/outlier_removal/mean_K float Number of neighbors to analyze for estimating statistics of a point. 10 pcl_grid_map_extraction/outlier_removal/stddev_threshold float Number of standard deviations under which points are considered to be inliers. 1.0"},{"location":"perception/elevation_map_loader/#subsampling-parameters","title":"Subsampling parameters","text":"<p>See https://pointclouds.org/documentation/tutorials/voxel_grid.html for more explanation on point cloud downsampling.</p> Name Type Description Default value pcl_grid_map_extraction/downsampling/is_downsample_cloud bool Whether to perform downsampling or not. false pcl_grid_map_extraction/downsampling/voxel_size float Voxel sizes (xyz) in meters. 0.02"},{"location":"perception/euclidean_cluster/","title":"euclidean_cluster","text":""},{"location":"perception/euclidean_cluster/#euclidean_cluster","title":"euclidean_cluster","text":""},{"location":"perception/euclidean_cluster/#purpose","title":"Purpose","text":"<p>euclidean_cluster is a package for clustering points into smaller parts to classify objects.</p> <p>This package has two clustering methods: <code>euclidean_cluster</code> and <code>voxel_grid_based_euclidean_cluster</code>.</p>"},{"location":"perception/euclidean_cluster/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/euclidean_cluster/#euclidean_cluster_1","title":"euclidean_cluster","text":"<p><code>pcl::EuclideanClusterExtraction</code> is applied to points. See official document for details.</p>"},{"location":"perception/euclidean_cluster/#voxel_grid_based_euclidean_cluster","title":"voxel_grid_based_euclidean_cluster","text":"<ol> <li>A centroid in each voxel is calculated by <code>pcl::VoxelGrid</code>.</li> <li>The centroids are clustered by <code>pcl::EuclideanClusterExtraction</code>.</li> <li>The input points are clustered based on the clustered centroids.</li> </ol>"},{"location":"perception/euclidean_cluster/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/euclidean_cluster/#input","title":"Input","text":"Name Type Description <code>input</code> <code>sensor_msgs::msg::PointCloud2</code> input pointcloud"},{"location":"perception/euclidean_cluster/#output","title":"Output","text":"Name Type Description <code>output</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> cluster pointcloud <code>debug/clusters</code> <code>sensor_msgs::msg::PointCloud2</code> colored cluster pointcloud for visualization"},{"location":"perception/euclidean_cluster/#parameters","title":"Parameters","text":""},{"location":"perception/euclidean_cluster/#core-parameters","title":"Core Parameters","text":""},{"location":"perception/euclidean_cluster/#euclidean_cluster_2","title":"euclidean_cluster","text":"Name Type Description <code>use_height</code> bool use point.z for clustering <code>min_cluster_size</code> int the minimum number of points that a cluster needs to contain in order to be considered valid <code>max_cluster_size</code> int the maximum number of points that a cluster needs to contain in order to be considered valid <code>tolerance</code> float the spatial cluster tolerance as a measure in the L2 Euclidean space"},{"location":"perception/euclidean_cluster/#voxel_grid_based_euclidean_cluster_1","title":"voxel_grid_based_euclidean_cluster","text":"Name Type Description <code>use_height</code> bool use point.z for clustering <code>min_cluster_size</code> int the minimum number of points that a cluster needs to contain in order to be considered valid <code>max_cluster_size</code> int the maximum number of points that a cluster needs to contain in order to be considered valid <code>tolerance</code> float the spatial cluster tolerance as a measure in the L2 Euclidean space <code>voxel_leaf_size</code> float the voxel leaf size of x and y <code>min_points_number_per_voxel</code> int the minimum number of points for a voxel"},{"location":"perception/euclidean_cluster/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/euclidean_cluster/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/euclidean_cluster/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/euclidean_cluster/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/euclidean_cluster/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<p>The <code>use_height</code> option of <code>voxel_grid_based_euclidean_cluster</code> isn't implemented yet.</p>"},{"location":"perception/front_vehicle_velocity_estimator/","title":"front_vehicle_velocity_estimator","text":""},{"location":"perception/front_vehicle_velocity_estimator/#front_vehicle_velocity_estimator","title":"front_vehicle_velocity_estimator","text":"<p>This package contains a front vehicle velocity estimation for offline perception module analysis. This package can:</p> <ul> <li>Attach velocity to 3D detections from velocity estimation with LiDAR pointcloud.</li> </ul>"},{"location":"perception/front_vehicle_velocity_estimator/#algorithm","title":"Algorithm","text":"<ul> <li>Processing flow<ol> <li>Choose front vehicle from front area objects.</li> <li>Choose nearest neighbor point within front vehicle.</li> <li>Estimate velocity of front vehicle by using the differentiated value from time series of nearest neighbor point positions.</li> <li>Compensate ego vehicle twist</li> </ol> </li> </ul>"},{"location":"perception/front_vehicle_velocity_estimator/#input","title":"Input","text":"Name Type Description <code>~/input/objects</code> autoware_auto_perception_msgs/msg/DetectedObject.msg 3D detected objects. <code>~/input/pointcloud</code> sensor_msgs/msg/PointCloud2.msg LiDAR pointcloud. <code>~/input/odometry</code> nav_msgs::msg::Odometry.msg Odometry data."},{"location":"perception/front_vehicle_velocity_estimator/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> autoware_auto_perception_msgs/msg/DetectedObjects.msg 3D detected object with twist. <code>~/debug/nearest_neighbor_pointcloud</code> sensor_msgs/msg/PointCloud2.msg The pointcloud msg of nearest neighbor point."},{"location":"perception/front_vehicle_velocity_estimator/#node-parameter","title":"Node parameter","text":"Name Type Description Default value <code>update_rate_hz</code> double The update rate [hz]. 10.0"},{"location":"perception/front_vehicle_velocity_estimator/#core-parameter","title":"Core parameter","text":"Name Type Description Default value <code>moving_average_num</code> int The moving average number for velocity estimation. 1 <code>threshold_pointcloud_z_high</code> float The threshold for z position value of point when choosing nearest neighbor point within front vehicle [m]. If z &gt; <code>threshold_pointcloud_z_high</code>, the point is considered to noise. 1.0f <code>threshold_pointcloud_z_low</code> float The threshold for z position value of point when choosing nearest neighbor point within front vehicle [m]. If z &lt; <code>threshold_pointcloud_z_low</code>, the point is considered to noise like ground. 0.6f <code>threshold_relative_velocity</code> double The threshold for min and max of estimated relative velocity (\\(v_{re}\\)) [m/s]. If \\(v_{re}\\) &lt; - <code>threshold_relative_velocity</code> , then \\(v_{re}\\) = - <code>threshold_relative_velocity</code>. If \\(v_{re}\\) &gt; <code>threshold_relative_velocity</code>, then \\(v_{re}\\) = <code>threshold_relative_velocity</code>. 10.0 <code>threshold_absolute_velocity</code> double The threshold for max of estimated absolute velocity (\\(v_{ae}\\)) [m/s]. If \\(v_{ae}\\) &gt; <code>threshold_absolute_velocity</code>, then \\(v_{ae}\\) = <code>threshold_absolute_velocity</code>. 20.0"},{"location":"perception/ground_segmentation/","title":"ground_segmentation","text":""},{"location":"perception/ground_segmentation/#ground_segmentation","title":"ground_segmentation","text":""},{"location":"perception/ground_segmentation/#purpose","title":"Purpose","text":"<p>The <code>ground_segmentation</code> is a node that remove the ground points from the input pointcloud.</p>"},{"location":"perception/ground_segmentation/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Detail description of each ground segmentation algorithm is in the following links.</p> Filter Name Description Detail ray_ground_filter A method of removing the ground based on the geometrical relationship between points lined up on radiation link scan_ground_filter Almost the same method as <code>ray_ground_filter</code>, but with slightly improved performance link ransac_ground_filter A method of removing the ground by approximating the ground to a plane link"},{"location":"perception/ground_segmentation/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/ground_segmentation/#input","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points <code>~/input/indices</code> <code>pcl_msgs::msg::Indices</code> reference indices"},{"location":"perception/ground_segmentation/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"perception/ground_segmentation/#parameters","title":"Parameters","text":""},{"location":"perception/ground_segmentation/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>input_frame</code> string \" \" input frame id <code>output_frame</code> string \" \" output frame id <code>max_queue_size</code> int 5 max queue size of input/output topics <code>use_indices</code> bool false flag to use pointcloud indices <code>latched_indices</code> bool false flag to latch pointcloud indices <code>approximate_sync</code> bool false flag to use approximate sync option"},{"location":"perception/ground_segmentation/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p><code>pointcloud_preprocessor::Filter</code> is implemented based on pcl_perception [1] because of this issue.</p>"},{"location":"perception/ground_segmentation/#referencesexternal-links","title":"References/External links","text":"<p>[1] https://github.com/ros-perception/perception_pcl/blob/ros2/pcl_ros/src/pcl_ros/filters/filter.cpp</p>"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/","title":"RANSAC Ground Filter","text":""},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#ransac-ground-filter","title":"RANSAC Ground Filter","text":""},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#purpose","title":"Purpose","text":"<p>The purpose of this node is that remove the ground points from the input pointcloud.</p>"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Apply the input points to the plane, and set the points at a certain distance from the plane as points other than the ground. Normally, whn using this method, the input points is filtered so that it is almost flat before use. Since the drivable area is often flat, there are methods such as filtering by lane.</p>"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#parameters","title":"Parameters","text":""},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description <code>base_frame</code> string base_link frame <code>unit_axis</code> string The axis which we need to search ground plane <code>max_iterations</code> int The maximum number of iterations <code>outlier_threshold</code> double The distance threshold to the model [m] <code>plane_slope_threshold</code> double The slope threshold to prevent mis-fitting [deg] <code>voxel_size_x</code> double voxel size x [m] <code>voxel_size_y</code> double voxel size y [m] <code>voxel_size_z</code> double voxel size z [m] <code>height_threshold</code> double The height threshold from ground plane for no ground points [m] <code>debug</code> bool whether to output debug information"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>This method can't handle slopes.</li> <li>The input points is filtered so that it is almost flat.</li> </ul>"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#referencesexternal-links","title":"References/External links","text":"<p>https://pcl.readthedocs.io/projects/tutorials/en/latest/planar_segmentation.html</p>"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/ground_segmentation/docs/ray-ground-filter/","title":"Ray Ground Filter","text":""},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#ray-ground-filter","title":"Ray Ground Filter","text":""},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#purpose","title":"Purpose","text":"<p>The purpose of this node is that remove the ground points from the input pointcloud.</p>"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The points is separated radially (Ray), and the ground is classified for each Ray sequentially from the point close to ego-vehicle based on the geometric information such as the distance and angle between the points.</p> <p></p>"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#parameters","title":"Parameters","text":""},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description <code>input_frame</code> string frame id of input pointcloud <code>output_frame</code> string frame id of output pointcloud <code>general_max_slope</code> double The triangle created by <code>general_max_slope</code> is called the global cone. If the point is outside the global cone, it is judged to be a point that is no on the ground <code>initial_max_slope</code> double Generally, the point where the object first hits is far from ego-vehicle because of sensor blind spot, so resolution is different from that point and thereafter, so this parameter exists to set a separate <code>local_max_slope</code> <code>local_max_slope</code> double The triangle created by <code>local_max_slope</code> is called the local cone. This parameter for classification based on the continuity of points <code>min_height_threshold</code> double This parameter is used instead of <code>height_threshold</code> because it's difficult to determine continuity in the local cone when the points are too close to each other. <code>radial_divider_angle</code> double The angle of ray <code>concentric_divider_distance</code> double Only check points which radius is larger than <code>concentric_divider_distance</code> <code>reclass_distance_threshold</code> double To check if point is to far from previous one, if so classify again <code>min_x</code> double The parameter to set vehicle footprint manually <code>max_x</code> double The parameter to set vehicle footprint manually <code>min_y</code> double The parameter to set vehicle footprint manually <code>max_y</code> double The parameter to set vehicle footprint manually"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The input_frame is set as parameter but it must be fixed as base_link for the current algorithm.</p>"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/ground_segmentation/docs/scan-ground-filter/","title":"Scan Ground Filter","text":""},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#scan-ground-filter","title":"Scan Ground Filter","text":""},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#purpose","title":"Purpose","text":"<p>The purpose of this node is that remove the ground points from the input pointcloud.</p>"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This algorithm works by following steps,</p> <ol> <li>Divide whole pointclouds into groups by horizontal angle and sort by xy-distance.</li> <li>Divide sorted pointclouds of each ray into grids</li> <li>Check the xy distance to previous pointcloud, if the distance is large and previous pointcloud is \"no ground\" and the height level of current point greater than previous point, the current pointcloud is classified as no ground.</li> <li>Check vertical angle of the point compared with previous ground grid</li> <li>Check the height of the point compared with predicted ground level</li> <li>If vertical angle is greater than local_slope_max and related height to predicted ground level is greater than \"non ground height threshold\", the point is classified as \"non ground\"</li> <li>If the vertical angle is in range of [-local_slope_max, local_slope_max] or related height to predicted ground level is smaller than non_ground_height_threshold, the point is classified as \"ground\"</li> <li>If the vertical angle is lower than -local_slope_max or the related height to ground level is greater than detection_range_z_max, the point will be classified as out of range</li> </ol>"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#parameters","title":"Parameters","text":""},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>input_frame</code> string \"base_link\" frame id of input pointcloud <code>output_frame</code> string \"base_link\" frame id of output pointcloud <code>global_slope_max_angle_deg</code> double 8.0 The global angle to classify as the ground or object [deg].A large threshold may reduce false positive of high slope road classification but it may lead to increase false negative of non-ground classification, particularly for small objects. <code>local_slope_max_angle_deg</code> double 10.0 The local angle to classify as the ground or object [deg] when comparing with adjacent point.A small value enhance accuracy classification of object with inclined surface. This should be considered together with <code>split_points_distance_tolerance</code> value. <code>radial_divider_angle_deg</code> double 1.0 The angle which divide the whole pointcloud to sliced group [deg] <code>split_points_distance_tolerance</code> double 0.2 The xy-distance threshold to distinguish far and near [m] <code>split_height_distance</code> double 0.2 The height threshold to distinguish ground and non-ground pointcloud when comparing with adjacent points [m]. A small threshold improves classification of non-ground point, especially for high elevation resolution pointcloud lidar. However, it might cause false positive for small step-like road surface or misaligned multiple lidar configuration. <code>use_virtual_ground_point</code> bool true whether to use the ground center of front wheels as the virtual ground point. <code>detection_range_z_max</code> float 2.5 Maximum height of detection range [m], applied only for elevation_grid_mode <code>center_pcl_shift</code> float 0.0 The x-axis offset of addition LiDARs from vehicle center of mass [m],  recommended to use only for additional LiDARs in elevation_grid_mode <code>non_ground_height_threshold</code> float 0.2 Height threshold of non ground objects [m] as <code>split_height_distance</code> and applied only for elevation_grid_mode <code>grid_mode_switch_radius</code> float 20.0 The distance where grid division mode change from by distance to by vertical angle [m], applied only for elevation_grid_mode <code>grid_size_m</code> float 0.5 The first grid size [m], applied only for elevation_grid_mode.A large value enhances the prediction stability for ground surface. suitable for rough surface or multiple lidar configuration. <code>gnd_grid_buffer_size</code> uint16 4 Number of grids using to estimate local ground slope, applied only for elevation_grid_mode <code>low_priority_region_x</code> float -20.0 The non-zero x threshold in back side from which small objects detection is low priority [m] <code>elevation_grid_mode</code> bool true Elevation grid scan mode option <code>use_recheck_ground_cluster</code> bool true Enable recheck ground cluster"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The input_frame is set as parameter but it must be fixed as base_link for the current algorithm.</p>"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>The elevation grid idea is referred from \"Shen Z, Liang H, Lin L, Wang Z, Huang W, Yu J. Fast Ground Segmentation for 3D LiDAR Point Cloud Based on Jump-Convolution-Process. Remote Sensing. 2021; 13(16):3239. https://doi.org/10.3390/rs13163239\"</p>"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<ul> <li>Horizontal check for classification is not implemented yet.</li> <li>Output ground visibility for diagnostic is not implemented yet.</li> </ul>"},{"location":"perception/heatmap_visualizer/","title":"heatmap_visualizer","text":""},{"location":"perception/heatmap_visualizer/#heatmap_visualizer","title":"heatmap_visualizer","text":""},{"location":"perception/heatmap_visualizer/#purpose","title":"Purpose","text":"<p>heatmap_visualizer is a package for visualizing heatmap of detected 3D objects' positions on the BEV space.</p> <p>This package is used for qualitative evaluation and trend analysis of the detector, it means, for instance, the heatmap shows \"This detector performs good for near around of our vehicle, but far is bad\".</p>"},{"location":"perception/heatmap_visualizer/#how-to-run","title":"How to run","text":"<pre><code>ros2 launch heatmap_visualizer heatmap_visualizer.launch.xml input/objects:=&lt;DETECTED_OBJECTS_TOPIC&gt;\n</code></pre>"},{"location":"perception/heatmap_visualizer/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>In this implementation, create heatmap of the center position of detected objects for each classes, for instance, CAR, PEDESTRIAN, etc, and publish them as occupancy grid maps.</p> <p></p> <p>In the above figure, the pink represents high detection frequency area and blue one is low, or black represents there is no detection.</p> <p></p> <p>As inner-workings, add center positions of detected objects to index of each corresponding grid map cell in a buffer. The created heatmap will be published by each specific frame, which can be specified with <code>frame_count</code>. Note that the buffer to be add the positions is not reset per publishing. When publishing, firstly these values are normalized to [0, 1] using maximum and minimum values in the buffer. Secondly, they are scaled to integer in [0, 100] because <code>nav_msgs::msg::OccupancyGrid</code> only allow the value in [0, 100].</p>"},{"location":"perception/heatmap_visualizer/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/heatmap_visualizer/#input","title":"Input","text":"Name Type Description <code>~/input/objects</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> detected objects"},{"location":"perception/heatmap_visualizer/#output","title":"Output","text":"Name Type Description <code>~/output/objects/&lt;CLASS_NAME&gt;</code> <code>nav_msgs::msg::OccupancyGrid</code> visualized heatmap"},{"location":"perception/heatmap_visualizer/#parameters","title":"Parameters","text":""},{"location":"perception/heatmap_visualizer/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>publish_frame_count</code> int <code>50</code> The number of frames to publish heatmap <code>heatmap_frame_id</code> string <code>base_link</code> The frame ID of heatmap to be respected <code>heatmap_length</code> float <code>200.0</code> A length of map in meter <code>heatmap_resolution</code> float <code>0.8</code> A resolution of map <code>use_confidence</code> bool <code>false</code> A flag if use confidence score as heatmap value <code>class_names</code> array <code>[\"UNKNOWN\", \"CAR\", \"TRUCK\", \"BUS\", \"TRAILER\", \"BICYCLE\", \"MOTORBIKE\", \"PEDESTRIAN\"]</code> An array of class names to be published <code>rename_to_car</code> bool <code>true</code> A flag if rename car like vehicle to car"},{"location":"perception/heatmap_visualizer/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The heatmap depends on the data to be used, so if the objects in data are sparse the heatmap will be sparse.</p>"},{"location":"perception/heatmap_visualizer/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/heatmap_visualizer/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/heatmap_visualizer/#referencesexternal-links","title":"References/External links","text":""},{"location":"perception/heatmap_visualizer/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/image_projection_based_fusion/","title":"image_projection_based_fusion","text":""},{"location":"perception/image_projection_based_fusion/#image_projection_based_fusion","title":"image_projection_based_fusion","text":""},{"location":"perception/image_projection_based_fusion/#purpose","title":"Purpose","text":"<p>The <code>image_projection_based_fusion</code> is a package to fuse detected obstacles (bounding box or segmentation) from image and 3d pointcloud or obstacles (bounding box, cluster or segmentation).</p>"},{"location":"perception/image_projection_based_fusion/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/image_projection_based_fusion/#sync-algorithm","title":"Sync Algorithm","text":""},{"location":"perception/image_projection_based_fusion/#matching","title":"matching","text":"<p>The offset between each camera and the lidar is set according to their shutter timing. After applying the offset to the timestamp, if the interval between the timestamp of pointcloud topic and the roi message is less than the match threshold, the two messages are matched.</p> <p></p> <p>current default value at autoware.universe for TIER IV Robotaxi are: - input_offset_ms: [61.67, 111.67, 45.0, 28.33, 78.33, 95.0] - match_threshold_ms: 30.0</p>"},{"location":"perception/image_projection_based_fusion/#fusion-and-timer","title":"fusion and timer","text":"<p>The subscription status of the message is signed with 'O'.</p> <p>1.if a pointcloud message is subscribed under the below condition:</p> pointcloud roi msg 1 roi msg 2 roi msg 3 subscription status O O O <p>If the roi msgs can be matched, fuse them and postprocess the pointcloud message. Otherwise, fuse the matched roi msgs and cache the pointcloud.</p> <p>2.if a pointcloud message is subscribed under the below condition:</p> pointcloud roi msg 1 roi msg 2 roi msg 3 subscription status O O <p>if the roi msgs can be matched, fuse them and cache the pointcloud.</p> <p>3.if a pointcloud message is subscribed under the below condition:</p> pointcloud roi msg 1 roi msg 2 roi msg 3 subscription status O O O <p>If the roi msg 3 is subscribed before the next pointcloud message coming or timeout, fuse it if matched, otherwise wait for the next roi msg 3. If the roi msg 3 is not subscribed before the next pointcloud message coming or timeout, postprocess the pointcloud message as it is.</p> <p>The timeout threshold should be set according to the postprocessing time. E.g, if the postprocessing time is around 50ms, the timeout threshold should be set smaller than 50ms, so that the whole processing time could be less than 100ms. current default value at autoware.universe for XX1: - timeout_ms: 50.0</p>"},{"location":"perception/image_projection_based_fusion/#known-limits","title":"Known Limits","text":"<p>The rclcpp::TimerBase timer could not break a for loop, therefore even if time is out when fusing a roi msg at the middle, the program will run until all msgs are fused.</p>"},{"location":"perception/image_projection_based_fusion/#detail-description-of-each-fusions-algorithm-is-in-the-following-links","title":"Detail description of each fusion's algorithm is in the following links","text":"Fusion Name Description Detail roi_cluster_fusion Overwrite a classification label of clusters by that of ROIs from a 2D object detector. link roi_detected_object_fusion Overwrite a classification label of detected objects by that of ROIs from a 2D object detector. link pointpainting_fusion Paint the point cloud with the ROIs from a 2D object detector and feed to a 3D object detector. link roi_pointcloud_fusion Matching pointcloud with ROIs from a 2D object detector to detect unknown-labeled objects link"},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/","title":"pointpainting_fusion","text":""},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/#pointpainting_fusion","title":"pointpainting_fusion","text":""},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/#purpose","title":"Purpose","text":"<p>The <code>pointpainting_fusion</code> is a package for utilizing the class information detected by a 2D object detection in 3D object detection.</p>"},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The lidar points are projected onto the output of an image-only 2d object detection network and the class scores are appended to each point. The painted point cloud can then be fed to the centerpoint network.</p> <p></p>"},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/#input","title":"Input","text":"Name Type Description <code>input</code> <code>sensor_msgs::msg::PointCloud2</code> pointcloud <code>input/camera_info[0-7]</code> <code>sensor_msgs::msg::CameraInfo</code> camera information to project 3d points onto image planes <code>input/rois[0-7]</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> ROIs from each image <code>input/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization"},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/#output","title":"Output","text":"Name Type Description <code>output</code> <code>sensor_msgs::msg::PointCloud2</code> painted pointcloud <code>~/output/objects</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> detected objects <code>~/debug/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization"},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/#parameters","title":"Parameters","text":""},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>score_threshold</code> float <code>0.4</code> detected objects with score less than threshold are ignored <code>densification_world_frame_id</code> string <code>map</code> the world frame id to fuse multi-frame pointcloud <code>densification_num_past_frames</code> int <code>0</code> the number of past frames to fuse with the current frame <code>trt_precision</code> string <code>fp16</code> TensorRT inference precision: <code>fp32</code> or <code>fp16</code> <code>encoder_onnx_path</code> string <code>\"\"</code> path to VoxelFeatureEncoder ONNX file <code>encoder_engine_path</code> string <code>\"\"</code> path to VoxelFeatureEncoder TensorRT Engine file <code>head_onnx_path</code> string <code>\"\"</code> path to DetectionHead ONNX file <code>head_engine_path</code> string <code>\"\"</code> path to DetectionHead TensorRT Engine file <code>build_only</code> bool <code>false</code> shutdown the node after TensorRT engine file is built"},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>The multi-frame painting is not implemented yet.</li> </ul>"},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/#referencesexternal-links","title":"References/External links","text":"<p>[1] Vora, Sourabh, et al. \"PointPainting: Sequential fusion for 3d object detection.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p> <p>[2] CVPR'20 Workshop on Scalability in Autonomous Driving] Waymo Open Dataset Challenge: https://youtu.be/9g9GsI33ol8?t=535 Ding, Zhuangzhuang, et al. \"1st Place Solution for Waymo Open Dataset Challenge--3D Detection and Domain Adaptation.\" arXiv preprint arXiv:2006.15505 (2020).</p>"},{"location":"perception/image_projection_based_fusion/docs/pointpainting-fusion/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/","title":"roi_cluster_fusion","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#roi_cluster_fusion","title":"roi_cluster_fusion","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#purpose","title":"Purpose","text":"<p>The <code>roi_cluster_fusion</code> is a package for filtering clusters that are less likely to be objects and overwriting labels of clusters with that of Region Of Interests (ROIs) by a 2D object detector.</p>"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The clusters are projected onto image planes, and then if the ROIs of clusters and ROIs by a detector are overlapped, the labels of clusters are overwritten with that of ROIs by detector. Intersection over Union (IoU) is used to determine if there are overlaps between them.</p> <p></p>"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#input","title":"Input","text":"Name Type Description <code>input</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> clustered pointcloud <code>input/camera_info[0-7]</code> <code>sensor_msgs::msg::CameraInfo</code> camera information to project 3d points onto image planes <code>input/rois[0-7]</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> ROIs from each image <code>input/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#output","title":"Output","text":"Name Type Description <code>output</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> labeled cluster pointcloud <code>~/debug/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#parameters","title":"Parameters","text":"<p>The following figure is an inner pipeline overview of RoI cluster fusion node. Please refer to it for your parameter settings.</p> <p></p>"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#core-parameters","title":"Core Parameters","text":"Name Type Description <code>fusion_distance</code> double If the detected object's distance to frame_id is less than the threshold, the fusion will be processed <code>trust_object_distance</code> double if the detected object's distance is less than the <code>trust_object_distance</code>, <code>trust_object_iou_mode</code> will be used, otherwise <code>non_trust_object_iou_mode</code> will be used <code>trust_object_iou_mode</code> string select mode from 3 options {<code>iou</code>, <code>iou_x</code>, <code>iou_y</code>} to calculate IoU in range of [<code>0</code>, <code>trust_distance</code>].  <code>iou</code>: IoU along x-axis and y-axis  <code>iou_x</code>: IoU along x-axis  <code>iou_y</code>: IoU along y-axis <code>non_trust_object_iou_mode</code> string the IOU mode using in range of [<code>trust_distance</code>, <code>fusion_distance</code>] if <code>trust_distance</code> &lt; <code>fusion_distance</code> <code>use_cluster_semantic_type</code> bool if <code>false</code>, the labels of clusters are overwritten by <code>UNKNOWN</code> before fusion <code>only_allow_inside_cluster</code> bool if <code>true</code>, the only clusters contained inside RoIs by a detector <code>roi_scale_factor</code> double the scale factor for offset of detector RoIs if <code>only_allow_inside_cluster=true</code> <code>iou_threshold</code> double the IoU threshold to overwrite a label of clusters with a label of roi <code>unknown_iou_threshold</code> double the IoU threshold to fuse cluster with unknown label of roi <code>remove_unknown</code> bool if <code>true</code>, remove all <code>UNKNOWN</code> labeled objects from output <code>rois_number</code> int the number of input rois <code>debug_mode</code> bool If <code>true</code>, subscribe and publish images for visualization."},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/","title":"roi_detected_object_fusion","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#roi_detected_object_fusion","title":"roi_detected_object_fusion","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#purpose","title":"Purpose","text":"<p>The <code>roi_detected_object_fusion</code> is a package to overwrite labels of detected objects with that of Region Of Interests (ROIs) by a 2D object detector.</p>"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>In what follows, we describe the algorithm utilized by <code>roi_detected_object_fusion</code> (the meaning of each parameter can be found in the <code>Parameters</code> section):</p> <ol> <li>If the <code>existence_probability</code> of a detected object is greater than the threshold, it is accepted without any further processing and published in <code>output</code>.</li> <li>The remaining detected objects are projected onto image planes, and if the resulting ROIs overlap with the ones from the 2D detector, they are published as fused objects in <code>output</code>. The Intersection over Union (IoU) is used to determine if there are overlaps between the detections from <code>input</code> and the ROIs from <code>input/rois</code>.</li> </ol> <p>The DetectedObject has three possible shape choices/implementations, where the polygon's vertices for each case are defined as follows:</p> <ul> <li><code>BOUNDING_BOX</code>: The 8 corners of a bounding box.</li> <li><code>CYLINDER</code>: The circle is approximated by a hexagon.</li> <li><code>POLYGON</code>: Not implemented yet.</li> </ul>"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#input","title":"Input","text":"Name Type Description <code>input</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> input detected objects <code>input/camera_info[0-7]</code> <code>sensor_msgs::msg::CameraInfo</code> camera information to project 3d points onto image planes. <code>input/rois[0-7]</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> ROIs from each image. <code>input/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization."},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#output","title":"Output","text":"Name Type Description <code>output</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> detected objects <code>~/debug/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization, <code>~/debug/fused_objects</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> fused detected objects <code>~/debug/ignored_objects</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> not fused detected objects"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#parameters","title":"Parameters","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#core-parameters","title":"Core Parameters","text":"Name Type Description <code>rois_number</code> int the number of input rois <code>debug_mode</code> bool If set to <code>true</code>, the node subscribes to the image topic and publishes an image with debug drawings. <code>passthrough_lower_bound_probability_thresholds</code> vector[double] If the <code>existence_probability</code> of a detected object is greater than the threshold, it is published in output. <code>trust_distances</code> vector[double] If the distance of a detected object from the origin of frame_id is greater than the threshold, it is published in output. <code>min_iou_threshold</code> double If the iou between detected objects and rois is greater than <code>min_iou_threshold</code>, the objects are classified as fused. <code>use_roi_probability</code> float If set to <code>true</code>, the algorithm uses <code>existence_probability</code> of ROIs to match with the that of detected objects. <code>roi_probability_threshold</code> double If the <code>existence_probability</code> of ROIs is greater than the threshold, matched detected objects are published in <code>output</code>. <code>can_assign_matrix</code> vector[int] association matrix between rois and detected_objects to check that two rois on images can be match"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p><code>POLYGON</code>, which is a shape of a detected object, isn't supported yet.</p>"},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/","title":"roi_pointcloud_fusion","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/#roi_pointcloud_fusion","title":"roi_pointcloud_fusion","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/#purpose","title":"Purpose","text":"<p>The node <code>roi_pointcloud_fusion</code> is to cluster the pointcloud based on Region Of Interests (ROIs) detected by a 2D object detector, specific for unknown labeled ROI.</p>"},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ul> <li>The pointclouds are projected onto image planes and extracted as cluster if they are inside the unknown labeled ROIs.</li> <li>Since the cluster might contain unrelated points from background, then a refinement step is added to filter the background pointcloud by distance to camera.</li> </ul>"},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/#input","title":"Input","text":"Name Type Description <code>input</code> <code>sensor_msgs::msg::PointCloud2</code> input pointcloud <code>input/camera_info[0-7]</code> <code>sensor_msgs::msg::CameraInfo</code> camera information to project 3d points onto image planes <code>input/rois[0-7]</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> ROIs from each image <code>input/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization"},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/#output","title":"Output","text":"Name Type Description <code>output</code> <code>sensor_msgs::msg::PointCloud2</code> output pointcloud as default of interface <code>output_clusters</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> output clusters <code>debug/clusters</code> <code>sensor_msgs/msg/PointCloud2</code> colored cluster pointcloud for visualization"},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/#parameters","title":"Parameters","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/#core-parameters","title":"Core Parameters","text":"Name Type Description <code>min_cluster_size</code> int the minimum number of points that a cluster needs to contain in order to be considered valid <code>cluster_2d_tolerance</code> double cluster tolerance measured in radial direction <code>rois_number</code> int the number of input rois"},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>Currently, the refinement is only based on distance to camera, the roi based clustering is supposed to work well with small object ROIs. Others criteria for refinement might needed in the future.</li> </ul>"},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/image_projection_based_fusion/docs/roi-pointcloud-fusion/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/lidar_apollo_instance_segmentation/","title":"lidar_apollo_instance_segmentation","text":""},{"location":"perception/lidar_apollo_instance_segmentation/#lidar_apollo_instance_segmentation","title":"lidar_apollo_instance_segmentation","text":""},{"location":"perception/lidar_apollo_instance_segmentation/#purpose","title":"Purpose","text":"<p>This node segments 3D pointcloud data from lidar sensors into obstacles, e.g., cars, trucks, bicycles, and pedestrians based on CNN based model and obstacle clustering method.</p>"},{"location":"perception/lidar_apollo_instance_segmentation/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>See the original design by Apollo.</p>"},{"location":"perception/lidar_apollo_instance_segmentation/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/lidar_apollo_instance_segmentation/#input","title":"Input","text":"Name Type Description <code>input/pointcloud</code> <code>sensor_msgs/PointCloud2</code> Pointcloud data from lidar sensors"},{"location":"perception/lidar_apollo_instance_segmentation/#output","title":"Output","text":"Name Type Description <code>output/labeled_clusters</code> <code>tier4_perception_msgs/DetectedObjectsWithFeature</code> Detected objects with labeled pointcloud cluster. <code>debug/instance_pointcloud</code> <code>sensor_msgs/PointCloud2</code> Segmented pointcloud for visualization."},{"location":"perception/lidar_apollo_instance_segmentation/#parameters","title":"Parameters","text":""},{"location":"perception/lidar_apollo_instance_segmentation/#node-parameters","title":"Node Parameters","text":"<p>None</p>"},{"location":"perception/lidar_apollo_instance_segmentation/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>score_threshold</code> double 0.8 If the score of a detected object is lower than this value, the object is ignored. <code>range</code> int 60 Half of the length of feature map sides. [m] <code>width</code> int 640 The grid width of feature map. <code>height</code> int 640 The grid height of feature map. <code>engine_file</code> string \"vls-128.engine\" The name of TensorRT engine file for CNN model. <code>prototxt_file</code> string \"vls-128.prototxt\" The name of prototxt file for CNN model. <code>caffemodel_file</code> string \"vls-128.caffemodel\" The name of caffemodel file for CNN model. <code>use_intensity_feature</code> bool true The flag to use intensity feature of pointcloud. <code>use_constant_feature</code> bool false The flag to use direction and distance feature of pointcloud. <code>target_frame</code> string \"base_link\" Pointcloud data is transformed into this frame. <code>z_offset</code> int 2 z offset from target frame. [m]"},{"location":"perception/lidar_apollo_instance_segmentation/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>There is no training code for CNN model.</p>"},{"location":"perception/lidar_apollo_instance_segmentation/#note","title":"Note","text":"<p>This package makes use of three external codes. The trained files are provided by apollo. The trained files are automatically downloaded when you build.</p> <p>Original URL</p> <ul> <li>VLP-16 :   https://github.com/ApolloAuto/apollo/raw/88bfa5a1acbd20092963d6057f3a922f3939a183/modules/perception/production/data/perception/lidar/models/cnnseg/velodyne16/deploy.caffemodel</li> <li>HDL-64 :   https://github.com/ApolloAuto/apollo/raw/88bfa5a1acbd20092963d6057f3a922f3939a183/modules/perception/production/data/perception/lidar/models/cnnseg/velodyne64/deploy.caffemodel</li> <li>VLS-128 :   https://github.com/ApolloAuto/apollo/raw/91844c80ee4bd0cc838b4de4c625852363c258b5/modules/perception/production/data/perception/lidar/models/cnnseg/velodyne128/deploy.caffemodel</li> </ul> <p>Supported lidars are velodyne 16, 64 and 128, but you can also use velodyne 32 and other lidars with good accuracy.</p> <ol> <li> <p>apollo 3D Obstacle Perception description</p> <pre><code>/******************************************************************************\n* Copyright 2017 The Apollo Authors. All Rights Reserved.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n* http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*****************************************************************************/\n</code></pre> </li> <li> <p>tensorRTWrapper :    It is used under the lib directory.</p> <pre><code>MIT License\n\nCopyright (c) 2018 lewes6369\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre> </li> <li> <p>autoware_perception description</p> <pre><code>/*\n* Copyright 2018-2019 Autoware Foundation. All rights reserved.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*     http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n</code></pre> </li> </ol>"},{"location":"perception/lidar_apollo_instance_segmentation/#special-thanks","title":"Special thanks","text":"<ul> <li>Apollo Project</li> <li>lewes6369</li> <li>Autoware Foundation</li> <li>Kosuke Takeuchi (TIER IV)</li> </ul>"},{"location":"perception/lidar_apollo_segmentation_tvm/","title":"lidar_apollo_segmentation_tvm","text":""},{"location":"perception/lidar_apollo_segmentation_tvm/#lidar_apollo_segmentation_tvm","title":"lidar_apollo_segmentation_tvm","text":""},{"location":"perception/lidar_apollo_segmentation_tvm/#design","title":"Design","text":""},{"location":"perception/lidar_apollo_segmentation_tvm/#usage","title":"Usage","text":""},{"location":"perception/lidar_apollo_segmentation_tvm/#neural-network","title":"Neural network","text":"<p>This package will not run without a neural network for its inference. The network is provided by ansible script during the installation of Autoware or can be downloaded manually according to Manual Downloading. This package uses 'get_neural_network' function from tvm_utility package to create and provide proper dependency. See its design page for more information on how to handle user-compiled networks.</p>"},{"location":"perception/lidar_apollo_segmentation_tvm/#backend","title":"Backend","text":"<p>The backend used for the inference can be selected by setting the <code>lidar_apollo_segmentation_tvm_BACKEND</code> cmake variable. The current available options are <code>llvm</code> for a CPU backend, and <code>vulkan</code> for a GPU backend. It defaults to <code>llvm</code>.</p>"},{"location":"perception/lidar_apollo_segmentation_tvm/#convolutional-neural-networks-cnn-segmentation","title":"Convolutional Neural Networks (CNN) Segmentation","text":"<p>See the original design by Apollo. The paragraph of interest goes up to, but excluding, the \"MinBox Builder\" paragraph. This package instead relies on further processing by a dedicated shape estimator.</p> <p>Note: the parameters described in the original design have been modified and are out of date.</p>"},{"location":"perception/lidar_apollo_segmentation_tvm/#inputs-outputs-api","title":"Inputs / Outputs / API","text":"<p>The package exports a boolean <code>lidar_apollo_segmentation_tvm_BUILT</code> cmake variable.</p>"},{"location":"perception/lidar_apollo_segmentation_tvm/#reference","title":"Reference","text":"<p>Lidar segmentation is based off a core algorithm by Apollo, with modifications from [TIER IV] (https://github.com/tier4/lidar_instance_segmentation_tvm) for the TVM backend.</p>"},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/","title":"Index","text":""},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#lidar_apollo_segmentation_tvm_nodes","title":"lidar_apollo_segmentation_tvm_nodes","text":""},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>An alternative to Euclidean clustering. This node detects and labels foreground obstacles (e.g. cars, motorcycles, pedestrians) from a point cloud, using a neural network.</p>"},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#design","title":"Design","text":"<p>See the design of the algorithm in the core (lidar_apollo_segmentation_tvm) package's design documents.</p>"},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#usage","title":"Usage","text":"<p><code>lidar_apollo_segmentation_tvm</code> and <code>lidar_apollo_segmentation_tvm_nodes</code> will not work without a neural network. See the lidar_apollo_segmentation_tvm usage for more information.</p>"},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The original node from Apollo has a Region Of Interest (ROI) filter. This has the benefit of working with a filtered point cloud that includes only the points inside the ROI (i.e., the drivable road and junction areas) with most of the background obstacles removed (such as buildings and trees around the road region). Not having this filter may negatively impact performance.</p>"},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#inputs-outputs-api","title":"Inputs / Outputs / API","text":""},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#inputs","title":"Inputs","text":"<p>The input are non-ground points as a PointCloud2 message from the sensor_msgs package.</p>"},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#outputs","title":"Outputs","text":"<p>The output is a DetectedObjectsWithFeature.</p>"},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#parameters","title":"Parameters","text":"Name Type Description Default Range range integer The range of the 2D grid with respect to the origin. 90 &gt;0 score_threshold float The detection confidence score threshold for filtering out the candidate clusters in the post-processing step. 0.1 \u22650.0\u22641.0 use_intensity_feature boolean Enable input channel intensity feature. false N/A use_constant_feature boolean Enable input channel constant feature. false N/A z_offset float Vertical translation of the pointcloud before inference. 0.0 N/A min_height float The minimum height with respect to the origin -5.0 N/A max_height float The maximum height with respect to the origin. 5.0 N/A objectness_thresh float The threshold of objectness for filtering out non-object cells in the obstacle clustering step. 0.5 \u22650.0\u22641.0 min_pts_num integer In the post-processing step, the candidate clusters with less than min_pts_num points are removed. 3 \u22650 height_thresh float If it is non-negative, the points that are higher than the predicted object height by height_thresh are filtered out in the post-processing step. 0.5 N/A data_path string Packages data and artifacts directory path. $(env HOME)/autoware_data N/A"},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#error-detection-and-handling","title":"Error detection and handling","text":"<p>Abort and warn when the input frame can't be converted to <code>base_link</code>.</p>"},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#security-considerations","title":"Security considerations","text":"<p>Both the input and output are controlled by the same actor, so the following security concerns are out-of-scope:</p> <ul> <li>Spoofing</li> <li>Tampering</li> </ul> <p>Leaking data to another actor would require a flaw in TVM or the host operating system that allows arbitrary memory to be read, a significant security flaw in itself. This is also true for an external actor operating the pipeline early: only the object that initiated the pipeline can run the methods to receive its output.</p> <p>A Denial-of-Service attack could make the target hardware unusable for other pipelines but would require being able to run code on the CPU, which would already allow a more severe Denial-of-Service attack.</p> <p>No elevation of privilege is required for this package.</p>"},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#related-issues","title":"Related issues","text":""},{"location":"perception/lidar_apollo_segmentation_tvm_nodes/#226-autowareauto-neural-networks-inference-architecture-design","title":"226: Autoware.Auto Neural Networks Inference Architecture Design","text":""},{"location":"perception/lidar_centerpoint/","title":"lidar_centerpoint","text":""},{"location":"perception/lidar_centerpoint/#lidar_centerpoint","title":"lidar_centerpoint","text":""},{"location":"perception/lidar_centerpoint/#purpose","title":"Purpose","text":"<p>lidar_centerpoint is a package for detecting dynamic 3D objects.</p>"},{"location":"perception/lidar_centerpoint/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>In this implementation, CenterPoint [1] uses a PointPillars-based [2] network to inference with TensorRT.</p> <p>We trained the models using https://github.com/open-mmlab/mmdetection3d.</p>"},{"location":"perception/lidar_centerpoint/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/lidar_centerpoint/#input","title":"Input","text":"Name Type Description <code>~/input/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> input pointcloud"},{"location":"perception/lidar_centerpoint/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> detected objects <code>debug/cyclic_time_ms</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> cyclic time (msg) <code>debug/processing_time_ms</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> processing time (ms)"},{"location":"perception/lidar_centerpoint/#parameters","title":"Parameters","text":""},{"location":"perception/lidar_centerpoint/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>score_threshold</code> float <code>0.4</code> detected objects with score less than threshold are ignored <code>densification_world_frame_id</code> string <code>map</code> the world frame id to fuse multi-frame pointcloud <code>densification_num_past_frames</code> int <code>1</code> the number of past frames to fuse with the current frame <code>trt_precision</code> string <code>fp16</code> TensorRT inference precision: <code>fp32</code> or <code>fp16</code> <code>encoder_onnx_path</code> string <code>\"\"</code> path to VoxelFeatureEncoder ONNX file <code>encoder_engine_path</code> string <code>\"\"</code> path to VoxelFeatureEncoder TensorRT Engine file <code>head_onnx_path</code> string <code>\"\"</code> path to DetectionHead ONNX file <code>head_engine_path</code> string <code>\"\"</code> path to DetectionHead TensorRT Engine file <code>nms_iou_target_class_names</code> list[string] - target classes for IoU-based Non Maximum Suppression <code>nms_iou_search_distance_2d</code> double - If two objects are farther than the value, NMS isn't applied. <code>nms_iou_threshold</code> double - IoU threshold for the IoU-based Non Maximum Suppression <code>build_only</code> bool <code>false</code> shutdown the node after TensorRT engine file is built"},{"location":"perception/lidar_centerpoint/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>The <code>object.existence_probability</code> is stored the value of classification confidence of a DNN, not probability.</li> </ul>"},{"location":"perception/lidar_centerpoint/#trained-models","title":"Trained Models","text":"<p>You can download the onnx format of trained models by clicking on the links below.</p> <ul> <li>Centerpoint : pts_voxel_encoder_centerpoint.onnx, pts_backbone_neck_head_centerpoint.onnx</li> <li>Centerpoint tiny: pts_voxel_encoder_centerpoint_tiny.onnx, pts_backbone_neck_head_centerpoint_tiny.onnx</li> </ul> <p><code>Centerpoint</code> was trained in <code>nuScenes</code> (~28k lidar frames) [8] and TIER IV's internal database (~11k lidar frames) for 60 epochs. <code>Centerpoint tiny</code> was trained in <code>Argoverse 2</code> (~110k lidar frames) [9] and TIER IV's internal database (~11k lidar frames) for 20 epochs.</p>"},{"location":"perception/lidar_centerpoint/#standalone-inference-and-visualization","title":"Standalone inference and visualization","text":"<p>In addition to its use as a standard ROS node, <code>lidar_centerpoint</code> can also be used to perform inferences in an isolated manner. To do so, execute the following launcher, where <code>pcd_path</code> is the path of the pointcloud to be used for inference.</p> <pre><code>ros2 launch lidar_centerpoint single_inference_lidar_centerpoint.launch.xml pcd_path:=test_pointcloud.pcd detections_path:=test_detections.ply\n</code></pre> <p><code>lidar_centerpoint</code> generates a <code>ply</code> file in the provided <code>detections_path</code>, which contains the detections as triangle meshes. These detections can be visualized by most 3D tools, but we also integrate a visualization UI using <code>Open3D</code> which is launched alongside <code>lidar_centerpoint</code>.</p>"},{"location":"perception/lidar_centerpoint/#changelog","title":"Changelog","text":""},{"location":"perception/lidar_centerpoint/#v1-20220706","title":"v1 (2022/07/06)","text":"Name URLs Description <code>centerpoint</code> pts_voxel_encoder pts_backbone_neck_head There is a single change due to the limitation in the implementation of this package. <code>num_filters=[32, 32]</code> of <code>PillarFeatureNet</code> <code>centerpoint_tiny</code> pts_voxel_encoder pts_backbone_neck_head The same model as <code>default</code> of <code>v0</code>. <p>These changes are compared with this configuration.</p>"},{"location":"perception/lidar_centerpoint/#v0-20211203","title":"v0 (2021/12/03)","text":"Name URLs Description <code>default</code> pts_voxel_encoder pts_backbone_neck_head There are two changes from the original CenterPoint architecture. <code>num_filters=[32]</code> of <code>PillarFeatureNet</code> and <code>ds_layer_strides=[2, 2, 2]</code> of <code>RPN</code>"},{"location":"perception/lidar_centerpoint/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/lidar_centerpoint/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/lidar_centerpoint/#referencesexternal-links","title":"References/External links","text":"<p>[1] Yin, Tianwei, Xingyi Zhou, and Philipp Kr\u00e4henb\u00fchl. \"Center-based 3d object detection and tracking.\" arXiv preprint arXiv:2006.11275 (2020).</p> <p>[2] Lang, Alex H., et al. \"PointPillars: Fast encoders for object detection from point clouds.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.</p> <p>[3] https://github.com/tianweiy/CenterPoint</p> <p>[4] https://github.com/open-mmlab/mmdetection3d</p> <p>[5] https://github.com/open-mmlab/OpenPCDet</p> <p>[6] https://github.com/yukkysaito/autoware_perception</p> <p>[7] https://github.com/NVIDIA-AI-IOT/CUDA-PointPillars</p> <p>[8] https://www.nuscenes.org/nuscenes</p> <p>[9] https://www.argoverse.org/av2.html</p>"},{"location":"perception/lidar_centerpoint/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/lidar_centerpoint/launch/centerpoint_vs_centerpoint-tiny/","title":"Run lidar_centerpoint and lidar_centerpoint-tiny simultaneously","text":""},{"location":"perception/lidar_centerpoint/launch/centerpoint_vs_centerpoint-tiny/#run-lidar_centerpoint-and-lidar_centerpoint-tiny-simultaneously","title":"Run lidar_centerpoint and lidar_centerpoint-tiny simultaneously","text":"<p>This tutorial is for showing <code>centerpoint</code> and <code>centerpoint_tiny</code>models\u2019 results simultaneously, making it easier to visualize and compare the performance.</p>"},{"location":"perception/lidar_centerpoint/launch/centerpoint_vs_centerpoint-tiny/#setup-development-environment","title":"Setup Development Environment","text":"<p>Follow the steps in the Source Installation (link) in Autoware doc.</p> <p>If you fail to build autoware environment according to lack of memory, then it is recommended to build autoware sequentially.</p> <p>Source the ROS 2 Galactic setup script.</p> <pre><code>source /opt/ros/galactic/setup.bash\n</code></pre> <p>Build the entire autoware repository.</p> <pre><code>colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release --parallel-workers=1\n</code></pre> <p>Or you can use a constrained number of CPU to build only one package.</p> <pre><code>export MAKEFLAGS=\"-j 4\" &amp;&amp; MAKE_JOBS=4 colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release --parallel-workers 1 --packages-select PACKAGE_NAME\n</code></pre> <p>Source the package.</p> <pre><code>source install/setup.bash\n</code></pre>"},{"location":"perception/lidar_centerpoint/launch/centerpoint_vs_centerpoint-tiny/#data-preparation","title":"Data Preparation","text":""},{"location":"perception/lidar_centerpoint/launch/centerpoint_vs_centerpoint-tiny/#using-rosbag-dataset","title":"Using rosbag dataset","text":"<pre><code>ros2 bag play /YOUR/ROSBAG/PATH/ --clock 100\n</code></pre> <p>Don't forget to add <code>clock</code> in order to sync between two rviz display.</p> <p>You can also use the sample rosbag provided by autoware here.</p> <p>If you want to merge several rosbags into one, you can refer to this tool.</p>"},{"location":"perception/lidar_centerpoint/launch/centerpoint_vs_centerpoint-tiny/#using-realtime-lidar-dataset","title":"Using realtime LiDAR dataset","text":"<p>Set up your Ethernet connection according to 1.1 - 1.3 in this website.</p> <p>Download Velodyne ROS driver</p> <pre><code>git clone -b ros2 https://github.com/ros-drivers/velodyne.git\n</code></pre> <p>Source the ROS 2 Galactic setup script.</p> <pre><code>source /opt/ros/galactic/setup.bash\n</code></pre> <p>Compile Velodyne driver</p> <pre><code>cd velodyne\nrosdep install -y --from-paths . --ignore-src --rosdistro $ROS_DISTRO\ncolcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release\n</code></pre> <p>Edit the configuration file. Specify the LiDAR device IP address in <code>./velodyne_driver/config/VLP32C-velodyne_driver_node-params.yaml</code></p> <pre><code>velodyne_driver_node:\nros__parameters:\ndevice_ip: 192.168.1.201 //change to your LiDAR device IP address\ngps_time: false\ntime_offset: 0.0\nenabled: true\nread_once: false\nread_fast: false\nrepeat_delay: 0.0\nframe_id: velodyne\nmodel: 32C\nrpm: 600.0\nport: 2368\n</code></pre> <p>Launch the velodyne driver.</p> <pre><code># Terminal 1\nros2 launch velodyne_driver velodyne_driver_node-VLP32C-launch.py\n</code></pre> <p>Launch the velodyne_pointcloud.</p> <pre><code># Terminal 2\nros2 launch velodyne_pointcloud velodyne_convert_node-VLP32C-launch.py\n</code></pre> <p>Point Cloud data will be available on topic <code>/velodyne_points</code>. You can check with <code>ros2 topic echo /velodyne_points</code>.</p> <p>Check this website if there is any unexpected issue.</p>"},{"location":"perception/lidar_centerpoint/launch/centerpoint_vs_centerpoint-tiny/#launch-file-setting","title":"Launch file setting","text":"<p>Several fields to check in <code>centerpoint_vs_centerpoint-tiny.launch.xml</code> before running lidar centerpoint.</p> <ul> <li><code>input/pointcloud</code> : set to the topic with input data you want to subscribe.</li> <li><code>model_path</code> : set to the path of the model.</li> <li><code>model_param_path</code> : set to the path of model's config file.</li> </ul>"},{"location":"perception/lidar_centerpoint/launch/centerpoint_vs_centerpoint-tiny/#run-centerpoint-and-centerpoint-tiny-simultaneously","title":"Run CenterPoint and CenterPoint-tiny simultaneously","text":"<p>Run</p> <pre><code>ros2 launch lidar_centerpoint centerpoint_vs_centerpoint-tiny.launch.xml\n</code></pre> <p>Then you will see two rviz window show immediately. On the left is the result for lidar centerpoint tiny, and on the right is the result for lidar centerpoint.</p> <p></p>"},{"location":"perception/lidar_centerpoint/launch/centerpoint_vs_centerpoint-tiny/#troubleshooting","title":"Troubleshooting","text":""},{"location":"perception/lidar_centerpoint/launch/centerpoint_vs_centerpoint-tiny/#bounding-box-blink-on-rviz","title":"Bounding Box blink on rviz","text":"<p>To avoid Bounding Boxes blinking on rviz, you can extend bbox marker lifetime.</p> <p>Set <code>marker_ptr-&gt;lifetime</code> and <code>marker.lifetime</code> to a longer lifetime.</p> <ul> <li><code>marker_ptr-&gt;lifetime</code> are in <code>PATH/autoware/src/universe/autoware.universe/common/autoware_auto_perception_rviz_plugin/src/object_detection/object_polygon_detail.cpp</code></li> <li><code>marker.lifetime</code> are in <code>PATH/autoware/src/universe/autoware.universe/common/tier4_autoware_utils/include/tier4_autoware_utils/ros/marker_helper.hpp</code></li> </ul> <p>Make sure to rebuild packages after any change.</p>"},{"location":"perception/lidar_centerpoint_tvm/","title":"lidar_centerpoint_tvm","text":""},{"location":"perception/lidar_centerpoint_tvm/#lidar_centerpoint_tvm","title":"lidar_centerpoint_tvm","text":""},{"location":"perception/lidar_centerpoint_tvm/#design","title":"Design","text":""},{"location":"perception/lidar_centerpoint_tvm/#usage","title":"Usage","text":"<p>lidar_centerpoint_tvm is a package for detecting dynamic 3D objects using TVM compiled centerpoint module for different backends. To use this package, replace <code>lidar_centerpoint</code> with <code>lidar_centerpoint_tvm</code> in perception launch files(for example, <code>lidar_based_detection.launch.xml</code> is lidar based detection is chosen.).</p>"},{"location":"perception/lidar_centerpoint_tvm/#neural-network","title":"Neural network","text":"<p>This package will not build without a neural network for its inference. The network is provided by the <code>tvm_utility</code> package. See its design page for more information on how to enable downloading pre-compiled networks (by setting the <code>DOWNLOAD_ARTIFACTS</code> cmake variable), or how to handle user-compiled networks.</p>"},{"location":"perception/lidar_centerpoint_tvm/#backend","title":"Backend","text":"<p>The backend used for the inference can be selected by setting the <code>lidar_centerpoint_tvm_BACKEND</code> cmake variable. The current available options are <code>llvm</code> for a CPU backend, and <code>vulkan</code> or <code>opencl</code> for a GPU backend. It defaults to <code>llvm</code>.</p>"},{"location":"perception/lidar_centerpoint_tvm/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/lidar_centerpoint_tvm/#input","title":"Input","text":"Name Type Description <code>~/input/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> input pointcloud"},{"location":"perception/lidar_centerpoint_tvm/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> detected objects <code>debug/cyclic_time_ms</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> cyclic time (msg) <code>debug/processing_time_ms</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> processing time (ms)"},{"location":"perception/lidar_centerpoint_tvm/#parameters","title":"Parameters","text":""},{"location":"perception/lidar_centerpoint_tvm/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>score_threshold</code> float <code>0.1</code> detected objects with score less than threshold are ignored <code>densification_world_frame_id</code> string <code>map</code> the world frame id to fuse multi-frame pointcloud <code>densification_num_past_frames</code> int <code>1</code> the number of past frames to fuse with the current frame"},{"location":"perception/lidar_centerpoint_tvm/#bounding-box","title":"Bounding Box","text":"<p>The lidar segmentation node establishes a bounding box for the detected obstacles. The <code>L-fit</code> method of fitting a bounding box to a cluster is used for that.</p>"},{"location":"perception/lidar_centerpoint_tvm/#limitation-and-known-issue","title":"Limitation and Known Issue","text":"<p>Due to an accuracy issue of <code>centerpoint</code> model, <code>vulkan</code> cannot be used at the moment. As for 'llvm' backend, real-time performance cannot be achieved.</p>"},{"location":"perception/lidar_centerpoint_tvm/#scatter-implementation","title":"Scatter Implementation","text":"<p>Scatter function can be implemented using either TVMScript or C++. For C++ implementation, please refer to https://github.com/angry-crab/autoware.universe/blob/c020419fe52e359287eccb1b77e93bdc1a681e24/perception/lidar_centerpoint_tvm/lib/network/scatter.cpp#L65</p>"},{"location":"perception/lidar_centerpoint_tvm/#reference","title":"Reference","text":"<p>[1] Yin, Tianwei, Xingyi Zhou, and Philipp Kr\u00e4henb\u00fchl. \"Center-based 3d object detection and tracking.\" arXiv preprint arXiv:2006.11275 (2020).</p> <p>[2] Lang, Alex H., et al. \"PointPillars: Fast encoders for object detection from point clouds.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.</p> <p>[3] https://github.com/tianweiy/CenterPoint</p> <p>[4] https://github.com/Abraham423/CenterPoint</p> <p>[5] https://github.com/open-mmlab/OpenPCDet</p>"},{"location":"perception/lidar_centerpoint_tvm/#related-issues","title":"Related issues","text":""},{"location":"perception/lidar_centerpoint_tvm/#908-run-lidar-centerpoint-with-tvm","title":"908: Run Lidar Centerpoint with TVM","text":""},{"location":"perception/map_based_prediction/","title":"map_based_prediction","text":""},{"location":"perception/map_based_prediction/#map_based_prediction","title":"map_based_prediction","text":""},{"location":"perception/map_based_prediction/#role","title":"Role","text":"<p><code>map_based_prediction</code> is a module to predict the future paths (and their probabilities) of other vehicles and pedestrians according to the shape of the map and the surrounding environment.</p>"},{"location":"perception/map_based_prediction/#assumptions","title":"Assumptions","text":"<ul> <li>The following information about the target obstacle is needed<ul> <li>Label (type of person, car, etc.)</li> <li>The object position in the current time and predicted position in the future time.</li> </ul> </li> <li>The following information about the surrounding environment is needed<ul> <li>Road network information with Lanelet2 format</li> </ul> </li> </ul>"},{"location":"perception/map_based_prediction/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/map_based_prediction/#flow-chart","title":"Flow chart","text":""},{"location":"perception/map_based_prediction/#path-prediction-for-road-users","title":"Path prediction for road users","text":""},{"location":"perception/map_based_prediction/#remove-old-object-history","title":"Remove old object history","text":"<p>Store time-series data of objects to determine the vehicle's route and to detect lane change for several duration. Object Data contains the object's position, speed, and time information.</p>"},{"location":"perception/map_based_prediction/#get-current-lanelet-and-update-object-history","title":"Get current lanelet and update Object history","text":"<p>Search one or more lanelets satisfying the following conditions for each target object and store them in the ObjectData.</p> <ul> <li>The CoG of the object must be inside the lanelet.</li> <li>The centerline of the lanelet must have two or more points.</li> <li>The angle difference between the lanelet and the direction of the object must be within the threshold given by the parameters.<ul> <li>The angle flip is allowed, the condition is <code>diff_yaw &lt; threshold or diff_yaw &gt; pi - threshold</code>.</li> </ul> </li> <li>The lanelet must be reachable from the lanelet recorded in the past history.</li> </ul>"},{"location":"perception/map_based_prediction/#get-predicted-reference-path","title":"Get predicted reference path","text":"<ul> <li>Get reference path:<ul> <li>Create a reference path for the object from the associated lanelet.</li> </ul> </li> <li>Predict object maneuver:<ul> <li>Generate predicted paths for the object.</li> <li>Assign probability to each maneuver of <code>Lane Follow</code>, <code>Left Lane Change</code>, and <code>Right Lane Change</code> based on the object history and the reference path obtained in the first step.</li> <li>Lane change decision is based on two domains:<ul> <li>Geometric domain: the lateral distance between the center of gravity of the object and left/right boundaries of the lane.</li> <li>Time domain: estimated time margin for the object to reach the left/right bound.</li> </ul> </li> </ul> </li> </ul> <p>The conditions for left lane change detection are:</p> <ul> <li>Check if the distance to the left lane boundary is less than the distance to the right lane boundary.</li> <li>Check if the distance to the left lane boundary is less than a <code>dist_threshold_to_bound_</code>.</li> <li>Check if the lateral velocity direction is towards the left lane boundary.</li> <li>Check if the time to reach the left lane boundary is less than <code>time_threshold_to_bound_</code>.</li> </ul> <p>Lane change logics is illustrated in the figure below.An example of how to tune the parameters is described later.</p> <p></p> <ul> <li>Calculate object probability:<ul> <li>The path probability obtained above is calculated based on the current position and angle of the object.</li> </ul> </li> <li>Refine predicted paths for smooth movement:<ul> <li>The generated predicted paths are recomputed to take the vehicle dynamics into account.</li> <li>The path is calculated with minimum jerk trajectory implemented by 4th/5th order spline for lateral/longitudinal motion.</li> </ul> </li> </ul>"},{"location":"perception/map_based_prediction/#tuning-lane-change-detection-logic","title":"Tuning lane change detection logic","text":"<p>Currently we provide two parameters to tune lane change detection:</p> <ul> <li><code>dist_threshold_to_bound_</code>: maximum distance from lane boundary allowed for lane changing vehicle</li> <li><code>time_threshold_to_bound_</code>: maximum time allowed for lane change vehicle to reach the boundary</li> <li><code>cutoff_freq_of_velocity_lpf_</code>: cutoff frequency of low pass filter for lateral velocity</li> </ul> <p>You can change these parameters in rosparam in the table below.</p> param name default value <code>dist_threshold_for_lane_change_detection</code> <code>1.0</code> [m] <code>time_threshold_for_lane_change_detection</code> <code>5.0</code> [s] <code>cutoff_freq_of_velocity_for_lane_change_detection</code> <code>0.1</code> [Hz]"},{"location":"perception/map_based_prediction/#tuning-threshold-parameters","title":"Tuning threshold parameters","text":"<p>Increasing these two parameters will slow down and stabilize the lane change estimation.</p> <p>Normally, we recommend tuning only <code>time_threshold_for_lane_change_detection</code> because it is the more important factor for lane change decision.</p>"},{"location":"perception/map_based_prediction/#tuning-lateral-velocity-calculation","title":"Tuning lateral velocity calculation","text":"<p>Lateral velocity calculation is also a very important factor for lane change decision because it is used in the time domain decision.</p> <p>The predicted time to reach the lane boundary is calculated by</p> \\[ t_{predicted} = \\dfrac{d_{lat}}{v_{lat}} \\] <p>where \\(d_{lat}\\) and \\(v_{lat}\\) represent the lateral distance to the lane boundary and the lateral velocity, respectively.</p> <p>Lowering the cutoff frequency of the low-pass filter for lateral velocity will make the lane change decision more stable but slower. Our setting is very conservative, so you may increase this parameter if you want to make the lane change decision faster.</p> <p>For the additional information, here we show how we calculate lateral velocity.</p> lateral velocity calculation method equation description [applied] time derivative of lateral distance \\(\\dfrac{\\Delta d_{lat}}{\\Delta t}\\) Currently, we use this method to deal with winding roads. Since this time differentiation easily becomes noisy, we also use a low-pass filter to get smoothed velocity. [not applied] Object Velocity Projection to Lateral Direction \\(v_{obj} \\sin(\\theta)\\) Normally, object velocities are less noisy than the time derivative of lateral distance. But the yaw difference \\(\\theta\\) between the lane and object directions sometimes becomes discontinuous, so we did not adopt this method. <p>Currently, we use the upper method with a low-pass filter to calculate lateral velocity.</p>"},{"location":"perception/map_based_prediction/#path-generation","title":"Path generation","text":"<p>Path generation is generated on the frenet frame. The path is generated by the following steps:</p> <ol> <li>Get the frenet frame of the reference path.</li> <li>Generate the frenet frame of the current position of the object and the finite position of the object.</li> <li>Optimize the path in each longitudinal and lateral coordinate of the frenet frame. (Use the quintic polynomial to fit start and end conditions.)</li> <li>Convert the path to the global coordinate.</li> </ol> <p>See paper [2] for more details.</p>"},{"location":"perception/map_based_prediction/#tuning-lateral-path-shape","title":"Tuning lateral path shape","text":"<p><code>lateral_control_time_horizon</code> parameter supports the tuning of the lateral path shape. This parameter is used to calculate the time to reach the reference path. The smaller the value, the more the path will be generated to reach the reference path quickly. (Mostly the center of the lane.)</p>"},{"location":"perception/map_based_prediction/#path-prediction-for-crosswalk-users","title":"Path prediction for crosswalk users","text":"<p>This module treats Pedestrians and Bicycles as objects using the crosswalk, and outputs prediction path based on map and estimated object's velocity, assuming the object has intention to cross the crosswalk, if the objects satisfies at least one of the following conditions:</p> <ul> <li>move toward the crosswalk</li> <li>stop near the crosswalk</li> </ul> <p>If there are a reachable crosswalk entry points within the <code>prediction_time_horizon</code> and the objects satisfies above condition, this module outputs additional predicted path to cross the opposite side via the crosswalk entry point.</p> <p>If the target object is inside the road or crosswalk, this module outputs one or two additional prediction path(s) to reach exit point of the crosswalk. The number of prediction paths are depend on whether object is moving or not. If the object is moving, this module outputs one prediction path toward an exit point that existed in the direction of object's movement. One the other hand, if the object has stopped, it is impossible to infer which exit points the object want to go, so this module outputs two prediction paths toward both side exit point.</p>"},{"location":"perception/map_based_prediction/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/map_based_prediction/#input","title":"Input","text":"Name Type Description <code>~/perception/object_recognition/tracking/objects</code> <code>autoware_auto_perception_msgs::msg::TrackedObjects</code> tracking objects without predicted path. <code>~/vector_map</code> <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> binary data of Lanelet2 Map."},{"location":"perception/map_based_prediction/#output","title":"Output","text":"Name Type Description <code>~/input/objects</code> <code>autoware_auto_perception_msgs::msg::TrackedObjects</code> tracking objects. Default is set to <code>/perception/object_recognition/tracking/objects</code> <code>~/output/objects</code> <code>autoware_auto_perception_msgs::msg::PredictedObjects</code> tracking objects with predicted path. <code>~/objects_path_markers</code> <code>visualization_msgs::msg::MarkerArray</code> marker for visualization."},{"location":"perception/map_based_prediction/#parameters","title":"Parameters","text":"Parameter Unit Type Description <code>enable_delay_compensation</code> [-] bool flag to enable the time delay compensation for the position of the object <code>prediction_time_horizon</code> [s] double predict time duration for predicted path <code>lateral_control_time_horizon</code> [s] double time duration for predicted path will reach the reference path (mostly center of the lane) <code>prediction_sampling_delta_time</code> [s] double sampling time for points in predicted path <code>min_velocity_for_map_based_prediction</code> [m/s] double apply map-based prediction to the objects with higher velocity than this value <code>min_crosswalk_user_velocity</code> [m/s] double minimum velocity used when crosswalk user's velocity is calculated <code>max_crosswalk_user_delta_yaw_threshold_for_lanelet</code> [rad] double maximum yaw difference between crosswalk user and lanelet to use in path prediction for crosswalk users <code>dist_threshold_for_searching_lanelet</code> [m] double The threshold of the angle used when searching for the lane to which the object belongs <code>delta_yaw_threshold_for_searching_lanelet</code> [rad] double The threshold of the angle used when searching for the lane to which the object belongs <code>sigma_lateral_offset</code> [m] double Standard deviation for lateral position of objects <code>sigma_yaw_angle_deg</code> [deg] double Standard deviation yaw angle of objects <code>object_buffer_time_length</code> [s] double Time span of object history to store the information <code>history_time_length</code> [s] double Time span of object information used for prediction <code>prediction_time_horizon_rate_for_validate_shoulder_lane_length</code> [-] double prediction path will disabled when the estimated path length exceeds lanelet length. This parameter control the estimated path length"},{"location":"perception/map_based_prediction/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>For object types of passenger car, bus, and truck<ul> <li>The predicted path of the object follows the road structure.</li> <li>For the object not being on any roads, the predicted path is generated by just a straight line prediction.</li> <li>For the object on a lanelet but moving in a different direction of the road, the predicted path is just straight.</li> <li>Vehicle dynamics may not be properly considered in the predicted path.</li> </ul> </li> <li>For object types of person and motorcycle<ul> <li>The predicted path is generated by just a straight line in all situations except for \"around crosswalk\".</li> </ul> </li> <li>For all obstacles<ul> <li>In the prediction, the vehicle motion is assumed to be a constant velocity due to a lack of acceleration information.</li> </ul> </li> </ul>"},{"location":"perception/map_based_prediction/#reference","title":"Reference","text":"<ol> <li>M. Werling, J. Ziegler, S. Kammel, and S. Thrun, \u201cOptimal trajectory generation for dynamic street scenario in a frenet frame,\u201d IEEE International Conference on Robotics and Automation, Anchorage, Alaska, USA, May 2010.</li> <li>A. Houenou, P. Bonnifait, V. Cherfaoui, and Wen Yao, \u201cVehicle trajectory prediction based on motion model and maneuver recognition,\u201d in 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, nov 2013, pp. 4363-4369.</li> </ol>"},{"location":"perception/multi_object_tracker/","title":"multi_object_tracker","text":""},{"location":"perception/multi_object_tracker/#multi_object_tracker","title":"multi_object_tracker","text":""},{"location":"perception/multi_object_tracker/#purpose","title":"Purpose","text":"<p>The results of the detection are processed by a time series. The main purpose is to give ID and estimate velocity.</p>"},{"location":"perception/multi_object_tracker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This multi object tracker consists of data association and EKF.</p> <p></p>"},{"location":"perception/multi_object_tracker/#data-association","title":"Data association","text":"<p>The data association performs maximum score matching, called min cost max flow problem. In this package, mussp[1] is used as solver. In addition, when associating observations to tracers, data association have gates such as the area of the object from the BEV, Mahalanobis distance, and maximum distance, depending on the class label.</p>"},{"location":"perception/multi_object_tracker/#ekf-tracker","title":"EKF Tracker","text":"<p>Models for pedestrians, bicycles (motorcycles), cars and unknown are available. The pedestrian or bicycle tracker is running at the same time as the respective EKF model in order to enable the transition between pedestrian and bicycle tracking. For big vehicles such as trucks and buses, we have separate models for passenger cars and large vehicles because they are difficult to distinguish from passenger cars and are not stable. Therefore, separate models are prepared for passenger cars and big vehicles, and these models are run at the same time as the respective EKF models to ensure stability.</p>"},{"location":"perception/multi_object_tracker/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/multi_object_tracker/#input","title":"Input","text":"Name Type Description <code>~/input</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> obstacles"},{"location":"perception/multi_object_tracker/#output","title":"Output","text":"Name Type Description <code>~/output</code> <code>autoware_auto_perception_msgs::msg::TrackedObjects</code> modified obstacles"},{"location":"perception/multi_object_tracker/#parameters","title":"Parameters","text":""},{"location":"perception/multi_object_tracker/#core-parameters","title":"Core Parameters","text":"<p>Node parameters are defined in multi_object_tracker.param.yaml and association parameters are defined in data_association.param.yaml.</p>"},{"location":"perception/multi_object_tracker/#node-parameters","title":"Node parameters","text":"Name Type Description <code>***_tracker</code> string EKF tracker name for each class <code>world_frame_id</code> double object kinematics definition frame <code>enable_delay_compensation</code> bool if True, tracker use timers to schedule publishers and use prediction step to extrapolate object state at desired timestamp <code>publish_rate</code> double Timer frequency to output with delay compensation"},{"location":"perception/multi_object_tracker/#association-parameters","title":"Association parameters","text":"Name Type Description <code>can_assign_matrix</code> double Assignment table for data association <code>max_dist_matrix</code> double Maximum distance table for data association <code>max_area_matrix</code> double Maximum area table for data association <code>min_area_matrix</code> double Minimum area table for data association <code>max_rad_matrix</code> double Maximum angle table for data association"},{"location":"perception/multi_object_tracker/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>See the model explanations.</p>"},{"location":"perception/multi_object_tracker/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/multi_object_tracker/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/multi_object_tracker/#evaluation-of-mussp","title":"Evaluation of muSSP","text":"<p>According to our evaluation, muSSP is faster than normal SSP when the matrix size is more than 100.</p> <p>Execution time for varying matrix size at 95% sparsity. In real data, the sparsity was often around 95%. </p> <p>Execution time for varying the sparsity with matrix size 100. </p>"},{"location":"perception/multi_object_tracker/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>This package makes use of external code.</p> Name License Original Repository muSSP Apache-2.0 https://github.com/yu-lab-vt/muSSP <p>[1] C. Wang, Y. Wang, Y. Wang, C.-t. Wu, and G. Yu, \u201cmuSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking,\u201d NeurIPS, 2019</p>"},{"location":"perception/multi_object_tracker/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/multi_object_tracker/models/","title":"Models used in this module","text":""},{"location":"perception/multi_object_tracker/models/#models-used-in-this-module","title":"Models used in this module","text":""},{"location":"perception/multi_object_tracker/models/#tracking-model","title":"Tracking model","text":""},{"location":"perception/multi_object_tracker/models/#ctrv-model-1","title":"CTRV model [1]","text":"<p>CTRV model is a model that assumes constant turn rate and velocity magnitude.</p> <ul> <li>state transition equation</li> </ul> \\[ \\begin{align*} x_{k+1}   &amp;= x_k + v_{x_k} \\cos(\\psi_k) \\cdot dt \\\\ y_{k+1}   &amp;= y_k + v_{x_k} \\sin(\\psi_k) \\cdot dt \\\\ \\psi_{k+1} &amp;= \\psi_k + \\dot{\\psi}_k \\cdot dt \\\\ v_{x_{k+1}}  &amp;= v_{x_k} \\\\ \\dot{\\psi}_{k+1}  &amp;= \\dot{\\psi}_k \\\\ \\end{align*} \\] <ul> <li>jacobian</li> </ul> \\[ A = \\begin{bmatrix} 1 &amp; 0 &amp; -v_x \\sin(\\psi) \\cdot dt &amp; \\cos(\\psi) \\cdot dt &amp; 0 \\\\ 0 &amp; 1 &amp; v_x \\cos(\\psi) \\cdot dt &amp; \\sin(\\psi) \\cdot dt &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; dt \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\]"},{"location":"perception/multi_object_tracker/models/#kinematic-bicycle-model-2","title":"Kinematic bicycle model [2]","text":"<p>Kinematic bicycle model uses slip angle \\(\\beta\\) and velocity \\(v\\) to calculate yaw update. The merit of using this model is that it can prevent unintended yaw rotation when the vehicle is stopped.</p> <p></p> <ul> <li>state variable<ul> <li>pose( \\(x,y\\) ), velocity( \\(v\\) ), yaw( \\(\\psi\\) ), and slip angle ( \\(\\beta\\) )</li> <li>\\([x_{k} ,y_{k} , v_{k} , \\psi_{k} , \\beta_{k} ]^\\mathrm{T}\\)</li> </ul> </li> <li>Prediction Equation<ul> <li>\\(dt\\): sampling time</li> </ul> </li> </ul> \\[ \\begin{aligned} x_{k+1} &amp; =x_{k}+v_{k} \\cos \\left(\\psi_{k}+\\beta_{k}\\right) d t \\\\ y_{k+1} &amp; =y_{k}+v_{k} \\sin \\left(\\psi_{k}+\\beta_{k}\\right) d t \\\\ v_{k+1} &amp; =v_{k} \\\\ \\psi_{k+1} &amp; =\\psi_{k}+\\frac{v_{k}}{l_{r}} \\sin \\beta_{k} d t \\\\ \\beta_{k+1} &amp; =\\beta_{k} \\end{aligned} \\] <ul> <li>Jacobian</li> </ul> \\[ \\frac{\\partial f}{\\partial \\mathrm x}=\\left[\\begin{array}{ccccc} 1 &amp; 0 &amp; -v \\sin (\\psi+\\beta) d t &amp; v \\cos (\\psi+\\beta) &amp; -v \\sin (\\psi+\\beta) d t \\\\ 0 &amp; 1 &amp; v \\cos (\\psi+\\beta) d t &amp; v \\sin (\\psi+\\beta) &amp; v \\cos (\\psi+\\beta) d t \\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{1}{l_r} \\sin \\beta d t &amp; \\frac{v}{l_r} \\cos \\beta d t \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right] \\]"},{"location":"perception/multi_object_tracker/models/#remarks-on-the-output-twist","title":"remarks on the output twist","text":"<p>Remarks that the velocity \\(v_{k}\\) is the norm of velocity of vehicle, not the longitudinal velocity. So the output twist in the object coordinate \\((x,y)\\) is calculated as follows.</p> \\[ \\begin{aligned} v_{x} &amp;= v_{k} \\cos \\left(\\beta_{k}\\right) \\\\ v_{y} &amp;= v_{k} \\sin \\left(\\beta_{k}\\right) \\end{aligned} \\]"},{"location":"perception/multi_object_tracker/models/#anchor-point-based-estimation","title":"Anchor point based estimation","text":"<p>To separate the estimation of the position and the shape, we use anchor point based position estimation.</p>"},{"location":"perception/multi_object_tracker/models/#anchor-point-and-tracking-relationships","title":"Anchor point and tracking relationships","text":"<p>Anchor point is set when the tracking is initialized. Its position is equal to the center of the bounding box of the first tracking bounding box.</p> <p>Here show how anchor point is used in tracking.</p> <p></p> <p>Raw detection is converted to anchor point coordinate, and tracking</p>"},{"location":"perception/multi_object_tracker/models/#manage-anchor-point-offset","title":"Manage anchor point offset","text":"<p>Anchor point should be kept in the same position of the object. In other words, the offset value must be adjusted so that the input BBOX and the output BBOX's closest plane to the ego vehicle are at the same position.</p>"},{"location":"perception/multi_object_tracker/models/#known-limits-drawbacks","title":"Known limits, drawbacks","text":"<ul> <li>When the anchor point is further than the detection center, the tracking will be more affected by the yaw detection noise.<ul> <li>This can be result in unintended yaw rotation or position drift.</li> </ul> </li> </ul>"},{"location":"perception/multi_object_tracker/models/#references","title":"References","text":"<p>[1] Schubert, Robin &amp; Richter, Eric &amp; Wanielik, Gerd. (2008). Comparison and evaluation of advanced motion models for vehicle tracking. 1 - 6. 10.1109/ICIF.2008.4632283.</p> <p>[2] Kong, Jason &amp; Pfeiffer, Mark &amp; Schildbach, Georg &amp; Borrelli, Francesco. (2015). Kinematic and dynamic vehicle models for autonomous driving control design. 1094-1099. 10.1109/IVS.2015.7225830.</p>"},{"location":"perception/object_merger/","title":"object_merger","text":""},{"location":"perception/object_merger/#object_merger","title":"object_merger","text":""},{"location":"perception/object_merger/#purpose","title":"Purpose","text":"<p>object_merger is a package for merging detected objects from two methods by data association.</p>"},{"location":"perception/object_merger/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The successive shortest path algorithm is used to solve the data association problem (the minimum-cost flow problem). The cost is calculated by the distance between two objects and gate functions are applied to reset cost, s.t. the maximum distance, the maximum area and the minimum area.</p>"},{"location":"perception/object_merger/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/object_merger/#input","title":"Input","text":"Name Type Description <code>input/object0</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> detection objects <code>input/object1</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> detection objects"},{"location":"perception/object_merger/#output","title":"Output","text":"Name Type Description <code>output/object</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> modified Objects"},{"location":"perception/object_merger/#parameters","title":"Parameters","text":"Name Type Description <code>can_assign_matrix</code> double Assignment table for data association <code>max_dist_matrix</code> double Maximum distance table for data association <code>max_area_matrix</code> double Maximum area table for data association <code>min_area_matrix</code> double Minimum area table for data association <code>max_rad_matrix</code> double Maximum angle table for data association <code>base_link_frame_id</code> double association frame <code>distance_threshold_list</code> <code>std::vector&lt;double&gt;</code> Distance threshold for each class used in judging overlap. The class order depends on ObjectClassification. <code>generalized_iou_threshold</code> <code>std::vector&lt;double&gt;</code> Generalized IoU threshold for each class"},{"location":"perception/object_merger/#tips","title":"Tips","text":"<ul> <li>False Positive Unknown object detected by clustering method sometimes raises the risk of sudden stop and interferes with Planning module. If ML based detector rarely misses objects, you can tune the parameter of object_merger and make Perception module ignore unknown objects.<ul> <li>If you want to remove unknown object close to large vehicle,<ul> <li>use HIGH <code>distance_threshold_list</code><ul> <li>However, this causes high computational load</li> </ul> </li> <li>use LOW <code>precision_threshold_to_judge_overlapped</code></li> <li>use LOW <code>generalized_iou_threshold</code><ul> <li>However, these 2 params raise the risk of overlooking object close to known object.</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"perception/object_merger/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/object_merger/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/object_merger/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/object_merger/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/object_merger/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<p>Data association algorithm was the same as that of multi_object_tracker, but the algorithm of multi_object_tracker was already updated.</p>"},{"location":"perception/object_range_splitter/","title":"object_range_splitter","text":""},{"location":"perception/object_range_splitter/#object_range_splitter","title":"object_range_splitter","text":""},{"location":"perception/object_range_splitter/#purpose","title":"Purpose","text":"<p>object_range_splitter is a package to divide detected objects into two messages by the distance from the origin.</p>"},{"location":"perception/object_range_splitter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/object_range_splitter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/object_range_splitter/#input","title":"Input","text":"Name Type Description <code>input/object</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> detected objects"},{"location":"perception/object_range_splitter/#output","title":"Output","text":"Name Type Description <code>output/long_range_object</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> long range detected objects <code>output/short_range_object</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> short range detected objects"},{"location":"perception/object_range_splitter/#parameters","title":"Parameters","text":"Name Type Description <code>split_range</code> float the distance boundary to divide detected objects [m]"},{"location":"perception/object_range_splitter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/object_range_splitter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/object_range_splitter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/object_range_splitter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/object_range_splitter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/object_velocity_splitter/","title":"object_velocity_splitter","text":""},{"location":"perception/object_velocity_splitter/#object_velocity_splitter","title":"object_velocity_splitter","text":"<p>This package contains a object filter module for autoware_auto_perception_msgs/msg/DetectedObject. This package can split DetectedObjects into two messages by object's speed.</p>"},{"location":"perception/object_velocity_splitter/#input","title":"Input","text":"Name Type Description <code>~/input/objects</code> autoware_auto_perception_msgs/msg/DetectedObjects.msg 3D detected objects."},{"location":"perception/object_velocity_splitter/#output","title":"Output","text":"Name Type Description <code>~/output/low_speed_objects</code> autoware_auto_perception_msgs/msg/DetectedObjects.msg Objects with low speed <code>~/output/high_speed_objects</code> autoware_auto_perception_msgs/msg/DetectedObjects.msg Objects with high speed"},{"location":"perception/object_velocity_splitter/#parameters","title":"Parameters","text":"Name Type Description Default value <code>velocity_threshold</code> double Velocity threshold parameter to split objects [m/s] 3.0"},{"location":"perception/occupancy_grid_map_outlier_filter/","title":"occupancy_grid_map_outlier_filter","text":""},{"location":"perception/occupancy_grid_map_outlier_filter/#occupancy_grid_map_outlier_filter","title":"occupancy_grid_map_outlier_filter","text":""},{"location":"perception/occupancy_grid_map_outlier_filter/#purpose","title":"Purpose","text":"<p>This node is an outlier filter based on a occupancy grid map. Depending on the implementation of occupancy grid map, it can be called an outlier filter in time series, since the occupancy grid map expresses the occupancy probabilities in time series.</p>"},{"location":"perception/occupancy_grid_map_outlier_filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ol> <li> <p>Use the occupancy grid map to separate point clouds into those with low occupancy probability and those with high occupancy probability.</p> </li> <li> <p>The point clouds that belong to the low occupancy probability are not necessarily outliers. In particular, the top of the moving object tends to belong to the low occupancy probability. Therefore, if <code>use_radius_search_2d_filter</code> is true, then apply an radius search 2d outlier filter to the point cloud that is determined to have a low occupancy probability.</p> <ol> <li>For each low occupancy probability point, determine the outlier from the radius (<code>radius_search_2d_filter/search_radius</code>) and the number of point clouds. In this case, the point cloud to be referenced is not only low occupancy probability points, but all point cloud including high occupancy probability points.</li> <li>The number of point clouds can be multiplied by <code>radius_search_2d_filter/min_points_and_distance_ratio</code> and distance from base link. However, the minimum and maximum number of point clouds is limited.</li> </ol> </li> </ol> <p>The following video is a sample. Yellow points are high occupancy probability, green points are low occupancy probability which is not an outlier, and red points are outliers. At around 0:15 and 1:16 in the first video, a bird crosses the road, but it is considered as an outlier.</p> <ul> <li>movie1</li> <li>movie2</li> </ul> <p></p>"},{"location":"perception/occupancy_grid_map_outlier_filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/occupancy_grid_map_outlier_filter/#input","title":"Input","text":"Name Type Description <code>~/input/pointcloud</code> <code>sensor_msgs/PointCloud2</code> Obstacle point cloud with ground removed. <code>~/input/occupancy_grid_map</code> <code>nav_msgs/OccupancyGrid</code> A map in which the probability of the presence of an obstacle is occupancy probability map"},{"location":"perception/occupancy_grid_map_outlier_filter/#output","title":"Output","text":"Name Type Description <code>~/output/pointcloud</code> <code>sensor_msgs/PointCloud2</code> Point cloud with outliers removed. trajectory <code>~/output/debug/outlier/pointcloud</code> <code>sensor_msgs/PointCloud2</code> Point clouds removed as outliers. <code>~/output/debug/low_confidence/pointcloud</code> <code>sensor_msgs/PointCloud2</code> Point clouds that had a low probability of occupancy in the occupancy grid map. However, it is not considered as an outlier. <code>~/output/debug/high_confidence/pointcloud</code> <code>sensor_msgs/PointCloud2</code> Point clouds that had a high probability of occupancy in the occupancy grid map. trajectory"},{"location":"perception/occupancy_grid_map_outlier_filter/#parameters","title":"Parameters","text":"Name Type Description <code>map_frame</code> string map frame id <code>base_link_frame</code> string base link frame id <code>cost_threshold</code> int Cost threshold of occupancy grid map (0~100). 100 means 100% probability that there is an obstacle, close to 50 means that it is indistinguishable whether it is an obstacle or free space, 0 means that there is no obstacle. <code>enable_debugger</code> bool Whether to output the point cloud for debugging. <code>use_radius_search_2d_filter</code> bool Whether or not to apply density-based outlier filters to objects that are judged to have low probability of occupancy on the occupancy grid map. <code>radius_search_2d_filter/search_radius</code> float Radius when calculating the density <code>radius_search_2d_filter/min_points_and_distance_ratio</code> float Threshold value of the number of point clouds per radius when the distance from baselink is 1m, because the number of point clouds varies with the distance from baselink. <code>radius_search_2d_filter/min_points</code> int Minimum number of point clouds per radius <code>radius_search_2d_filter/max_points</code> int Maximum number of point clouds per radius"},{"location":"perception/occupancy_grid_map_outlier_filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/occupancy_grid_map_outlier_filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/occupancy_grid_map_outlier_filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/occupancy_grid_map_outlier_filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/occupancy_grid_map_outlier_filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/probabilistic_occupancy_grid_map/","title":"probabilistic_occupancy_grid_map","text":""},{"location":"perception/probabilistic_occupancy_grid_map/#probabilistic_occupancy_grid_map","title":"probabilistic_occupancy_grid_map","text":""},{"location":"perception/probabilistic_occupancy_grid_map/#purpose","title":"Purpose","text":"<p>This package outputs the probability of having an obstacle as occupancy grid map. </p>"},{"location":"perception/probabilistic_occupancy_grid_map/#referencesexternal-links","title":"References/External links","text":"<ul> <li>Pointcloud based occupancy grid map</li> <li>Laserscan based occupancy grid map</li> </ul>"},{"location":"perception/probabilistic_occupancy_grid_map/#settings","title":"Settings","text":"<p>Occupancy grid map is generated on <code>map_frame</code>, and grid orientation is fixed.</p> <p>You may need to choose <code>scan_origin_frame</code> and <code>gridmap_origin_frame</code> which means sensor origin and gridmap origin respectively. Especially, set your main LiDAR sensor frame (e.g. <code>velodyne_top</code> in sample_vehicle) as a <code>scan_origin_frame</code> would result in better performance.</p> <p></p>"},{"location":"perception/probabilistic_occupancy_grid_map/#each-config-parameters","title":"Each config parameters","text":"<p>Config parameters are managed in <code>config/*.yaml</code> and here shows its outline.</p> <ul> <li>Pointcloud based occupancy grid map</li> </ul> Ros param name Default value map_frame \"map\" base_link_frame \"base_link\" scan_origin_frame \"base_link\" gridmap_origin_frame \"base_link\" use_height_filter true enable_single_frame_mode false filter_obstacle_pointcloud_by_raw_pointcloud false map_length 150.0 [m] map_resolution 0.5 [m] use_projection false projection_dz_threshold 0.01 obstacle_separation_threshold 1.0 input_obstacle_pointcloud true input_obstacle_and_raw_pointcloud true <ul> <li>Laserscan based occupancy grid map</li> </ul> Ros param name Default value map_length 150 [m] map_width 150 [m] map_resolution 0.5 [m] use_height_filter true enable_single_frame_mode false map_frame \"map\" base_link_frame \"base_link\" scan_origin_frame \"base_link\" gridmap_origin_frame \"base_link\""},{"location":"perception/probabilistic_occupancy_grid_map/#other-parameters","title":"Other parameters","text":"<p>Additional argument is shown below:</p> Name Default Description <code>use_multithread</code> <code>false</code> whether to use multithread <code>use_intra_process</code> <code>false</code> <code>map_origin</code> `` parameter to override <code>map_origin_frame</code> which means grid map origin <code>scan_origin</code> `` parameter to override <code>scan_origin_frame</code> which means scanning center <code>output</code> <code>occupancy_grid</code> output name <code>use_pointcloud_container</code> <code>false</code> <code>container_name</code> <code>occupancy_grid_map_container</code> <code>input_obstacle_pointcloud</code> <code>false</code> only for laserscan based method. If true, the node subscribe obstacle pointcloud <code>input_obstacle_and_raw_pointcloud</code> <code>true</code> only for laserscan based method. If true, the node subscribe both obstacle and raw pointcloud"},{"location":"perception/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/","title":"laserscan based occupancy grid map","text":""},{"location":"perception/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#laserscan-based-occupancy-grid-map","title":"laserscan based occupancy grid map","text":""},{"location":"perception/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The basic idea is to take a 2D laserscan and ray trace it to create a time-series processed occupancy grid map.</p> <ol> <li>the node take a laserscan and make an occupancy grid map with one frame. ray trace is done by Bresenham's line algorithm.    </li> <li> <p>Optionally, obstacle point clouds and raw point clouds can be received and reflected in the occupancy grid map. The reason is that laserscan only uses the most foreground point in the polar coordinate system, so it throws away a lot of information. As a result, the occupancy grid map is almost an UNKNOWN cell.    Therefore, the obstacle point cloud and the raw point cloud are used to reflect what is judged to be the ground and what is judged to be an obstacle in the occupancy grid map. The black and red dots represent raw point clouds, and the red dots represent obstacle point clouds. In other words, the black points are determined as the ground, and the red point cloud is the points determined as obstacles. The gray cells are represented as UNKNOWN cells.    </p> </li> <li> <p>Using the previous occupancy grid map, update the existence probability using a binary Bayesian filter (1). Also, the unobserved cells are time-decayed like the system noise of the Kalman filter (2).</p> </li> </ol> \\[ \\hat{P_{o}} = \\frac{(P_{o} *P_{z})}{(P_{o}* P_{z} + (1 - P_{o}) * \\bar{P_{z}})} \\tag{1} \\] \\[ \\hat{P_{o}} = \\frac{(P_{o} + 0.5 * \\frac{1}{ratio})}{(\\frac{1}{ratio} + 1)} \\tag{2} \\]"},{"location":"perception/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#input","title":"Input","text":"Name Type Description <code>~/input/laserscan</code> <code>sensor_msgs::LaserScan</code> laserscan <code>~/input/obstacle_pointcloud</code> <code>sensor_msgs::PointCloud2</code> obstacle pointcloud <code>~/input/raw_pointcloud</code> <code>sensor_msgs::PointCloud2</code> The overall point cloud used to input the obstacle point cloud"},{"location":"perception/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#output","title":"Output","text":"Name Type Description <code>~/output/occupancy_grid_map</code> <code>nav_msgs::OccupancyGrid</code> occupancy grid map"},{"location":"perception/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#parameters","title":"Parameters","text":""},{"location":"perception/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#node-parameters","title":"Node Parameters","text":"Name Type Description <code>map_frame</code> string map frame <code>base_link_frame</code> string base_link frame <code>input_obstacle_pointcloud</code> bool whether to use the optional obstacle point cloud? If this is true, <code>~/input/obstacle_pointcloud</code> topics will be received. <code>input_obstacle_and_raw_pointcloud</code> bool whether to use the optional obstacle and raw point cloud? If this is true, <code>~/input/obstacle_pointcloud</code> and <code>~/input/raw_pointcloud</code> topics will be received. <code>use_height_filter</code> bool whether to height filter for <code>~/input/obstacle_pointcloud</code> and <code>~/input/raw_pointcloud</code>? By default, the height is set to -1~2m. <code>map_length</code> double The length of the map. -100 if it is 50~50[m] <code>map_resolution</code> double The map cell resolution [m]"},{"location":"perception/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>In several places we have modified the external code written in BSD3 license.</p> <ul> <li>occupancy_grid_map.hpp</li> <li>cost_value.hpp</li> <li>occupancy_grid_map.cpp</li> </ul>"},{"location":"perception/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>Bresenham's_line_algorithm</p> <ul> <li>https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm</li> <li>https://ieeexplore.ieee.org/document/5388473</li> </ul>"},{"location":"perception/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<ul> <li>The update probability of the binary Bayesian filter is currently hard-coded and requires a code change to be modified.</li> <li>Since there is no special support for moving objects, the probability of existence is not increased for fast objects.</li> </ul>"},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/","title":"pointcloud based occupancy grid map","text":""},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#pointcloud-based-occupancy-grid-map","title":"pointcloud based occupancy grid map","text":""},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#1st-step","title":"1st step","text":"<p>First of all, input obstacle/raw pointcloud are transformed into the polar coordinate centered around <code>scan_origin</code> and divided int circular bins per angle_increment respectively. At this time, each point belonging to each bin is stored as range data. In addition, the x,y information in the map coordinate is also stored for ray-tracing on the map coordinate. The bin contains the following information for each point</p> <ul> <li>range data from origin of raytrace</li> <li>x on map coordinate</li> <li>y on map coordinate</li> </ul> <p></p> <p>The following figure shows each of the bins from side view. </p>"},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#2nd-step","title":"2nd step","text":"<p>The ray trace is performed in three steps for each cell. The ray trace is done by Bresenham's line algorithm. </p> <ol> <li> <p>Initialize freespace to the farthest point of each bin.</p> <p></p> </li> <li> <p>Fill in the unknown cells.    Based on the assumption that <code>UNKNOWN</code> is behind the obstacle, the cells that are more than a distance margin from each obstacle point are filled with <code>UNKNOWN</code></p> <p></p> <p>There are three reasons for setting a distance margin.</p> <ul> <li>It is unlikely that a point on the ground will be immediately behind an obstacle.</li> <li>The obstacle point cloud is processed and may not match the raw pointcloud.</li> <li>The input may be inaccurate and obstacle points may not be determined as obstacles.</li> </ul> <p>When the parameter <code>grid_map_type</code> is \"OccupancyGridMapProjectiveBlindSpot\" and the <code>scan_origin</code> is a sensor frame like <code>velodyne_top</code> for instance, for each obstacle pointcloud, if there are no visible raw pointclouds that are located above the projected ray from the <code>scan_origin</code> to that obstacle pointcloud, the cells between the obstacle pointcloud and the <code>projected point</code> are filled with <code>UNKNOWN</code>. Note that the <code>scan_origin</code> should not be <code>base_link</code> if this flag is true because otherwise all the cells behind the obstacle point clouds would be filled with <code>UNKNOWN</code>.</p> <p></p> </li> <li> <p>Fill in the occupied cells.    Fill in the point where the obstacle point is located with occupied.    In addition, If the distance between obstacle points is less than or equal to the distance margin, that interval is filled with <code>OCCUPIED</code> because the input may be inaccurate and obstacle points may not be determined as obstacles.</p> <p></p> </li> </ol>"},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#3rd-step","title":"3rd step","text":"<p>Using the previous occupancy grid map, update the existence probability using a binary Bayesian filter (1). Also, the unobserved cells are time-decayed like the system noise of the Kalman filter (2).</p> \\[ \\hat{P_{o}} = \\frac{(P_{o} *P_{z})}{(P_{o}* P_{z} + (1 - P_{o}) * \\bar{P_{z}})} \\tag{1} \\] \\[ \\hat{P_{o}} = \\frac{(P_{o} + 0.5 * \\frac{1}{ratio})}{(\\frac{1}{ratio} + 1)} \\tag{2} \\]"},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#input","title":"Input","text":"Name Type Description <code>~/input/obstacle_pointcloud</code> <code>sensor_msgs::PointCloud2</code> obstacle pointcloud <code>~/input/raw_pointcloud</code> <code>sensor_msgs::PointCloud2</code> The overall point cloud used to input the obstacle point cloud"},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#output","title":"Output","text":"Name Type Description <code>~/output/occupancy_grid_map</code> <code>nav_msgs::OccupancyGrid</code> occupancy grid map"},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#parameters","title":"Parameters","text":""},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#node-parameters","title":"Node Parameters","text":"Name Type Description <code>map_frame</code> string map frame <code>base_link_frame</code> string base_link frame <code>use_height_filter</code> bool whether to height filter for <code>~/input/obstacle_pointcloud</code> and <code>~/input/raw_pointcloud</code>? By default, the height is set to -1~2m. <code>map_length</code> double The length of the map. -100 if it is 50~50[m] <code>map_resolution</code> double The map cell resolution [m] <code>grid_map_type</code> string The type of grid map for estimating <code>UNKNOWN</code> region behind obstacle point clouds"},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>In several places we have modified the external code written in BSD3 license.</p> <ul> <li>occupancy_grid_map.hpp</li> <li>cost_value.hpp</li> <li>occupancy_grid_map.cpp</li> </ul>"},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<ul> <li>The update probability of the binary Bayesian filter is currently hard-coded and requires a code change to be modified.</li> <li>Since there is no special support for moving objects, the probability of existence is not increased for fast objects.</li> </ul>"},{"location":"perception/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#how-to-debug","title":"How to debug","text":"<p>If <code>grid_map_type</code> is \"OccupancyGridMapProjectiveBlindSpot\" and <code>pub_debug_grid</code> is <code>true</code>, it is possible to check the each process of grid map generation by running</p> <pre><code>ros2 launch probabilistic_occupancy_grid_map debug.launch.xml\n</code></pre> <p>and visualizing the following occupancy grid map topics (which are listed in config/grid_map_param.yaml):</p> <ul> <li><code>/perception/occupancy_grid_map/grid_1st_step</code>: <code>FREE</code> cells are filled</li> <li><code>/perception/occupancy_grid_map/grid_2nd_step</code>: <code>UNKNOWN</code> cells are filled</li> <li><code>/perception/occupancy_grid_map/grid_3rd_step</code>: <code>OCCUPIED</code> cells are filled</li> </ul>"},{"location":"perception/radar_crossing_objects_noise_filter/","title":"radar_crossing_objects_noise_filter","text":""},{"location":"perception/radar_crossing_objects_noise_filter/#radar_crossing_objects_noise_filter","title":"radar_crossing_objects_noise_filter","text":"<p>This package contains a radar noise filter module for autoware_auto_perception_msgs/msg/DetectedObject. This package can filter the noise objects which cross to the ego vehicle.</p>"},{"location":"perception/radar_crossing_objects_noise_filter/#algorithm","title":"Algorithm","text":""},{"location":"perception/radar_crossing_objects_noise_filter/#background","title":"Background","text":"<p>This package aim to filter the noise objects which cross from the ego vehicle. The reason why these objects are noise is as below.</p> <ul> <li> <ol> <li>The objects with doppler velocity can be trusted more than those with vertical velocity to it.</li> </ol> </li> </ul> <p>Radars can get velocity information of objects as doppler velocity, but cannot get vertical velocity to doppler velocity directory. Some radars can output the objects with not only doppler velocity but also vertical velocity by estimation. If the vertical velocity estimation is poor, it leads to output noise objects. In other words, the above situation is that the objects which has vertical twist viewed from ego vehicle can tend to be noise objects.</p> <p>The example is below figure. Velocity estimation fails on static objects, resulting in ghost objects crossing in front of ego vehicles.</p> <p></p> <ul> <li> <ol> <li>Turning around by ego vehicle affect the output from radar.</li> </ol> </li> </ul> <p>When the ego vehicle turns around, the radars outputting at the object level sometimes fail to estimate the twist of objects correctly even if radar_tracks_msgs_converter compensates by the ego vehicle twist. So if an object detected by radars has circular motion viewing from base_link, it is likely that the speed is estimated incorrectly and that the object is a static object.</p> <p>The example is below figure. When the ego vehicle turn right, the surrounding objects have left circular motion.</p> <p></p>"},{"location":"perception/radar_crossing_objects_noise_filter/#detail-algorithm","title":"Detail Algorithm","text":"<p>To filter the objects crossing to ego vehicle, this package filter the objects as below algorithm.</p> <p></p> <pre><code>  // If velocity of an object is rather than the velocity_threshold,\n// and crossing_yaw is near to vertical\n// angle_threshold &lt; crossing_yaw &lt; pi - angle_threshold\nif (\nvelocity &gt; node_param_.velocity_threshold &amp;&amp;\nabs(std::cos(crossing_yaw)) &lt; abs(std::cos(node_param_.angle_threshold))) {\n// Object is noise object;\n} else {\n// Object is not noise object;\n}\n</code></pre>"},{"location":"perception/radar_crossing_objects_noise_filter/#input","title":"Input","text":"Name Type Description <code>~/input/objects</code> autoware_auto_perception_msgs/msg/DetectedObjects.msg Radar objects."},{"location":"perception/radar_crossing_objects_noise_filter/#output","title":"Output","text":"Name Type Description <code>~/output/noise_objects</code> autoware_auto_perception_msgs/msg/DetectedObjects.msg Noise objects <code>~/output/filtered_objects</code> autoware_auto_perception_msgs/msg/DetectedObjects.msg Filtered objects"},{"location":"perception/radar_crossing_objects_noise_filter/#parameters","title":"Parameters","text":"Name Type Description Default value <code>angle_threshold</code> double The angle threshold parameter to filter [rad]. This parameter has condition that 0 &lt; <code>angle_threshold</code> &lt; pi / 2. See algorithm chapter for details. 1.0472 <code>velocity_threshold</code> double The velocity threshold parameter to filter [m/s]. See algorithm chapter for details. 3.0"},{"location":"perception/radar_fusion_to_detected_object/","title":"radar_fusion_to_detected_object","text":""},{"location":"perception/radar_fusion_to_detected_object/#radar_fusion_to_detected_object","title":"radar_fusion_to_detected_object","text":"<p>This package contains a sensor fusion module for radar-detected objects and 3D detected objects. The fusion node can:</p> <ul> <li>Attach velocity to 3D detections when successfully matching radar data. The tracking modules use the velocity information to enhance the tracking results while planning modules use it to execute actions like adaptive cruise control.</li> <li>Improve the low confidence 3D detections when corresponding radar detections are found.</li> </ul> <p></p>"},{"location":"perception/radar_fusion_to_detected_object/#core-algorithm","title":"Core algorithm","text":"<p>The document of core algorithm is here</p>"},{"location":"perception/radar_fusion_to_detected_object/#parameters-for-sensor-fusion","title":"Parameters for sensor fusion","text":"Name Type Description Default value bounding_box_margin double The distance to extend the 2D bird's-eye view Bounding Box on each side. This distance is used as a threshold to find radar centroids falling inside the extended box. [m] 2.0 split_threshold_velocity double The object's velocity threshold to decide to split for two objects from radar information (currently not implemented) [m/s] 5.0 threshold_yaw_diff double The yaw orientation threshold. If \u2223 \u03b8_ob \u2212 \u03b8_ra \u2223 &lt; threshold \u00d7 yaw_diff attached to radar information include estimated velocity, where\u03b8obis yaw angle from 3d detected object,*\u03b8_ra is yaw angle from radar object. [rad] 0.35"},{"location":"perception/radar_fusion_to_detected_object/#weight-parameters-for-velocity-estimation","title":"Weight parameters for velocity estimation","text":"<p>To tune these weight parameters, please see document in detail.</p> Name Type Description Default value velocity_weight_average double The twist coefficient of average twist of radar data in velocity estimation. 0.0 velocity_weight_median double The twist coefficient of median twist of radar data in velocity estimation. 0.0 velocity_weight_min_distance double The twist coefficient of radar data nearest to the center of bounding box in velocity estimation. 1.0 velocity_weight_target_value_average double The twist coefficient of target value weighted average in velocity estimation. Target value is amplitude if using radar pointcloud. Target value is probability if using radar objects. 0.0 velocity_weight_target_value_top double The twist coefficient of top target value radar data in velocity estimation. Target value is amplitude if using radar pointcloud. Target value is probability if using radar objects. 0.0"},{"location":"perception/radar_fusion_to_detected_object/#parameters-for-fixed-object-information","title":"Parameters for fixed object information","text":"Name Type Description Default value convert_doppler_to_twist bool Convert doppler velocity to twist using the yaw information of a detected object. false threshold_probability float If the probability of an output object is lower than this parameter, and the output object does not have radar points/objects, then delete the object. 0.4 compensate_probability bool If this parameter is true, compensate probability of objects to threshold probability. false"},{"location":"perception/radar_fusion_to_detected_object/#radar_object_fusion_to_detected_object","title":"radar_object_fusion_to_detected_object","text":"<p>Sensor fusion with radar objects and a detected object.</p> <ul> <li>Calculation cost is O(nm).<ul> <li>n: the number of radar objects.</li> <li>m: the number of objects from 3d detection.</li> </ul> </li> </ul>"},{"location":"perception/radar_fusion_to_detected_object/#how-to-launch","title":"How to launch","text":"<pre><code>ros2 launch radar_fusion_to_detected_object radar_object_to_detected_object.launch.xml\n</code></pre>"},{"location":"perception/radar_fusion_to_detected_object/#input","title":"Input","text":"Name Type Description <code>~/input/objects</code> autoware_auto_perception_msgs/msg/DetectedObject.msg 3D detected objects. <code>~/input/radar_objects</code> autoware_auto_perception_msgs/msg/DetectedObjects.msg Radar objects. Note that frame_id need to be same as <code>~/input/objects</code>"},{"location":"perception/radar_fusion_to_detected_object/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> autoware_auto_perception_msgs/msg/DetectedObjects.msg 3D detected object with twist. <code>~/debug/low_confidence_objects</code> autoware_auto_perception_msgs/msg/DetectedObjects.msg 3D detected object that doesn't output as <code>~/output/objects</code> because of low confidence"},{"location":"perception/radar_fusion_to_detected_object/#parameters","title":"Parameters","text":"Name Type Description Default value update_rate_hz double The update rate [hz]. 20.0"},{"location":"perception/radar_fusion_to_detected_object/#radar_scan_fusion_to_detected_object-tbd","title":"radar_scan_fusion_to_detected_object (TBD)","text":"<p>TBD</p>"},{"location":"perception/radar_fusion_to_detected_object/docs/algorithm/","title":"Algorithm","text":""},{"location":"perception/radar_fusion_to_detected_object/docs/algorithm/#common-algorithm","title":"Common Algorithm","text":""},{"location":"perception/radar_fusion_to_detected_object/docs/algorithm/#1-link-between-3d-bounding-box-and-radar-data","title":"1. Link between 3d bounding box and radar data","text":"<p>Choose radar pointcloud/objects within 3D bounding box from lidar-base detection with margin space from bird's-eye view.</p> <p></p>"},{"location":"perception/radar_fusion_to_detected_object/docs/algorithm/#2-feature-support-split-the-object-going-in-a-different-direction","title":"2. [Feature support] Split the object going in a different direction","text":"<ul> <li>Split two object for the low confidence object that can be estimated to derive two object.</li> </ul>"},{"location":"perception/radar_fusion_to_detected_object/docs/algorithm/#3-estimate-twist-of-object","title":"3. Estimate twist of object","text":"<p>Estimate twist from chosen radar pointcloud/objects using twist and target value (Target value is amplitude if using radar pointcloud. Target value is probability if using radar objects). First, the estimation function calculate</p> <ul> <li>Average twist for radar pointcloud/objects.</li> <li>Median twist for radar pointcloud/objects.</li> <li>Twist for radar pointcloud/objects nearest of the center of bounding box in velocity.</li> <li>Weighted average twist with target value of radar pointcloud/objects.</li> <li>Twist with max target value of radar pointcloud/objects.</li> </ul> <p>Second, the estimation function calculate weighted average of these list. Third, twist information of estimated twist is attached to an object.</p> <p></p>"},{"location":"perception/radar_fusion_to_detected_object/docs/algorithm/#4-feature-support-option-convert-doppler-velocity-to-twist","title":"4. [Feature support] [Option] Convert doppler velocity to twist","text":"<p>If the twist information of radars is doppler velocity, convert from doppler velocity to twist using yaw angle of DetectedObject. Because radar pointcloud has only doppler velocity information, radar pointcloud fusion should use this feature. On the other hand, because radar objects have twist information, radar object fusion should not use this feature.</p> <p></p>"},{"location":"perception/radar_fusion_to_detected_object/docs/algorithm/#5-delete-objects-with-low-probability","title":"5. Delete objects with low probability","text":"<ul> <li>Delete low confidence objects that do not have some radar points/objects.</li> </ul>"},{"location":"perception/radar_object_clustering/","title":"radar_object_clustering","text":""},{"location":"perception/radar_object_clustering/#radar_object_clustering","title":"radar_object_clustering","text":"<p>This package contains a radar object clustering for autoware_auto_perception_msgs/msg/DetectedObject input.</p> <p>This package can make clustered objects from radar DetectedObjects, the objects which is converted from RadarTracks by radar_tracks_msgs_converter and is processed by noise filter. In other word, this package can combine multiple radar detections from one object into one and adjust class and size.</p> <p></p>"},{"location":"perception/radar_object_clustering/#algorithm","title":"Algorithm","text":""},{"location":"perception/radar_object_clustering/#background","title":"Background","text":"<p>In radars with object output, there are cases that multiple detection results are obtained from one object, especially for large vehicles such as trucks and trailers. Its multiple detection results cause separation of objects in tracking module. Therefore, by this package the multiple detection results are clustered into one object in advance.</p>"},{"location":"perception/radar_object_clustering/#detail-algorithm","title":"Detail Algorithm","text":"<ul> <li>Sort by distance from <code>base_link</code></li> </ul> <p>At first, to prevent changing the result from depending on the order of objects in DetectedObjects, input objects are sorted by distance from <code>base_link</code>. In addition, to apply matching in closeness order considering occlusion, objects are sorted in order of short distance in advance.</p> <ul> <li>Clustering</li> </ul> <p>If two radar objects are near, and yaw angle direction and velocity between two radar objects is similar (the degree of these is defined by parameters), then these are clustered. Note that radar characteristic affect parameters for this matching. For example, if resolution of range distance or angle is low and accuracy of velocity is high, then <code>distance_threshold</code> parameter should be bigger and should set matching that strongly looks at velocity similarity.</p> <p></p> <p>After grouping for all radar objects, if multiple radar objects are grouping, the kinematics of the new clustered object is calculated from average of that and label and shape of the new clustered object is calculated from top confidence in radar objects.</p> <ul> <li>Fixed label correction</li> </ul> <p>When the label information from radar outputs lack accuracy, <code>is_fixed_label</code> parameter is recommended to set <code>true</code>. If the parameter is true, the label of a clustered object is overwritten by the label set by <code>fixed_label</code> parameter. If this package use for faraway dynamic object detection with radar, the parameter is recommended to set to <code>VEHICLE</code>.</p> <ul> <li>Fixed size correction</li> </ul> <p>When the size information from radar outputs lack accuracy, <code>is_fixed_size</code> parameter is recommended to set <code>true</code>. If the parameter is true, the size of a clustered object is overwritten by the label set by <code>size_x</code>, <code>size_y</code>, and <code>size_z</code> parameters. If this package use for faraway dynamic object detection with radar, the parameter is recommended to set to <code>size_x</code>, <code>size_y</code>, <code>size_z</code>, as average of vehicle size. Note that to use for multi_objects_tracker, the size parameters need to exceed <code>min_area_matrix</code> parameters of it.</p>"},{"location":"perception/radar_object_clustering/#limitation","title":"Limitation","text":"<p>For now, size estimation for clustered object is not implemented. So <code>is_fixed_size</code> parameter is recommended to set <code>true</code>, and size parameters is recommended to set to value near to average size of vehicles.</p>"},{"location":"perception/radar_object_clustering/#input","title":"Input","text":"Name Type Description <code>~/input/objects</code> autoware_auto_perception_msgs/msg/DetectedObjects.msg Radar objects."},{"location":"perception/radar_object_clustering/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> autoware_auto_perception_msgs/msg/DetectedObjects.msg Output objects"},{"location":"perception/radar_object_clustering/#parameters","title":"Parameters","text":"Name Type Description Default value <code>angle_threshold</code> double Angle threshold to judge whether radar detections come from one object. [rad] 0.174 <code>distance_threshold</code> double Distance threshold to judge whether radar detections come from one object. [m] 4.0 <code>velocity_threshold</code> double Velocity threshold to judge whether radar detections come from one object. [m/s] 2.0 <code>is_fixed_label</code> bool If this parameter is true, the label of a clustered object is overwritten by the label set by <code>fixed_label</code> parameter. false <code>fixed_label</code> string If <code>is_fixed_label</code> is true, the label of a clustered object is overwritten by this parameter. \"UNKNOWN\" <code>is_fixed_size</code> bool If this parameter is true, the size of a clustered object is overwritten by the label set by <code>size_x</code>, <code>size_y</code>, and <code>size_z</code> parameters. false <code>size_x</code> double If <code>is_fixed_size</code> is true, the x-axis size of a clustered object is overwritten by this parameter. [m] 4.0 <code>size_y</code> double If <code>is_fixed_size</code> is true, the y-axis size of a clustered object is overwritten by this parameter. [m] 1.5 <code>size_z</code> double If <code>is_fixed_size</code> is true, the z-axis size of a clustered object is overwritten by this parameter. [m] 1.5"},{"location":"perception/radar_object_tracker/","title":"Radar Object Tracker","text":""},{"location":"perception/radar_object_tracker/#radar-object-tracker","title":"Radar Object Tracker","text":""},{"location":"perception/radar_object_tracker/#purpose","title":"Purpose","text":"<p>This package provides a radar object tracking node that processes sequences of detected objects to assign consistent identities to them and estimate their velocities.</p>"},{"location":"perception/radar_object_tracker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This radar object tracker is a combination of data association and tracking algorithms.</p>"},{"location":"perception/radar_object_tracker/#data-association","title":"Data Association","text":"<p>The data association algorithm matches detected objects to existing tracks.</p>"},{"location":"perception/radar_object_tracker/#tracker-models","title":"Tracker Models","text":"<p>The tracker models used in this package vary based on the class of the detected object. See more details in the models.md.</p>"},{"location":"perception/radar_object_tracker/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/radar_object_tracker/#input","title":"Input","text":"Name Type Description <code>~/input</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> Detected objects <code>/vector/map</code> <code>autoware_auto_msgs::msg::HADMapBin</code> Map data"},{"location":"perception/radar_object_tracker/#output","title":"Output","text":"Name Type Description <code>~/output</code> <code>autoware_auto_perception_msgs::msg::TrackedObjects</code> Tracked objects"},{"location":"perception/radar_object_tracker/#parameters","title":"Parameters","text":""},{"location":"perception/radar_object_tracker/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>publish_rate</code> double 10.0 The rate at which to publish the output messages <code>world_frame_id</code> string \"map\" The frame ID of the world coordinate system <code>enable_delay_compensation</code> bool false Whether to enable delay compensation. If set to <code>true</code>, output topic is published by timer with <code>publish_rate</code>. <code>tracking_config_directory</code> string \"./config/tracking/\" The directory containing the tracking configuration files <code>enable_logging</code> bool false Whether to enable logging <code>logging_file_path</code> string \"/tmp/association_log.json\" The path to the file where logs should be written <code>tracker_lifetime</code> double 1.0 The lifetime of the tracker in seconds <code>use_distance_based_noise_filtering</code> bool true Whether to use distance based filtering <code>minimum_range_threshold</code> double 70.0 Minimum distance threshold for filtering in meters <code>use_map_based_noise_filtering</code> bool true Whether to use map based filtering <code>max_distance_from_lane</code> double 5.0 Maximum distance from lane for filtering in meters <code>max_angle_diff_from_lane</code> double 0.785398 Maximum angle difference from lane for filtering in radians <code>max_lateral_velocity</code> double 5.0 Maximum lateral velocity for filtering in m/s <code>can_assign_matrix</code> array An array of integers used in the data association algorithm <code>max_dist_matrix</code> array An array of doubles used in the data association algorithm <code>max_area_matrix</code> array An array of doubles used in the data association algorithm <code>min_area_matrix</code> array An array of doubles used in the data association algorithm <code>max_rad_matrix</code> array An array of doubles used in the data association algorithm <code>min_iou_matrix</code> array An array of doubles used in the data association algorithm <p>See more details in the models.md.</p>"},{"location":"perception/radar_object_tracker/#tracker-parameters","title":"Tracker parameters","text":"<p>Currently, this package supports the following trackers:</p> <ul> <li><code>linear_motion_tracker</code></li> <li><code>constant_turn_rate_motion_tracker</code></li> </ul> <p>Default settings for each tracker are defined in the ./config/tracking/, and described in models.md.</p>"},{"location":"perception/radar_object_tracker/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/radar_object_tracker/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/radar_object_tracker/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/radar_object_tracker/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/radar_object_tracker/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/radar_object_tracker/models/","title":"models","text":""},{"location":"perception/radar_object_tracker/models/#models","title":"models","text":"<p>Tracking models can be chosen from the ros parameter <code>~tracking_model</code>:</p> <p>Each model has its own parameters, which can be set in the ros parameter server.</p> <ul> <li>model name<ul> <li>parameter name for general</li> <li>override parameter name for each tracking object class</li> </ul> </li> </ul>"},{"location":"perception/radar_object_tracker/models/#linear-constant-acceleration-model","title":"linear constant acceleration model","text":"<ul> <li>prediction</li> </ul> \\[ \\begin{bmatrix} x_{k+1} \\\\ y_{k+1} \\\\ v_{x_{k+1}} \\\\ v_{y_{k+1}} \\\\ a_{x_{k+1}} \\\\ a_{y_{k+1}} \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; dt &amp; 0 &amp; \\frac{1}{2}dt^2 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; dt &amp; 0 &amp; \\frac{1}{2}dt^2 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; dt &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; dt \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} x_k \\\\ y_k \\\\ v_{x_k} \\\\ v_{y_k} \\\\ a_{x_k} \\\\ a_{y_k} \\end{bmatrix} + noise \\] <ul> <li> <p>noise model</p> <ul> <li>random walk in acc: 2 parameters(currently disabled)</li> <li>random state noise: 6 parameters    </li> </ul> </li> </ul> <ul> <li>observation<ul> <li>observation: x,y,vx,vy</li> <li>observation noise: 4 parameters</li> </ul> </li> </ul>"},{"location":"perception/radar_object_tracker/models/#constant-turn-rate-and-velocity-model","title":"constant turn rate and velocity model","text":"<p>Just idea, not implemented yet.</p> \\[ \\begin{align} x_{k+1} &amp;= x_k + \\frac{v_k}{\\omega_k} (sin(\\theta_k + \\omega_k dt) - sin(\\theta_k)) \\\\ y_{k+1} &amp;= y_k + \\frac{v_k}{\\omega_k} (cos(\\theta_k) - cos(\\theta_k + \\omega_k dt)) \\\\ v_{k+1} &amp;= v_k \\\\ \\theta_{k+1} &amp;= \\theta_k + \\omega_k dt \\\\ \\omega_{k+1} &amp;= \\omega_k \\end{align} \\]"},{"location":"perception/radar_object_tracker/models/#noise-filtering","title":"Noise filtering","text":"<p>Radar sensors often have noisy measurement. So we use the following filter to reduce the false positive objects.</p> <p>The figure below shows the current noise filtering process.</p> <p></p>"},{"location":"perception/radar_object_tracker/models/#minimum-range-filter","title":"minimum range filter","text":"<p>In most cases, Radar sensors are used with other sensors such as LiDAR and Camera, and Radar sensors are used to detect objects far away. So we can filter out objects that are too close to the sensor.</p> <p><code>use_distance_based_noise_filtering</code> parameter is used to enable/disable this filter, and <code>minimum_range_threshold</code> parameter is used to set the threshold.</p>"},{"location":"perception/radar_object_tracker/models/#lanelet-based-filter","title":"lanelet based filter","text":"<p>With lanelet map information, We can filter out false positive objects that are not likely important obstacles.</p> <p>We filter out objects that satisfy the following conditions:</p> <ul> <li>too large lateral distance from lane</li> <li>velocity direction is too different from lane direction</li> <li>too large lateral velocity</li> </ul> <p>Each condition can be set by the following parameters:</p> <ul> <li><code>max_distance_from_lane</code></li> <li><code>max_angle_diff_from_lane</code></li> <li><code>max_lateral_velocity</code></li> </ul>"},{"location":"perception/radar_tracks_msgs_converter/","title":"radar_tracks_msgs_converter","text":""},{"location":"perception/radar_tracks_msgs_converter/#radar_tracks_msgs_converter","title":"radar_tracks_msgs_converter","text":"<p>This package convert from radar_msgs/msg/RadarTracks into autoware_auto_perception_msgs/msg/DetectedObject and autoware_auto_perception_msgs/msg/TrackedObject.</p> <ul> <li>Calculation cost is O(n).<ul> <li>n: The number of radar objects</li> </ul> </li> </ul>"},{"location":"perception/radar_tracks_msgs_converter/#design","title":"Design","text":""},{"location":"perception/radar_tracks_msgs_converter/#input-output","title":"Input / Output","text":"<ul> <li>Input<ul> <li><code>~/input/radar_objects</code> (radar_msgs/msg/RadarTracks.msg): Input radar topic</li> <li><code>~/input/odometry</code> (nav_msgs/msg/Odometry.msg): Ego vehicle odometry topic</li> </ul> </li> <li>Output<ul> <li><code>~/output/radar_detected_objects</code> (autoware_auto_perception_msgs/msg/DetectedObject.idl): The topic converted to Autoware's message. This is used for radar sensor fusion detection and radar detection.</li> <li><code>~/output/radar_tracked_objects</code> (autoware_auto_perception_msgs/msg/TrackedObject.idl): The topic converted to Autoware's message. This is used for tracking layer sensor fusion.</li> </ul> </li> </ul>"},{"location":"perception/radar_tracks_msgs_converter/#parameters","title":"Parameters","text":"<ul> <li><code>update_rate_hz</code> (double): The update rate [hz].<ul> <li>Default parameter is 20.0</li> </ul> </li> <li><code>new_frame_id</code> (string): The header frame of output topic.<ul> <li>Default parameter is \"base_link\"</li> </ul> </li> <li><code>use_twist_compensation</code> (bool): If the parameter is true, then the twist of output objects' topic is compensated by ego vehicle motion.<ul> <li>Default parameter is \"false\"</li> </ul> </li> </ul>"},{"location":"perception/radar_tracks_msgs_converter/#note","title":"Note","text":"<p>This package convert the label from <code>radar_msgs/msg/RadarTrack.msg</code> to Autoware label. Label id is defined as below.</p> RadarTrack Autoware UNKNOWN 32000 0 CAR 32001 1 TRUCK 32002 2 BUS 32003 3 TRAILER 32004 4 MOTORCYCLE 32005 5 BICYCLE 32006 6 PEDESTRIAN 32007 7 <ul> <li>radar_msgs/msg/RadarTrack.msg: additional vendor-specific classifications are permitted starting from 32000.</li> <li>Autoware objects label</li> </ul>"},{"location":"perception/shape_estimation/","title":"shape_estimation","text":""},{"location":"perception/shape_estimation/#shape_estimation","title":"shape_estimation","text":""},{"location":"perception/shape_estimation/#purpose","title":"Purpose","text":"<p>This node calculates a refined object shape (bounding box, cylinder, convex hull) in which a pointcloud cluster fits according to a label.</p>"},{"location":"perception/shape_estimation/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/shape_estimation/#fitting-algorithms","title":"Fitting algorithms","text":"<ul> <li> <p>bounding box</p> <p>L-shape fitting. See reference below for details.</p> </li> </ul> <ul> <li> <p>cylinder</p> <p><code>cv::minEnclosingCircle</code></p> </li> </ul> <ul> <li> <p>convex hull</p> <p><code>cv::convexHull</code></p> </li> </ul>"},{"location":"perception/shape_estimation/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/shape_estimation/#input","title":"Input","text":"Name Type Description <code>input</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> detected objects with labeled cluster"},{"location":"perception/shape_estimation/#output","title":"Output","text":"Name Type Description <code>output/objects</code> <code>autoware_auto_perception_msgs::msg::DetectedObjects</code> detected objects with refined shape"},{"location":"perception/shape_estimation/#parameters","title":"Parameters","text":"Name Type Default Value Description <code>use_corrector</code> bool true The flag to apply rule-based filter <code>use_filter</code> bool true The flag to apply rule-based corrector <code>use_vehicle_reference_yaw</code> bool true The flag to use vehicle reference yaw for corrector"},{"location":"perception/shape_estimation/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD</p>"},{"location":"perception/shape_estimation/#referencesexternal-links","title":"References/External links","text":"<p>L-shape fitting implementation of the paper:</p> <pre><code>@conference{Zhang-2017-26536,\nauthor = {Xiao Zhang and Wenda Xu and Chiyu Dong and John M. Dolan},\ntitle = {Efficient L-Shape Fitting for Vehicle Detection Using Laser Scanners},\nbooktitle = {2017 IEEE Intelligent Vehicles Symposium},\nyear = {2017},\nmonth = {June},\nkeywords = {autonomous driving, laser scanner, perception, segmentation},\n}\n</code></pre>"},{"location":"perception/simple_object_merger/","title":"simple_object_merger","text":""},{"location":"perception/simple_object_merger/#simple_object_merger","title":"simple_object_merger","text":"<p>This package can merge multiple topics of autoware_auto_perception_msgs/msg/DetectedObject without data association algorithm.</p>"},{"location":"perception/simple_object_merger/#algorithm","title":"Algorithm","text":""},{"location":"perception/simple_object_merger/#background","title":"Background","text":"<p>This package can merge multiple DetectedObjects without matching processing. Object_merger solve data association algorithm like Hungarian algorithm for matching problem, but it needs computational cost. In addition, for now, object_merger can handle only 2 DetectedObjects topics and cannot handle more than 2 topics in one node. To merge 6 DetectedObjects topics, 6 object_merger nodes need to stand.</p> <p>So this package aim to merge DetectedObjects simply. This package do not use data association algorithm to reduce the computational cost, and it can handle more than 2 topics in one node to prevent launching a large number of nodes.</p>"},{"location":"perception/simple_object_merger/#limitation","title":"Limitation","text":"<ul> <li>Sensor data drops and delay</li> </ul> <p>Merged objects will not be published until all topic data is received when initializing. In addition, to care sensor data drops and delayed, this package has a parameter to judge timeout. When the latest time of the data of a topic is older than the timeout parameter, it is not merged for output objects. For now specification of this package, if all topic data is received at first and after that the data drops, and the merged objects are published without objects which is judged as timeout. The timeout parameter should be determined by sensor cycle time.</p> <ul> <li>Post-processing</li> </ul> <p>Because this package does not have matching processing, so it can be used only when post-processing is used. For now, clustering processing can be used as post-processing.</p>"},{"location":"perception/simple_object_merger/#use-case","title":"Use case","text":"<p>Use case is as below.</p> <ul> <li>Multiple radar detection</li> </ul> <p>This package can be used for multiple radar detection. Since clustering processing will be included later process in radar faraway detection, this package can be used.</p>"},{"location":"perception/simple_object_merger/#input","title":"Input","text":"Name Type Description std::vector 3D detected objects. Topic names are set by parameters"},{"location":"perception/simple_object_merger/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> autoware_auto_perception_msgs/msg/DetectedObjects.msg Merged objects"},{"location":"perception/simple_object_merger/#parameters","title":"Parameters","text":"Name Type Description Default value <code>update_rate_hz</code> double Update rate. [hz] 20.0 <code>new_frame_id</code> string The header frame_id of output topic. \"base_link\" <code>timeout_threshold</code> double Threshold for timeout judgement [s]. 1.0 <code>input_topics</code> List[string] Input topics name. \"[]\""},{"location":"perception/tensorrt_classifier/","title":"TensorRT Classification for Efficient Dynamic Batched Inference","text":""},{"location":"perception/tensorrt_classifier/#tensorrt-classification-for-efficient-dynamic-batched-inference","title":"TensorRT Classification for Efficient Dynamic Batched Inference","text":""},{"location":"perception/tensorrt_classifier/#purpose","title":"Purpose","text":"<p>This package classifies arbitrary categories using TensorRT for efficient and faster inference. Specifically, this optimizes preprocessing for efficient inference on embedded platform. Moreover, we support dynamic batched inference in GPUs and DLAs.</p>"},{"location":"perception/tensorrt_yolo/","title":"tensorrt_yolo","text":""},{"location":"perception/tensorrt_yolo/#tensorrt_yolo","title":"tensorrt_yolo","text":""},{"location":"perception/tensorrt_yolo/#purpose","title":"Purpose","text":"<p>This package detects 2D bounding boxes for target objects e.g., cars, trucks, bicycles, and pedestrians on a image based on YOLO(You only look once) model.</p>"},{"location":"perception/tensorrt_yolo/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/tensorrt_yolo/#cite","title":"Cite","text":"<p>yolov3</p> <p>Redmon, J., &amp; Farhadi, A. (2018). Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767.</p> <p>yolov4</p> <p>Bochkovskiy, A., Wang, C. Y., &amp; Liao, H. Y. M. (2020). Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934.</p> <p>yolov5</p> <p>Jocher, G., et al. (2021). ultralytics/yolov5: v6.0 - YOLOv5n 'Nano' models, Roboflow integration, TensorFlow export, OpenCV DNN support (v6.0). Zenodo. https://doi.org/10.5281/zenodo.5563715</p>"},{"location":"perception/tensorrt_yolo/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/tensorrt_yolo/#input","title":"Input","text":"Name Type Description <code>in/image</code> <code>sensor_msgs/Image</code> The input image"},{"location":"perception/tensorrt_yolo/#output","title":"Output","text":"Name Type Description <code>out/objects</code> <code>tier4_perception_msgs/DetectedObjectsWithFeature</code> The detected objects with 2D bounding boxes <code>out/image</code> <code>sensor_msgs/Image</code> The image with 2D bounding boxes for visualization"},{"location":"perception/tensorrt_yolo/#parameters","title":"Parameters","text":""},{"location":"perception/tensorrt_yolo/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>anchors</code> double array [10.0, 13.0, 16.0, 30.0, 33.0, 23.0, 30.0, 61.0, 62.0, 45.0, 59.0, 119.0, 116.0, 90.0, 156.0, 198.0, 373.0, 326.0] The anchors to create bounding box candidates <code>scale_x_y</code> double array [1.0, 1.0, 1.0] The scale parameter to eliminate grid sensitivity <code>score_thresh</code> double 0.1 If the objectness score is less than this value, the object is ignored in yolo layer. <code>iou_thresh</code> double 0.45 The iou threshold for NMS method <code>detections_per_im</code> int 100 The maximum detection number for one frame <code>use_darknet_layer</code> bool true The flag to use yolo layer in darknet <code>ignore_thresh</code> double 0.5 If the output score is less than this value, ths object is ignored."},{"location":"perception/tensorrt_yolo/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>data_path</code> string \"\" Packages data and artifacts directory path <code>onnx_file</code> string \"\" The onnx file name for yolo model <code>engine_file</code> string \"\" The tensorrt engine file name for yolo model <code>label_file</code> string \"\" The label file with label names for detected objects written on it <code>calib_image_directory</code> string \"\" The directory name including calibration images for int8 inference <code>calib_cache_file</code> string \"\" The calibration cache file for int8 inference <code>mode</code> string \"FP32\" The inference mode: \"FP32\", \"FP16\", \"INT8\" <code>gpu_id</code> int 0 GPU device ID that runs the model"},{"location":"perception/tensorrt_yolo/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>This package includes multiple licenses.</p>"},{"location":"perception/tensorrt_yolo/#onnx-model","title":"Onnx model","text":"<p>All YOLO ONNX models are converted from the officially trained model. If you need information about training datasets and conditions, please refer to the official repositories.</p> <p>All models are downloaded during env preparation by ansible (as mention in installation). It is also possible to download them manually, see Manual downloading of artifacts . When launching the node with a model for the first time, the model is automatically converted to TensorRT, although this may take some time.</p>"},{"location":"perception/tensorrt_yolo/#yolov3","title":"YOLOv3","text":"<p>YOLOv3: Converted from darknet weight file and conf file.</p> <ul> <li>This code is used for converting darknet weight file and conf file to onnx.</li> </ul>"},{"location":"perception/tensorrt_yolo/#yolov4","title":"YOLOv4","text":"<p>YOLOv4: Converted from darknet weight file and conf file.</p> <p>YOLOv4-tiny: Converted from darknet weight file and conf file.</p> <ul> <li>This code is used for converting darknet weight file and conf file to onnx.</li> </ul>"},{"location":"perception/tensorrt_yolo/#yolov5","title":"YOLOv5","text":"<p>Refer to this guide</p> <ul> <li>YOLOv5s</li> </ul> <ul> <li>YOLOv5m</li> </ul> <ul> <li>YOLOv5l</li> </ul> <ul> <li>YOLOv5x</li> </ul>"},{"location":"perception/tensorrt_yolo/#limitations","title":"Limitations","text":"<ul> <li>If you want to run multiple instances of this node for multiple cameras using \"yolo.launch.xml\", first of all, create a TensorRT engine by running the \"tensorrt_yolo.launch.xml\" launch file separately for each GPU. Otherwise, multiple instances of the node trying to create the same TensorRT engine can cause potential problems.</li> </ul>"},{"location":"perception/tensorrt_yolo/#reference-repositories","title":"Reference repositories","text":"<ul> <li>https://github.com/pjreddie/darknet</li> </ul> <ul> <li>https://github.com/AlexeyAB/darknet</li> </ul> <ul> <li>https://github.com/ultralytics/yolov5</li> </ul> <ul> <li>https://github.com/wang-xinyu/tensorrtx</li> </ul> <ul> <li>https://github.com/NVIDIA/retinanet-examples</li> </ul>"},{"location":"perception/tensorrt_yolox/","title":"tensorrt_yolox","text":""},{"location":"perception/tensorrt_yolox/#tensorrt_yolox","title":"tensorrt_yolox","text":""},{"location":"perception/tensorrt_yolox/#purpose","title":"Purpose","text":"<p>This package detects target objects e.g., cars, trucks, bicycles, and pedestrians on a image based on YOLOX model.</p>"},{"location":"perception/tensorrt_yolox/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/tensorrt_yolox/#cite","title":"Cite","text":"<p>Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun, \"YOLOX: Exceeding YOLO Series in 2021\", arXiv preprint arXiv:2107.08430, 2021 [ref]</p>"},{"location":"perception/tensorrt_yolox/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/tensorrt_yolox/#input","title":"Input","text":"Name Type Description <code>in/image</code> <code>sensor_msgs/Image</code> The input image"},{"location":"perception/tensorrt_yolox/#output","title":"Output","text":"Name Type Description <code>out/objects</code> <code>tier4_perception_msgs/DetectedObjectsWithFeature</code> The detected objects with 2D bounding boxes <code>out/image</code> <code>sensor_msgs/Image</code> The image with 2D bounding boxes for visualization"},{"location":"perception/tensorrt_yolox/#parameters","title":"Parameters","text":""},{"location":"perception/tensorrt_yolox/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>score_threshold</code> float 0.3 If the objectness score is less than this value, the object is ignored in yolox layer. <code>nms_threshold</code> float 0.7 The IoU threshold for NMS method <p>NOTE: These two parameters are only valid for \"plain\" model (described later).</p>"},{"location":"perception/tensorrt_yolox/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>model_path</code> string \"\" The onnx file name for yolox model <code>label_path</code> string \"\" The label file with label names for detected objects written on it <code>precision</code> string \"fp16\" The inference mode: \"fp32\", \"fp16\", \"int8\" <code>build_only</code> bool false shutdown node after TensorRT engine file is built <code>calibration_algorithm</code> string \"MinMax\" Calibration algorithm to be used for quantization when precision==int8. Valid value is one of: Entropy\",(\"Legacy\" | \"Percentile\"), \"MinMax\"] <code>dla_core_id</code> int -1 If positive ID value is specified, the node assign inference task to the DLA core <code>quantize_first_layer</code> bool false If true, set the operating precision for the first (input) layer to be fp16. This option is valid only when precision==int8 <code>quantize_last_layer</code> bool false If true, set the operating precision for the last (output) layer to be fp16. This option is valid only when precision==int8 <code>profile_per_layer</code> bool false If true, profiler function will be enabled. Since the profile function may affect execution speed, it is recommended to set this flag true only for development purpose. <code>clip_value</code> double 0.0 If positive value is specified, the value of each layer output will be clipped between [0.0, clip_value]. This option is valid only when precision==int8 and used to manually specify the dynamic range instead of using any calibration <code>preprocess_on_gpu</code> bool true If true, pre-processing is performed on GPU <code>calibration_image_list_path</code> string \"\" Path to a file which contains path to images. Those images will be used for int8 quantization."},{"location":"perception/tensorrt_yolox/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The label contained in detected 2D bounding boxes (i.e., <code>out/objects</code>) will be either one of the followings:</p> <ul> <li>CAR</li> <li>PEDESTRIAN (\"PERSON\" will also be categorized as \"PEDESTRIAN\")</li> <li>BUS</li> <li>TRUCK</li> <li>BICYCLE</li> <li>MOTORCYCLE</li> </ul> <p>If other labels (case insensitive) are contained in the file specified via the <code>label_file</code> parameter, those are labeled as <code>UNKNOWN</code>, while detected rectangles are drawn in the visualization result (<code>out/image</code>).</p>"},{"location":"perception/tensorrt_yolox/#onnx-model","title":"Onnx model","text":"<p>A sample model (named <code>yolox-tiny.onnx</code>) is downloaded by ansible script on env preparation stage, if not, please, follow Manual downloading of artifacts. To accelerate Non-maximum-suppression (NMS), which is one of the common post-process after object detection inference, <code>EfficientNMS_TRT</code> module is attached after the ordinal YOLOX (tiny) network. The <code>EfficientNMS_TRT</code> module contains fixed values for <code>score_threshold</code> and <code>nms_threshold</code> in it, hence these parameters are ignored when users specify ONNX models including this module.</p> <p>This package accepts both <code>EfficientNMS_TRT</code> attached ONNXs and models published from the official YOLOX repository (we referred to them as \"plain\" models).</p> <p>In addition to <code>yolox-tiny.onnx</code>, a custom model named <code>yolox-sPlus-opt.onnx</code> is either available. This model is based on YOLOX-s and tuned to perform more accurate detection with almost comparable execution speed with <code>yolox-tiny</code>. To get better results with this model, users are recommended to use some specific running arguments such as <code>precision:=int8</code>, <code>calibration_algorithm:=Entropy</code>, <code>clip_value:=6.0</code>. Users can refer <code>launch/yolox_sPlus_opt.launch.xml</code> to see how this model can be used.</p> <p>All models are automatically converted to TensorRT format. These converted files will be saved in the same directory as specified ONNX files with <code>.engine</code> filename extension and reused from the next run. The conversion process may take a while (typically 10 to 20 minutes) and the inference process is blocked until complete the conversion, so it will take some time until detection results are published (even until appearing in the topic list) on the first run</p>"},{"location":"perception/tensorrt_yolox/#package-acceptable-model-generation","title":"Package acceptable model generation","text":"<p>To convert users' own model that saved in PyTorch's <code>pth</code> format into ONNX, users can exploit the converter offered by the official repository. For the convenience, only procedures are described below. Please refer the official document for more detail.</p>"},{"location":"perception/tensorrt_yolox/#for-plain-models","title":"For plain models","text":"<ol> <li> <p>Install dependency</p> <pre><code>git clone git@github.com:Megvii-BaseDetection/YOLOX.git\ncd YOLOX\npython3 setup.py develop --user\n</code></pre> </li> <li> <p>Convert pth into ONNX</p> <pre><code>python3 tools/export_onnx.py \\\n--output-name YOUR_YOLOX.onnx \\\n-f YOUR_YOLOX.py \\\n-c YOUR_YOLOX.pth\n</code></pre> </li> </ol>"},{"location":"perception/tensorrt_yolox/#for-efficientnms_trt-embedded-models","title":"For EfficientNMS_TRT embedded models","text":"<ol> <li> <p>Install dependency</p> <pre><code>git clone git@github.com:Megvii-BaseDetection/YOLOX.git\ncd YOLOX\npython3 setup.py develop --user\npip3 install git+ssh://git@github.com/wep21/yolox_onnx_modifier.git --user\n</code></pre> </li> <li> <p>Convert pth into ONNX</p> <pre><code>python3 tools/export_onnx.py \\\n--output-name YOUR_YOLOX.onnx \\\n-f YOUR_YOLOX.py \\\n-c YOUR_YOLOX.pth\n  --decode_in_inference\n</code></pre> </li> <li> <p>Embed <code>EfficientNMS_TRT</code> to the end of YOLOX</p> <pre><code>yolox_onnx_modifier YOUR_YOLOX.onnx -o YOUR_YOLOX_WITH_NMS.onnx\n</code></pre> </li> </ol>"},{"location":"perception/tensorrt_yolox/#label-file","title":"Label file","text":"<p>A sample label file (named <code>label.txt</code>)is also downloaded automatically during env preparation process (NOTE: This file is incompatible with models that output labels for the COCO dataset (e.g., models from the official YOLOX repository)).</p> <p>This file represents the correspondence between class index (integer outputted from YOLOX network) and class label (strings making understanding easier). This package maps class IDs (incremented from 0) with labels according to the order in this file.</p>"},{"location":"perception/tensorrt_yolox/#reference-repositories","title":"Reference repositories","text":"<ul> <li>https://github.com/Megvii-BaseDetection/YOLOX</li> <li>https://github.com/wep21/yolox_onnx_modifier</li> </ul>"},{"location":"perception/tracking_object_merger/","title":"Tracking Object Merger","text":""},{"location":"perception/tracking_object_merger/#tracking-object-merger","title":"Tracking Object Merger","text":""},{"location":"perception/tracking_object_merger/#purpose","title":"Purpose","text":"<p>This package try to merge two tracking objects from different sensor.</p>"},{"location":"perception/tracking_object_merger/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Merging tracking objects from different sensor is a combination of data association and state fusion algorithms.</p> <p>Detailed process depends on the merger policy.</p>"},{"location":"perception/tracking_object_merger/#decorative_tracker_merger","title":"decorative_tracker_merger","text":"<p>In decorative_tracker_merger, we assume there are dominant tracking objects and sub tracking objects. The name <code>decorative</code> means that sub tracking objects are used to complement the main objects.</p> <p>Usually the dominant tracking objects are from LiDAR and sub tracking objects are from Radar or Camera.</p> <p>Here show the processing pipeline.</p> <p></p>"},{"location":"perception/tracking_object_merger/#time-sync","title":"time sync","text":"<p>Sub object(Radar or Camera) often has higher frequency than dominant object(LiDAR). So we need to sync the time of sub object to dominant object.</p> <p></p>"},{"location":"perception/tracking_object_merger/#data-association","title":"data association","text":"<p>In the data association, we use the following rules to determine whether two tracking objects are the same object.</p> <ul> <li>gating<ul> <li><code>distance gate</code>: distance between two tracking objects</li> <li><code>angle gate</code>: angle between two tracking objects</li> <li><code>mahalanobis_distance_gate</code>: Mahalanobis distance between two tracking objects</li> <li><code>min_iou_gate</code>: minimum IoU between two tracking objects</li> <li><code>max_velocity_gate</code>: maximum velocity difference between two tracking objects</li> </ul> </li> <li>score<ul> <li>score used in matching is equivalent to the distance between two tracking objects</li> </ul> </li> </ul>"},{"location":"perception/tracking_object_merger/#tracklet-update","title":"tracklet update","text":"<p>Sub tracking objects are merged into dominant tracking objects.</p> <p>Depends on the tracklet input sensor state, we update the tracklet state with different rules.</p> state\\priority 1st 2nd 3rd Kinematics except velocity LiDAR Radar Camera Forward velocity Radar LiDAR Camera Object classification Camera LiDAR Radar"},{"location":"perception/tracking_object_merger/#tracklet-management","title":"tracklet management","text":"<p>We use the <code>existence_probability</code> to manage tracklet.</p> <ul> <li>When we create a new tracklet, we set the <code>existence_probability</code> to \\(p_{sensor}\\) value.</li> <li>In each update with specific sensor, we set the <code>existence_probability</code> to \\(p_{sensor}\\) value.</li> <li>When tracklet does not have update with specific sensor, we reduce the <code>existence_probability</code> by <code>decay_rate</code></li> <li>Object can be published if <code>existence_probability</code> is larger than <code>publish_probability_threshold</code></li> <li>Object will be removed if <code>existence_probability</code> is smaller than <code>remove_probability_threshold</code></li> </ul> <p></p> <p>These parameter can be set in <code>config/decorative_tracker_merger.param.yaml</code>.</p> <pre><code>tracker_state_parameter:\nremove_probability_threshold: 0.3\npublish_probability_threshold: 0.6\ndefault_lidar_existence_probability: 0.7\ndefault_radar_existence_probability: 0.6\ndefault_camera_existence_probability: 0.6\ndecay_rate: 0.1\nmax_dt: 1.0\n</code></pre>"},{"location":"perception/tracking_object_merger/#inputparameters","title":"input/parameters","text":"topic name message type description <code>~input/main_object</code> <code>autoware_auto_perception_msgs::TrackedObjects</code> Dominant tracking objects. Output will be published with this dominant object stamps. <code>~input/sub_object</code> <code>autoware_auto_perception_msgs::TrackedObjects</code> Sub tracking objects. <code>output/object</code> <code>autoware_auto_perception_msgs::TrackedObjects</code> Merged tracking objects. <code>debug/interpolated_sub_object</code> <code>autoware_auto_perception_msgs::TrackedObjects</code> Interpolated sub tracking objects. <p>Default parameters are set in config/decorative_tracker_merger.param.yaml.</p> parameter name description default value <code>base_link_frame_id</code> base link frame id. This is used to transform the tracking object. \"base_link\" <code>time_sync_threshold</code> time sync threshold. If the time difference between two tracking objects is smaller than this value, we consider these two tracking objects are the same object. 0.05 <code>sub_object_timeout_sec</code> sub object timeout. If the sub object is not updated for this time, we consider this object is not exist. 0.5 <code>main_sensor_type</code> main sensor type. This is used to determine the dominant tracking object. \"lidar\" <code>sub_sensor_type</code> sub sensor type. This is used to determine the sub tracking object. \"radar\" <code>tracker_state_parameter</code> tracker state parameter. This is used to manage the tracklet. <ul> <li>the detail of <code>tracker_state_parameter</code> is described in tracklet management</li> </ul>"},{"location":"perception/tracking_object_merger/#tuning","title":"tuning","text":"<p>As explained in tracklet management, this tracker merger tend to maintain the both input tracking objects.</p> <p>If there are many false positive tracking objects,</p> <ul> <li>decrease <code>default_&lt;sensor&gt;_existence_probability</code> of that sensor</li> <li>increase <code>decay_rate</code></li> <li>increase <code>publish_probability_threshold</code> to publish only reliable tracking objects</li> </ul>"},{"location":"perception/tracking_object_merger/#equivalent_tracker_merger","title":"equivalent_tracker_merger","text":"<p>This is future work.</p>"},{"location":"perception/traffic_light_arbiter/","title":"traffic_light_arbiter","text":""},{"location":"perception/traffic_light_arbiter/#traffic_light_arbiter","title":"traffic_light_arbiter","text":""},{"location":"perception/traffic_light_arbiter/#purpose","title":"Purpose","text":"<p>This package receives traffic signals from perception and external (e.g., V2X) components and combines them using either a confidence-based or a external-preference based approach.</p>"},{"location":"perception/traffic_light_arbiter/#trafficlightarbiter","title":"TrafficLightArbiter","text":"<p>A node that merges traffic light/signal state from image recognition and external (e.g., V2X) systems to provide to a planning component.</p>"},{"location":"perception/traffic_light_arbiter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/traffic_light_arbiter/#input","title":"Input","text":"Name Type Description ~/sub/vector_map autoware_auto_mapping_msgs::msg::HADMapBin The vector map to get valid traffic signal ids. ~/sub/perception_traffic_signals autoware_perception_msgs::msg::TrafficSignalArray The traffic signals from the image recognition pipeline. ~/sub/external_traffic_signals autoware_perception_msgs::msg::TrafficSignalArray The traffic signals from an external system."},{"location":"perception/traffic_light_arbiter/#output","title":"Output","text":"Name Type Description ~/pub/traffic_signals autoware_perception_msgs::msg::TrafficSignalArray The merged traffic signal state."},{"location":"perception/traffic_light_arbiter/#parameters","title":"Parameters","text":""},{"location":"perception/traffic_light_arbiter/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>external_time_tolerance</code> double 5.0 The duration in seconds an external message is considered valid for merging <code>perception_time_tolerance</code> double 1.0 The duration in seconds a perception message is considered valid for merging <code>external_priority</code> bool false Whether or not externals signals take precedence over perception-based ones. If false, the merging uses confidence as a criteria"},{"location":"perception/traffic_light_classifier/","title":"traffic_light_classifier","text":""},{"location":"perception/traffic_light_classifier/#traffic_light_classifier","title":"traffic_light_classifier","text":""},{"location":"perception/traffic_light_classifier/#purpose","title":"Purpose","text":"<p>traffic_light_classifier is a package for classifying traffic light labels using cropped image around a traffic light. This package has two classifier models: <code>cnn_classifier</code> and <code>hsv_classifier</code>.</p>"},{"location":"perception/traffic_light_classifier/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/traffic_light_classifier/#cnn_classifier","title":"cnn_classifier","text":"<p>Traffic light labels are classified by EfficientNet-b1 or MobileNet-v2. Totally 83400 (58600 for training, 14800 for evaluation and 10000 for test) TIER IV internal images of Japanese traffic lights were used for fine-tuning. The information of the models is listed here:</p> Name Input Size Test Accuracy EfficientNet-b1 128 x 128 99.76% MobileNet-v2 224 x 224 99.81%"},{"location":"perception/traffic_light_classifier/#hsv_classifier","title":"hsv_classifier","text":"<p>Traffic light colors (green, yellow and red) are classified in HSV model.</p>"},{"location":"perception/traffic_light_classifier/#about-label","title":"About Label","text":"<p>The message type is designed to comply with the unified road signs proposed at the Vienna Convention. This idea has been also proposed in Autoware.Auto.</p> <p>There are rules for naming labels that nodes receive. One traffic light is represented by the following character string separated by commas. <code>color1-shape1, color2-shape2</code> .</p> <p>For example, the simple red and red cross traffic light label must be expressed as \"red-circle, red-cross\".</p> <p>These colors and shapes are assigned to the message as follows: </p>"},{"location":"perception/traffic_light_classifier/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/traffic_light_classifier/#input","title":"Input","text":"Name Type Description <code>~/input/image</code> <code>sensor_msgs::msg::Image</code> input image <code>~/input/rois</code> <code>tier4_perception_msgs::msg::TrafficLightRoiArray</code> rois of traffic lights"},{"location":"perception/traffic_light_classifier/#output","title":"Output","text":"Name Type Description <code>~/output/traffic_signals</code> <code>tier4_perception_msgs::msg::TrafficSignalArray</code> classified signals <code>~/output/debug/image</code> <code>sensor_msgs::msg::Image</code> image for debugging"},{"location":"perception/traffic_light_classifier/#parameters","title":"Parameters","text":""},{"location":"perception/traffic_light_classifier/#node-parameters","title":"Node Parameters","text":"Name Type Description <code>classifier_type</code> int if the value is <code>1</code>, cnn_classifier is used <code>data_path</code> str packages data and artifacts directory path"},{"location":"perception/traffic_light_classifier/#core-parameters","title":"Core Parameters","text":""},{"location":"perception/traffic_light_classifier/#cnn_classifier_1","title":"cnn_classifier","text":"Name Type Description <code>classifier_label_path</code> str path to the model file <code>classifier_model_path</code> str path to the label file <code>classifier_precision</code> str TensorRT precision, <code>fp16</code> or <code>int8</code> <code>classifier_mean</code> vector\\ 3-channel input image mean <code>classifier_std</code> vector\\ 3-channel input image std <code>apply_softmax</code> bool whether or not apply softmax"},{"location":"perception/traffic_light_classifier/#hsv_classifier_1","title":"hsv_classifier","text":"Name Type Description <code>green_min_h</code> int the minimum hue of green color <code>green_min_s</code> int the minimum saturation of green color <code>green_min_v</code> int the minimum value (brightness) of green color <code>green_max_h</code> int the maximum hue of green color <code>green_max_s</code> int the maximum saturation of green color <code>green_max_v</code> int the maximum value (brightness) of green color <code>yellow_min_h</code> int the minimum hue of yellow color <code>yellow_min_s</code> int the minimum saturation of yellow color <code>yellow_min_v</code> int the minimum value (brightness) of yellow color <code>yellow_max_h</code> int the maximum hue of yellow color <code>yellow_max_s</code> int the maximum saturation of yellow color <code>yellow_max_v</code> int the maximum value (brightness) of yellow color <code>red_min_h</code> int the minimum hue of red color <code>red_min_s</code> int the minimum saturation of red color <code>red_min_v</code> int the minimum value (brightness) of red color <code>red_max_h</code> int the maximum hue of red color <code>red_max_s</code> int the maximum saturation of red color <code>red_max_v</code> int the maximum value (brightness) of red color"},{"location":"perception/traffic_light_classifier/#training-traffic-light-classifier-model","title":"Training Traffic Light Classifier Model","text":""},{"location":"perception/traffic_light_classifier/#overview","title":"Overview","text":"<p>This guide provides detailed instructions on training a traffic light classifier model using the mmlab/mmpretrain repository and deploying it using mmlab/mmdeploy. If you wish to create a custom traffic light classifier model with your own dataset, please follow the steps outlined below.</p>"},{"location":"perception/traffic_light_classifier/#data-preparation","title":"Data Preparation","text":""},{"location":"perception/traffic_light_classifier/#use-sample-dataset","title":"Use Sample Dataset","text":"<p>Autoware offers a sample dataset that illustrates the training procedures for traffic light classification. This dataset comprises 1045 images categorized into red, green, and yellow labels. To utilize this sample dataset, please download it from link and extract it to a designated folder of your choice.</p>"},{"location":"perception/traffic_light_classifier/#use-your-custom-dataset","title":"Use Your Custom Dataset","text":"<p>To train a traffic light classifier, adopt a structured subfolder format where each subfolder represents a distinct class. Below is an illustrative dataset structure example;</p> <pre><code>DATASET_ROOT\n    \u251c\u2500\u2500 TRAIN\n    \u2502    \u251c\u2500\u2500 RED\n    \u2502    \u2502   \u251c\u2500\u2500 001.png\n    \u2502    \u2502   \u251c\u2500\u2500 002.png\n    \u2502    \u2502   \u2514\u2500\u2500 ...\n    \u2502    \u2502\n    \u2502    \u251c\u2500\u2500 GREEN\n    \u2502    \u2502    \u251c\u2500\u2500 001.png\n    \u2502    \u2502    \u251c\u2500\u2500 002.png\n    \u2502    \u2502    \u2514\u2500\u2500...\n    \u2502    \u2502\n    \u2502    \u251c\u2500\u2500 YELLOW\n    \u2502    \u2502    \u251c\u2500\u2500 001.png\n    \u2502    \u2502    \u251c\u2500\u2500 002.png\n    \u2502    \u2502    \u2514\u2500\u2500...\n    \u2502    \u2514\u2500\u2500 ...\n    \u2502\n    \u251c\u2500\u2500 VAL\n    \u2502       \u2514\u2500\u2500...\n    \u2502\n    \u2502\n    \u2514\u2500\u2500 TEST\n           \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"perception/traffic_light_classifier/#installation","title":"Installation","text":""},{"location":"perception/traffic_light_classifier/#prerequisites","title":"Prerequisites","text":"<p>Step 1. Download and install Miniconda from the official website.</p> <p>Step 2. Create a conda virtual environment and activate it</p> <pre><code>conda create --name tl-classifier python=3.8 -y\nconda activate tl-classifier\n</code></pre> <p>Step 3. Install PyTorch</p> <p>Please ensure you have PyTorch installed, compatible with CUDA 11.6, as it is a requirement for current Autoware</p> <pre><code>conda install pytorch==1.13.1 torchvision==0.14.1 pytorch-cuda=11.6 -c pytorch -c nvidia\n</code></pre>"},{"location":"perception/traffic_light_classifier/#install-mmlabmmpretrain","title":"Install mmlab/mmpretrain","text":"<p>Step 1. Install mmpretrain from source</p> <pre><code>cd ~/\ngit clone https://github.com/open-mmlab/mmpretrain.git\ncd mmpretrain\npip install -U openmim &amp;&amp; mim install -e .\n</code></pre>"},{"location":"perception/traffic_light_classifier/#training","title":"Training","text":"<p>MMPretrain offers a training script that is controlled through a configuration file. Leveraging an inheritance design pattern, you can effortlessly tailor the training script using Python files as configuration files.</p> <p>In the example, we demonstrate the training steps on the MobileNetV2 model, but you have the flexibility to employ alternative classification models such as EfficientNetV2, EfficientNetV3, ResNet, and more.</p>"},{"location":"perception/traffic_light_classifier/#create-a-config-file","title":"Create a config file","text":"<p>Generate a configuration file for your preferred model within the <code>configs</code> folder</p> <pre><code>touch ~/mmpretrain/configs/mobilenet_v2/mobilenet-v2_8xb32_custom.py\n</code></pre> <p>Open the configuration file in your preferred text editor and make a copy of the provided content. Adjust the data_root variable to match the path of your dataset. You are welcome to customize the configuration parameters for the model, dataset, and scheduler to suit your preferences</p> <pre><code># Inherit model, schedule and default_runtime from base model\n_base_ = [\n    '../_base_/models/mobilenet_v2_1x.py',\n    '../_base_/schedules/imagenet_bs256_epochstep.py',\n    '../_base_/default_runtime.py'\n]\n\n# Set the number of classes to the model\n# You can also change other model parameters here\n# For detailed descriptions of model parameters, please refer to link below\n# (Customize model)[https://mmpretrain.readthedocs.io/en/latest/advanced_guides/modules.html]\nmodel = dict(head=dict(num_classes=3, topk=(1, 3)))\n\n# Set max epochs and validation interval\ntrain_cfg = dict(by_epoch=True, max_epochs=50, val_interval=5)\n\n# Set optimizer and lr scheduler\noptim_wrapper = dict(\n    optimizer=dict(type='SGD', lr=0.001, momentum=0.9))\nparam_scheduler = dict(type='StepLR', by_epoch=True, step_size=1, gamma=0.98)\n\ndataset_type = 'CustomDataset'\ndata_root = \"/PATH/OF/YOUR/DATASET\"\n\n# Customize data preprocessing and dataloader pipeline for training set\n# These parameters calculated for the sample dataset\ndata_preprocessor = dict(\n    mean=[0.2888 * 256, 0.2570 * 256, 0.2329 * 256],\n    std=[0.2106 * 256, 0.2037 * 256, 0.1864 * 256],\n    num_classes=3,\n    to_rgb=True,\n)\n\n# Customize data preprocessing and dataloader pipeline for train set\n# For detailed descriptions of data pipeline, please refer to link below\n# (Customize data pipeline)[https://mmpretrain.readthedocs.io/en/latest/advanced_guides/pipeline.html]\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='Resize', scale=224),\n    dict(type='RandomFlip', prob=0.5, direction='horizontal'),\n    dict(type='PackInputs'),\n]\ntrain_dataloader = dict(\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        ann_file='',\n        data_prefix='train',\n        with_label=True,\n        pipeline=train_pipeline,\n    ),\n    num_workers=8,\n    batch_size=32,\n    sampler=dict(type='DefaultSampler', shuffle=True)\n)\n\n# Customize data preprocessing and dataloader pipeline for test set\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='Resize', scale=224),\n    dict(type='PackInputs'),\n]\n\n# Customize data preprocessing and dataloader pipeline for validation set\nval_cfg = dict()\nval_dataloader = dict(\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        ann_file='',\n        data_prefix='val',\n        with_label=True,\n        pipeline=test_pipeline,\n    ),\n    num_workers=8,\n    batch_size=32,\n    sampler=dict(type='DefaultSampler', shuffle=True)\n)\n\nval_evaluator = dict(topk=(1, 3,), type='Accuracy')\n\ntest_dataloader = val_dataloader\ntest_evaluator = val_evaluator\n</code></pre>"},{"location":"perception/traffic_light_classifier/#start-training","title":"Start training","text":"<pre><code>cd ~/mmpretrain\npython tools/train.py configs/mobilenet_v2/mobilenet-v2_8xb32_custom.py\n</code></pre> <p>Training logs and weights will be saved in the <code>work_dirs/mobilenet-v2_8xb32_custom</code> folder.</p>"},{"location":"perception/traffic_light_classifier/#convert-pytorch-model-to-onnx-model","title":"Convert PyTorch model to ONNX model","text":""},{"location":"perception/traffic_light_classifier/#install-mmdeploy","title":"Install mmdeploy","text":"<p>The 'mmdeploy' toolset is designed for deploying your trained model onto various target devices. With its capabilities, you can seamlessly convert PyTorch models into the ONNX format.</p> <pre><code># Activate your conda environment\nconda activate tl-classifier\n\n# Install mmenigne and mmcv\nmim install mmengine\nmim install \"mmcv&gt;=2.0.0rc2\"\n\n# Install mmdeploy\npip install mmdeploy==1.2.0\n\n# Support onnxruntime\npip install mmdeploy-runtime==1.2.0\npip install mmdeploy-runtime-gpu==1.2.0\npip install onnxruntime-gpu==1.8.1\n\n#Clone mmdeploy repository\ncd ~/\ngit clone -b main https://github.com/open-mmlab/mmdeploy.git\n</code></pre>"},{"location":"perception/traffic_light_classifier/#convert-pytorch-model-to-onnx-model_1","title":"Convert PyTorch model to ONNX model","text":"<pre><code>cd ~/mmdeploy\n\n# Run deploy.py script\n# deploy.py script takes 5 main arguments with these order; config file path, train config file path,\n# checkpoint file path, demo image path, and work directory path\npython tools/deploy.py \\\n~/mmdeploy/configs/mmpretrain/classification_onnxruntime_static.py\\\n~/mmpretrain/configs/mobilenet_v2/train_mobilenet_v2.py \\\n~/mmpretrain/work_dirs/train_mobilenet_v2/epoch_300.pth \\\n/SAMPLE/IAMGE/DIRECTORY \\\n--work-dir mmdeploy_model/mobilenet_v2\n</code></pre> <p>Converted ONNX model will be saved in the <code>mmdeploy/mmdeploy_model/mobilenet_v2</code> folder.</p> <p>After obtaining your onnx model, update parameters defined in the launch file (e.g. <code>model_file_path</code>, <code>label_file_path</code>, <code>input_h</code>, <code>input_w</code>...). Note that, we only support labels defined in tier4_perception_msgs::msg::TrafficLightElement.</p>"},{"location":"perception/traffic_light_classifier/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/traffic_light_classifier/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/traffic_light_classifier/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/traffic_light_classifier/#referencesexternal-links","title":"References/External links","text":"<p>[1] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L. Chen, \"MobileNetV2: Inverted Residuals and Linear Bottlenecks,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 4510-4520, doi: 10.1109/CVPR.2018.00474.</p> <p>[2] Tan, Mingxing, and Quoc Le. \"EfficientNet: Rethinking model scaling for convolutional neural networks.\" International conference on machine learning. PMLR, 2019.</p>"},{"location":"perception/traffic_light_classifier/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/traffic_light_fine_detector/","title":"traffic_light_fine_detector","text":""},{"location":"perception/traffic_light_fine_detector/#traffic_light_fine_detector","title":"traffic_light_fine_detector","text":""},{"location":"perception/traffic_light_fine_detector/#purpose","title":"Purpose","text":"<p>It is a package for traffic light detection using YoloX-s.</p>"},{"location":"perception/traffic_light_fine_detector/#training-information","title":"Training Information","text":""},{"location":"perception/traffic_light_fine_detector/#pretrained-model","title":"Pretrained Model","text":"<p>The model is based on YOLOX and the pretrained model could be downloaded from here.</p>"},{"location":"perception/traffic_light_fine_detector/#training-data","title":"Training Data","text":"<p>The model was fine-tuned on around 17,000 TIER IV internal images of Japanese traffic lights.</p>"},{"location":"perception/traffic_light_fine_detector/#trained-onnx-model","title":"Trained Onnx model","text":"<p>You can download the ONNX file using these instructions. Please visit autoware-documentation for more information.</p>"},{"location":"perception/traffic_light_fine_detector/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Based on the camera image and the global ROI array detected by <code>map_based_detection</code> node, a CNN-based detection method enables highly accurate traffic light detection.</p>"},{"location":"perception/traffic_light_fine_detector/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/traffic_light_fine_detector/#input","title":"Input","text":"Name Type Description <code>~/input/image</code> <code>sensor_msgs/Image</code> The full size camera image <code>~/input/rois</code> <code>tier4_perception_msgs::msg::TrafficLightRoiArray</code> The array of ROIs detected by map_based_detector <code>~/expect/rois</code> <code>tier4_perception_msgs::msg::TrafficLightRoiArray</code> The array of ROIs detected by map_based_detector without any offset"},{"location":"perception/traffic_light_fine_detector/#output","title":"Output","text":"Name Type Description <code>~/output/rois</code> <code>tier4_perception_msgs::msg::TrafficLightRoiArray</code> The detected accurate rois <code>~/debug/exe_time_ms</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> The time taken for inference"},{"location":"perception/traffic_light_fine_detector/#parameters","title":"Parameters","text":""},{"location":"perception/traffic_light_fine_detector/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>fine_detector_score_thresh</code> double 0.3 If the objectness score is less than this value, the object is ignored <code>fine_detector_nms_thresh</code> double 0.65 IoU threshold to perform Non-Maximum Suppression"},{"location":"perception/traffic_light_fine_detector/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>data_path</code> string \"$(env HOME)/autoware_data\" packages data and artifacts directory path <code>fine_detector_model_path</code> string \"\" The onnx file name for yolo model <code>fine_detector_label_path</code> string \"\" The label file with label names for detected objects written on it <code>fine_detector_precision</code> string \"fp32\" The inference mode: \"fp32\", \"fp16\" <code>approximate_sync</code> bool false Flag for whether to ues approximate sync policy"},{"location":"perception/traffic_light_fine_detector/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/traffic_light_fine_detector/#reference-repositories","title":"Reference repositories","text":"<p>YOLOX github repository</p> <ul> <li>https://github.com/Megvii-BaseDetection/YOLOX</li> </ul>"},{"location":"perception/traffic_light_map_based_detector/","title":"The `traffic_light_map_based_detector` Package","text":""},{"location":"perception/traffic_light_map_based_detector/#the-traffic_light_map_based_detector-package","title":"The <code>traffic_light_map_based_detector</code> Package","text":""},{"location":"perception/traffic_light_map_based_detector/#overview","title":"Overview","text":"<p><code>traffic_light_map_based_detector</code> calculates where the traffic lights will appear in the image based on the HD map.</p> <p>Calibration and vibration errors can be entered as parameters, and the size of the detected RegionOfInterest will change according to the error.</p> <p></p> <p>If the node receives route information, it only looks at traffic lights on that route. If the node receives no route information, it looks at a radius of 200 meters and the angle between the traffic light and the camera is less than 40 degrees.</p>"},{"location":"perception/traffic_light_map_based_detector/#input-topics","title":"Input topics","text":"Name Type Description <code>~input/vector_map</code> autoware_auto_mapping_msgs::HADMapBin vector map <code>~input/camera_info</code> sensor_msgs::CameraInfo target camera parameter <code>~input/route</code> autoware_planning_msgs::LaneletRoute optional: route"},{"location":"perception/traffic_light_map_based_detector/#output-topics","title":"Output topics","text":"Name Type Description <code>~output/rois</code> tier4_perception_msgs::TrafficLightRoiArray location of traffic lights in image corresponding to the camera info <code>~expect/rois</code> tier4_perception_msgs::TrafficLightRoiArray location of traffic lights in image without any offset <code>~debug/markers</code> visualization_msgs::MarkerArray visualization to debug"},{"location":"perception/traffic_light_map_based_detector/#node-parameters","title":"Node parameters","text":"Parameter Type Description <code>max_vibration_pitch</code> double Maximum error in pitch direction. If -5~+5, it will be 10. <code>max_vibration_yaw</code> double Maximum error in yaw direction. If -5~+5, it will be 10. <code>max_vibration_height</code> double Maximum error in height direction. If -5~+5, it will be 10. <code>max_vibration_width</code> double Maximum error in width direction. If -5~+5, it will be 10. <code>max_vibration_depth</code> double Maximum error in depth direction. If -5~+5, it will be 10. <code>max_detection_range</code> double Maximum detection range in meters. Must be positive <code>min_timestamp_offset</code> double Minimum timestamp offset when searching for corresponding tf <code>max_timestamp_offset</code> double Maximum timestamp offset when searching for corresponding tf <code>timestamp_sample_len</code> double sampling length between min_timestamp_offset and max_timestamp_offset"},{"location":"perception/traffic_light_multi_camera_fusion/","title":"The `traffic_light_multi_camera_fusion` Package","text":""},{"location":"perception/traffic_light_multi_camera_fusion/#the-traffic_light_multi_camera_fusion-package","title":"The <code>traffic_light_multi_camera_fusion</code> Package","text":""},{"location":"perception/traffic_light_multi_camera_fusion/#overview","title":"Overview","text":"<p><code>traffic_light_multi_camera_fusion</code> performs traffic light signal fusion which can be summarized as the following two tasks:</p> <ol> <li>Multi-Camera-Fusion: performed on single traffic light signal detected by different cameras.</li> <li>Group-Fusion: performed on traffic light signals within the same group, which means traffic lights sharing the same regulatory element id defined in lanelet2 map.</li> </ol>"},{"location":"perception/traffic_light_multi_camera_fusion/#input-topics","title":"Input topics","text":"<p>For every camera, the following three topics are subscribed:</p> Name Type Description <code>~/&lt;camera_namespace&gt;/camera_info</code> sensor_msgs::CameraInfo camera info from traffic_light_map_based_detector <code>~/&lt;camera_namespace&gt;/rois</code> tier4_perception_msgs::TrafficLightRoiArray detection roi from traffic_light_fine_detector <code>~/&lt;camera_namespace&gt;/traffic_signals</code> tier4_perception_msgs::TrafficLightSignalArray classification result from traffic_light_classifier <p>You don't need to configure these topics manually. Just provide the <code>camera_namespaces</code> parameter and the node will automatically extract the <code>&lt;camera_namespace&gt;</code> and create the subscribers.</p>"},{"location":"perception/traffic_light_multi_camera_fusion/#output-topics","title":"Output topics","text":"Name Type Description <code>~/output/traffic_signals</code> autoware_perception_msgs::TrafficLightSignalArray traffic light signal fusion result"},{"location":"perception/traffic_light_multi_camera_fusion/#node-parameters","title":"Node parameters","text":"Parameter Type Description <code>camera_namespaces</code> vector\\ Camera Namespaces to be fused <code>message_lifespan</code> double The maximum timestamp span to be fused <code>approximate_sync</code> bool Whether work in Approximate Synchronization Mode <code>perform_group_fusion</code> bool Whether perform Group Fusion"},{"location":"perception/traffic_light_occlusion_predictor/","title":"The `traffic_light_occlusion_predictor` Package","text":""},{"location":"perception/traffic_light_occlusion_predictor/#the-traffic_light_occlusion_predictor-package","title":"The <code>traffic_light_occlusion_predictor</code> Package","text":""},{"location":"perception/traffic_light_occlusion_predictor/#overview","title":"Overview","text":"<p><code>traffic_light_occlusion_predictor</code> receives the detected traffic lights rois and calculates the occlusion ratios of each roi with point cloud.</p> <p>For each traffic light roi, hundreds of pixels would be selected and projected into the 3D space. Then from the camera point of view, the number of projected pixels that are occluded by the point cloud is counted and used for calculating the occlusion ratio for the roi. As shown in follow image, the red pixels are occluded and the occlusion ratio is the number of red pixels divided by the total pixel numbers.</p> <p></p> <p>If no point cloud is received or all point clouds have very large stamp difference with the camera image, the occlusion ratio of each roi would be set as 0.</p>"},{"location":"perception/traffic_light_occlusion_predictor/#input-topics","title":"Input topics","text":"Name Type Description <code>~input/vector_map</code> autoware_auto_mapping_msgs::HADMapBin vector map <code>~/input/rois</code> autoware_auto_perception_msgs::TrafficLightRoiArray traffic light detections <code>~input/camera_info</code> sensor_msgs::CameraInfo target camera parameter <code>~/input/cloud</code> sensor_msgs::PointCloud2 LiDAR point cloud"},{"location":"perception/traffic_light_occlusion_predictor/#output-topics","title":"Output topics","text":"Name Type Description <code>~/output/occlusion</code> autoware_auto_perception_msgs::TrafficLightOcclusionArray occlusion ratios of each roi"},{"location":"perception/traffic_light_occlusion_predictor/#node-parameters","title":"Node parameters","text":"Parameter Type Description <code>azimuth_occlusion_resolution_deg</code> double azimuth resolution of LiDAR point cloud (degree) <code>elevation_occlusion_resolution_deg</code> double elevation resolution of LiDAR point cloud (degree) <code>max_valid_pt_dist</code> double The points within this distance would be used for calculation <code>max_image_cloud_delay</code> double The maximum delay between LiDAR point cloud and camera image <code>max_wait_t</code> double The maximum time waiting for the LiDAR point cloud"},{"location":"perception/traffic_light_ssd_fine_detector/","title":"traffic_light_ssd_fine_detector","text":""},{"location":"perception/traffic_light_ssd_fine_detector/#traffic_light_ssd_fine_detector","title":"traffic_light_ssd_fine_detector","text":""},{"location":"perception/traffic_light_ssd_fine_detector/#purpose","title":"Purpose","text":"<p>It is a package for traffic light detection using MobileNetV2 and SSDLite.</p>"},{"location":"perception/traffic_light_ssd_fine_detector/#training-information","title":"Training Information","text":"<p>NOTE:</p> <ul> <li>Currently, Autoware supports SSD trained both pytorch-ssd and mmdetection.<ul> <li>Please specify either <code>pytorch</code> or <code>mmdetection</code> in <code>dnn_header_type</code>.</li> </ul> </li> <li>Note that, tensor names of your onnx model conform the following.<ul> <li>Input tensor: <code>input</code></li> <li>Output box tensor: <code>boxes</code>.</li> <li>Output score tensor: <code>scores</code>.</li> </ul> </li> </ul>"},{"location":"perception/traffic_light_ssd_fine_detector/#pretrained-model","title":"Pretrained Model","text":"<p>The model is based on pytorch-ssd and the pretrained model could be downloaded from here.</p>"},{"location":"perception/traffic_light_ssd_fine_detector/#training-data","title":"Training Data","text":"<p>The model was fine-tuned on 1750 TIER IV internal images of Japanese traffic lights.</p>"},{"location":"perception/traffic_light_ssd_fine_detector/#trained-onnx-model","title":"Trained Onnx model","text":"<ul> <li>https://drive.google.com/uc?id=1USFDPRH9JrVdGoqt27qHjRgittwc0kcO</li> </ul>"},{"location":"perception/traffic_light_ssd_fine_detector/#customization-of-cnn-model","title":"Customization of CNN model","text":"<p>In order to train models and export onnx model, we recommend open-mmlab/mmdetection. Please follow the official document to install and experiment with mmdetection. If you get into troubles, FAQ page would help you.</p> <p>The following steps are example of a quick-start.</p>"},{"location":"perception/traffic_light_ssd_fine_detector/#step-0-install-mmcv-and-mim","title":"step 0. Install MMCV and MIM","text":"<p>NOTE :</p> <ul> <li>First of all, install PyTorch suitable for your CUDA version (CUDA11.6 is supported in Autoware).</li> <li>Our tested library versions are following. If the scripts as shown below would be old, please update to be suited to your version.<ul> <li>MMCV == 1.x</li> <li>MMDetection == 2.x</li> <li>MMDeploy == 0.x</li> <li>MIM == 0.3.x</li> </ul> </li> </ul> <p>In order to install mmcv suitable for your CUDA version, install it specifying a url.</p> <pre><code># Install mim\n$ pip install -U openmim\n\n# Install mmcv on a machine with CUDA11.6 and PyTorch1.13.0\n$ pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.13/index.html\n</code></pre>"},{"location":"perception/traffic_light_ssd_fine_detector/#step-1-install-mmdetection","title":"step 1. Install MMDetection","text":"<p>You can install mmdetection as a Python package or from source.</p> <pre><code># As a Python package\n$ pip install mmdet\n\n# From source\n$ git clone https://github.com/open-mmlab/mmdetection.git\n$ cd mmdetection\n$ pip install -v -e .\n</code></pre>"},{"location":"perception/traffic_light_ssd_fine_detector/#step-2-train-your-model","title":"step 2. Train your model","text":"<p>Train model with your experiment configuration file. For the details of config file, see here.</p> <pre><code># [] is optional, you can start training from pre-trained checkpoint\n$ mim train mmdet YOUR_CONFIG.py [--resume-from YOUR_CHECKPOINT.pth]\n</code></pre>"},{"location":"perception/traffic_light_ssd_fine_detector/#step-3-export-onnx-model","title":"step 3. Export onnx model","text":"<p>In exporting onnx, use <code>mmdetection/tools/deployment/pytorch2onnx.py</code> or open-mmlab/mmdeploy. NOTE:</p> <ul> <li>Currently, autoware does not support TensorRT plugin for NMS defined by open-mmlab. Therefore, please deploy onnx model excluding NMS layer.</li> </ul> <pre><code>cd ~/mmdetection/tools/deployment\npython3 pytorch2onnx.py YOUR_CONFIG.py ...\n</code></pre>"},{"location":"perception/traffic_light_ssd_fine_detector/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Based on the camera image and the global ROI array detected by <code>map_based_detection</code> node, a CNN-based detection method enables highly accurate traffic light detection.</p>"},{"location":"perception/traffic_light_ssd_fine_detector/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/traffic_light_ssd_fine_detector/#input","title":"Input","text":"Name Type Description <code>~/input/image</code> <code>sensor_msgs/Image</code> The full size camera image <code>~/input/rois</code> <code>tier4_perception_msgs::msg::TrafficLightRoiArray</code> The array of ROIs detected by map_based_detector"},{"location":"perception/traffic_light_ssd_fine_detector/#output","title":"Output","text":"Name Type Description <code>~/output/rois</code> <code>tier4_perception_msgs::msg::TrafficLightRoiArray</code> The detected accurate rois <code>~/debug/exe_time_ms</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> The time taken for inference"},{"location":"perception/traffic_light_ssd_fine_detector/#parameters","title":"Parameters","text":""},{"location":"perception/traffic_light_ssd_fine_detector/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>score_thresh</code> double 0.7 If the objectness score is less than this value, the object is ignored <code>mean</code> std::vector [0.5,0.5,0.5] Average value of the normalized values of the image data used for training <code>std</code> std::vector [0.5,0.5,0.5] Standard deviation of the normalized values of the image data used for training"},{"location":"perception/traffic_light_ssd_fine_detector/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>data_path</code> string \"$(env HOME)/autoware_data\" packages data and artifacts directory path <code>onnx_file</code> string \"$(var data_path)/traffic_light_ssd_fine_detector/mb2-ssd-lite-tlr.onnx\" The onnx file name for yolo model <code>label_file</code> string \"$(var data_path)/traffic_light_ssd_fine_detector/voc_labels_tl.txt\" The label file with label names for detected objects written on it <code>dnn_header_type</code> string \"pytorch\" Name of DNN trained toolbox: \"pytorch\" or \"mmdetection\" <code>mode</code> string \"FP32\" The inference mode: \"FP32\", \"FP16\", \"INT8\" <code>max_batch_size</code> int 8 The size of the batch processed at one time by inference by TensorRT <code>approximate_sync</code> bool false Flag for whether to ues approximate sync policy <code>build_only</code> bool false shutdown node after TensorRT engine file is built"},{"location":"perception/traffic_light_ssd_fine_detector/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/traffic_light_ssd_fine_detector/#reference-repositories","title":"Reference repositories","text":"<p>pytorch-ssd github repository</p> <ul> <li>https://github.com/qfgaohao/pytorch-ssd</li> </ul> <p>MobileNetV2</p> <ul> <li>M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L. Chen, \"MobileNetV2: Inverted Residuals and Linear Bottlenecks,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 4510-4520, doi: 10.1109/CVPR.2018.00474.</li> </ul>"},{"location":"perception/traffic_light_visualization/","title":"traffic_light_visualization","text":""},{"location":"perception/traffic_light_visualization/#traffic_light_visualization","title":"traffic_light_visualization","text":""},{"location":"perception/traffic_light_visualization/#purpose","title":"Purpose","text":"<p>The <code>traffic_light_visualization</code> is a package that includes two visualizing nodes:</p> <ul> <li>traffic_light_map_visualizer is a node that shows traffic lights color status and position on rviz as markers.</li> <li>traffic_light_roi_visualizer is a node that draws the result of traffic light recognition nodes (traffic light status, position and classification probability) on the input image as shown in the following figure and publishes it.</li> </ul> <p></p>"},{"location":"perception/traffic_light_visualization/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/traffic_light_visualization/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/traffic_light_visualization/#traffic_light_map_visualizer","title":"traffic_light_map_visualizer","text":""},{"location":"perception/traffic_light_visualization/#input","title":"Input","text":"Name Type Description <code>~/input/tl_state</code> <code>tier4_perception_msgs::msg::TrafficSignalArray</code> status of traffic lights <code>~/input/vector_map</code> <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> vector map"},{"location":"perception/traffic_light_visualization/#output","title":"Output","text":"Name Type Description <code>~/output/traffic_light</code> <code>visualization_msgs::msg::MarkerArray</code> marker array that indicates status of traffic lights"},{"location":"perception/traffic_light_visualization/#traffic_light_roi_visualizer","title":"traffic_light_roi_visualizer","text":""},{"location":"perception/traffic_light_visualization/#input_1","title":"Input","text":"Name Type Description <code>~/input/tl_state</code> <code>tier4_perception_msgs::msg::TrafficSignalArray</code> status of traffic lights <code>~/input/image</code> <code>sensor_msgs::msg::Image</code> the image captured by perception cameras <code>~/input/rois</code> <code>tier4_perception_msgs::msg::TrafficLightRoiArray</code> the ROIs detected by <code>traffic_light_ssd_fine_detector</code> <code>~/input/rough/rois</code> (option) <code>tier4_perception_msgs::msg::TrafficLightRoiArray</code> the ROIs detected by <code>traffic_light_map_based_detector</code>"},{"location":"perception/traffic_light_visualization/#output_1","title":"Output","text":"Name Type Description <code>~/output/image</code> <code>sensor_msgs::msg::Image</code> output image with ROIs"},{"location":"perception/traffic_light_visualization/#parameters","title":"Parameters","text":""},{"location":"perception/traffic_light_visualization/#traffic_light_map_visualizer_1","title":"traffic_light_map_visualizer","text":"<p>None</p>"},{"location":"perception/traffic_light_visualization/#traffic_light_roi_visualizer_1","title":"traffic_light_roi_visualizer","text":""},{"location":"perception/traffic_light_visualization/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>enable_fine_detection</code> bool false whether to visualize result of the traffic light fine detection"},{"location":"perception/traffic_light_visualization/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/traffic_light_visualization/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/traffic_light_visualization/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/traffic_light_visualization/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/traffic_light_visualization/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"planning/","title":"Planning Components","text":""},{"location":"planning/#planning-components","title":"Planning Components","text":""},{"location":"planning/#getting-started","title":"Getting Started","text":"<p>The Autoware.Universe Planning Modules represent a cutting-edge component within the broader open-source autonomous driving software stack. These modules play a pivotal role in autonomous vehicle navigation, skillfully handling route planning, dynamic obstacle avoidance, and real-time adaptation to varied traffic conditions.</p> <ul> <li>For high level concept of Planning Components, please refer to Planning Component Design Document</li> <li>To understand how Planning Components interacts with other components, please refer to Planning Component Interface Document</li> <li>The Node Diagram illustrates the interactions, inputs, and outputs of all modules in the Autoware.Universe, including planning modules.</li> </ul>"},{"location":"planning/#planning-module","title":"Planning Module","text":"<p>The Module in the Planning Component refers to the various components that collectively form the planning system of the software. These modules cover a range of functionalities necessary for autonomous vehicle planning. Autoware's planning modules are modularized, meaning users can customize which functions are enabled by changing the configuration. This modular design allows for flexibility and adaptability to different scenarios and requirements in autonomous vehicle operations.</p>"},{"location":"planning/#how-to-enable-or-disable-planning-module","title":"How to Enable or Disable Planning Module","text":"<p>Enabling and disabling modules involves managing settings in key configuration and launch files.</p>"},{"location":"planning/#key-files-for-configuration","title":"Key Files for Configuration","text":"<p>The <code>default_preset.yaml</code> file acts as the primary configuration file, where planning modules can be disable or enabled. Furthermore, users can also set the type of motion planner across various motion planners. For example:</p> <ul> <li><code>launch_avoidance_module</code>: Set to <code>true</code> to enable the avoidance module, or <code>false</code> to disable it.</li> <li><code>motion_stop_planner_type</code>: Set <code>default</code> to either <code>obstacle_stop_planner</code> or <code>obstacle_cruise_planner</code>.</li> </ul> <p>Note</p> <p>Click here to view the <code>default_preset.yaml</code>.</p> <p>The launch files reference the settings defined in <code>default_preset.yaml</code> to apply the configurations when the behavior path planner's node is running. For instance, the parameter <code>avoidance.enable_module</code> in</p> <pre><code>&lt;param name=\"avoidance.enable_module\" value=\"$(var launch_avoidance_module)\"/&gt;\n</code></pre> <p>corresponds to launch_avoidance_module from <code>default_preset.yaml</code>.</p>"},{"location":"planning/#parameters-configuration","title":"Parameters configuration","text":"<p>There are multiple parameters available for configuration, and users have the option to modify them in here. It's important to note that not all parameters are adjustable via <code>rqt_reconfigure</code>. To ensure the changes are effective, modify the parameters and then restart Autoware. Additionally, detailed information about each parameter is available in the corresponding documents under the planning tab.</p>"},{"location":"planning/#integrating-a-custom-module-into-autoware-a-step-by-step-guide","title":"Integrating a Custom Module into Autoware: A Step-by-Step Guide","text":"<p>This guide outlines the steps for integrating your custom module into Autoware:</p> <ul> <li>Add your modules to the <code>default_preset.yaml</code> file. For example</li> </ul> <pre><code>- arg:\nname: launch_intersection_module\ndefault: \"true\"\n</code></pre> <ul> <li>Incorporate your modules into the launcher. For example in behavior_planning.launch.xml:</li> </ul> <pre><code>&lt;arg name=\"launch_intersection_module\" default=\"true\"/&gt;\n\n&lt;let\nname=\"behavior_velocity_planner_launch_modules\"\nvalue=\"$(eval &amp;quot;'$(var behavior_velocity_planner_launch_modules)' + 'behavior_velocity_planner::IntersectionModulePlugin, '&amp;quot;)\"\nif=\"$(var launch_intersection_module)\"\n/&gt;\n</code></pre> <ul> <li>If applicable, place your parameter folder within the appropriate existing parameter folder. For example intersection_module's parameters is in behavior_velocity_planner.</li> <li>Insert the path of your parameters in the tier4_planning_component.launch.xml. For example <code>behavior_velocity_planner_intersection_module_param_path</code> is used.</li> </ul> <pre><code>&lt;arg name=\"behavior_velocity_planner_intersection_module_param_path\" value=\"$(var behavior_velocity_config_path)/intersection.param.yaml\"/&gt;\n</code></pre> <ul> <li>Define your parameter path variable within the corresponding launcher. For example in behavior_planning.launch.xml</li> </ul> <pre><code>&lt;param from=\"$(var behavior_velocity_planner_intersection_module_param_path)\"/&gt;\n</code></pre> <p>Note</p> <p>Depending on the specific module you wish to add, the relevant files and steps may vary. This guide provides a general overview and serves as a starting point. It's important to adapt these instructions to the specifics of your module.</p>"},{"location":"planning/#join-our-community-driven-effort","title":"Join Our Community-Driven Effort","text":"<p>Autoware thrives on community collaboration. Every contribution, big or small, is invaluable to us. Whether it's reporting bugs, suggesting improvements, offering new ideas, or anything else you can think of \u2013 we welcome it all with open arms.</p>"},{"location":"planning/#how-to-contribute","title":"How to Contribute?","text":"<p>Ready to contribute? Great! To get started, simply visit our Contributing Guidelines where you'll find all the information you need to jump in. This includes instructions on submitting bug reports, proposing feature enhancements, and even contributing to the codebase.</p>"},{"location":"planning/#join-our-planning-control-working-group-meetings","title":"Join Our Planning &amp; Control Working Group Meetings","text":"<p>The Planning &amp; Control working group is an integral part of our community. We meet bi-weekly to discuss our current progress, upcoming challenges, and brainstorm new ideas. These meetings are a fantastic opportunity to directly contribute to our discussions and decision-making processes.</p> <p>Meeting Details:</p> <ul> <li>Frequency: Bi-weekly</li> <li>Day: Thursday</li> <li>Time: 08:00 AM UTC (05:00 PM JST)</li> <li>Agenda: Discuss current progress, plan future developments. You can view and comment on the minutes of past meetings here.</li> </ul> <p>Interested in joining our meetings? We\u2019d love to have you! For more information on how to participate, visit the following link: How to participate in the working group.</p>"},{"location":"planning/#citations","title":"Citations","text":"<p>Occasionally, we publish papers specific to the Planning Component in Autoware. We encourage you to explore these publications and find valuable insights for your work. If you find them useful and incorporate any of our methodologies or algorithms in your projects, citing our papers would be immensely helpful. This support allows us to reach a broader audience and continue contributing to the field.</p> <p>If you use the Jerk Constrained Velocity Planning algorithm in Motion Velocity Smoother module in the Planning Component, we kindly request you to cite the relevant paper.</p> <p>Y. Shimizu, T. Horibe, F. Watanabe and S. Kato, \"Jerk Constrained Velocity Planning for an Autonomous Vehicle: Linear Programming Approach,\" 2022 International Conference on Robotics and Automation (ICRA)</p> <pre><code>@inproceedings{shimizu2022,\n  author={Shimizu, Yutaka and Horibe, Takamasa and Watanabe, Fumiya and Kato, Shinpei},\n  booktitle={2022 International Conference on Robotics and Automation (ICRA)},\n  title={Jerk Constrained Velocity Planning for an Autonomous Vehicle: Linear Programming Approach},\n  year={2022},\n  pages={5814-5820},\n  doi={10.1109/ICRA46639.2022.9812155}}\n</code></pre>"},{"location":"planning/behavior_path_lane_change_module/","title":"Lane Change design","text":""},{"location":"planning/behavior_path_lane_change_module/#lane-change-design","title":"Lane Change design","text":"<p>The Lane Change module is activated when lane change is needed and can be safely executed.</p>"},{"location":"planning/behavior_path_lane_change_module/#lane-change-requirement","title":"Lane Change Requirement","text":"<ul> <li>During lane change request condition<ul> <li>The ego-vehicle isn\u2019t on a <code>preferred_lane</code>.</li> <li>There is neither intersection nor crosswalk on the path of the lane change</li> </ul> </li> <li>lane change ready condition<ul> <li>Path of the lane change does not collide with other dynamic objects (see the figure below)</li> <li>Lane change candidate path is approved by an operator.</li> </ul> </li> </ul>"},{"location":"planning/behavior_path_lane_change_module/#generating-lane-change-candidate-path","title":"Generating Lane Change Candidate Path","text":"<p>The lane change candidate path is divided into two phases: preparation and lane-changing. The following figure illustrates each phase of the lane change candidate path.</p> <p></p>"},{"location":"planning/behavior_path_lane_change_module/#preparation-phase","title":"Preparation phase","text":"<p>The preparation trajectory is the candidate path's first and the straight portion generated along the ego vehicle's current lane. The length of the preparation trajectory is computed as follows.</p> <pre><code>lane_change_prepare_distance = current_speed * lane_change_prepare_duration + 0.5 * deceleration * lane_change_prepare_duration^2\n</code></pre> <p>During the preparation phase, the turn signal will be activated when the remaining distance is equal to or less than <code>lane_change_search_distance</code>.</p>"},{"location":"planning/behavior_path_lane_change_module/#lane-changing-phase","title":"Lane-changing phase","text":"<p>The lane-changing phase consist of the shifted path that moves ego from current lane to the target lane. Total distance of lane-changing phase is as follows. Note that during the lane changing phase, the ego vehicle travels at a constant speed.</p> <pre><code>lane_change_prepare_velocity = std::max(current_speed + deceleration * lane_change_prepare_duration, minimum_lane_changing_velocity)\nlane_changing_distance = lane_change_prepare_velocity * lane_changing_duration\n</code></pre> <p>The <code>backward_length_buffer_for_end_of_lane</code> is added to allow some window for any possible delay, such as control or mechanical delay during brake lag.</p>"},{"location":"planning/behavior_path_lane_change_module/#multiple-candidate-path-samples-longitudinal-acceleration","title":"Multiple candidate path samples (longitudinal acceleration)","text":"<p>Lane change velocity is affected by the ego vehicle's current velocity. High velocity requires longer preparation and lane changing distance. However we also need to plan lane changing trajectories in case ego vehicle slows down. Computing candidate paths that assumes ego vehicle's slows down is performed by substituting predetermined deceleration value into <code>prepare_length</code>, <code>prepare_velocity</code> and <code>lane_changing_length</code> equation.</p> <p>The predetermined longitudinal acceleration values are a set of value that starts from <code>longitudinal_acceleration = maximum_longitudinal_acceleration</code>, and decrease by <code>longitudinal_acceleration_resolution</code> until it reaches <code>longitudinal_acceleration = -maximum_longitudinal_deceleration</code>. Both <code>maximum_longitudinal_acceleration</code> and <code>maximum_longitudinal_deceleration</code> are calculated as: defined in the <code>common.param</code> file as <code>normal.min_acc</code>.</p> <pre><code>maximum_longitudinal_acceleration = min(common_param.max_acc, lane_change_param.max_acc)\nmaximum_longitudinal_deceleration = max(common_param.min_acc, lane_change_param.min_acc)\n</code></pre> <p>where <code>common_param</code> is vehicle common parameter, which defines vehicle common maximum longitudinal acceleration and deceleration. Whereas, <code>lane_change_param</code> has maximum longitudinal acceleration and deceleration for the lane change module. For example, if a user set and <code>common_param.max_acc=1.0</code> and <code>lane_change_param.max_acc=0.0</code>, <code>maximum_longitudinal_acceleration</code> becomes <code>0.0</code>, and the lane change does not accelerate in the lane change phase.</p> <p>The <code>longitudinal_acceleration_resolution</code> is determine by the following</p> <pre><code>longitudinal_acceleration_resolution = (maximum_longitudinal_acceleration - minimum_longitudinal_acceleration) / longitudinal_acceleration_sampling_num\n</code></pre> <p>Note that when the <code>current_velocity</code> is lower than <code>minimum_lane_changing_velocity</code>, the vehicle needs to accelerate its velocity to <code>minimum_lane_changing_velocity</code>. Therefore, longitudinal acceleration becomes positive value (not decelerate).</p> <p>The following figure illustrates when <code>longitudinal_acceleration_sampling_num = 4</code>. Assuming that <code>maximum_deceleration = 1.0</code> then <code>a0 == 0.0 == no deceleration</code>, <code>a1 == 0.25</code>, <code>a2 == 0.5</code>, <code>a3 == 0.75</code> and <code>a4 == 1.0 == maximum_deceleration</code>. <code>a0</code> is the expected lane change trajectories should ego vehicle do not decelerate, and <code>a1</code>'s path is the expected lane change trajectories should ego vehicle decelerate at <code>0.25 m/s^2</code>.</p> <p></p> <p>Which path will be chosen will depend on validity and collision check.</p>"},{"location":"planning/behavior_path_lane_change_module/#multiple-candidate-path-samples-lateral-acceleration","title":"Multiple candidate path samples (lateral acceleration)","text":"<p>In addition to sampling longitudinal acceleration, we also sample lane change paths by adjusting the value of lateral acceleration. Since lateral acceleration influences the duration of a lane change, a lower lateral acceleration value results in a longer lane change path, while a higher lateral acceleration value leads to a shorter lane change path. This allows the lane change module to generate a shorter lane change path by increasing the lateral acceleration when there is limited space for the lane change.</p> <p>The maximum and minimum lateral accelerations are defined in the lane change parameter file as a map. The range of lateral acceleration is determined for each velocity by linearly interpolating the values in the map. Let's assume we have the following map</p> Ego Velocity Minimum lateral acceleration Maximum lateral acceleration 0.0 0.2 0.3 2.0 0.2 0.4 4.0 0.3 0.4 6.0 0.3 0.5 <p>In this case, when the current velocity of the ego vehicle is 3.0, the minimum and maximum lateral accelerations are 0.25 and 0.4 respectively. These values are obtained by linearly interpolating the second and third rows of the map, which provide the minimum and maximum lateral acceleration values.</p> <p>Within this range, we sample the lateral acceleration for the ego vehicle. Similar to the method used for sampling longitudinal acceleration, the resolution of lateral acceleration (lateral_acceleration_resolution) is determined by the following:</p> <pre><code>lateral_acceleration_resolution = (maximum_lateral_acceleration - minimum_lateral_acceleration) / lateral_acceleration_sampling_num\n</code></pre>"},{"location":"planning/behavior_path_lane_change_module/#candidate-paths-validity-check","title":"Candidate Path's validity check","text":"<p>A candidate path is valid if the total lane change distance is less than</p> <ol> <li>distance to the end of current lane</li> <li>distance to the next intersection</li> <li>distance from current pose to the goal.</li> <li>distance to the crosswalk.</li> </ol> <p>The goal must also be in the list of the preferred lane.</p> <p>The following flow chart illustrates the validity check.</p> <p></p>"},{"location":"planning/behavior_path_lane_change_module/#candidate-paths-safety-check","title":"Candidate Path's Safety check","text":"<p>See safety check utils explanation</p>"},{"location":"planning/behavior_path_lane_change_module/#objects-selection-and-classification","title":"Objects selection and classification","text":"<p>First, we divide the target objects into obstacles in the target lane, obstacles in the current lane, and obstacles in other lanes. Target lane indicates the lane that the ego vehicle is going to reach after the lane change and current lane mean the current lane where the ego vehicle is following before the lane change. Other lanes are lanes that do not belong to the target and current lanes. The following picture describes objects on each lane. Note that users can remove objects either on current and other lanes from safety check by changing the flag, which are <code>check_objects_on_current_lanes</code> and <code>check_objects_on_other_lanes</code>.</p> <p></p> <p>Furthermore, to change lanes behind a vehicle waiting at a traffic light, we skip the safety check for the stopping vehicles near the traffic light.\u3000The explanation for parked car detection is written in documentation for avoidance module.</p>"},{"location":"planning/behavior_path_lane_change_module/#collision-check-in-prepare-phase","title":"Collision check in prepare phase","text":"<p>The ego vehicle may need to secure ample inter-vehicle distance ahead of the target vehicle before attempting a lane change. The flag <code>enable_collision_check_at_prepare_phase</code> can be enabled to gain this behavior. The following image illustrates the differences between the <code>false</code> and <code>true</code> cases.</p> <p></p> <p>The parameter <code>prepare_phase_ignore_target_speed_thresh</code> can be configured to ignore the prepare phase collision check for targets whose speeds are less than a specific threshold, such as stationary or very slow-moving objects.</p>"},{"location":"planning/behavior_path_lane_change_module/#if-the-lane-is-blocked-and-multiple-lane-changes","title":"If the lane is blocked and multiple lane changes","text":"<p>When driving on the public road with other vehicles, there exist scenarios where lane changes cannot be executed. Suppose the candidate path is evaluated as unsafe, for example, due to incoming vehicles in the adjacent lane. In that case, the ego vehicle can't change lanes, and it is impossible to reach the goal. Therefore, the ego vehicle must stop earlier at a certain distance and wait for the adjacent lane to be evaluated as safe. The minimum stopping distance can be computed from shift length and minimum lane changing velocity.</p> <pre><code>lane_changing_time = f(shift_length, lat_acceleration, lat_jerk)\nminimum_lane_change_distance = minimum_prepare_length + minimum_lane_changing_velocity * lane_changing_time + lane_change_finish_judge_buffer\n</code></pre> <p>The following figure illustrates when the lane is blocked in multiple lane changes cases.</p> <p></p>"},{"location":"planning/behavior_path_lane_change_module/#stopping-position-when-an-object-exists-ahead","title":"Stopping position when an object exists ahead","text":"<p>When an obstacle is in front of the ego vehicle, stop with keeping a distance for lane change. The position to be stopped depends on the situation, such as when the lane change is blocked by the target lane obstacle, or when the lane change is not needed immediately.The following shows the division in that case.</p>"},{"location":"planning/behavior_path_lane_change_module/#when-the-ego-vehicle-is-near-the-end-of-the-lane-change","title":"When the ego vehicle is near the end of the lane change","text":"<p>Regardless of the presence or absence of objects in the lane change target lane, stop by keeping the distance necessary for lane change to the object ahead.</p> <p></p> <p></p>"},{"location":"planning/behavior_path_lane_change_module/#when-the-ego-vehicle-is-not-near-the-end-of-the-lane-change","title":"When the ego vehicle is not near the end of the lane change","text":"<p>If there are NO objects in the lane change section of the target lane, stop by keeping the distance necessary for lane change to the object ahead.</p> <p></p> <p>If there are objects in the lane change section of the target lane, stop WITHOUT keeping the distance necessary for lane change to the object ahead.</p> <p></p>"},{"location":"planning/behavior_path_lane_change_module/#when-the-target-lane-is-far-away","title":"When the target lane is far away","text":"<p>When the target lane for lane change is far away and not next to the current lane, do not keep the distance necessary for lane change to the object ahead.</p> <p></p>"},{"location":"planning/behavior_path_lane_change_module/#lane-change-when-stuck","title":"Lane Change When Stuck","text":"<p>The ego vehicle is considered stuck if it is stopped and meets any of the following conditions:</p> <ul> <li>There is an obstacle in front of the current lane</li> <li>The ego vehicle is at the end of the current lane</li> </ul> <p>In this case, the safety check for lane change is relaxed compared to normal times. Please refer to the 'stuck' section under the 'Collision checks during lane change' for more details. The function to stop by keeping a margin against forward obstacle in the previous section is being performed to achieve this feature.</p>"},{"location":"planning/behavior_path_lane_change_module/#lane-change-regulations","title":"Lane change regulations","text":"<p>If you want to regulate lane change on crosswalks or intersections, the lane change module finds a lane change path excluding it includes crosswalks or intersections. To regulate lane change on crosswalks or intersections, change <code>regulation.crosswalk</code> or <code>regulation.intersection</code> to <code>true</code>. If the ego vehicle gets stuck, to avoid stuck, it enables lane change in crosswalk/intersection. If the ego vehicle stops more than <code>stuck_detection.stop_time</code> seconds, it is regarded as a stuck. If the ego vehicle velocity is smaller than <code>stuck_detection.velocity</code>, it is regarded as stopping.</p>"},{"location":"planning/behavior_path_lane_change_module/#aborting-lane-change","title":"Aborting lane change","text":"<p>The abort process may result in three different outcome; Cancel, Abort and Stop/Cruise.</p> <p>The following depicts the flow of the abort lane change check.</p> <p></p>"},{"location":"planning/behavior_path_lane_change_module/#cancel","title":"Cancel","text":"<p>Suppose the lane change trajectory is evaluated as unsafe. In that case, if the ego vehicle has not departed from the current lane yet, the trajectory will be reset, and the ego vehicle will resume the lane following the maneuver.</p> <p>The function can be enabled by setting <code>enable_on_prepare_phase</code> to <code>true</code>.</p> <p>The following image illustrates the cancel process.</p> <p></p>"},{"location":"planning/behavior_path_lane_change_module/#abort","title":"Abort","text":"<p>Assume the ego vehicle has already departed from the current lane. In that case, it is dangerous to cancel the path, and it will cause the ego vehicle to change the heading direction abruptly. In this case, planning a trajectory that allows the ego vehicle to return to the current path while minimizing the heading changes is necessary. In this case, the lane change module will generate an abort path. The following images show an example of the abort path. Do note that the function DOESN'T GUARANTEE a safe abort process, as it didn't check the presence of the surrounding objects and/or their reactions. The function can be enable manually by setting both <code>enable_on_prepare_phase</code> and <code>enable_on_lane_changing_phase</code> to <code>true</code>. The parameter <code>max_lateral_jerk</code> need to be set to a high value in order for it to work.</p> <p></p>"},{"location":"planning/behavior_path_lane_change_module/#stopcruise","title":"Stop/Cruise","text":"<p>The last behavior will also occur if the ego vehicle has departed from the current lane. If the abort function is disabled or the abort is no longer possible, the ego vehicle will attempt to stop or transition to the obstacle cruise mode. Do note that the module DOESN'T GUARANTEE safe maneuver due to the unexpected behavior that might've occurred during these critical scenarios. The following images illustrate the situation.</p> <p></p>"},{"location":"planning/behavior_path_lane_change_module/#parameters","title":"Parameters","text":""},{"location":"planning/behavior_path_lane_change_module/#essential-lane-change-parameters","title":"Essential lane change parameters","text":"<p>The following parameters are configurable in <code>lane_change.param.yaml</code>.</p> Name Unit Type Description Default value <code>backward_lane_length</code> [m] double The backward length to check incoming objects in lane change target lane. 200.0 <code>prepare_duration</code> [m] double The preparation time for the ego vehicle to be ready to perform lane change. 4.0 <code>backward_length_buffer_for_end_of_lane</code> [m] double The end of lane buffer to ensure ego vehicle has enough distance to start lane change 3.0 <code>backward_length_buffer_for_blocking_object</code> [m] double The end of lane buffer to ensure ego vehicle has enough distance to start lane change when there is an object in front 3.0 <code>lane_change_finish_judge_buffer</code> [m] double The additional buffer used to confirm lane change process completion 3.0 <code>finish_judge_lateral_threshold</code> [m] double Lateral distance threshold to confirm lane change process completion 0.2 <code>lane_changing_lateral_jerk</code> [m/s3] double Lateral jerk value for lane change path generation 0.5 <code>minimum_lane_changing_velocity</code> [m/s] double Minimum speed during lane changing process. 2.78 <code>prediction_time_resolution</code> [s] double Time resolution for object's path interpolation and collision check. 0.5 <code>longitudinal_acceleration_sampling_num</code> [-] int Number of possible lane-changing trajectories that are being influenced by longitudinal acceleration 5 <code>lateral_acceleration_sampling_num</code> [-] int Number of possible lane-changing trajectories that are being influenced by lateral acceleration 3 <code>object_check_min_road_shoulder_width</code> [m] double Width considered as a road shoulder if the lane does not have a road shoulder 0.5 <code>object_shiftable_ratio_threshold</code> [-] double Vehicles around the center line within this distance ratio will be excluded from parking objects 0.6 <code>min_length_for_turn_signal_activation</code> [m] double Turn signal will be activated if the ego vehicle approaches to this length from minimum lane change length 10.0 <code>length_ratio_for_turn_signal_deactivation</code> [-] double Turn signal will be deactivated if the ego vehicle approaches to this length ratio for lane change finish point 0.8 <code>max_longitudinal_acc</code> [-] double maximum longitudinal acceleration for lane change 1.0 <code>min_longitudinal_acc</code> [-] double maximum longitudinal deceleration for lane change -1.0 <code>lateral_acceleration.velocity</code> [m/s] double Reference velocity for lateral acceleration calculation (look up table) [0.0, 4.0, 10.0] <code>lateral_acceleration.min_values</code> [m/ss] double Min lateral acceleration values corresponding to velocity (look up table) [0.15, 0.15, 0.15] <code>lateral_acceleration.max_values</code> [m/ss] double Max lateral acceleration values corresponding to velocity (look up table) [0.5, 0.5, 0.5] <code>target_object.car</code> [-] boolean Include car objects for safety check true <code>target_object.truck</code> [-] boolean Include truck objects for safety check true <code>target_object.bus</code> [-] boolean Include bus objects for safety check true <code>target_object.trailer</code> [-] boolean Include trailer objects for safety check true <code>target_object.unknown</code> [-] boolean Include unknown objects for safety check true <code>target_object.bicycle</code> [-] boolean Include bicycle objects for safety check true <code>target_object.motorcycle</code> [-] boolean Include motorcycle objects for safety check true <code>target_object.pedestrian</code> [-] boolean Include pedestrian objects for safety check true"},{"location":"planning/behavior_path_lane_change_module/#lane-change-regulations_1","title":"Lane change regulations","text":"Name Unit Type Description Default value <code>regulation.crosswalk</code> [-] boolean Regulate lane change on crosswalks false <code>regulation.intersection</code> [-] boolean Regulate lane change on intersections false"},{"location":"planning/behavior_path_lane_change_module/#ego-vehicle-stuck-detection","title":"Ego vehicle stuck detection","text":"Name Unit Type Description Default value <code>stuck_detection.velocity</code> [m/s] double Velocity threshold for ego vehicle stuck detection 0.1 <code>stuck_detection.stop_time</code> [s] double Stop time threshold for ego vehicle stuck detection 3.0"},{"location":"planning/behavior_path_lane_change_module/#collision-checks-during-lane-change","title":"Collision checks during lane change","text":"<p>The following parameters are configurable in <code>behavior_path_planner.param.yaml</code> and <code>lane_change.param.yaml</code>.</p>"},{"location":"planning/behavior_path_lane_change_module/#execution","title":"execution","text":"Name Unit Type Description Default value <code>safety_check.execution.lateral_distance_max_threshold</code> [m] double The lateral distance threshold that is used to determine whether lateral distance between two object is enough and whether lane change is safe. 2.0 <code>safety_check.execution.longitudinal_distance_min_threshold</code> [m] double The longitudinal distance threshold that is used to determine whether longitudinal distance between two object is enough and whether lane change is safe. 3.0 <code>safety_check.execution.expected_front_deceleration</code> [m/s^2] double The front object's maximum deceleration when the front vehicle perform sudden braking. (*1) -1.0 <code>safety_check.execution.expected_rear_deceleration</code> [m/s^2] double The rear object's maximum deceleration when the rear vehicle perform sudden braking. (*1) -1.0 <code>safety_check.execution.rear_vehicle_reaction_time</code> [s] double The reaction time of the rear vehicle driver which starts from the driver noticing the sudden braking of the front vehicle until the driver step on the brake. 2.0 <code>safety_check.execution.rear_vehicle_safety_time_margin</code> [s] double The time buffer for the rear vehicle to come into complete stop when its driver perform sudden braking. 2.0 <code>safety_check.execution.enable_collision_check_at_prepare_phase</code> [-] boolean Perform collision check starting from prepare phase. If <code>false</code>, collision check only evaluated for lane changing phase. true <code>safety_check.execution.prepare_phase_ignore_target_speed_thresh</code> [m/s] double Ignore collision check in prepare phase of object speed that is lesser that the configured value. <code>enable_collision_check_at_prepare_phase</code> must be <code>true</code> 0.1 <code>safety_check.execution.check_objects_on_current_lanes</code> [-] boolean If true, the lane change module include objects on current lanes. true <code>safety_check.execution.check_objects_on_other_lanes</code> [-] boolean If true, the lane change module include objects on other lanes. true <code>safety_check.execution.use_all_predicted_path</code> [-] boolean If false, use only the predicted path that has the maximum confidence. true"},{"location":"planning/behavior_path_lane_change_module/#cancel_1","title":"cancel","text":"Name Unit Type Description Default value <code>safety_check.cancel.lateral_distance_max_threshold</code> [m] double The lateral distance threshold that is used to determine whether lateral distance between two object is enough and whether lane change is safe. 1.5 <code>safety_check.cancel.longitudinal_distance_min_threshold</code> [m] double The longitudinal distance threshold that is used to determine whether longitudinal distance between two object is enough and whether lane change is safe. 3.0 <code>safety_check.cancel.expected_front_deceleration</code> [m/s^2] double The front object's maximum deceleration when the front vehicle perform sudden braking. (*1) -1.5 <code>safety_check.cancel.expected_rear_deceleration</code> [m/s^2] double The rear object's maximum deceleration when the rear vehicle perform sudden braking. (*1) -2.5 <code>safety_check.cancel.rear_vehicle_reaction_time</code> [s] double The reaction time of the rear vehicle driver which starts from the driver noticing the sudden braking of the front vehicle until the driver step on the brake. 2.0 <code>safety_check.cancel.rear_vehicle_safety_time_margin</code> [s] double The time buffer for the rear vehicle to come into complete stop when its driver perform sudden braking. 2.5 <code>safety_check.cancel.enable_collision_check_at_prepare_phase</code> [-] boolean Perform collision check starting from prepare phase. If <code>false</code>, collision check only evaluated for lane changing phase. false <code>safety_check.cancel.prepare_phase_ignore_target_speed_thresh</code> [m/s] double Ignore collision check in prepare phase of object speed that is lesser that the configured value. <code>enable_collision_check_at_prepare_phase</code> must be <code>true</code> 0.2 <code>safety_check.cancel.check_objects_on_current_lanes</code> [-] boolean If true, the lane change module include objects on current lanes. false <code>safety_check.cancel.check_objects_on_other_lanes</code> [-] boolean If true, the lane change module include objects on other lanes. false <code>safety_check.cancel.use_all_predicted_path</code> [-] boolean If false, use only the predicted path that has the maximum confidence. false"},{"location":"planning/behavior_path_lane_change_module/#stuck","title":"stuck","text":"Name Unit Type Description Default value <code>safety_check.stuck.lateral_distance_max_threshold</code> [m] double The lateral distance threshold that is used to determine whether lateral distance between two object is enough and whether lane change is safe. 2.0 <code>safety_check.stuck.longitudinal_distance_min_threshold</code> [m] double The longitudinal distance threshold that is used to determine whether longitudinal distance between two object is enough and whether lane change is safe. 3.0 <code>safety_check.stuck.expected_front_deceleration</code> [m/s^2] double The front object's maximum deceleration when the front vehicle perform sudden braking. (*1) -1.0 <code>safety_check.stuck.expected_rear_deceleration</code> [m/s^2] double The rear object's maximum deceleration when the rear vehicle perform sudden braking. (*1) -1.0 <code>safety_check.stuck.rear_vehicle_reaction_time</code> [s] double The reaction time of the rear vehicle driver which starts from the driver noticing the sudden braking of the front vehicle until the driver step on the brake. 2.0 <code>safety_check.stuck.rear_vehicle_safety_time_margin</code> [s] double The time buffer for the rear vehicle to come into complete stop when its driver perform sudden braking. 2.0 <code>safety_check.stuck.enable_collision_check_at_prepare_phase</code> [-] boolean Perform collision check starting from prepare phase. If <code>false</code>, collision check only evaluated for lane changing phase. true <code>safety_check.stuck.prepare_phase_ignore_target_speed_thresh</code> [m/s] double Ignore collision check in prepare phase of object speed that is lesser that the configured value. <code>enable_collision_check_at_prepare_phase</code> must be <code>true</code> 0.1 <code>safety_check.stuck.check_objects_on_current_lanes</code> [-] boolean If true, the lane change module include objects on current lanes. true <code>safety_check.stuck.check_objects_on_other_lanes</code> [-] boolean If true, the lane change module include objects on other lanes. true <code>safety_check.stuck.use_all_predicted_path</code> [-] boolean If false, use only the predicted path that has the maximum confidence. true <p>(*1) the value must be negative.</p>"},{"location":"planning/behavior_path_lane_change_module/#abort-lane-change","title":"Abort lane change","text":"<p>The following parameters are configurable in <code>lane_change.param.yaml</code>.</p> Name Unit Type Description Default value <code>cancel.enable_on_prepare_phase</code> [-] boolean Enable cancel lane change true <code>cancel.enable_on_lane_changing_phase</code> [-] boolean Enable abort lane change. false <code>cancel.delta_time</code> [s] double The time taken to start steering to return to the center line. 3.0 <code>cancel.duration</code> [s] double The time taken to complete returning to the center line. 3.0 <code>cancel.max_lateral_jerk</code> [m/sss] double The maximum lateral jerk for abort path 1000.0 <code>cancel.overhang_tolerance</code> [m] double Lane change cancel is prohibited if the vehicle head exceeds the lane boundary more than this tolerance distance 0.0"},{"location":"planning/behavior_path_lane_change_module/#debug","title":"Debug","text":"<p>The following parameters are configurable in <code>lane_change.param.yaml</code>.</p> Name Unit Type Description Default value <code>publish_debug_marker</code> [-] boolean Flag to publish debug marker false"},{"location":"planning/behavior_path_lane_change_module/#debug-marker-visualization","title":"Debug Marker &amp; Visualization","text":"<p>To enable the debug marker, execute (no restart is needed)</p> <pre><code>ros2 param set /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner lane_change.publish_debug_marker true\n</code></pre> <p>or simply set the <code>publish_debug_marker</code> to <code>true</code> in the <code>lane_change.param.yaml</code> for permanent effect (restart is needed).</p> <p>Then add the marker</p> <pre><code>/planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/debug/lane_change_left\n</code></pre> <p>in <code>rviz2</code>.</p> <p></p> <p></p> <p></p> <p>Available information</p> <ol> <li>Ego to object relation, plus safety check information</li> <li>Ego vehicle interpolated pose up to the latest safety check position.</li> <li>Object is safe or not, shown by the color of the polygon (Green = Safe, Red = unsafe)</li> <li>Valid candidate paths.</li> <li>Position when lane changing start and end.</li> </ol>"},{"location":"planning/behavior_path_planner/","title":"Behavior Path Planner","text":""},{"location":"planning/behavior_path_planner/#behavior-path-planner","title":"Behavior Path Planner","text":"<p>The Behavior Path Planner's main objective is to significantly enhance the safety of autonomous vehicles by minimizing the risk of accidents. It improves driving efficiency through time conservation and underpins reliability with its rule-based approach. Additionally, it allows users to integrate their own custom behavior modules or use it with different types of vehicles, such as cars, buses, and delivery robots, as well as in various environments, from busy urban streets to open highways.</p> <p>The module begins by thoroughly analyzing the ego vehicle's current situation, including its position, speed, and surrounding environment. This analysis leads to essential driving decisions about lane changes or stopping and subsequently generates a path that is both safe and efficient. It considers road geometry, traffic rules, and dynamic conditions while also incorporating obstacle avoidance to respond to static and dynamic obstacles such as other vehicles, pedestrians, or unexpected roadblocks, ensuring safe navigation.</p> <p>Moreover, the planner actively interacts with other traffic participants, predicting their actions and accordingly adjusting the vehicle's path. This ensures not only the safety of the autonomous vehicle but also contributes to smooth traffic flow. Its adherence to traffic laws, including speed limits and traffic signals, further guarantees lawful and predictable driving behavior. The planner is also designed to minimize sudden or abrupt maneuvers, aiming for a comfortable and natural driving experience.</p> <p>Note</p> <p>The Planning Component Design Document outlines the foundational philosophy guiding the design and future development of the Behavior Path Planner module. We strongly encourage readers to consult this document to understand the rationale behind its current configuration and the direction of its ongoing development.</p>"},{"location":"planning/behavior_path_planner/#purpose-use-cases","title":"Purpose / Use Cases","text":"<p>Essentially, the module has three primary responsibilities:</p> <ol> <li>Creating a path based on the traffic situation.</li> <li>Generating drivable area, i.e. the area within which the vehicle can maneuver.</li> <li>Generating turn signal commands to be relayed to the vehicle interface.</li> </ol>"},{"location":"planning/behavior_path_planner/#features","title":"Features","text":""},{"location":"planning/behavior_path_planner/#supported-scene-modules","title":"Supported Scene Modules","text":"<p>Behavior Path Planner has following scene modules</p> Name Description Details Lane Following this module generates reference path from lanelet centerline. LINK Avoidance this module generates avoidance path when there is objects that should be avoid. LINK Dynamic Avoidance WIP LINK Avoidance By Lane Change this module generates lane change path when there is objects that should be avoid. LINK Lane Change this module is performed when it is necessary and a collision check with other vehicles is cleared. LINK External Lane Change WIP LINK Start Planner this module is performed when ego-vehicle is in the road lane and goal is in the shoulder lane. ego-vehicle will stop at the goal. LINK Goal Planner this module is performed when ego-vehicle is stationary and footprint of ego-vehicle is included in shoulder lane. This module ends when ego-vehicle merges into the road. LINK Side Shift (for remote control) shift the path to left or right according to an external instruction. LINK <p>Note</p> <p>click on the following images to view the video of their execution</p> <p></p> <p>Note</p> <p>Users can refer to Planning component design for some additional behavior.</p>"},{"location":"planning/behavior_path_planner/#how-to-add-or-implement-new-module","title":"How to add or implement new module?","text":"<p>All scene modules are implemented by inheriting base class <code>scene_module_interface.hpp</code>.</p> <p>Warning</p> <p>The remainder of this subsection is work in progress (WIP).</p>"},{"location":"planning/behavior_path_planner/#planner-manager","title":"Planner Manager","text":"<p>The Planner Manager's responsibilities include:</p> <ol> <li>Activating the relevant scene module in response to the specific situation faced by the autonomous vehicle. For example, when a parked vehicle blocks the ego vehicle's driving lane, the manager would engage the avoidance module.</li> <li>Managing the execution order when multiple modules are running simultaneously. For instance, if both the lane-changing and avoidance modules are operational, the manager decides which should take precedence.</li> <li>Merging paths from multiple modules when they are activated simultaneously and each generates its own path, thereby creating a single functional path.</li> </ol> <p>Note</p> <p>To check the scene module's transition, i.e.: registered, approved and candidate modules, set <code>verbose: true</code> in the behavior path planner configuration file.</p> <p></p> <p>Note</p> <p>For more in-depth information, refer to Manager design document.</p>"},{"location":"planning/behavior_path_planner/#inputs-outputs-api","title":"Inputs / Outputs / API","text":""},{"location":"planning/behavior_path_planner/#input","title":"Input","text":"Name Required? Type Description ~/input/odometry \u25cb <code>nav_msgs::msg::Odometry</code> for ego velocity. ~/input/accel \u25cb <code>geometry_msgs::msg::AccelWithCovarianceStamped</code> for ego acceleration. ~/input/objects \u25cb <code>autoware_auto_perception_msgs::msg::PredictedObjects</code> dynamic objects from perception module. ~/input/occupancy_grid_map \u25cb <code>nav_msgs::msg::OccupancyGrid</code> occupancy grid map from perception module. This is used for only Goal Planner module. ~/input/traffic_signals \u25cb <code>autoware_perception_msgs::msg::TrafficSignalArray</code> traffic signals information from the perception module ~/input/vector_map \u25cb <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> vector map information. ~/input/route \u25cb <code>autoware_auto_mapping_msgs::msg::LaneletRoute</code> current route from start to goal. ~/input/scenario \u25cb <code>tier4_planning_msgs::msg::Scenario</code> Launches behavior path planner if current scenario == <code>Scenario:LaneDriving</code>. ~/input/lateral_offset \u25b3 <code>tier4_planning_msgs::msg::LateralOffset</code> lateral offset to trigger side shift ~/system/operation_mode/state \u25cb <code>autoware_adapi_v1_msgs::msg::OperationModeState</code> Allows planning module to know if vehicle is in autonomous mode or can be controlledref <ul> <li>\u25cb Mandatory: Planning Module would not work if anyone of this is not present.</li> <li>\u25b3 Optional: Some module would not work, but Planning Module can still be operated.</li> </ul>"},{"location":"planning/behavior_path_planner/#output","title":"Output","text":"Name Type Description QoS Durability ~/output/path <code>autoware_auto_planning_msgs::msg::PathWithLaneId</code> the path generated by modules. <code>volatile</code> ~/output/turn_indicators_cmd <code>autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand</code> turn indicators command. <code>volatile</code> ~/output/hazard_lights_cmd <code>autoware_auto_vehicle_msgs::msg::HazardLightsCommand</code> hazard lights command. <code>volatile</code> ~/output/modified_goal <code>autoware_planning_msgs::msg::PoseWithUuidStamped</code> output modified goal commands. <code>transient_local</code> ~/output/stop_reasons <code>tier4_planning_msgs::msg::StopReasonArray</code> describe the reason for ego vehicle stop <code>volatile</code> ~/output/reroute_availability <code>tier4_planning_msgs::msg::RerouteAvailability</code> the path the module is about to take. to be executed as soon as external approval is obtained. <code>volatile</code>"},{"location":"planning/behavior_path_planner/#debug","title":"Debug","text":"Name Type Description QoS Durability ~/debug/avoidance_debug_message_array <code>tier4_planning_msgs::msg::AvoidanceDebugMsgArray</code> debug message for avoidance. notify users reasons for avoidance path cannot be generated. <code>volatile</code> ~/debug/lane_change_debug_message_array <code>tier4_planning_msgs::msg::LaneChangeDebugMsgArray</code> debug message for lane change. notify users unsafe reason during lane changing process <code>volatile</code> ~/debug/maximum_drivable_area <code>visualization_msgs::msg::MarkerArray</code> shows maximum static drivable area. <code>volatile</code> ~/debug/turn_signal_info <code>visualization_msgs::msg::MarkerArray</code> TBA <code>volatile</code> ~/debug/bound <code>visualization_msgs::msg::MarkerArray</code> debug for static drivable area <code>volatile</code> ~/planning/path_candidate/* <code>autoware_auto_planning_msgs::msg::Path</code> the path before approval. <code>volatile</code> ~/planning/path_reference/* <code>autoware_auto_planning_msgs::msg::Path</code> reference path generated by each modules. <code>volatile</code> <p>Note</p> <p>For specific information of which topics are being subscribed and published, refer to behavior_path_planner.xml.</p>"},{"location":"planning/behavior_path_planner/#how-to-enable-or-disable-the-modules","title":"How to enable or disable the modules","text":"<p>Enabling and disabling the modules in the behavior path planner is primarily managed through two key files: <code>default_preset.yaml</code> and <code>behavior_path_planner.launch.xml</code>.</p> <p>The <code>default_preset.yaml</code> file acts as a configuration file for enabling or disabling specific modules within the planner. It contains a series of arguments which represent the behavior path planner's modules or features. For example:</p> <ul> <li><code>launch_avoidance_module</code>: Set to <code>true</code> to enable the avoidance module, or <code>false</code> to disable it.</li> <li><code>use_experimental_lane_change_function</code>: Set to <code>true</code> to enable experimental features in the lane change module.</li> </ul> <p>Note</p> <p>Click here to view the <code>default_preset.yaml</code>.</p> <p>The <code>behavior_path_planner.launch.xml</code> file references the settings defined in <code>default_preset.yaml</code> to apply the configurations when the behavior path planner's node is running. For instance, the parameter <code>avoidance.enable_module</code> in</p> <pre><code>&lt;param name=\"avoidance.enable_module\" value=\"$(var launch_avoidance_module)\"/&gt;\n</code></pre> <p>corresponds to launch_avoidance_module from <code>default_preset.yaml</code>.</p> <p>Therefore, to enable or disable a module, simply set the corresponding module in <code>default_preset.yaml</code> to <code>true</code> or <code>false</code>. These changes will be applied upon the next launch of Autoware.</p>"},{"location":"planning/behavior_path_planner/#generating-path","title":"Generating Path","text":"<p>A sophisticated methodology is used for path generation, particularly focusing on maneuvers like lane changes and avoidance. At the core of this design is the smooth lateral shifting of the reference path, achieved through a constant-jerk profile. This approach ensures a consistent rate of change in acceleration, facilitating smooth transitions and minimizing abrupt changes in lateral dynamics, crucial for passenger comfort and safety.</p> <p>The design involves complex mathematical formulations for calculating the lateral shift of the vehicle's path over time. These calculations include determining lateral displacement, velocity, and acceleration, while considering the vehicle's lateral acceleration and velocity limits. This is essential for ensuring that the vehicle's movements remain safe and manageable.</p> <p>The <code>ShiftLine</code> struct (as seen here) is utilized to represent points along the path where the lateral shift starts and ends. It includes details like the start and end points in absolute coordinates, the relative shift lengths at these points compared to the reference path, and the associated indexes on the reference path. This struct is integral to managing the path shifts, as it allows the path planner to dynamically adjust the trajectory based on the vehicle's current position and planned maneuver.</p> <p>Furthermore, the design and its implementation incorporate various equations and mathematical models to calculate essential parameters for the path shift. These include the total distance of the lateral shift, the maximum allowable lateral acceleration and jerk, and the total time required for the shift. Practical considerations are also noted, such as simplifying assumptions in the absence of a specific time interval for most lane change and avoidance cases.</p> <p>The shifted path generation logic enables the behavior path planner to dynamically generate safe and efficient paths, precisely controlling the vehicle\u2019s lateral movements to ensure the smooth execution of lane changes and avoidance maneuvers. This careful planning and execution adhere to the vehicle's dynamic capabilities and safety constraints, maximizing efficiency and safety in autonomous vehicle navigation.</p> <p>Note</p> <p>If you're a math lover, refer to Path Generation Design for the nitty-gritty.</p>"},{"location":"planning/behavior_path_planner/#collision-assessment-safety-check","title":"Collision Assessment / Safety check","text":"<p>The purpose of the collision assessment function in the Behavior Path Planner is to evaluate the potential for collisions with target objects across all modules. It is utilized in two scenarios:</p> <ol> <li>During candidate path generation, to ensure that the generated candidate path is collision-free.</li> <li>When the path is approved by the manager, and the ego vehicle is executing the current module. If the current situation is deemed unsafe, depending on each module's requirements, the planner will either cancel the execution or opt to execute another module.</li> </ol> <p>The safety check process involves several steps. Initially, it obtains the pose of the target object at a specific time, typically through interpolation of the predicted path. It then checks for any overlap between the ego vehicle and the target object at this time. If an overlap is detected, the path is deemed unsafe. The function also identifies which vehicle is in front by using the arc length along the given path. The function operates under the assumption that accurate data on the position, velocity, and shape of both the ego vehicle (the autonomous vehicle) and any target objects are available. It also relies on the yaw angle of each point in the predicted paths of these objects, which is expected to point towards the next path point.</p> <p>A critical part of the safety check is the calculation of the RSS (Responsibility-Sensitive Safety) distance-inspired algorithm. This algorithm considers factors such as reaction time, safety time margin, and the velocities and decelerations of both vehicles. Extended object polygons are created for both the ego and target vehicles. Notably, the rear object\u2019s polygon is extended by the RSS distance longitudinally and by a lateral margin. The function finally checks for overlap between this extended rear object polygon and the front object polygon. Any overlap indicates a potential unsafe situation.</p> <p>However, the module does have a limitation concerning the yaw angle of each point in the predicted paths of target objects, which may not always accurately point to the next point, leading to potential inaccuracies in some edge cases.</p> <p>Note</p> <p>For further reading on the collision assessment  method, please refer to Safety check utils</p>"},{"location":"planning/behavior_path_planner/#generating-drivable-area","title":"Generating Drivable Area","text":""},{"location":"planning/behavior_path_planner/#static-drivable-area-logic","title":"Static Drivable Area logic","text":"<p>The drivable area is used to determine the area in which the ego vehicle can travel. The primary goal of static drivable area expansion is to ensure safe travel by generating an area that encompasses only the necessary spaces for the vehicle's current behavior, while excluding non-essential areas. For example, while <code>avoidance</code> module is running, the drivable area includes additional space needed for maneuvers around obstacles, and it limits the behavior by not extending the avoidance path outside of lanelet areas.</p> <p>Static drivable area expansion operates under assumptions about the correct arrangement of lanes and the coverage of both the front and rear of the vehicle within the left and right boundaries. Key parameters for drivable area generation include extra footprint offsets for the ego vehicle, the handling of dynamic objects, maximum expansion distance, and specific methods for expansion. Additionally, since each module generates its own drivable area, before passing it as the input to generate the next running module's drivable area, or before generating a unified drivable area, the system sorts drivable lanes based on the vehicle's passage order. This ensures the correct definition of the lanes used in drivable area generation.</p> <p>Note</p> <p>Further details can is provided in Drivable Area Design.</p>"},{"location":"planning/behavior_path_planner/#dynamic-drivable-area-logic","title":"Dynamic Drivable Area Logic","text":"<p>Large vehicles require much more space, which sometimes causes them to veer out of their current lane. A typical example being a bus making a turn at a corner. In such cases, relying on a static drivable area is insufficient, since the static method depends on lane information provided by high-definition maps. To overcome the limitations of the static approach, the dynamic drivable area expansion algorithm adjusts the navigable space for an autonomous vehicle in real-time. It conserves computational power by reusing previously calculated path data, updating only when there is a significant change in the vehicle's position. The system evaluates the minimum lane width necessary to accommodate the vehicle's turning radius and other dynamic factors. It then calculates the optimal expansion of the drivable area's boundaries to ensure there is adequate space for safe maneuvering, taking into account the vehicle's path curvature. The rate at which these boundaries can expand or contract is moderated to maintain stability in the vehicle's navigation. The algorithm aims to maximize the drivable space while avoiding fixed obstacles and adhering to legal driving limits. Finally, it applies these boundary adjustments and smooths out the path curvature calculations to ensure a safe and legally compliant navigable path is maintained throughout the vehicle's operation.</p> <p>Note</p> <p>The feature can be enabled in the drivable_area_expansion.param.yaml.</p>"},{"location":"planning/behavior_path_planner/#generating-turn-signal","title":"Generating Turn Signal","text":"<p>The Behavior Path Planner module uses the <code>autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand</code> to output turn signal commands (see TurnIndicatorsCommand.idl). The system evaluates the driving context and determines when to activate turn signals based on its maneuver planning\u2014like turning, lane changing, or obstacle avoidance.</p> <p>Within this framework, the system differentiates between desired and required blinker activations. Desired activations are those recommended by traffic laws for typical driving scenarios, such as signaling before a lane change or turn. Required activations are those that are deemed mandatory for safety reasons, like signaling an abrupt lane change to avoid an obstacle.</p> <p>The <code>TurnIndicatorsCommand</code> message structure has a command field that can take one of several constants: <code>NO_COMMAND</code> indicates no signal is necessary, <code>DISABLE</code> to deactivate signals, <code>ENABLE_LEFT</code> to signal a left turn, and <code>ENABLE_RIGHT</code> to signal a right turn. The Behavior Path Planner sends these commands at the appropriate times, based on its rules-based system that considers both the desired and required scenarios for blinker activation.</p> <p>Note</p> <p>For more in-depth information, refer to Turn Signal Design document.</p>"},{"location":"planning/behavior_path_planner/#rerouting","title":"Rerouting","text":"<p>Warning</p> <p>Rerouting is a feature that was still under progress. Further information will be included on a later date.</p>"},{"location":"planning/behavior_path_planner/#parameters-and-configuration","title":"Parameters and Configuration","text":"<p>The configuration files are organized in a hierarchical directory structure for ease of navigation and management. Each subdirectory contains specific configuration files relevant to its module. The root directory holds general configuration files that apply to the overall behavior of the planner. The following is an overview of the directory structure with the respective configuration files.</p> <pre><code>behavior_path_planner\n\u251c\u2500\u2500 behavior_path_planner.param.yaml\n\u251c\u2500\u2500 drivable_area_expansion.param.yaml\n\u251c\u2500\u2500 scene_module_manager.param.yaml\n\u251c\u2500\u2500 avoidance\n\u2502   \u2514\u2500\u2500 avoidance.param.yaml\n\u251c\u2500\u2500 avoidance_by_lc\n\u2502   \u2514\u2500\u2500 avoidance_by_lc.param.yaml\n\u251c\u2500\u2500 dynamic_avoidance\n\u2502   \u2514\u2500\u2500 dynamic_avoidance.param.yaml\n\u251c\u2500\u2500 goal_planner\n\u2502   \u2514\u2500\u2500 goal_planner.param.yaml\n\u251c\u2500\u2500 lane_change\n\u2502   \u2514\u2500\u2500 lane_change.param.yaml\n\u251c\u2500\u2500 side_shift\n\u2502   \u2514\u2500\u2500 side_shift.param.yaml\n\u2514\u2500\u2500 start_planner\n    \u2514\u2500\u2500 start_planner.param.yaml\n</code></pre> <p>Similarly, the common directory contains configuration files that are used across various modules, providing shared parameters and settings essential for the functioning of the Behavior Path Planner:</p> <pre><code>common\n\u251c\u2500\u2500 common.param.yaml\n\u251c\u2500\u2500 costmap_generator.param.yaml\n\u2514\u2500\u2500 nearest_search.param.yaml\n</code></pre> <p>The preset directory contains the configurations for managing the operational state of various modules. It includes the default_preset.yaml file, which specifically caters to enabling and disabling modules within the system.</p> <pre><code>preset\n\u2514\u2500\u2500 default_preset.yaml\n</code></pre>"},{"location":"planning/behavior_path_planner/#limitations-future-work","title":"Limitations &amp; Future Work","text":"<ol> <li>Goal Planner module cannot be simultaneously executed together with other modules.</li> <li>Module is not designed as plugin. Integrating custom module is not straightforward and user have to modify some part of the behavior path planner main code.</li> </ol>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_by_lane_change_design/","title":"Avoidance by lane change design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_by_lane_change_design/#avoidance-by-lane-change-design","title":"Avoidance by lane change design","text":"<p>This is a sub-module to avoid obstacles by lane change maneuver.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_by_lane_change_design/#purpose-role","title":"Purpose / Role","text":"<p>This module is designed as one of the obstacle avoidance features and generates a lane change path if the following conditions are satisfied.</p> <ul> <li>Exist lane changeable lanelet.</li> <li>Exist avoidance target objects on ego driving lane.</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_by_lane_change_design/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Basically, this module is implemented by reusing the avoidance target filtering logic of the existing Normal Avoidance Module and the path generation logic of the Normal Lane Change Module. On the other hand, the conditions under which the module is activated differ from those of a normal avoidance module.</p> <p>Check that the following conditions are satisfied after the filtering process for the avoidance target.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_by_lane_change_design/#number-of-the-avoidance-target-objects","title":"Number of the avoidance target objects","text":"<p>This module is launched when the number of avoidance target objects on EGO DRIVING LANE is greater than <code>execute_object_num</code>. If there are no avoidance targets in the ego driving lane or their number is less than the parameter, the obstacle is avoided by normal avoidance behavior (if the normal avoidance module is registered).</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_by_lane_change_design/#lane-change-end-point-condition","title":"Lane change end point condition","text":"<p>Unlike the normal avoidance module, which specifies the shift line end point, this module does not specify its end point when generating a lane change path. On the other hand, setting <code>execute_only_when_lane_change_finish_before_object</code> to <code>true</code> will activate this module only if the lane change can be completed before the avoidance target object.</p> <p>Although setting the parameter to <code>false</code> would increase the scene of avoidance by lane change, it is assumed that sufficient lateral margin may not be ensured in some cases because the vehicle passes by the side of obstacles during the lane change.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_by_lane_change_design/#parameters","title":"Parameters","text":"Name Unit Type Description Default value execute_object_num [-] int Number of avoidance target objects on ego driving lane is greater than this value, this module will be launched. 1 execute_object_longitudinal_margin [m] double [maybe unused] Only when distance between the ego and avoidance target object is longer than this value, this module will be launched. 0.0 execute_only_when_lane_change_finish_before_object [-] bool If this flag set <code>true</code>, this module will be launched only when the lane change end point is NOT behind the avoidance target object. true"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/","title":"Avoidance design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#avoidance-design","title":"Avoidance design","text":"<p>This is a rule-based path planning module designed for obstacle avoidance.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#purpose-role","title":"Purpose / Role","text":"<p>This module is designed for rule-based avoidance that is easy for developers to design its behavior. It generates avoidance path parameterized by intuitive parameters such as lateral jerk and avoidance distance margin. This makes it possible to pre-define avoidance behavior.</p> <p>In addition, the approval interface of behavior_path_planner allows external users / modules (e.g. remote operation) to intervene the decision of the vehicle behavior.\u3000 This function is expected to be used, for example, for remote intervention in emergency situations or gathering information on operator decisions during development.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#limitations","title":"Limitations","text":"<p>This module allows developers to design vehicle behavior in avoidance planning using specific rules. Due to the property of rule-based planning, the algorithm can not compensate for not colliding with obstacles in complex cases. This is a trade-off between \"be intuitive and easy to design\" and \"be hard to tune but can handle many cases\". This module adopts the former policy and therefore this output should be checked more strictly in the later stage. In the .iv reference implementation, there is another avoidance module in motion planning module that uses optimization to handle the avoidance in complex cases. (Note that, the motion planner needs to be adjusted so that the behavior result will not be changed much in the simple case and this is a typical challenge for the behavior-motion hierarchical architecture.)</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#why-is-avoidance-in-behavior-module","title":"Why is avoidance in behavior module?","text":"<p>This module executes avoidance over lanes, and the decision requires the lane structure information to take care of traffic rules (e.g. it needs to send an indicator signal when the vehicle crosses a lane). The difference between motion and behavior module in the planning stack is whether the planner takes traffic rules into account, which is why this avoidance module exists in the behavior module.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The following figure shows a simple explanation of the logic for avoidance path generation. First, target objects are picked up, and shift requests are generated for each object. These shift requests are generated by taking into account the lateral jerk required for avoidance (red lines). Then these requests are merged and the shift points are created on the reference path (blue line). Filtering operations are performed on the shift points such as removing unnecessary shift points (yellow line), and finally a smooth avoidance path is generated by combining Clothoid-like curve primitives (green line).</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#overview-of-algorithm-for-target-object-filtering","title":"Overview of algorithm for target object filtering","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#how-to-decide-the-target-obstacles","title":"How to decide the target obstacles","text":"<p>The avoidance target should be limited to stationary objects (you should not avoid a vehicle waiting at a traffic light even if it blocks your path). Therefore, target vehicles for avoidance should meet the following specific conditions.</p> <ul> <li>It is in the vicinity of your lane (parametrized)</li> <li>It is stopped<ul> <li><code>threshold_speed_object_is_stopped</code>: parameter that be used for judge the object has stopped or not.</li> <li><code>threshold_time_object_is_moving</code>: parameter that be used for chattering prevention.</li> </ul> </li> <li>It is a specific class.<ul> <li>User can limit avoidance targets.</li> <li>Fo now, avoidance module supports only vehicle.</li> </ul> </li> <li>It is not being in the center of the route<ul> <li>This means that the vehicle is parked on the edge of the lane. This prevents the vehicle from avoiding a vehicle waiting at a traffic light in the middle of the lane. However, this is not an appropriate implementation for the purpose. Even if a vehicle is in the center of the lane, it should be avoided if it has its hazard lights on, and this is a point that should be improved in the future as the recognition performance improves.</li> </ul> </li> <li>Object is not behind ego(default: &gt; -<code>2.0 m</code>) or too far(default: &lt; <code>150.0 m</code>) and object is not behind the path goal.</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#parked-car-detection","title":"Parked-car detection","text":"<p>Not only the length from the centerline, but also the length from the road shoulder is calculated and used for the filtering process. It calculates the ratio of the actual length between the the object's center and the center line <code>shift_length</code> and the maximum length the object can shift <code>shiftable_length</code>.</p> \\[ l_D = l_a - \\frac{width}{2}, \\\\ ratio =  \\frac{l_d}{l_D} \\] <ul> <li>\\(l_d\\) : actual shift length</li> <li>\\(l_D\\) : shiftable length</li> <li>\\(l_a\\) : distance between centerline and most left boundary.</li> <li>\\(width\\) : object width</li> </ul> <p>The closer the object is to the shoulder, the larger the value of \\(ratio\\) (theoretical max value is 1.0), and it compares the value and <code>object_check_shiftable_ratio</code> to determine whether the object is a parked-car. If the road has no road shoulders, it uses <code>object_check_min_road_shoulder_width</code> as a road shoulder width virtually.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#compensation-for-detection-lost","title":"Compensation for detection lost","text":"<p>In order to prevent chattering of recognition results, once an obstacle is targeted, it is hold for a while even if it disappears. This is effective when recognition is unstable. However, since it will result in over-detection (increase a number of false-positive), it is necessary to adjust parameters according to the recognition accuracy (if <code>object_last_seen_threshold = 0.0</code>, the recognition result is 100% trusted).</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#flowchart_1","title":"Flowchart","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#overview-of-algorithm-for-avoidance-path-generation","title":"Overview of algorithm for avoidance path generation","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#how-to-prevent-shift-line-chattering-that-is-caused-by-perception-noise","title":"How to prevent shift line chattering that is caused by perception noise","text":"<p>Since object recognition results contain noise related to position ,orientation and boundary size, if the raw object recognition results are used in path generation, the avoidance path will be directly affected by the noise.</p> <p>Therefore, in order to reduce the influence of the noise, avoidance module generate a envelope polygon for the avoidance target that covers it, and the avoidance path should be generated based on that polygon. The envelope polygons are generated so that they are parallel to the reference path and the polygon size is larger than the avoidance target (define by <code>object_envelope_buffer</code>). The position and size of the polygon is not updated as long as the avoidance target exists within that polygon.</p> <pre><code># default value\nobject_envelope_buffer: 0.3 # [m]\n</code></pre> <p></p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#computing-shift-length-and-shift-points","title":"Computing Shift Length and Shift Points","text":"<p>The lateral shift length is affected by 4 variables, namely <code>lateral_collision_safety_buffer</code>, <code>lateral_collision_margin</code>, <code>vehicle_width</code> and <code>overhang_distance</code>. The equation is as follows</p> <pre><code>avoid_margin = lateral_collision_margin + lateral_collision_safety_buffer + 0.5 * vehicle_width\nmax_allowable_lateral_distance = to_road_shoulder_distance - road_shoulder_safety_margin - 0.5 * vehicle_width\nif(isOnRight(o))\n{\nshift_length = avoid_margin + overhang_distance\n}\nelse\n{\nshift_length = avoid_margin - overhang_distance\n}\n</code></pre> <p>The following figure illustrates these variables(This figure just shows the max value of lateral shift length).</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#rationale-of-having-safety-buffer-and-safety-margin","title":"Rationale of having safety buffer and safety margin","text":"<p>To compute the shift length, additional parameters that can be tune are <code>lateral_collision_safety_buffer</code> and <code>road_shoulder_safety_margin</code>.</p> <ul> <li>The <code>lateral_collision_safety_buffer</code> parameter is used to set a safety gap that will act as the final line of defense when computing avoidance path.<ul> <li>The rationale behind having this parameter is that the parameter <code>lateral_collision_margin</code> might be changing according to the situation for various reasons. Therefore, <code>lateral_collision_safety_buffer</code> will act as the final line of defense in case of the usage of <code>lateral_collision_margin</code> fails.</li> <li>It is recommended to set the value to more than half of the ego vehicle's width.</li> </ul> </li> <li>The <code>road_shoulder_safety_margin</code> will prevent the module from generating a path that might cause the vehicle to go too near the road shoulder or adjacent lane dividing line.</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#generating-path-only-within-lanelet-boundaries","title":"Generating path only within lanelet boundaries","text":"<p>The shift length is set as a constant value before the feature is implemented. Setting the shift length like this will cause the module to generate an avoidance path regardless of actual environmental properties. For example, the path might exceed the actual road boundary or go towards a wall. Therefore, to address this limitation, in addition to how to decide the target obstacle, the module also takes into account the following additional element</p> <ul> <li>The obstacles' current lane and position.</li> <li>The road shoulder with reference to the direction to avoid.</li> </ul> <p>These elements are used to compute the distance from the object to the road's shoulder (<code>to_road_shoulder_distance</code>). The parameters <code>use_adjacent_lane</code> and <code>use_opposite_lane</code> allows further configuration of the to <code>to_road_shoulder_distance</code>. The following image illustrates the configuration.</p> <p></p> <p>If one of the following conditions is <code>false</code>, then the shift point will not be generated.</p> <ul> <li>The distance to shoulder of road is enough</li> </ul> <pre><code>avoid_margin = lateral_collision_margin + lateral_collision_safety_buffer + 0.5 * vehicle_width\navoid_margin &lt;= (to_road_shoulder_distance - 0.5 * vehicle_width - road_shoulder_safety_margin)\n</code></pre> <ul> <li> <p>The obstacle intrudes into the current driving path.</p> <ul> <li> <p>when the object is on right of the path</p> <pre><code>-overhang_dist&lt;(lateral_collision_margin + lateral_collision_safety_buffer + 0.5 * vehicle_width)\n</code></pre> </li> </ul> <ul> <li> <p>when the object is on left of the path</p> <pre><code>overhang_dist&lt;(lateral_collision_margin + lateral_collision_safety_buffer + 0.5 * vehicle_width)\n</code></pre> </li> </ul> </li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#details-of-algorithm-for-avoidance-path-generation","title":"Details of algorithm for avoidance path generation","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#flow-chart-of-the-process","title":"Flow-chart of the process","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#how-to-decide-the-path-shape","title":"How to decide the path shape","text":"<p>Generate shift points for obstacles with given lateral jerk. These points are integrated to generate an avoidance path. The detailed process flow for each case corresponding to the obstacle placement are described below. The actual implementation is not separated for each case, but the function corresponding to <code>multiple obstacle case (both directions)</code> is always running.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#one-obstacle-case","title":"One obstacle case","text":"<p>The lateral shift distance to the obstacle is calculated, and then the shift point is generated from the ego vehicle speed and the given lateral jerk as shown in the figure below. A smooth avoidance path is then calculated based on the shift point.</p> <p>Additionally, the following processes are executed in special cases.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#lateral-jerk-relaxation-conditions","title":"Lateral jerk relaxation conditions","text":"<ul> <li>If the ego vehicle is close to the avoidance target, the lateral jerk will be relaxed up to the maximum jerk</li> <li>When returning to the center line after avoidance, if there is not enough distance left to the goal (end of path), the jerk condition will be relaxed as above.</li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#minimum-velocity-relaxation-conditions","title":"Minimum velocity relaxation conditions","text":"<p>There is a problem that we can not know the actual speed during avoidance in advance. This is especially critical when the ego vehicle speed is 0. To solve that, this module provides a parameter for the minimum avoidance speed, which is used for the lateral jerk calculation when the vehicle speed is low.</p> <ul> <li>If the ego vehicle speed is lower than \"nominal\" minimum speed, use the minimum speed in the calculation of the jerk.</li> <li>If the ego vehicle speed is lower than \"sharp\" minimum speed and a nominal lateral jerk is not enough for avoidance (the case where the ego vehicle is stopped close to the obstacle), use the \"sharp\" minimum speed in the calculation of the jerk (it should be lower than \"nominal\" speed).</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#multiple-obstacle-case-one-direction","title":"Multiple obstacle case (one direction)","text":"<p>Generate shift points for multiple obstacles. All of them are merged to generate new shift points along the reference path. The new points are filtered (e.g. remove small-impact shift points), and the avoidance path is computed for the filtered shift points.</p> <p>Merge process of raw shift points: check the shift length on each path points. If the shift points are overlapped, the maximum shift value is selected for the same direction.</p> <p>For the details of the shift point filtering, see filtering for shift points.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#multiple-obstacle-case-both-direction","title":"Multiple obstacle case (both direction)","text":"<p>Generate shift points for multiple obstacles. All of them are merged to generate new shift points. If there are areas where the desired shifts conflict in different directions, the sum of the maximum shift amounts of these areas is used as the final shift amount. The rest of the process is the same as in the case of one direction.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#filtering-for-shift-points","title":"Filtering for shift points","text":"<p>The shift points are modified by a filtering process in order to get the expected shape of the avoidance path. It contains the following filters.</p> <ul> <li>Quantization: Quantize the avoidance width in order to ignore small shifts.</li> <li>Small shifts removal: Shifts with small changes with respect to the previous shift point are unified in the previous shift width.</li> <li>Similar gradient removal: Connect two shift points with a straight line, and remove the shift points in between if their shift amount is in the vicinity of the straight line.</li> <li>Remove momentary returns: For shift points that reduce the avoidance width (for going back to the center line), if there is enough long distance in the longitudinal direction, remove them.</li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#other-features","title":"Other features","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#drivable-area-expansion","title":"Drivable area expansion","text":"<p>This module has following parameters that sets which areas the path may extend into when generating an avoidance path.</p> <pre><code># drivable area setting\nuse_adjacent_lane: true\nuse_opposite_lane: true\nuse_intersection_areas: false\nuse_hatched_road_markings: false\n</code></pre>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#adjacent-lane","title":"adjacent lane","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#opposite-lane","title":"opposite lane","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#intersection-areas","title":"intersection areas","text":"<p>The intersection area is defined on Lanelet map. See here</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#hatched-road-markings","title":"hatched road markings","text":"<p>The hatched road marking is defined on Lanelet map. See here</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#safety-check","title":"Safety check","text":"<p>The avoidance module has a safety check logic. The result of safe check is used for yield maneuver. It is enable by setting <code>enable</code> as <code>true</code>.</p> <pre><code># safety check configuration\nenable: true # [-]\ncheck_current_lane: false # [-]\ncheck_shift_side_lane: true # [-]\ncheck_other_side_lane: false # [-]\ncheck_unavoidable_object: false # [-]\ncheck_other_object: true # [-]\n\n# collision check parameters\ncheck_all_predicted_path: false # [-]\ntime_horizon: 10.0 # [s]\nidling_time: 1.5 # [s]\nsafety_check_backward_distance: 50.0 # [m]\nsafety_check_accel_for_rss: 2.5 # [m/ss]\n</code></pre> <p></p> <p><code>safety_check_backward_distance</code> is the parameter related to the safety check area. The module checks a collision risk for all vehicle that is within shift side lane and between object <code>object_check_forward_distance</code> ahead and <code>safety_check_backward_distance</code> behind.</p> <p></p> <p>NOTE: Even if a part of an object polygon overlaps the detection area, if the center of gravity of the object does not exist on the lane, the vehicle is excluded from the safety check target.</p> <p>Judge the risk of collision based on ego future position and object prediction path. The module calculates Ego's future position in the time horizon (<code>safety_check_time_horizon</code>), and use object's prediction path as object future position.</p> <p></p> <p>After calculating the future position of Ego and object, the module calculates the lateral/longitudinal deviation of Ego and the object. The module also calculates the lateral/longitudinal margin necessary to determine that it is safe to execute avoidance maneuver, and if both the lateral and longitudinal distances are less than the margins, it determines that there is a risk of a collision at that time.</p> <p></p> <p>The value of the longitudinal margin is calculated based on Responsibility-Sensitive Safety theory (RSS). The <code>safety_check_idling_time</code> represents \\(T_{idle}\\), and <code>safety_check_accel_for_rss</code> represents \\(a_{max}\\).</p> \\[ D_{lon} = V_{ego}T_{idle} + \\frac{1}{2}a_{max}T_{idle}^2 + \\frac{(V_{ego} + a_{max}T_{idle})^2}{2a_{max}} - \\frac{V_{obj}^2}{2a_{max}} \\] <p>The lateral margin is changeable based on ego longitudinal velocity. If the vehicle is driving at a high speed, the lateral margin should be larger, and if the vehicle is driving at a low speed, the value of the lateral margin should be set to a smaller value. Thus, the lateral margin for each vehicle speed is set as a parameter, and the module determines the lateral margin from the current vehicle speed as shown in the following figure.</p> <p></p> <pre><code>target_velocity_matrix:\ncol_size: 5\nmatrix: [2.78 5.56 ... 16.7  # target velocity [m/s]\n0.50 0.75 ... 1.50] # margin [m]\n</code></pre>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#yield-maneuver","title":"Yield maneuver","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#overview","title":"Overview","text":"<p>If an avoidance path can be generated and it is determined that avoidance maneuver should not be executed due to surrounding traffic conditions, the module executes YIELD maneuver. In yield maneuver, the vehicle slows down to the target vehicle velocity (<code>yield_velocity</code>) and keep that speed until the module judge that avoidance path is safe. If the YIELD condition goes on and the vehicle approaches the avoidance target, it stops at the avoidable position and waits until the safety is confirmed.</p> <pre><code># For yield maneuver\nyield_velocity: 2.78 # [m/s]\n</code></pre> <p></p> <p>NOTE: In yield maneuver, the vehicle decelerates target velocity under constraints.</p> <pre><code>nominal_deceleration: -1.0 # [m/ss]\nnominal_jerk: 0.5 # [m/sss]\n</code></pre> <p>If it satisfies following all of three conditions, the module inserts stop point in front of the avoidance target with an avoidable interval.</p> <ul> <li>Can't pass through the side of target object without avoidance.</li> <li>There is enough lane width to avoid target object.</li> <li>In waiting approval or yield maneuver.</li> </ul> <p>The module determines that it is NOT passable without avoidance if the object overhang is less than the threshold.</p> <pre><code>lateral_passable_collision_margin: 0.5 # [-]\n</code></pre> \\[ L_{overhang} &lt; \\frac{W}{2} + L_{margin} (not passable) \\] <p>The \\(W\\) represents vehicle width, and \\(L_{margin}\\) represents <code>lateral_passable_collision_margin</code>.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#limitation","title":"Limitation","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#limitation1","title":"Limitation1","text":"<p>The current behavior in unsafe condition is just slow down and it is so conservative. It is difficult to achieve aggressive behavior in the current architecture because of modularity. There are many modules in autoware that change the vehicle speed, and the avoidance module cannot know what speed planning they will output, so it is forced to choose a behavior that is as independent of other modules' processing as possible.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#limitation2","title":"Limitation2","text":"<p>The YIELD maneuver is executed ONLY when the vehicle has NOT initiated avoidance maneuver. The module has a threshold parameter (<code>avoidance_initiate_threshold</code>) for the amount of shifting and determines that the vehicle is initiating avoidance if the vehicle current shift exceeds the threshold.</p> \\[ SHIFT_{current} &gt; L_{threshold} \\] <p></p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#avoidance-cancelling-maneuver","title":"Avoidance cancelling maneuver","text":"<p>If <code>enable_cancel_maneuver</code> parameter is true, Avoidance Module takes different actions according to the situations as follows:</p> <ul> <li>If vehicle stops: If there is any object in the path of the vehicle, the avoidance path is generated. If this object goes away while the vehicle is stopping, the avoidance path will cancelled.</li> <li>If vehicle is in motion, but avoidance maneuver doesn't started: If there is any object in the path of the vehicle, the avoidance path is generated. If this object goes away while the vehicle is not started avoidance maneuver, the avoidance path will cancelled.</li> <li>If vehicle is in motion, avoidance maneuver started: If there is any object in the path of the vehicle, the avoidance path is generated,but if this object goes away while the vehicle is started avoidance maneuver, the avoidance path will not cancelled.</li> </ul> <p>If <code>enable_cancel_maneuver</code> parameter is false, Avoidance Module doesn't revert generated avoidance path even if path objects are gone.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#how-to-keep-the-consistency-of-the-optimize-base-path-generation-logic","title":"How to keep the consistency of the optimize-base path generation logic","text":"<p>WIP</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#parameters","title":"Parameters","text":"<p>The avoidance specific parameter configuration file can be located at <code>src/autoware/launcher/planning_launch/config/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/avoidance/avoidance.param.yaml</code>.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#general-parameters","title":"General parameters","text":"<p>namespace: <code>avoidance.</code></p> Name Unit Type Description Default value resample_interval_for_planning [m] double Path resample interval for avoidance planning path. 0.3 resample_interval_for_output [m] double Path resample interval for output path. Too short interval increases computational cost for latter modules. 4.0 detection_area_right_expand_dist [m] double Lanelet expand length for right side to find avoidance target vehicles. 0.0 detection_area_left_expand_dist [m] double Lanelet expand length for left side to find avoidance target vehicles. 1.0 enable_cancel_maneuver [-] bool Reset trajectory when avoided objects are gone. If false, shifted path points remain same even though the avoided objects are gone. false enable_yield_maneuver [-] bool Flag to enable yield maneuver. false enable_yield_maneuver_during_shifting [-] bool Flag to enable yield maneuver during shifting. false Name Unit Type Description Default value enable_bound_clipping [-] bool Enable clipping left and right bound of drivable area when obstacles are in the drivable area false use_adjacent_lane [-] bool Extend avoidance trajectory to adjacent lanes that has same direction. If false, avoidance only happen in current lane. true use_opposite_lane [-] bool Extend avoidance trajectory to opposite direction lane. <code>use_adjacent_lane</code> must be <code>true</code> to take effects true use_intersection_areas [-] bool Extend drivable to intersection area. false use_hatched_road_markings [-] bool Extend drivable to hatched road marking area. false Name Unit Type Description Default value output_debug_marker [-] bool Flag to publish debug marker (set <code>false</code> as default since it takes considerable cost). false output_debug_info [-] bool Flag to print debug info (set <code>false</code> as default since it takes considerable cost). false"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#avoidance-target-filtering-parameters","title":"Avoidance target filtering parameters","text":"<p>namespace: <code>avoidance.target_object.</code></p> <p>This module supports all object classes, and it can set following parameters independently.</p> <pre><code>car:\nis_target: true # [-]\nmoving_speed_threshold: 1.0 # [m/s]\nmoving_time_threshold: 1.0 # [s]\nmax_expand_ratio: 0.0 # [-]\nenvelope_buffer_margin: 0.3 # [m]\navoid_margin_lateral: 1.0 # [m]\nsafety_buffer_lateral: 0.7 # [m]\nsafety_buffer_longitudinal: 0.0 # [m]\n</code></pre> Name Unit Type Description Default value is_target [-] bool By setting this flag <code>true</code>, this module avoid those class objects. false moving_speed_threshold [m/s] double Objects with speed greater than this will be judged as moving ones. 1.0 moving_time_threshold [s] double Objects keep moving longer duration than this will be excluded from avoidance target. 1.0 envelope_buffer_margin [m] double The buffer between raw boundary box of detected objects and enveloped polygon that is used for avoidance path generation. 0.3 avoid_margin_lateral [m] double The lateral distance between ego and avoidance targets. 1.0 safety_buffer_lateral [m] double Creates an additional lateral gap that will prevent the vehicle from getting to near to the obstacle. 0.5 safety_buffer_longitudinal [m] double Creates an additional longitudinal gap that will prevent the vehicle from getting to near to the obstacle. 0.0 <p>Parameters for the logic to compensate perception noise of the far objects.</p> Name Unit Type Description Default value max_expand_ratio [-] double This value will be applied <code>envelope_buffer_margin</code> according to the distance between the ego and object. 0.0 lower_distance_for_polygon_expansion [-] double If the distance between the ego and object is less than this, the expand ratio will be zero. 30.0 upper_distance_for_polygon_expansion [-] double If the distance between the ego and object is larger than this, the expand ratio will be <code>max_expand_ratio</code>. 100.0 <p>namespace: <code>avoidance.target_filtering.</code></p> Name Unit Type Description Default value threshold_distance_object_is_on_center [m] double Vehicles around the center line within this distance will be excluded from avoidance target. 1.0 object_ignore_section_traffic_light_in_front_distance [m] double If the distance between traffic light and vehicle is less than this parameter, this module will ignore it. 30.0 object_ignore_section_crosswalk_in_front_distance [m] double If the front distance between crosswalk and vehicle is less than this parameter, this module will ignore it. 30.0 object_ignore_section_crosswalk_behind_distance [m] double If the back distance between crosswalk and vehicle is less than this parameter, this module will ignore it. 30.0 object_check_forward_distance [m] double Forward distance to search the avoidance target. 150.0 object_check_backward_distance [m] double Backward distance to search the avoidance target. 2.0 object_check_goal_distance [m] double Backward distance to search the avoidance target. 20.0 object_check_shiftable_ratio [m] double Vehicles around the center line within this distance will be excluded from avoidance target. 0.6 object_check_min_road_shoulder_width [m] double Width considered as a road shoulder if the lane does not have a road shoulder target. 0.5 object_last_seen_threshold [s] double For the compensation of the detection lost. The object is registered once it is observed as an avoidance target. When the detection loses, the timer will start and the object will be un-registered when the time exceeds this limit. 2.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#safety-check-parameters","title":"Safety check parameters","text":"<p>namespace: <code>avoidance.safety_check.</code></p> Name Unit Type Description Default value enable [-] bool Enable to use safety check feature. true check_current_lane [-] bool Check objects on current driving lane. false check_shift_side_lane [-] bool Check objects on shift side lane. true check_other_side_lane [-] bool Check objects on other side lane. false check_unavoidable_object [-] bool Check collision between ego and unavoidable objects. false check_other_object [-] bool Check collision between ego and non avoidance target objects. false check_all_predicted_path [-] bool Check all prediction path of safety check target objects. false time_horizon [s] double Time horizon to check lateral/longitudinal margin is enough or not. 10.0 time_resolution [s] double Time resolution to check lateral/longitudinal margin is enough or not. 0.5 safety_check_backward_distance [m] double Backward distance to search the dynamic objects. 50.0 safety_check_hysteresis_factor [-] double Hysteresis factor that be used for chattering prevention. 2.0 safety_check_ego_offset [m] double Output new avoidance path only when the offset between ego and previous output path is less than this. 1.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#avoidance-maneuver-parameters","title":"Avoidance maneuver parameters","text":"<p>namespace: <code>avoidance.avoidance.lateral.</code></p> Name Unit Type Description Default value road_shoulder_safety_margin [m] double Prevents the generated path to come too close to the road shoulders. 0.0 lateral_execution_threshold [m] double The lateral distance deviation threshold between the current path and suggested avoidance point to execute avoidance. (*2) 0.499 lateral_small_shift_threshold [m] double The shift lines whose lateral offset is less than this will be applied with other ones. 0.501 max_right_shift_length [m] double Maximum shift length for right direction 5.0 max_left_shift_length [m] double Maximum shift length for left direction 5.0 <p>namespace: <code>avoidance.avoidance.longitudinal.</code></p> Name Unit Type Description Default value prepare_time [s] double Avoidance shift starts from point ahead of this time x ego_speed to avoid sudden path change. 2.0 longitudinal_collision_safety_buffer [s] double Longitudinal collision buffer between target object and shift line. 0.0 min_prepare_distance [m] double Minimum distance for \"prepare_time\" x \"ego_speed\". 1.0 min_avoidance_distance [m] double Minimum distance of avoidance path (i.e. this distance is needed even if its lateral jerk is very low) 10.0 min_nominal_avoidance_speed [m/s] double Minimum speed for jerk calculation in a nominal situation (*1). 7.0 min_sharp_avoidance_speed [m/s] double Minimum speed for jerk calculation in a sharp situation (*1). 1.0 min_slow_down_speed [m/s] double Minimum slow speed for avoidance prepare section. 1.38 (5km/h) buf_slow_down_speed [m/s] double Buffer for controller tracking error. Basically, vehicle always cannot follow velocity profile precisely. Therefore, the module inserts lower speed than target speed that satisfies conditions to avoid object within accel/jerk constraints so that the avoidance path always can be output even if the current speed is a little bit higher than target speed. 0.57 (2.0km/h)"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#yield-maneuver-parameters","title":"Yield maneuver parameters","text":"<p>namespace: <code>avoidance.yield.</code></p> Name Unit Type Description Default value yield_velocity [m/s] double The ego will decelerate yield velocity in the yield maneuver. 2.78"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#stop-maneuver-parameters","title":"Stop maneuver parameters","text":"<p>namespace: <code>avoidance.stop.</code></p> Name Unit Type Description Default value min_distance [m] double Minimum stop distance in the situation where avoidance maneuver is not approved or in yield maneuver. 10.0 max_distance [m] double Maximum stop distance in the situation where avoidance maneuver is not approved or in yield maneuver. 20.0 stop_buffer [m] double Buffer distance in the situation where avoidance maneuver is not approved or in yield maneuver. 1.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#constraints-parameters","title":"Constraints parameters","text":"<p>namespace: <code>avoidance.constraints.</code></p> Name Unit Type Description Default value use_constraints_for_decel [-] bool Flag to decel under longitudinal constraints. <code>TRUE: allow to control breaking mildness</code> false <p>namespace: <code>avoidance.constraints.lateral.</code></p> a Name Unit Type Description Default value prepare_time [s] double Avoidance shift starts from point ahead of this time x ego_speed to avoid sudden path change. 2.0 min_prepare_distance [m] double Minimum distance for \"prepare_time\" x \"ego_speed\". 1.0 nominal_lateral_jerk [m/s3] double Avoidance path is generated with this jerk when there is enough distance from ego. 0.2 max_lateral_jerk [m/s3] double Avoidance path gets sharp up to this jerk limit when there is not enough distance from ego. 1.0 max_lateral_acceleration [m/s3] double Avoidance path gets sharp up to this accel limit when there is not enough distance from ego. 0.5 <p>namespace: <code>avoidance.constraints.longitudinal.</code></p> Name Unit Type Description Default value nominal_deceleration [m/ss] double Nominal deceleration limit. -1.0 nominal_jerk [m/sss] double Nominal jerk limit. 0.5 max_deceleration [m/ss] double Max decelerate limit. -2.0 max_jerk [m/sss] double Max jerk limit. 1.0 max_acceleration [m/ss] double Maximum acceleration during avoidance. 1.0 <p>(*2) If there are multiple vehicles in a row to be avoided, no new avoidance path will be generated unless their lateral margin difference exceeds this value.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":"<ul> <li>Planning on the intersection<ul> <li>If it is known that the ego vehicle is going to stop in the middle of avoidance execution (for example, at a red traffic light), sometimes the avoidance should not be executed until the vehicle is ready to move. This is because it is impossible to predict how the environment will change during the stop.\u3000 This is especially important at intersections.</li> </ul> </li> </ul> <ul> <li> <p>Safety Check</p> <ul> <li>In the current implementation, it is only the jerk limit that permits the avoidance execution. It is needed to consider the collision with other vehicles when change the path shape.</li> </ul> </li> </ul> <ul> <li> <p>Consideration of the speed of the avoidance target</p> <ul> <li>In the current implementation, only stopped vehicle is targeted as an avoidance target. It is needed to support the overtaking function for low-speed vehicles, such as a bicycle. (It is actually possible to overtake the low-speed objects by changing the parameter, but the logic is not supported and thus the safety cannot be guaranteed.)</li> <li>The overtaking (e.g., to overtake a vehicle running in front at 20 km/h at 40 km/h) may need to be handled outside the avoidance module. It should be discussed which module should handle it.</li> </ul> </li> </ul> <ul> <li> <p>Cancel avoidance when target disappears</p> <ul> <li>In the current implementation, even if the avoidance target disappears, the avoidance path will remain. If there is no longer a need to avoid, it must be canceled.</li> </ul> </li> </ul> <ul> <li> <p>Improved performance of avoidance target selection</p> <ul> <li>Essentially, avoidance targets are judged based on whether they are static objects or not. For example, a vehicle waiting at a traffic light should not be avoided because we know that it will start moving in the future. However this decision cannot be made in the current Autoware due to the lack of the perception functions. Therefore, the current avoidance module limits the avoidance target to vehicles parked on the shoulder of the road, and executes avoidance only for vehicles that are stopped away from the center of the lane. However, this logic cannot avoid a vehicle that has broken down and is stopped in the center of the lane, which should be recognized as a static object by the perception module. There is room for improvement in the performance of this decision.</li> </ul> </li> </ul> <ul> <li>Resampling path<ul> <li>Now the rough resolution resampling is processed to the output path in order to reduce the computational cost for the later modules. This resolution is set to a uniformly large value \u3000(e.g. <code>5m</code>), but small resolution should be applied for complex paths.</li> </ul> </li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#how-to-debug","title":"How to debug","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#publishing-visualization-marker","title":"Publishing Visualization Marker","text":"<p>Developers can see what is going on in each process by visualizing all the avoidance planning process outputs. The example includes target vehicles, shift points for each object, shift points after each filtering process, etc.</p> <p></p> <p>To enable the debug marker, execute <code>ros2 param set /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner avoidance.publish_debug_marker true</code> (no restart is needed) or simply set the <code>publish_debug_marker</code> to <code>true</code> in the <code>avoidance.param.yaml</code> for permanent effect (restart is needed). Then add the marker <code>/planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/debug/avoidance</code> in <code>rviz2</code>.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_avoidance_design/#echoing-debug-message-to-find-out-why-the-objects-were-ignored","title":"Echoing debug message to find out why the objects were ignored","text":"<p>If for some reason, no shift point is generated for your object, you can check for the failure reason via <code>ros2 topic echo</code>.</p> <p></p> <p>To print the debug message, just run the following</p> <pre><code>ros2 topic echo /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/debug/avoidance_debug_message_array\n</code></pre>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/","title":"Drivable Area design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/#drivable-area-design","title":"Drivable Area design","text":"<p>Drivable Area represents the area where ego vehicle can pass.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/#purpose-role","title":"Purpose / Role","text":"<p>In order to defined the area that ego vehicle can travel safely, we generate drivable area in behavior path planner module. Our drivable area is represented by two line strings, which are <code>left_bound</code> line and <code>right_bound</code> line respectively. Both <code>left_bound</code> and <code>right_bound</code> are created from left and right boundaries of lanelets. Note that <code>left_bound</code> and <code>right bound</code> are generated by <code>generateDrivableArea</code> function.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/#assumption","title":"Assumption","text":"<p>Our drivable area has several assumptions.</p> <ul> <li>Drivable Area should have all of the necessary area but should not represent unnecessary area for current behaviors. For example, when ego vehicle is in <code>follow lane</code> mode, drivable area should not contain adjacent lanes.</li> </ul> <ul> <li>When generating a drivable area, lanes need to be arranged in the order in which cars pass by (More details can be found in following sections).</li> </ul> <ul> <li>Both left and right bounds should cover the front of the path and the end of the path.</li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/#limitations","title":"Limitations","text":"<p>Currently, when clipping left bound or right bound, it can clip the bound more than necessary and the generated path might be conservative.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/#parameters-for-drivable-area-generation","title":"Parameters for drivable area generation","text":"Name Unit Type Description Default value enabled [-] boolean whether to dynamically the drivable area using the ego footprint false ego.extra_footprint_offset.front [m] double extra length to add to the front of the ego footprint 0.0 ego.extra_footprint_offset.rear [m] double extra length to add to the rear of the ego footprint 0.0 ego.extra_footprint_offset.left [m] double extra length to add to the left of the ego footprint 0.0 ego.extra_footprint_offset.right [m] double extra length to add to the rear of the ego footprint 0.0 dynamic_objects.avoid [-] boolean if true, the drivable area is not expanded in the predicted path of dynamic objects true dynamic_objects.extra_footprint_offset.front [m] double extra length to add to the front of the ego footprint 0.5 dynamic_objects.extra_footprint_offset.rear [m] double extra length to add to the rear of the ego footprint 0.5 dynamic_objects.extra_footprint_offset.left [m] double extra length to add to the left of the ego footprint 0.5 dynamic_objects.extra_footprint_offset.right [m] double extra length to add to the rear of the ego footprint 0.5 max_distance [m] double maximum distance by which the drivable area can be expended. A value of 0.0 means no maximum distance. 0.0 expansion.method [-] string method to use for the expansion: \"polygon\" will expand the drivable area around the ego footprint polygons; \"lanelet\" will expand to the whole lanelets overlapped by the ego footprints \"polygon\" expansion.max_arc_path_length [m] double maximum length along the path where the ego footprint is projected 50.0 expansion.extra_arc_length [m] double extra arc length (relative to the path) around ego footprints where the drivable area is expanded 0.5 expansion.avoid_linestring.types [-] string array linestring types in the lanelet maps that will not be crossed when expanding the drivable area [guard_rail, road_border] avoid_linestring.distance [m] double distance to keep between the drivable area and the linestrings to avoid 0.0 avoid_linestring.compensate.enable [-] bool if true, when the expansion is blocked by a linestring on one side of the path, we try to compensate and expand on the other side true avoid_linestring.compensate.extra_distance [m] double extra distance added to the expansion when compensating 3.0 <p>The following parameters are defined for each module. Please refer to the <code>config/drivable_area_expansion.yaml</code> file.</p> Name Unit Type Description Default value drivable_area_right_bound_offset [m] double right offset length to expand drivable area 5.0 drivable_area_left_bound_offset [m] double left offset length to expand drivable area 5.0 drivable_area_types_to_skip [-] string linestring types (as defined in the lanelet map) that will not be expanded road_border"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This section gives details of the generation of the drivable area (<code>left_bound</code> and <code>right_bound</code>).</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/#drivable-lanes-generation","title":"Drivable Lanes Generation","text":"<p>Before generating drivable areas, drivable lanes need to be sorted. Drivable Lanes are selected in each module (<code>Lane Follow</code>, <code>Avoidance</code>, <code>Lane Change</code>, <code>Goal Planner</code>, <code>Pull Out</code> and etc.), so more details about selection of drivable lanes can be found in each module's document. We use the following structure to define the drivable lanes.</p> <pre><code>struct DrivalbleLanes\n{\nlanelet::ConstLanelet right_lanelet; // right most lane\nlanelet::ConstLanelet left_lanelet; // left most lane\nlanelet::ConstLanelets middle_lanelets; // middle lanes\n};\n</code></pre> <p>The image of the sorted drivable lanes is depicted in the following picture.</p> <p></p> <p>Note that, the order of drivable lanes become</p> <pre><code>drivable_lanes = {DrivableLane1, DrivableLanes2, DrivableLanes3, DrivableLanes4, DrivableLanes5}\n</code></pre>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/#drivable-area-generation","title":"Drivable Area Generation","text":"<p>In this section, a drivable area is created using drivable lanes arranged in the order in which vehicles pass by. We created <code>left_bound</code> from left boundary of the leftmost lanelet and <code>right_bound</code> from right boundary of the rightmost lanelet. The image of the created drivable area will be the following blue lines. Note that the drivable area is defined in the <code>Path</code> and <code>PathWithLaneId</code> messages as</p> <pre><code>std::vector&lt;geometry_msgs::msg::Point&gt; left_bound;\nstd::vector&lt;geometry_msgs::msg::Point&gt; right_bound;\n</code></pre> <p>and each point of right bound and left bound has a position in the absolute coordinate system.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/#drivable-area-expansion","title":"Drivable Area Expansion","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/#static-expansion","title":"Static Expansion","text":"<p>Each module can statically expand the left and right bounds of the target lanes by the parameter defined values. This enables large vehicles to pass narrow curve. The image of this process can be described as</p> <p></p> <p>Note that we only expand right bound of the rightmost lane and left bound of the leftmost lane.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/#dynamic-expansion","title":"Dynamic Expansion","text":"<p>The drivable area can also be expanded dynamically by considering the ego vehicle footprint projected on each path point. This expansion can be summarized with the following steps:</p> <ol> <li>Build the ego path footprint.</li> <li>Build the dynamic objects' predicted footprints (optional).</li> <li>Build \"uncrossable\" lines.</li> <li>Remove the footprints from step 2 and the lines from step 3 from the ego path footprint from step 1.</li> <li>Expand the drivable area with the result of step 4.</li> </ol> Inputs Footprints and uncrossable lines Expanded drivable area <p>Please note that the dynamic expansion can only increase the size of the drivable area and cannot remove any part from the original drivable area.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/#visualizing-maximum-drivable-area-debug","title":"Visualizing maximum drivable area (Debug)","text":"<p>Sometimes, the developers might get a different result between two maps that may look identical during visual inspection.</p> <p>For example, in the same area, one can perform avoidance and another cannot. This might be related to the maximum drivable area issues due to the non-compliance vector map design from the user.</p> <p>To debug the issue, the maximum drivable area boundary can be visualized.</p> <p></p> <p></p> <p>The maximum drivable area can be visualize by adding the marker from <code>/planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/maximum_drivable_area</code></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_drivable_area_design/#expansion-with-hatched-road-markings-area","title":"Expansion with hatched road markings area","text":"<p>If the hatched road markings area is defined in the lanelet map, the area can be used as a drivable area. Since the area is expressed as a polygon format of Lanelet2, several steps are required for correct expansion.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_dynamic_avoidance_design/","title":"Dynamic avoidance design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_dynamic_avoidance_design/#dynamic-avoidance-design","title":"Dynamic avoidance design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_dynamic_avoidance_design/#purpose-role","title":"Purpose / Role","text":"<p>This is a module designed for avoiding obstacles which are running. Static obstacles such as parked vehicles are dealt with by the avoidance module.</p> <p>This module is under development. In the current implementation, the dynamic obstacles to avoid is extracted from the drivable area. Then the motion planner, in detail obstacle_avoidance_planner, will generate an avoiding trajectory.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_dynamic_avoidance_design/#overview-of-drivable-area-modification","title":"Overview of drivable area modification","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_dynamic_avoidance_design/#filtering-obstacles-to-avoid","title":"Filtering obstacles to avoid","text":"<p>The dynamics obstacles meeting the following condition will be avoided.</p> <ul> <li>The type is designated as <code>target_object.*</code>.</li> <li>The norm of the obstacle's velocity projected to the ego's path is smaller than <code>target_object.min_obstacle_vel</code>.</li> <li>The obstacle is in the next lane to the ego's lane, which will not cut-into the ego's lane according to the highest-prioritized predicted path.</li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_dynamic_avoidance_design/#drivable-area-modification","title":"Drivable area modification","text":"<p>To realize dynamic obstacles for avoidance, the time dimension should be take into an account considering the dynamics. However, it will make the planning problem much harder to solve. Therefore, we project the time dimension to the 2D pose dimension.</p> <p>Currently, the predicted paths of predicted objects are not so stable. Therefore, instead of using the predicted paths, we assume that the obstacle will run parallel to the ego's path.</p> <p>First, a maximum lateral offset to avoid is calculated as follows. The polygon's width to extract from the drivable area is the obstacle width and double <code>drivable_area_generation.lat_offset_from_obstacle</code>. We can limit the lateral shift offset by <code>drivable_area_generation.max_lat_offset_to_avoid</code>.</p> <p></p> <p>Then, extracting the same directional and opposite directional obstacles from the drivable area will work as follows considering TTC (time to collision). Regarding the same directional obstacles, obstacles whose TTC is negative will be ignored (e.g. The obstacle is in front of the ego, and the obstacle's velocity is larger than the ego's velocity.).</p> <p>Same directional obstacles </p> <p>Opposite directional obstacles </p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_dynamic_avoidance_design/#parameters","title":"Parameters","text":"Name Unit Type Description Default value target_object.car [-] bool The flag whether to avoid cars or not true target_object.truck [-] bool The flag whether to avoid trucks or not true ... [-] bool ... ... target_object.min_obstacle_vel [m/s] double Minimum obstacle velocity to avoid 1.0 drivable_area_generation.lat_offset_from_obstacle [m] double Lateral offset to avoid from obstacles 0.8 drivable_area_generation.max_lat_offset_to_avoid [m] double Maximum lateral offset to avoid 0.5 drivable_area_generation.overtaking_object.max_time_to_collision [s] double Maximum value when calculating time to collision 3.0 drivable_area_generation.overtaking_object.start_duration_to_avoid [s] double Duration to consider avoidance before passing by obstacles 4.0 drivable_area_generation.overtaking_object.end_duration_to_avoid [s] double Duration to consider avoidance after passing by obstacles 5.0 drivable_area_generation.overtaking_object.duration_to_hold_avoidance [s] double Duration to hold avoidance after passing by obstacles 3.0 drivable_area_generation.oncoming_object.max_time_to_collision [s] double Maximum value when calculating time to collision 3.0 drivable_area_generation.oncoming_object.start_duration_to_avoid [s] double Duration to consider avoidance before passing by obstacles 9.0 drivable_area_generation.oncoming_object.end_duration_to_avoid [s] double Duration to consider avoidance after passing by obstacles 0.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/","title":"Goal Planner design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#goal-planner-design","title":"Goal Planner design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#purpose-role","title":"Purpose / Role","text":"<p>Plan path around the goal.</p> <ul> <li>Park at the designated goal.</li> <li>Modify the goal to avoid obstacles or to pull over at the side of tha lane.</li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#design","title":"Design","text":"<p>If goal modification is not allowed, park at the designated fixed goal. (<code>fixed_goal_planner</code> in the figure below) When allowed, park in accordance with the specified policy(e.g pull over on left/right side of the lane). (<code>rough_goal_planner</code> in the figure below). Currently rough goal planner only support pull_over feature, but it would be desirable to be able to accommodate various parking policies in the future.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#start-condition","title":"start condition","text":"<p>Either one is activated when all conditions are met.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#fixed_goal_planner","title":"fixed_goal_planner","text":"<ul> <li>Route is set with <code>allow_goal_modification=false</code> by default.</li> <li>ego-vehicle is in the same lane as the goal.</li> </ul> <p>If the target path contains a goal, modify the points of the path so that the path and the goal are connected smoothly. This process will change the shape of the path by the distance of <code>refine_goal_search_radius_range</code> from the goal. Note that this logic depends on the interpolation algorithm that will be executed in a later module (at the moment it uses spline interpolation), so it needs to be updated in the future.</p> <p></p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#rough_goal_planner","title":"rough_goal_planner","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#pull-over-on-road-lane","title":"pull over on road lane","text":"<ul> <li>The distance between the goal and ego-vehicle is shorter than <code>pull_over_minimum_request_length</code>.</li> <li>Route is set with <code>allow_goal_modification=true</code> .<ul> <li>We can set this option with SetRoute api service.</li> <li>We support <code>2D Rough Goal Pose</code> with the key bind <code>r</code> in RViz, but in the future there will be a panel of tools to manipulate various Route API from RViz.</li> </ul> </li> <li>ego-vehicle is in the same lane as the goal.</li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#pull-over-on-shoulder-lane","title":"pull over on shoulder lane","text":"<ul> <li>The distance between the goal and ego-vehicle is shorter than <code>pull_over_minimum_request_length</code>.</li> <li>Goal is set in the <code>road_shoulder</code>.</li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#finish-condition","title":"finish condition","text":"<ul> <li>The distance to the goal from your vehicle is lower than threshold (default: &lt; <code>1m</code>).</li> <li>The ego-vehicle is stopped.<ul> <li>The speed is lower than threshold (default: &lt; <code>0.01m/s</code>).</li> </ul> </li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#general-parameters-for-goal_planner","title":"General parameters for goal_planner","text":"Name Unit Type Description Default value th_arrived_distance [m] double distance threshold for arrival of path termination 1.0 th_stopped_velocity [m/s] double velocity threshold for arrival of path termination 0.01 th_stopped_time [s] double time threshold for arrival of path termination 2.0 center_line_path_interval [m] double reference center line path point interval 1.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#collision-check","title":"collision check","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#occupancy-grid-based-collision-check","title":"occupancy grid based collision check","text":"<p>Generate footprints from ego-vehicle path points and determine obstacle collision from the value of occupancy_grid of the corresponding cell.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#parameters-for-occupancy-grid-based-collision-check","title":"Parameters for occupancy grid based collision check","text":"Name Unit Type Description Default value use_occupancy_grid_for_goal_search [-] bool flag whether to use occupancy grid for goal search collision check true use_occupancy_grid_for_goal_longitudinal_margin [-] bool flag whether to use occupancy grid for keeping longitudinal margin false use_occupancy_grid_for_path_collision_check [-] bool flag whether to use occupancy grid for collision check false occupancy_grid_collision_check_margin [m] double margin to calculate ego-vehicle cells from footprint. 0.0 theta_size [-] int size of theta angle to be considered. angular resolution for collision check will be 2\\(\\pi\\) / theta_size [rad]. 360 obstacle_threshold [-] int threshold of cell values to be considered as obstacles 60"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#object-recognition-based-collision-check","title":"object recognition based collision check","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#parameters-for-object-recognition-based-collision-check","title":"Parameters for object recognition based collision check","text":"Name Unit Type Description Default value use_object_recognition [-] bool flag whether to use object recognition for collision check true object_recognition_collision_check_margin [m] double margin to calculate ego-vehicle cells from footprint. 0.6 object_recognition_collision_check_max_extra_stopping_margin [m] double maximum value when adding longitudinal distance margin for collision check considering stopping distance 1.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#goal-search","title":"Goal Search","text":"<p>If it is not possible to park safely at a given goal, <code>/planning/scenario_planning/modified_goal</code> is searched for in certain range of the shoulder lane.</p> <p>goal search video</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#parameters-for-goal-search","title":"Parameters for goal search","text":"Name Unit Type Description Default value goal_priority [-] string In case <code>minimum_weighted_distance</code>, sort with smaller longitudinal distances taking precedence over smaller lateral distances. In case <code>minimum_longitudinal_distance</code>, sort with weighted lateral distance against longitudinal distance. <code>minimum_weighted_distance</code> prioritize_goals_before_objects [-] bool If there are objects that may need to be avoided, prioritize the goal in front of them true forward_goal_search_length [m] double length of forward range to be explored from the original goal 20.0 backward_goal_search_length [m] double length of backward range to be explored from the original goal 20.0 goal_search_interval [m] double distance interval for goal search 2.0 longitudinal_margin [m] double margin between ego-vehicle at the goal position and obstacles 3.0 max_lateral_offset [m] double maximum offset of goal search in the lateral direction 0.5 lateral_offset_interval [m] double distance interval of goal search in the lateral direction 0.25 ignore_distance_from_lane_start [m] double distance from start of pull over lanes for ignoring goal candidates 0.0 ignore_distance_from_lane_start [m] double distance from start of pull over lanes for ignoring goal candidates 0.0 margin_from_boundary [m] double distance margin from edge of the shoulder lane 0.5"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#pull-over","title":"Pull Over","text":"<p>There are three path generation methods. The path is generated with a certain margin (default: <code>0.5 m</code>) from the boundary of shoulder lane.</p> Name Unit Type Description Default value pull_over_minimum_request_length [m] double when the ego-vehicle approaches the goal by this distance or a safe distance to stop, pull over is activated. 100.0 pull_over_velocity [m/s] double decelerate to this speed by the goal search area 3.0 pull_over_minimum_velocity [m/s] double speed of pull_over after stopping once. this prevents excessive acceleration. 1.38 decide_path_distance [m] double decide path if it approaches this distance relative to the parking position. after that, no path planning and goal search are performed 10.0 maximum_deceleration [m/s2] double maximum deceleration. it prevents sudden deceleration when a parking path cannot be found suddenly 1.0 path_priority [-] string In case <code>efficient_path</code> use a goal that can generate an efficient path which is set in <code>efficient_path_order</code>. In case <code>close_goal</code> use the closest goal to the original one. efficient_path efficient_path_order [-] string efficient order of pull over planner along lanes\u3000excluding freespace pull over [\"SHIFT\", \"ARC_FORWARD\", \"ARC_BACKWARD\"]"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#shift-parking","title":"shift parking","text":"<p>Pull over distance is calculated by the speed, lateral deviation, and the lateral jerk. The lateral jerk is searched for among the predetermined minimum and maximum values, and the one satisfies ready conditions described above is output.</p> <ol> <li>Apply uniform offset to centerline of shoulder lane for ensuring margin</li> <li>In the section between merge start and end, path is shifted by a method that is used to generate avoidance path (four segmental constant jerk polynomials)</li> <li>Combine this path with center line of road lane</li> </ol> <p></p> <p>shift_parking video</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#parameters-for-shift-parking","title":"Parameters for shift parking","text":"Name Unit Type Description Default value enable_shift_parking [-] bool flag whether to enable shift parking true shift_sampling_num [-] int Number of samplings in the minimum to maximum range of lateral_jerk 4 maximum_lateral_jerk [m/s3] double maximum lateral jerk 2.0 minimum_lateral_jerk [m/s3] double minimum lateral jerk 0.5 deceleration_interval [m] double distance of deceleration section 15.0 after_shift_straight_distance [m] double straight line distance after pull over end point 1.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#geometric-parallel-parking","title":"geometric parallel parking","text":"<p>Generate two arc paths with discontinuous curvature. It stops twice in the middle of the path to control the steer on the spot. There are two path generation methods: forward and backward. See also [1] for details of the algorithm. There is also a simple python implementation.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#parameters-geometric-parallel-parking","title":"Parameters geometric parallel parking","text":"Name Unit Type Description Default value arc_path_interval [m] double interval between arc path points 1.0 pull_over_max_steer_rad [rad] double maximum steer angle for path generation. it may not be possible to control steer up to max_steer_angle in vehicle_info when stopped 0.35"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#arc-forward-parking","title":"arc forward parking","text":"<p>Generate two forward arc paths.</p> <p></p> <p>arc_forward_parking video</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#parameters-arc-forward-parking","title":"Parameters arc forward parking","text":"Name Unit Type Description Default value enable_arc_forward_parking [-] bool flag whether to enable arc forward parking true after_forward_parking_straight_distance [m] double straight line distance after pull over end point 2.0 forward_parking_velocity [m/s] double velocity when forward parking 1.38 forward_parking_lane_departure_margin [m/s] double lane departure margin for front left corner of ego-vehicle when forward parking 0.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#arc-backward-parking","title":"arc backward parking","text":"<p>Generate two backward arc paths.</p> <p>.</p> <p>arc_backward_parking video</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#parameters-arc-backward-parking","title":"Parameters arc backward parking","text":"Name Unit Type Description Default value enable_arc_backward_parking [-] bool flag whether to enable arc backward parking true after_backward_parking_straight_distance [m] double straight line distance after pull over end point 2.0 backward_parking_velocity [m/s] double velocity when backward parking -1.38 backward_parking_lane_departure_margin [m/s] double lane departure margin for front right corner of ego-vehicle when backward 0.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#freespace-parking","title":"freespace parking","text":"<p>If the vehicle gets stuck with <code>lane_parking</code>, run <code>freespace_parking</code>. To run this feature, you need to set <code>parking_lot</code> to the map, <code>activate_by_scenario</code> of costmap_generator to <code>false</code> and <code>enable_freespace_parking</code> to <code>true</code></p> <p></p> <p>Simultaneous execution with <code>avoidance_module</code> in the flowchart is under development.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#unimplemented-parts-limitations-for-freespace-parking","title":"Unimplemented parts / limitations for freespace parking","text":"<ul> <li>When a short path is generated, the ego does can not drive with it.</li> <li>Complex cases take longer to generate or fail.</li> <li>The drivable area is not guaranteed to fit in the parking_lot.</li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_goal_planner_design/#parameters-freespace-parking","title":"Parameters freespace parking","text":"Name Unit Type Description Default value enable_freespace_parking [-] bool This flag enables freespace parking, which runs when the vehicle is stuck due to e.g. obstacles in the parking area. true <p>See freespace_planner for other parameters.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_interface_design/","title":"Interface design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_interface_design/#interface-design","title":"Interface design","text":"<p>Warning</p> <p>Under Construction</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_limitations/","title":"Limitations","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_limitations/#limitations","title":"Limitations","text":"<p>The document describes the limitations that are currently present in the <code>behavior_path_planner</code> module.</p> <p>The following items (but not limited to) fall in the scope of limitation:</p> <ul> <li>limitations due to the third-party API design and requirement</li> <li>limitations due to any shortcoming out of the developer's control.</li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_limitations/#limitation-multiple-connected-opposite-lanes-require-linestring-with-shared-id","title":"Limitation: Multiple connected opposite lanes require Linestring with shared ID","text":"<p>To fully utilize the <code>Lanelet2</code>'s API, the design of the vector map (<code>.osm</code>) needs to follow all the criteria described in <code>Lanelet2</code> documentation. Specifically, in the case of 2 or more lanes, the Linestrings that divide the current lane with the opposite/adjacent lane need to have a matching <code>Linestring ID</code>. Assume the following ideal case.</p> <p></p> <p>In the image, <code>Linestring ID51</code> is shared by <code>Lanelet A</code> and <code>Lanelet B</code>. Hence we can directly use the available <code>left</code>, <code>adjacentLeft</code>, <code>right</code>, <code>adjacentRight</code> and <code>findUsages</code> method within <code>Lanelet2</code>'s API to directly query the direction and opposite lane availability.</p> <pre><code>const auto right_lane = routing_graph_ptr_-&gt;right(lanelet);\nconst auto adjacent_right_lane = routing_graph_ptr_-&gt;adjacentRight(lanelet);\nconst auto opposite_right_lane = lanelet_map_ptr_-&gt;laneletLayer.findUsages(lanelet.rightBound().invert());\n</code></pre> <p>The following images show the situation where these API does not work directly. This means that we cannot use them straight away, and several assumptions and logical instruction are needed to make these APIs work.</p> <p></p> <p>In this example (multiple linestring issues), <code>Lanelet C</code> contains <code>Linestring ID61</code> and <code>ID62</code>, while <code>Lanelet D</code> contains <code>Linestring ID63</code> and <code>ID 64</code>. Although the <code>Linestring ID62</code> and <code>ID64</code> have identical point IDs and seem visually connected, the API will treat these Linestring as though they are separated. When it searches for any <code>Lanelet</code> that is connected via <code>Linestring ID62</code>, it will return <code>NULL</code>, since <code>ID62</code> only connects to <code>Lanelet C</code> and not other <code>Lanelet</code>.</p> <p>Although, in this case, it is possible to forcefully search the lanelet availability by checking the lanelet that contains the points, using<code>getLaneletFromPoint</code> method. But, the implementation requires complex rules for it to work. Take the following images as an example.</p> <p></p> <p>Assume <code>Object X</code> is in <code>Lanelet F</code>. We can forcefully search <code>Lanelet E</code> via <code>Point 7</code>, and it will work if <code>Point 7</code> is utilized by only 2 lanelet. However, the complexity increases when we want to start searching for the direction of the opposite lane. We can infer the direction of the lanelet by using mathematical operations (dot product of vector <code>V_ID72</code> (<code>Point 6</code> minus <code>Point 9</code>), and <code>V_ID74</code> (<code>Point 7</code> minus <code>Point 8</code>). But, notice that we did not use Point 7 in V_ID72. This is because searching it requires an iteration, adding additional non-beneficial computation.</p> <p>Suppose the points are used by more than 2 lanelets. In that case, we have to find the differences for all lanelet, and the result might be undefined. The reason is that the differences between the coordinates do not reflect the actual shape of the lanelet. The following image demonstrates this point.</p> <p></p> <p></p> <p>There are many other available solutions to try. However, further attempt to solve this might cause issues in the future, especially for maintaining or scaling up the software.</p> <p>In conclusion, the multiple Linestring issues will not be supported. Covering these scenarios might give the user an \"everything is possible\" impression. This is dangerous since any attempt to create a non-standardized vector map is not compliant with safety regulations.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_limitations/#limitation-avoidance-at-corners-and-intersections","title":"Limitation: Avoidance at Corners and Intersections","text":"<p>Currently, the implementation doesn't cover avoidance at corners and intersections. The reason is similar to here. However, this case can still be supported in the future (assuming the vector map is defined correctly).</p> <p></p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_limitations/#limitation-chattering-shifts","title":"Limitation: Chattering shifts","text":"<p>There are possibilities that the shifted path chatters as a result of various factors. For example, bounded box shape or position from the perception input. Sometimes, it is difficult for the perception to get complete information about the object's size. As the object size is updated, the object length will also be updated. This might cause shifts point to be re-calculated, therefore resulting in chattering shift points.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/","title":"Manager design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#manager-design","title":"Manager design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#purpose-role","title":"Purpose / Role","text":"<p>The manager launches and executes scene modules in <code>behavior_path_planner</code> depending on the use case, and has been developed to achieve following features:</p> <ul> <li>Multiple modules can run simultaneously in series in order to achieve more complex use cases. For example, as shown in the following video, this manager make it possible to avoid a parked vehicle during lane change maneuver.</li> <li>Flexible development by not relying on framework from external libraries.</li> </ul> <p></p> <p>Movie</p> <p>Support status:</p> Name Simple exclusive execution Advanced simultaneous execution Avoidance Avoidance By Lane Change Lane Change External Lane Change Goal Planner (without goal modification) Goal Planner (with goal modification) Pull Out Side Shift <p>Click here for supported scene modules.</p> <p>Warning</p> <p>It is still under development and some functions may be unstable.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#overview","title":"Overview","text":"<p>The manager is the core part of the <code>behavior_path_planner</code> implementation. It outputs path based on the latest data.</p> <p>The manager has sub-managers for each scene module, and its main task is</p> <ul> <li>set latest planner data to scene modules via sub-managers.</li> <li>check scene module's request status via sub-managers.</li> <li>launch scene modules that make execution request.</li> <li>execute launched modules.</li> <li>delete scene expired modules.</li> </ul> <p>Additionally, the manager generates root reference path, and if any other modules don't request execution, the path is used as the planning result of <code>behavior_path_planner</code>.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#sub-managers","title":"Sub-managers","text":"<p>The sub-manager's main task is</p> <ul> <li>store the launched modules in internal vectors <code>registered_modules_</code>.</li> <li>create scene module instance.</li> <li>pass scene module's instance to the manager.</li> <li>delete expired scene module instance from <code>registered_modules_</code>.</li> <li>publish debug markers.</li> </ul> <p> </p> sub-managers <p>Sub-manager is registered on the manager with the following function.</p> <pre><code>/**\n * @brief register managers.\n * @param manager pointer.\n */\nvoid registerSceneModuleManager(const SceneModuleManagerPtr &amp; manager_ptr)\n{\nRCLCPP_INFO(logger_, \"register %s module\", manager_ptr-&gt;getModuleName().c_str());\nmanager_ptrs_.push_back(manager_ptr);\nprocessing_time_.emplace(manager_ptr-&gt;getModuleName(), 0.0);\n}\n</code></pre> <p>Code is here</p> <p>Sub-manager has the following parameters that are needed by the manager to manage the launched modules, and these parameters can be set for each module.</p> <pre><code>struct ModuleConfigParameters\n{\nbool enable_module{false};\nbool enable_rtc{false};\nbool enable_simultaneous_execution_as_approved_module{false};\nbool enable_simultaneous_execution_as_candidate_module{false};\nuint8_t priority{0};\nuint8_t max_module_size{0};\n};\n</code></pre> <p>Code is here</p> Name Type Description <code>enable_module</code> bool if true, the sub-manager is registered on the manager. <code>enable_rtc</code> bool if true, the scene modules should be approved by (request to cooperate)rtc function. if false, the module can be run without approval from rtc. <code>enable_simultaneous_execution_as_candidate_module</code> bool if true, the manager allows its scene modules to run with other scene modules as candidate module. <code>enable_simultaneous_execution_as_approved_module</code> bool if true, the manager allows its scene modules to run with other scene modules as approved module. <code>priority</code> uint8_t the manager decides execution priority based on this parameter. The smaller the number is, the higher the priority is. <code>max_module_size</code> uint8_t the sub-manager can run some modules simultaneously. this parameter set the maximum number of the launched modules."},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#scene-modules","title":"Scene modules","text":"<p>Scene modules receives necessary data and RTC command, and outputs candidate path(s), reference path and RTC cooperate status. When multiple modules run in series, the output of the previous module is received as input and the information is used to generate a new modified path, as shown in the following figure. And, when one module is running alone, it receives a reference path generated from the centerline of the lane in which Ego is currently driving as previous module output.</p> <p> </p> scene module I/O Type Description IN <code>behavior_path_planner::BehaviorModuleOutput</code> previous module output. contains data necessary for path planning. IN <code>behavior_path_planner::PlannerData</code> contains data necessary for path planning. IN <code>tier4_planning_msgs::srv::CooperateCommands</code> contains approval data for scene module's path modification. (details) OUT <code>behavior_path_planner::BehaviorModuleOutput</code> contains modified path, turn signal information, etc... OUT <code>tier4_planning_msgs::msg::CooperateStatus</code> contains RTC cooperate status. (details) OUT <code>autoware_auto_planning_msgs::msg::Path</code> candidate path output by a module that has not received approval for path change. when it approved, the ego's following path is switched to this path. (just for visualization) OUT <code>autoware_auto_planning_msgs::msg::Path</code> reference path generated from the centerline of the lane the ego is going to follow. (just for visualization) OUT <code>visualization_msgs::msg::MarkerArray</code> virtual wall, debug info, etc... <p>Scene modules running on the manager are stored on the candidate modules stack or approved modules stack depending on the condition whether the path modification has been approved or not.</p> Stack Approval condition Description candidate modules Not approved The candidate modules whose modified path has not been approved by RTC is stored in vector <code>candidate_module_ptrs_</code> in the manager. The candidate modules stack is updated in the following order. 1. The manager selects only those modules that can be executed based on the configuration of the sub-manager whose scene module requests execution. 2. Determines the execution priority. 3. Executes them as candidate module. All of these modules receive the decided (approved) path from approved modules stack and RUN in PARALLEL.  approved modules Already approved When the path modification is approved via RTC commands, the manager moves the candidate module to approved modules stack. These modules are stored in <code>approved_module_ptrs_</code>. In this stack, all scene modules RUN in SERIES."},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#process-flow","title":"Process flow","text":"<p>There are 6 steps in one process:</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#step1","title":"Step1","text":"<p>At first, the manager set latest planner data, and run all approved modules and get output path. At this time, the manager checks module status and removes expired modules from approved modules stack.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#step2","title":"Step2","text":"<p>Input approved modules output and necessary data to all registered modules, and the modules judge the necessity of path modification based on it. The manager checks which module makes execution request.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#step3","title":"Step3","text":"<p>Check request module existence.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#step4","title":"Step4","text":"<p>The manager decides which module to execute as candidate modules from the modules that requested to execute path modification.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#step5","title":"Step5","text":"<p>Decides the priority order of execution among candidate modules. And, run all candidate modules. Each modules outputs reference path and RTC cooperate status.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#step6","title":"Step6","text":"<p>Move approved module to approved modules stack from candidate modules stack.</p> <p></p> <p>and, within a single planning cycle, these steps are repeated until the following conditions are satisfied.</p> <ul> <li>Any modules don't make a request of path modification. (Check in Step3)</li> <li>Any candidate modules' request are not approved. (Check in Step5)</li> </ul> <p></p> <pre><code>  while (rclcpp::ok()) {\n/**\n     * STEP1: get approved modules' output\n     */\nconst auto approved_modules_output = runApprovedModules(data);\n\n/**\n     * STEP2: check modules that need to be launched\n     */\nconst auto request_modules = getRequestModules(approved_modules_output);\n\n/**\n     * STEP3: if there is no module that need to be launched, return approved modules' output\n     */\nif (request_modules.empty()) {\nprocessing_time_.at(\"total_time\") = stop_watch_.toc(\"total_time\", true);\nreturn approved_modules_output;\n}\n\n/**\n     * STEP4: if there is module that should be launched, execute the module\n     */\nconst auto [highest_priority_module, candidate_modules_output] =\nrunRequestModules(request_modules, data, approved_modules_output);\nif (!highest_priority_module) {\nprocessing_time_.at(\"total_time\") = stop_watch_.toc(\"total_time\", true);\nreturn approved_modules_output;\n}\n\n/**\n     * STEP5: if the candidate module's modification is NOT approved yet, return the result.\n     * NOTE: the result is output of the candidate module, but the output path don't contains path\n     * shape modification that needs approval. On the other hand, it could include velocity profile\n     * modification.\n     */\nif (highest_priority_module-&gt;isWaitingApproval()) {\nprocessing_time_.at(\"total_time\") = stop_watch_.toc(\"total_time\", true);\nreturn candidate_modules_output;\n}\n\n/**\n     * STEP6: if the candidate module is approved, push the module into approved_module_ptrs_\n     */\naddApprovedModule(highest_priority_module);\nclearCandidateModules();\n}\n</code></pre> <p>Code is here</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#priority-of-execution-request","title":"Priority of execution request","text":"<p>Compare priorities parameter among sub-managers to determine the order of execution based on config. Therefore, the priority between sub-modules does NOT change at runtime.</p> <pre><code>  /**\n   * @brief swap the modules order based on it's priority.\n   * @param modules.\n   * @details for now, the priority is decided in config file and doesn't change runtime.\n   */\nvoid sortByPriority(std::vector&lt;SceneModulePtr&gt; &amp; modules) const\n{\n// TODO(someone) enhance this priority decision method.\nstd::sort(modules.begin(), modules.end(), [this](auto a, auto b) {\nreturn getManager(a)-&gt;getPriority() &lt; getManager(b)-&gt;getPriority();\n});\n}\n</code></pre> <p>Code is here</p> <p>In the future, however, we are considering having the priorities change dynamically depending on the situation in order to achieve more complex use cases.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#how-to-decide-which-request-modules-to-run","title":"How to decide which request modules to run?","text":"<p>On this manager, it is possible that multiple scene modules may request path modification at same time. In that case, the modules to be executed as candidate module is determined in the following order.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#step1_1","title":"Step1","text":"<p>Push back the modules that make a request to <code>request_modules</code>.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#step2_1","title":"Step2","text":"<p>Check approved modules stack, and remove non-executable modules from<code>request_modules</code> based on the following condition.</p> <ul> <li>Condition A. approved module stack is empty.</li> <li>Condition B. all modules in approved modules stack support simultaneous execution as approved module (<code>enable_simultaneous_execution_as_approved_module</code> is <code>true</code>).</li> <li>Condition C. the request module supports simultaneous execution as approved module.</li> </ul> <p></p> <p>Executable or not:</p> Condition A Condition B Condition C Executable as candidate modules? YES - YES YES YES - NO YES NO YES YES YES NO YES NO NO NO NO YES NO NO NO NO NO <p>If a module that doesn't support simultaneous execution exists in approved modules stack (NOT satisfy Condition B), no more modules can be added to the stack, and therefore none of the modules can be executed as candidate.</p> <p>For example, if approved module's setting of <code>enable_simultaneous_execution_as_approved_module</code> is ENABLE, then only modules whose the setting is ENABLE proceed to the next step.</p> <p></p> <p>Other examples:</p> Process Description If approved modules stack is empty, then all request modules proceed to the next step, regardless of the setting of <code>enable_simultaneous_execution_as_approved_module</code>. If approved module's setting of <code>enable_simultaneous_execution_as_approved_module</code> is DISABLE, then all request modules are discarded."},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#step3_1","title":"Step3","text":"<p>Sort <code>request_modules</code> by priority.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#step4_1","title":"Step4","text":"<p>Check and pick up executable modules as candidate in order of priority based on the following conditions.</p> <ul> <li>Condition A. candidate module stack is empty.</li> <li>Condition B. all modules in candidate modules stack support simultaneous execution as candidate module (<code>enable_simultaneous_execution_as_candidate_module</code> is <code>true</code>).</li> <li>Condition C. the request module supports simultaneous execution as candidate module.</li> </ul> <p></p> <p>Executable or not:</p> Condition A Condition B Condition C Executable as candidate modules? YES - YES YES YES - NO YES NO YES YES YES NO YES NO NO NO NO YES NO NO NO NO NO <p>For example, if the highest priority module's setting of <code>enable_simultaneous_execution_as_candidate_module</code> is DISABLE, then all modules after the second priority are discarded.</p> <p></p> <p>Other examples:</p> Process Description If a module with a higher priority exists, lower priority modules whose setting of <code>enable_simultaneous_execution_as_candidate_module</code> is DISABLE are discarded. If all modules' setting of <code>enable_simultaneous_execution_as_candidate_module</code> is ENABLE, then all modules proceed to the next step."},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#step5_1","title":"Step5","text":"<p>Run all candidate modules.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#how-to-decide-which-modules-output-to-use","title":"How to decide which module's output to use?","text":"<p>Sometimes, multiple candidate modules are running simultaneously.</p> <p></p> <p>In this case, the manager selects a candidate modules which output path is used as <code>behavior_path_planner</code> output by approval condition in the following rules.</p> <ul> <li>Rule A. Regardless of the priority in the sub-manager (<code>priority</code>), approved modules always have a higher priority than unapproved modules.</li> <li>Rule B. If the approval status is the same, sort according to the sub-manager's priority.</li> </ul> Module A's approval condition Module A's priority Module B's approval condition Module B's priority Final priority Approved 1 Approved 99 Module A &gt; Module B Approved 1 Not approved 99 Module A &gt; Module B Not approved 1 Approved 99 Module B &gt; Module A Not approved 1 Not approved 99 Module A &gt; Module B <p>Note</p> <p>The smaller the number is, the higher the priority is.</p> <p> </p> module priority <p></p> <p>Additionally, the manager moves the highest priority module to approved modules stack if it is already approved.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#scene-module-unregister-process","title":"Scene module unregister process","text":"<p>The manager removes expired module in approved modules stack based on the module's status.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#waiting-approval-modules","title":"Waiting approval modules","text":"<p>If one module requests multiple path changes, the module may be back to waiting approval condition again. In this case, the manager moves the module to candidate modules stack. If there are some modules that was pushed back to approved modules stack later than the waiting approved module, it is also removed from approved modules stack.</p> <p>This is because module C is planning output path with the output of module B as input, and if module B is removed from approved modules stack and the input of module C changes, the output path of module C may also change greatly, and the output path will be unstable.</p> <p>As a result, the module A's output is used as approved modules stack.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#failure-modules","title":"Failure modules","text":"<p>The failure modules return the status <code>ModuleStatus::FAILURE</code>. The manager removes the module from approved modules stack as well as waiting approval modules, but the failure module is not moved to candidate modules stack.</p> <p>As a result, the module A's output is used as approved modules stack.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#succeeded-modules","title":"Succeeded modules","text":"<p>The succeeded modules return the status <code>ModuleStatus::SUCCESS</code>. The manager removes those modules based on Last In First Out policy. In other words, if a module added later to approved modules stack is still running (is in <code>ModuleStatus::RUNNING</code>), the manager doesn't remove the succeeded module. The reason for this is the same as in removal for waiting approval modules, and is to prevent sudden changes of the running module's output.</p> <p></p> <p></p> <p>As an exception, if Lane Change module returns status <code>ModuleStatus::SUCCESS</code>, the manager doesn't remove any modules until all modules is in status <code>ModuleStatus::SUCCESS</code>. This is because when the manager removes the Lane Change (normal LC, external LC, avoidance by LC) module as succeeded module, the manager updates the information of the lane Ego is currently driving in, so root reference path (= module A's input path) changes significantly at that moment.</p> <p></p> <p></p> <p>When the manager removes succeeded modules, the last added module's output is used as approved modules stack.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#reference-path-generation","title":"Reference path generation","text":"<p>The root reference path is generated from the centerline of the lanelet sequence that obtained from the root lanelet, and it is not only used as an input to the first added module of approved modules stack, but also used as the output of <code>behavior_path_planner</code> if none of the modules are running.</p> <p> </p> root reference path generation <p>The root lanelet is the closest lanelet within the route, and the update timing is based on Ego's operation mode state.</p> <ul> <li>the state is <code>OperationModeState::AUTONOMOUS</code>: Update only when the ego moves to right or left lane by lane change module.</li> <li>the state is NOT <code>OperationModeState::AUTONOMOUS</code>: Update at the beginning of every planning cycle.</li> </ul> <p></p> <p>The manager needs to know the ego behavior and then generate a root reference path from the lanes that Ego should follow.</p> <p>For example, during autonomous driving, even if Ego moves into the next lane in order to avoid a parked vehicle, the target lanes that Ego should follow will NOT change because Ego will return to the original lane after the avoidance maneuver. Therefore, the manager does NOT update root lanelet even if the avoidance maneuver is finished.</p> <p></p> <p>On the other hand, if the lane change is successful, the manager updates root lanelet because the lane that Ego should follow changes.</p> <p></p> <p>In addition, while manual driving, the manager always updates root lanelet because the pilot may move to an adjacent lane regardless of the decision of the autonomous driving system.</p> <pre><code>  /**\n   * @brief get reference path from root_lanelet_ centerline.\n   * @param planner data.\n   * @return reference path.\n   */\nBehaviorModuleOutput getReferencePath(const std::shared_ptr&lt;PlannerData&gt; &amp; data) const\n{\nconst auto &amp; route_handler = data-&gt;route_handler;\nconst auto &amp; pose = data-&gt;self_odometry-&gt;pose.pose;\nconst auto p = data-&gt;parameters;\n\nconstexpr double extra_margin = 10.0;\nconst auto backward_length =\nstd::max(p.backward_path_length, p.backward_path_length + extra_margin);\n\nconst auto lanelet_sequence = route_handler-&gt;getLaneletSequence(\nroot_lanelet_.value(), pose, backward_length, std::numeric_limits&lt;double&gt;::max());\n\nlanelet::ConstLanelet closest_lane{};\nif (lanelet::utils::query::getClosestLaneletWithConstrains(\nlanelet_sequence, pose, &amp;closest_lane, p.ego_nearest_dist_threshold,\np.ego_nearest_yaw_threshold)) {\nreturn utils::getReferencePath(closest_lane, data);\n}\n\nif (lanelet::utils::query::getClosestLanelet(lanelet_sequence, pose, &amp;closest_lane)) {\nreturn utils::getReferencePath(closest_lane, data);\n}\n\nreturn {};  // something wrong.\n}\n</code></pre> <p>Code is here</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#drivable-area-generation","title":"Drivable area generation","text":"<p>Warning</p> <p>Under Construction</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_manager_design/#turn-signal-management","title":"Turn signal management","text":"<p>Warning</p> <p>Under Construction</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_path_generation_design/","title":"Path Generation design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_path_generation_design/#path-generation-design","title":"Path Generation design","text":"<p>This document explains how the path is generated for lane change and avoidance, etc. The implementation can be found in path_shifter.hpp.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_path_generation_design/#overview","title":"Overview","text":"<p>The base idea of the path generation in lane change and avoidance is to smoothly shift the reference path, such as the center line, in the lateral direction. This is achieved by using a constant-jerk profile as in the figure below. More details on how it is used can be found in README. It is assumed that the reference path is smooth enough for this algorithm.</p> <p>The figure below explains how the application of a constant lateral jerk \\(l^{'''}(s)\\) can be used to induce lateral shifting. In order to comply with the limits on lateral acceleration and velocity, zero-jerk time is employed in the figure ( \\(T_a\\) and \\(T_v\\) ). In each interval where constant jerk is applied, the shift position \\(l(s)\\) can be characterized by a third-degree polynomial. Therefore the shift length from the reference path can then be calculated by combining spline curves.</p> <p></p> <p>Note that, due to the rarity of the \\(T_v\\) in almost all cases of lane change and avoidance, \\(T_v\\) is not considered in the current implementation.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_path_generation_design/#mathematical-derivation","title":"Mathematical Derivation","text":"<p>With initial longitudinal velocity \\(v_0^{\\rm lon}\\) and longitudinal acceleration \\(a^{\\rm lon}\\), longitudinal position \\(s(t)\\) and longitudinal velocity at each time \\(v^{\\rm lon}(t)\\) can be derived as:</p> \\[ \\begin{align} s_1&amp;= v^{\\rm lon}_0 T_j + \\frac{1}{2} a^{\\rm lon} T_j^2 \\\\ v_1&amp;= v^{\\rm lon}_0 + a^{\\rm lon} T_j \\\\ s_2&amp;= v^{\\rm lon}_1 T_a + \\frac{1}{2} a^{\\rm lon} T_a^2 \\\\ v_2&amp;= v^{\\rm lon}_1 + a^{\\rm lon} T_a \\\\ s_3&amp;= v^{\\rm lon}_2 T_j + \\frac{1}{2} a^{\\rm lon} T_j^2 \\\\ v_3&amp;= v^{\\rm lon}_2 + a^{\\rm lon} T_j \\\\ s_4&amp;= v^{\\rm lon}_3 T_v + \\frac{1}{2} a^{\\rm lon} T_v^2 \\\\ v_4&amp;= v^{\\rm lon}_3 + a^{\\rm lon} T_v \\\\ s_5&amp;= v^{\\rm lon}_4 T_j + \\frac{1}{2} a^{\\rm lon} T_j^2 \\\\ v_5&amp;= v^{\\rm lon}_4 + a^{\\rm lon} T_j \\\\ s_6&amp;= v^{\\rm lon}_5 T_a + \\frac{1}{2} a^{\\rm lon} T_a^2 \\\\ v_6&amp;= v^{\\rm lon}_5 + a^{\\rm lon} T_a \\\\ s_7&amp;= v^{\\rm lon}_6 T_j + \\frac{1}{2} a^{\\rm lon} T_j^2 \\\\ v_7&amp;= v^{\\rm lon}_6 + a^{\\rm lon} T_j \\end{align} \\] <p>By applying simple integral operations, the following analytical equations can be derived to describe the shift distance \\(l(t)\\) at each time under lateral jerk, lateral acceleration, and velocity constraints.</p> \\[ \\begin{align} l_1&amp;= \\frac{1}{6}jT_j^3\\\\[10pt] l_2&amp;= \\frac{1}{6}j T_j^3 + \\frac{1}{2} j T_a T_j^2 + \\frac{1}{2} j T_a^2 T_j\\\\[10pt] l_3&amp;= j  T_j^3 + \\frac{3}{2} j T_a T_j^2 + \\frac{1}{2} j T_a^2 T_j\\\\[10pt] l_4&amp;= j T_j^3 + \\frac{3}{2} j T_a T_j^2 + \\frac{1}{2} j T_a^2 T_j + j(T_a + T_j)T_j T_v\\\\[10pt] l_5&amp;= \\frac{11}{6} j T_j^3 + \\frac{5}{2} j T_a T_j^2 + \\frac{1}{2} j T_a^2 T_j + j(T_a + T_j)T_j T_v \\\\[10pt] l_6&amp;= \\frac{11}{6} j T_j^3 + 3 j T_a T_j^2 + j T_a^2 T_j + j(T_a + T_j)T_j T_v\\\\[10pt] l_7&amp;= 2 j T_j^3 + 3 j T_a T_j^2 + j T_a^2 T_j + j(T_a + T_j)T_j T_v \\end{align} \\] <p>These equations are used to determine the shape of a path. Additionally, by applying further mathematical operations to these basic equations, the expressions of the following subsections can be derived.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_path_generation_design/#calculation-of-maximum-acceleration-from-transition-time-and-final-shift-length","title":"Calculation of Maximum Acceleration from transition time and final shift length","text":"<p>In the case where there are no limitations on lateral velocity and lateral acceleration, the maximum lateral acceleration during the shifting can be calculated as follows. The constant-jerk time is given by \\(T_j = T_{\\rm total}/4\\) because of its symmetric property. Since \\(T_a=T_v=0\\), the final shift length \\(L=l_7=2jT_j^3\\) can be determined using the above equation. The maximum lateral acceleration is then given by \\(a_{\\rm max} =jT_j\\). This results in the following expression for the maximum lateral acceleration:</p> \\[ \\begin{align} a_{\\rm max}^{\\rm lat}  = \\frac{8L}{T_{\\rm total}^2} \\end{align} \\]"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_path_generation_design/#calculation-of-ta-tj-and-jerk-from-acceleration-limit","title":"Calculation of Ta, Tj and jerk from acceleration limit","text":"<p>In the case where there are no limitations on lateral velocity, the constant-jerk and acceleration times, as well as the required jerk can be calculated from the acceleration limit, total time, and final shift length as follows. Since \\(T_v=0\\), the final shift length is given by:</p> \\[ \\begin{align} L = l_7 = 2 j T_j^3 + 3 j T_a T_j^2 + j T_a^2 T_j \\end{align} \\] <p>Additionally, the velocity profile reveals the following relations:</p> \\[ \\begin{align} a_{\\rm lim}^{\\rm lat} &amp;= j T_j\\\\ T_{\\rm total} &amp;= 4T_j + 2T_a \\end{align} \\] <p>By solving these three equations, the following can be obtained:</p> \\[ \\begin{align} T_j&amp;=\\frac{T_{\\rm total}}{2} - \\frac{2L}{a_{\\rm lim}^{\\rm lat} T_{\\rm total}}\\\\[10pt] T_a&amp;=\\frac{4L}{a_{\\rm lim}^{\\rm lat} T_{\\rm total}} - \\frac{T_{\\rm total}}{2}\\\\[10pt] jerk&amp;=\\frac{2a_{\\rm lim} ^2T_{\\rm total}}{a_{\\rm lim}^{\\rm lat} T_{\\rm total}^2-4L} \\end{align} \\] <p>where \\(T_j\\) is the constant-jerk time, \\(T_a\\) is the constant acceleration time, \\(j\\) is the required jerk, \\(a_{\\rm lim}^{\\rm lat}\\) is the lateral acceleration limit, and \\(L\\) is the final shift length.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_path_generation_design/#calculation-of-required-time-from-jerk-and-acceleration-constraint","title":"Calculation of Required Time from Jerk and Acceleration Constraint","text":"<p>In the case where there are no limitations on lateral velocity, the total time required for shifting can be calculated from the lateral jerk and lateral acceleration limits and the final shift length as follows. By solving the two equations given above:</p> \\[ L = l_7 = 2 j T_j^3 + 3 j T_a T_j^2 + j T_a^2 T_j,\\quad a_{\\rm lim}^{\\rm lat} = j T_j \\] <p>we obtain the following expressions:</p> \\[ \\begin{align} T_j &amp;= \\frac{a_{\\rm lim}^{\\rm lat}}{j}\\\\[10pt] T_a &amp;= \\frac{1}{2}\\sqrt{\\frac{a_{\\rm lim}^{\\rm lat}}{j}^2 + \\frac{4L}{a_{\\rm lim}^{\\rm lat}}} - \\frac{3a_{\\rm lim}^{\\rm lat}}{2j} \\end{align} \\] <p>The total time required for shifting can then be calculated as \\(T_{\\rm total}=4T_j+2T_a\\).</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_path_generation_design/#limitation","title":"Limitation","text":"<ul> <li>Since \\(T_v\\) is zero in almost all cases of lane change and avoidance, \\(T_v\\) is not considered in the current implementation.</li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_safety_check/","title":"Safety Check Utils","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_safety_check/#safety-check-utils","title":"Safety Check Utils","text":"<p>Safety check function checks if the given path will collide with a given target object.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_safety_check/#purpose-role","title":"Purpose / Role","text":"<p>In the behavior path planner, certain modules (e.g., lane change) need to perform collision checks to ensure the safe navigation of the ego vehicle. These utility functions assist the user in conducting safety checks with other road participants.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_safety_check/#assumptions","title":"Assumptions","text":"<p>The safety check module is based on the following assumptions:</p> <ol> <li>Users must provide the position, velocity, and shape of both the ego and target objects to the utility functions.</li> <li>The yaw angle of each point in the predicted path of both the ego and target objects should point to the next point in the path.</li> <li>The safety check module uses RSS distance to determine the safety of a potential collision with other objects.</li> </ol>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_safety_check/#limitations","title":"Limitations","text":"<p>Currently the yaw angle of each point of predicted paths of a target object does not point to the next point. Therefore, the safety check function might returns incorrect result for some edge case.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_safety_check/#inner-working-algorithm","title":"Inner working / Algorithm","text":"<p>The flow of the safety check algorithm is described in the following explanations.</p> <p></p> <p>Here we explain each step of the algorithm flow.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_safety_check/#1-get-pose-of-the-target-object-at-a-given-time","title":"1. Get pose of the target object at a given time","text":"<p>For the first step, we obtain the pose of the target object at a given time. This can be done by interpolating the predicted path of the object.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_safety_check/#2-check-overlap","title":"2. Check overlap","text":"<p>With the interpolated pose obtained in the step.1, we check if the object and ego vehicle overlaps at a given time. If they are overlapped each other, the given path is unsafe.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_safety_check/#3-get-front-object","title":"3. Get front object","text":"<p>After the overlap check, it starts to perform the safety check for the broader range. In this step, it judges if ego or target object is in front of the other vehicle. We use arc length of the front point of each object along the given path to judge which one is in front of the other. In the following example, target object (red rectangle) is running in front of the ego vehicle (black rectangle).</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_safety_check/#4-calculate-rss-distance","title":"4. Calculate RSS distance","text":"<p>After we find which vehicle is running ahead of the other vehicle, we start to compute the RSS distance. With the reaction time \\(t_{reaction}\\) and safety time margin \\(t_{margin}\\), RSS distance can be described as:</p> \\[ rss_{dist} = v_{rear} (t_{reaction} + t_{margin}) + \\frac{v_{rear}^2}{2|a_{rear, decel}|} - \\frac{v_{front}^2}{2|a_{front, decel|}} \\] <p>where \\(V_{front}\\), \\(v_{rear}\\) are front and rear vehicle velocity respectively and \\(a_{rear, front}\\), \\(a_{rear, decel}\\) are front and rear vehicle deceleration.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_safety_check/#5-create-extended-ego-and-target-object-polygons","title":"5. Create extended ego and target object polygons","text":"<p>In this step, we compute extended ego and target object polygons. The extended polygons can be described as:</p> <p></p> <p>As the picture shows, we expand the rear object polygon. For the longitudinal side, we extend it with the RSS distance, and for the lateral side, we extend it by the lateral margin</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_safety_check/#6-check-overlap","title":"6. Check overlap","text":"<p>Similar to the previous step, we check the overlap of the extended rear object polygon and front object polygon. If they are overlapped each other, we regard it as the unsafe situation.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_side_shift_design/","title":"Side Shift design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_side_shift_design/#side-shift-design","title":"Side Shift design","text":"<p>(For remote control) Shift the path to left or right according to an external instruction.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_side_shift_design/#flowchart","title":"Flowchart","text":"<p>```plantuml --&gt; @startuml skinparam monochrome true skinparam defaultTextAlignment center skinparam noteTextAlignment left</p> <p>title path generation</p> <p>start partition plan { if (lateral_offset_change_request_ == true \\n &amp;&amp; \\n (shifting_status_ == BEFORE_SHIFT \\n || \\n shifting_status_ == AFTER_SHIFT)) then ( true)   partition replace-shift-line {     if ( shift line is inserted in the path ) then ( yes)       :erase left shift line;     else ( no)     endif     :calcShiftLines;     :add new shift lines;     :inserted_lateral_offset_ = requested_lateral_offset_ \\n inserted_shift_lines_ = new_shift_line;   } else( false) endif stop @enduml ```</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/","title":"Start Planner design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#start-planner-design","title":"Start Planner design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#purpose-role","title":"Purpose / Role","text":"<p>The Start Planner module is designed to generate a path from the current ego position to the driving lane, avoiding static obstacles and implementing safety checks against dynamic obstacles. (Note: The feature of safety checks against dynamic obstacles is currently a work in progress.) This module is activated when a new route is received.</p> <p>Use cases are as follows</p> <ul> <li>start smoothly from the current ego position to centerline.   </li> <li>pull out from the side of the road lane to centerline.   </li> <li>pull out from the shoulder lane to the road lane centerline.   </li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#design","title":"Design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#general-parameters-for-start_planner","title":"General parameters for start_planner","text":"Name Unit Type Description Default value th_arrived_distance_m [m] double distance threshold for arrival of path termination 1.0 th_distance_to_middle_of_the_road [m] double distance threshold to determine if the vehicle is on the middle of the road 0.1 th_stopped_velocity_mps [m/s] double velocity threshold for arrival of path termination 0.01 th_stopped_time_sec [s] double time threshold for arrival of path termination 1.0 th_turn_signal_on_lateral_offset [m] double lateral distance threshold for turning on blinker 1.0 intersection_search_length [m] double check if intersections exist within this length 30.0 length_ratio_for_turn_signal_deactivation_near_intersection [m] double deactivate turn signal of this module near intersection 0.5 collision_check_margin [m] double Obstacle collision check margin 1.0 collision_check_distance_from_end [m] double collision check distance from end point. currently only for pull out 15.0 center_line_path_interval [m] double reference center line path point interval 1.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#safety-check-with-static-obstacles","title":"Safety check with static obstacles","text":"<ol> <li>Calculate ego-vehicle's footprint on pull out path between from current position to pull out end point. (Illustrated by blue frame)</li> <li>Calculate object's polygon</li> <li>If a distance between the footprint and the polygon is lower than the threshold (default: <code>1.0 m</code>), that is judged as a unsafe path</li> </ol>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#safety-check-with-dynamic-obstacles","title":"Safety check with dynamic obstacles","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#basic-concept-of-safety-check-against-dynamic-obstacles","title":"Basic concept of safety check against dynamic obstacles","text":"<p>This is based on the concept of RSS. For the logic used, refer to the link below. See safety check feature explanation</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#collision-check-performed-range","title":"Collision check performed range","text":"<p>A collision check with dynamic objects is primarily performed between the shift start point and end point. The range for safety check varies depending on the type of path generated, so it will be explained for each pattern.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#shift-pull-out","title":"Shift pull out","text":"<p>For the \"shift pull out\", safety verification starts at the beginning of the shift and ends at the shift's conclusion.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#geometric-pull-out","title":"Geometric pull out","text":"<p>Since there's a stop at the midpoint during the shift, this becomes the endpoint for safety verification. After stopping, safety verification resumes.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#backward-pull-out-start-point-search","title":"Backward pull out start point search","text":"<p>During backward movement, no safety check is performed. Safety check begins at the point where the backward movement ends.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#ego-vehicles-velocity-planning","title":"Ego vehicle's velocity planning","text":"<p>WIP</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#safety-check-in-free-space-area","title":"Safety check in free space area","text":"<p>WIP</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#parameters-for-safety-check","title":"Parameters for safety check","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#stop-condition-parameters","title":"Stop Condition Parameters","text":"<p>Parameters under <code>stop_condition</code> define the criteria for stopping conditions.</p> Name Unit Type Description Default value maximum_deceleration_for_stop [m/s^2] double Maximum deceleration allowed for a stop 1.0 maximum_jerk_for_stop [m/s^3] double Maximum jerk allowed for a stop 1.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#ego-predicted-path-parameters","title":"Ego Predicted Path Parameters","text":"<p>Parameters under <code>path_safety_check.ego_predicted_path</code> specify the ego vehicle's predicted path characteristics.</p> Name Unit Type Description Default value min_velocity [m/s] double Minimum velocity of the ego vehicle's predicted path 0.0 acceleration [m/s^2] double Acceleration for the ego vehicle's predicted path 1.0 max_velocity [m/s] double Maximum velocity of the ego vehicle's predicted path 1.0 time_horizon_for_front_object [s] double Time horizon for predicting front objects 10.0 time_horizon_for_rear_object [s] double Time horizon for predicting rear objects 10.0 time_resolution [s] double Time resolution for the ego vehicle's predicted path 0.5 delay_until_departure [s] double Delay until the ego vehicle departs 1.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#target-object-filtering-parameters","title":"Target Object Filtering Parameters","text":"<p>Parameters under <code>target_filtering</code> are related to filtering target objects for safety check.</p> Name Unit Type Description Default value safety_check_time_horizon [s] double Time horizon for safety check 5.0 safety_check_time_resolution [s] double Time resolution for safety check 1.0 object_check_forward_distance [m] double Forward distance for object detection 10.0 object_check_backward_distance [m] double Backward distance for object detection 100.0 ignore_object_velocity_threshold [m/s] double Velocity threshold below which objects are ignored 1.0 object_types_to_check.check_car - bool Flag to check cars true object_types_to_check.check_truck - bool Flag to check trucks true object_types_to_check.check_bus - bool Flag to check buses true object_types_to_check.check_trailer - bool Flag to check trailers true object_types_to_check.check_bicycle - bool Flag to check bicycles true object_types_to_check.check_motorcycle - bool Flag to check motorcycles true object_types_to_check.check_pedestrian - bool Flag to check pedestrians true object_types_to_check.check_unknown - bool Flag to check unknown object types false object_lane_configuration.check_current_lane - bool Flag to check the current lane true object_lane_configuration.check_right_side_lane - bool Flag to check the right side lane true object_lane_configuration.check_left_side_lane - bool Flag to check the left side lane true object_lane_configuration.check_shoulder_lane - bool Flag to check the shoulder lane true object_lane_configuration.check_other_lane - bool Flag to check other lanes false include_opposite_lane - bool Flag to include the opposite lane in check false invert_opposite_lane - bool Flag to invert the opposite lane check false check_all_predicted_path - bool Flag to check all predicted paths true use_all_predicted_path - bool Flag to use all predicted paths true use_predicted_path_outside_lanelet - bool Flag to use predicted paths outside of lanelets false"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#safety-check-parameters","title":"Safety Check Parameters","text":"<p>Parameters under <code>safety_check_params</code> define the configuration for safety check.</p> Name Unit Type Description Default value enable_safety_check - bool Flag to enable safety check true check_all_predicted_path - bool Flag to check all predicted paths true publish_debug_marker - bool Flag to publish debug markers false rss_params.rear_vehicle_reaction_time [s] double Reaction time for rear vehicles 2.0 rss_params.rear_vehicle_safety_time_margin [s] double Safety time margin for rear vehicles 1.0 rss_params.lateral_distance_max_threshold [m] double Maximum lateral distance threshold 2.0 rss_params.longitudinal_distance_min_threshold [m] double Minimum longitudinal distance threshold 3.0 rss_params.longitudinal_velocity_delta_time [s] double Delta time for longitudinal velocity 0.8 hysteresis_factor_expand_rate - double Rate to expand/shrink the hysteresis factor 1.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#path-generation","title":"Path Generation","text":"<p>There are two path generation methods.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#shift-pull-out_1","title":"shift pull out","text":"<p>This is the most basic method of starting path planning and is used on road lanes and shoulder lanes when there is no particular obstruction.</p> <p>Pull out distance is calculated by the speed, lateral deviation, and the lateral jerk. The lateral jerk is searched for among the predetermined minimum and maximum values, and the one that generates a safe path is selected.</p> <ul> <li>Generate the road lane centerline and shift it to the current position.</li> <li>In the section between merge start and end, path is shifted by a method that is used to generate avoidance path (four segmental constant jerk polynomials)</li> <li>Combine this path with center line of road lane</li> </ul> <p></p> <p>shift pull out video</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#parameters-for-shift-pull-out","title":"parameters for shift pull out","text":"Name Unit Type Description Default value enable_shift_pull_out [-] bool flag whether to enable shift pull out true check_shift_path_lane_departure [-] bool flag whether to check if shift path footprints are out of lane false shift_pull_out_velocity [m/s] double velocity of shift pull out 2.0 pull_out_sampling_num [-] int Number of samplings in the minimum to maximum range of lateral_jerk 4 maximum_lateral_jerk [m/s3] double maximum lateral jerk 2.0 minimum_lateral_jerk [m/s3] double minimum lateral jerk 0.1 minimum_shift_pull_out_distance [m] double minimum shift pull out distance. if calculated pull out distance is shorter than this, use this for path generation. 0.0 maximum_curvature [m] double maximum curvature. The pull out distance is calculated so that the curvature is smaller than this value. 0.07"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#geometric-pull-out_1","title":"geometric pull out","text":"<p>Generate two arc paths with discontinuous curvature. Ego-vehicle stops once in the middle of the path to control the steer on the spot. See also [1] for details of the algorithm.</p> <p></p> <p>geometric pull out video</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#parameters-for-geometric-pull-out","title":"parameters for geometric pull out","text":"Name Unit Type Description Default value enable_geometric_pull_out [-] bool flag whether to enable geometric pull out true divide_pull_out_path [-] bool flag whether to divide arc paths. The path is assumed to be divided because the curvature is not continuous. But it requires a stop during the departure. false geometric_pull_out_velocity [m/s] double velocity of geometric pull out 1.0 arc_path_interval [m] double path points interval of arc paths of geometric pull out 1.0 lane_departure_margin [m] double margin of deviation to lane right 0.2 pull_out_max_steer_angle [rad] double maximum steer angle for path generation 0.26"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#backward-pull-out-start-point-search_1","title":"backward pull out start point search","text":"<p>If a safe path cannot be generated from the current position, search backwards for a pull out start point at regular intervals(default: <code>2.0</code>).</p> <p></p> <p>pull out after backward driving video</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#parameters-for-backward-pull-out-start-point-search","title":"parameters for backward pull out start point search","text":"Name Unit Type Description Default value enable_back [-] bool flag whether to search backward for start_point true search_priority [-] string In the case of <code>efficient_path</code>, use efficient paths even if the back distance is longer. In case of <code>short_back_distance</code>, use a path with as short a back distance efficient_path max_back_distance [m] double maximum back distance 30.0 backward_search_resolution [m] double distance interval for searching backward pull out start point 2.0 backward_path_update_duration [s] double time interval for searching backward pull out start point. this prevents chattering between back driving and pull_out 3.0 ignore_distance_from_lane_end [m] double If distance from shift start pose to end of shoulder lane is less than this value, this start pose candidate is ignored 15.0"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#freespace-pull-out","title":"freespace pull out","text":"<p>If the vehicle gets stuck with pull out along lanes, execute freespace pull out. To run this feature, you need to set <code>parking_lot</code> to the map, <code>activate_by_scenario</code> of costmap_generator to <code>false</code> and <code>enable_freespace_planner</code> to <code>true</code></p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#unimplemented-parts-limitations-for-freespace-pull-out","title":"Unimplemented parts / limitations for freespace pull out","text":"<ul> <li>When a short path is generated, the ego does can not drive with it.</li> <li>Complex cases take longer to generate or fail.</li> <li>The drivable area is not guaranteed to fit in the parking_lot.</li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_start_planner_design/#parameters-freespace-parking","title":"Parameters freespace parking","text":"Name Unit Type Description Default value enable_freespace_planner [-] bool this flag activates a free space pullout that is executed when a vehicle is stuck due to obstacles in the lanes where the ego is located true end_pose_search_start_distance [m] double distance from ego to the start point of the search for the end point in the freespace_pull_out driving lane 20.0 end_pose_search_end_distance [m] double distance from ego to the end point of the search for the end point in the freespace_pull_out driving lane 30.0 end_pose_search_interval [m] bool interval to search for the end point in the freespace_pull_out driving lane 2.0 <p>See freespace_planner for other parameters.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/","title":"Turn Signal design","text":""},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#turn-signal-design","title":"Turn Signal design","text":"<p>Turn Signal decider determines necessary blinkers.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#purpose-role","title":"Purpose / Role","text":"<p>This module is responsible for activating a necessary blinker during driving. It uses rule-based algorithm to determine blinkers, and the details of this algorithm are described in the following sections. Note that this algorithm is strictly based on the Japanese Road Traffic Raw.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#assumptions","title":"Assumptions","text":"<p>Autoware has following order of priorities for turn signals.</p> <ol> <li>Activate turn signal to safely navigate ego vehicle and protect other road participants</li> <li>Follow traffic laws</li> <li>Follow human driving practices</li> </ol>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#limitations","title":"Limitations","text":"<p>Currently, this algorithm can sometimes give unnatural (not wrong) blinkers in a complicated situations. This is because it tries to follow the road traffic raw and cannot solve <code>blinker conflicts</code> clearly in that environment.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#parameters-for-turn-signal-decider","title":"Parameters for turn signal decider","text":"Name Unit Type Description Default value turn_signal_intersection_search_distance [m] double constant search distance to decide activation of blinkers at intersections 30 turn_signal_intersection_angle_threshold_degree deg double angle threshold to determined the end point of intersection required section 15 turn_signal_minimum_search_distance [m] double minimum search distance for avoidance and lane change 10 turn_signal_search_time [s] double search time for to decide activation of blinkers 3.0 turn_signal_shift_length_threshold [m] double shift length threshold to decide activation of blinkers 0.3 turn_signal_on_swerving [-] bool flag to activate blinkers when swerving true <p>Note that the default values for <code>turn_signal_intersection_search_distance</code> and <code>turn_signal_search_time</code> is strictly followed by Japanese Road Traffic Laws. So if your country does not allow to use these default values, you should change these values in configuration files.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>In this algorithm, it assumes that each blinker has two sections, which are <code>desired section</code> and <code>required section</code>. The image of these two sections are depicted in the following diagram.</p> <p></p> <p>These two sections have the following meanings.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#-desired-section","title":"- Desired Section","text":"<pre><code>- This section is defined by road traffic laws. It cannot be longer or shorter than the designated length defined by the law.\n- In this section, you do not have to activate the designated blinkers if it is dangerous to do so.\n</code></pre>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#-required-section","title":"- Required Section","text":"<pre><code>- In this section, ego vehicle must activate designated blinkers. However, if there are blinker conflicts, it must solve them based on the algorithm we mention later in this document.\n- Required section cannot be longer than desired section.\n</code></pre> <p>When turning on the blinker, it decides whether or not to turn on the specified blinker based on the distance from the front of the ego vehicle to the start point of each section. Conversely, when turning off the blinker, it calculates the distance from the base link of the ego vehicle to the end point of each section and decide whether or not to turn it off based on that.</p> <p></p> <p>For left turn, right turn, avoidance, lane change, goal planner and pull out, we define these two sections, which are elaborated in the following part.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#1-left-and-right-turn","title":"1. Left and Right turn","text":"<p>Turn signal decider checks each lanelet on the map if it has <code>turn_direction</code> information. If a lanelet has this information, it activates necessary blinker based on this information.</p> <ul> <li>desired start point   The <code>search_distance</code> for blinkers at intersections is <code>v * turn_signal_search_time + turn_signal_intersection_search_distance</code>. Then the start point becomes <code>search_distance</code> meters before the start point of the intersection lanelet(depicted in gree in the following picture), where <code>v</code> is the velocity of the ego vehicle. However, if we set <code>turn_signal_distance</code> in the lanelet, we use that length as search distance.</li> </ul> <ul> <li>desired end point   Terminal point of the intersection lanelet.</li> </ul> <ul> <li>required start point   Initial point of the intersection lanelet.</li> </ul> <ul> <li>required end point   The earliest point that satisfies the following condition. \\(\\theta - \\theta_{\\textrm{end}} &lt; \\delta\\), where \\(\\theta_{\\textrm{end}}\\) is yaw angle of the terminal point of the lanelet, \\(\\theta\\) is the angle of a required end point and \\(\\delta\\) is the threshold defined by the user.</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#2-avoidance","title":"2. Avoidance","text":"<p>Avoidance can be separated into two sections, first section and second section. The first section is from the start point of the path shift to the end of the path shift. The second section is from the end of shift point to the end of avoidance. Note that avoidance module will not activate turn signal when its shift length is below <code>turn_signal_shift_length_threshold</code>.</p> <p>First section</p> <ul> <li>desired start point <code>v * turn_signal_search_time</code> meters before the start point of the avoidance shift path.</li> </ul> <ul> <li>desired end point   Shift complete point where the path shift is completed.</li> </ul> <ul> <li>required start point   Avoidance start point.</li> </ul> <ul> <li>required end point   Shift complete point same as the desired end point.</li> </ul> <p></p> <p>Second section</p> <ul> <li>desired start point   Shift complete point.</li> </ul> <ul> <li>desired end point   Avoidance terminal point</li> </ul> <ul> <li>required start point   Shift complete point.</li> </ul> <ul> <li>required end point   Avoidance terminal point.</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#3-lane-change","title":"3. Lane Change","text":"<ul> <li>desired start point <code>v * turn_signal_search_time</code> meters before the start point of the lane change path.</li> </ul> <ul> <li>desired end point   Terminal point of the lane change path.</li> </ul> <ul> <li>required start point   Initial point of the lane change path.</li> </ul> <ul> <li>required end point   Cross point with lane change path and boundary line of the adjacent lane.</li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#4-pull-out","title":"4. Pull out","text":"<ul> <li>desired start point   Start point of the path of pull out.</li> </ul> <ul> <li>desired end point   Terminal point of the path of pull out.</li> </ul> <ul> <li>required start point   Start point of the path pull out.</li> </ul> <ul> <li>required end point   Terminal point of the path of pull out.</li> </ul>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#5-goal-planner","title":"5. Goal Planner","text":"<ul> <li>desired start point <code>v * turn_signal_search_time</code> meters before the start point of the pull over path.</li> </ul> <ul> <li>desired end point   Terminal point of the path of pull over.</li> </ul> <ul> <li>required start point   Start point of the path of pull over.</li> </ul> <ul> <li>required end point   Terminal point of the path of pull over.</li> </ul> <p>When it comes to handle several blinkers, it gives priority to the first blinker that comes first. However, this rule sometimes activate unnatural blinkers, so turn signal decider uses the following five rules to decide the necessary turn signal.</p> <ul> <li>pattern1 </li> </ul> <ul> <li>pattern2 </li> </ul> <ul> <li>pattern3 </li> </ul> <ul> <li>pattern4 </li> </ul> <ul> <li>pattern5 </li> </ul> <p>Based on these five rules, turn signal decider can solve <code>blinker conflicts</code>. The following pictures show some examples of this kind of conflicts.</p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#-several-right-and-left-turns-on-short-sections","title":"- Several right and left turns on short sections","text":"<p>In this scenario, ego vehicle has to pass several turns that are close each other. Since this pattern can be solved by the pattern1 rule, the overall result is depicted in the following picture.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#-avoidance-with-left-turn-1","title":"- Avoidance with left turn (1)","text":"<p>In this scene, ego vehicle has to deal with the obstacle that is on its original path as well as make a left turn. The overall result can be varied by the position of the obstacle, but the image of the result is described in the following picture.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#-avoidance-with-left-turn-2","title":"- Avoidance with left turn (2)","text":"<p>Same as the previous scenario, ego vehicle has to avoid the obstacle as well as make a turn left. However, in this scene, the obstacle is parked after the intersection. Similar to the previous one, the overall result can be varied by the position of the obstacle, but the image of the result is described in the following picture.</p> <p></p>"},{"location":"planning/behavior_path_planner/docs/behavior_path_planner_turn_signal_design/#-lane-change-and-left-turn","title":"- Lane change and left turn","text":"<p>In this scenario, ego vehicle has to do lane change before making a left turn. In the following example, ego vehicle does not activate left turn signal until it reaches the end point of the lane change path.</p> <p></p>"},{"location":"planning/behavior_velocity_blind_spot_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_blind_spot_module/#blind-spot","title":"Blind Spot","text":""},{"location":"planning/behavior_velocity_blind_spot_module/#role","title":"Role","text":"<p>Blind spot module checks possible collisions with bicycles and pedestrians running on its left/right side while turing left/right before junctions.</p> <p></p>"},{"location":"planning/behavior_velocity_blind_spot_module/#activation-timing","title":"Activation Timing","text":"<p>This function is activated when the lane id of the target path has an intersection label (i.e. the <code>turn_direction</code> attribute is <code>left</code> or <code>right</code>).</p>"},{"location":"planning/behavior_velocity_blind_spot_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Sets a stop line, a pass judge line, a detection area and conflict area based on a map information and a self position.</p> <ul> <li>Stop line : Automatically created based on crossing lane information.</li> </ul> <ul> <li>Pass judge line : A position to judge if stop or not to avoid a rapid brake.</li> </ul> <ul> <li>Detection area : Right/left side area of the self position.</li> </ul> <ul> <li>Conflict area : Right/left side area from the self position to the stop line.</li> </ul> <p>Stop/Go state: When both conditions are met for any of each object, this module state is transited to the \"stop\" state and insert zero velocity to stop the vehicle.</p> <ul> <li>Object is on the detection area</li> <li>Object\u2019s predicted path is on the conflict area</li> </ul> <p>In order to avoid a rapid stop, the \u201cstop\u201d judgement is not executed after the judgment line is passed.</p> <p>Once a \"stop\" is judged, it will not transit to the \"go\" state until the \"go\" judgment continues for a certain period in order to prevent chattering of the state (e.g. 2 seconds).</p>"},{"location":"planning/behavior_velocity_blind_spot_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>stop_line_margin</code> double [m] a margin that the vehicle tries to stop before stop_line <code>backward_length</code> double [m] distance from closest path point to the edge of beginning point. <code>ignore_width_from_center_line</code> double [m] ignore threshold that vehicle behind is collide with ego vehicle or not <code>max_future_movement_time</code> double [s] maximum time for considering future movement of object <code>adjacent_extend_width</code> double [m] if adjacent lane e.g. bicycle only lane exists, blind_spot area is expanded by this length"},{"location":"planning/behavior_velocity_blind_spot_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_velocity_crosswalk_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_crosswalk_module/#crosswalk","title":"Crosswalk","text":""},{"location":"planning/behavior_velocity_crosswalk_module/#role","title":"Role","text":"<p>This module judges whether the ego should stop in front of the crosswalk in order to provide safe passage of pedestrians and bicycles based on object's behavior and surround traffic.</p> <p> </p> crosswalk module"},{"location":"planning/behavior_velocity_crosswalk_module/#activation-timing","title":"Activation Timing","text":"<p>The manager launch crosswalk scene modules when the reference path conflicts crosswalk lanelets.</p>"},{"location":"planning/behavior_velocity_crosswalk_module/#module-parameters","title":"Module Parameters","text":""},{"location":"planning/behavior_velocity_crosswalk_module/#common-parameters","title":"Common parameters","text":"Parameter Type Description <code>common.show_processing_time</code> bool whether to show processing time"},{"location":"planning/behavior_velocity_crosswalk_module/#parameters-for-input-data","title":"Parameters for input data","text":"Parameter Type Description <code>common.traffic_light_state_timeout</code> double [s] timeout threshold for traffic light signal"},{"location":"planning/behavior_velocity_crosswalk_module/#parameters-for-stop-position","title":"Parameters for stop position","text":"<p>The crosswalk module determines a stop position at least <code>stop_distance_from_object</code> away from the object.</p> <p> </p> stop margin <p>The stop line is the reference point for the stopping position of the vehicle, but if there is no stop line in front of the crosswalk, the position <code>stop_distance_from_crosswalk</code> meters before the crosswalk is the virtual stop line for the vehicle. Then, if the stop position determined from <code>stop_distance_from_object</code> exists in front of the stop line determined from the HDMap or <code>stop_distance_from_crosswalk</code>, the actual stop position is determined according to <code>stop_distance_from_object</code> in principle, and vice versa.</p> <p> </p> explicit stop line <p> </p> virtual stop point <p>On the other hand, if pedestrian (bicycle) is crossing wide crosswalks seen in scramble intersections, and the pedestrian position is more than <code>far_object_threshold</code> meters away from the stop line, the actual stop position is determined to be <code>stop_distance_from_object</code> and pedestrian position, not at the stop line.</p> <p> </p> stop at wide crosswalk <p>See the workflow in algorithms section.</p> Parameter Type Description <code>stop_position.stop_distance_from_object</code> double [m] the vehicle decelerates to be able to stop in front of object with margin <code>stop_position.stop_distance_from_crosswalk</code> double [m] make stop line away from crosswalk when no explicit stop line exists <code>stop_position.far_object_threshold</code> double [m] if objects cross X meters behind the stop line, the stop position is determined according to the object position (stop_distance_from_object meters before the object) <code>stop_position.stop_position_threshold</code> double [m] threshold for check whether the vehicle stop in front of crosswalk"},{"location":"planning/behavior_velocity_crosswalk_module/#parameters-for-egos-slow-down-velocity","title":"Parameters for ego's slow down velocity","text":"Parameter Type Description <code>slow_velocity</code> double [m/s] target vehicle velocity when module receive slow down command from FOA <code>max_slow_down_jerk</code> double [m/sss] minimum jerk deceleration for safe brake <code>max_slow_down_accel</code> double [m/ss] minimum accel deceleration for safe brake <code>no_relax_velocity</code> double [m/s] if the current velocity is less than X m/s, ego always stops at the stop position(not relax deceleration constraints)"},{"location":"planning/behavior_velocity_crosswalk_module/#parameters-for-stuck-vehicle","title":"Parameters for stuck vehicle","text":"<p>If there are low speed or stop vehicle ahead of the crosswalk, and there is not enough space between the crosswalk and the vehicle (see following figure), closing the distance to that vehicle could cause Ego to be stuck on the crosswalk. So, in this situation, this module plans to stop before the crosswalk and wait until the vehicles move away, even if there are no pedestrians or bicycles.</p> <p> </p> stuck vehicle attention range Parameter Type Description <code>stuck_vehicle.stuck_vehicle_velocity</code> double [m/s] maximum velocity threshold whether the vehicle is stuck <code>stuck_vehicle.max_stuck_vehicle_lateral_offset</code> double [m] maximum lateral offset for stuck vehicle position should be looked <code>stuck_vehicle.stuck_vehicle_attention_range</code> double [m] the detection area is defined as X meters behind the crosswalk"},{"location":"planning/behavior_velocity_crosswalk_module/#parameters-for-pass-judge-logic","title":"Parameters for pass judge logic","text":"<p>Also see algorithm section.</p> Parameter Type Description <code>pass_judge.ego_pass_first_margin</code> double [s] time margin for ego pass first situation <code>pass_judge.ego_pass_later_margin</code> double [s] time margin for object pass first situation <code>pass_judge.stop_object_velocity_threshold</code> double [m/s] velocity threshold for the module to judge whether the objects is stopped <code>pass_judge.min_object_velocity</code> double [m/s] minimum object velocity (compare the estimated velocity by perception module with this parameter and adopt the larger one to calculate TTV.) <code>pass_judge.timeout_no_intention_to_walk</code> double [s] if the pedestrian does not move for X seconds after stopping before the crosswalk, the module judge that ego is able to pass first. <code>pass_judge.timeout_ego_stop_for_yield</code> double [s] the amount of time which ego should be stopping to query whether it yields or not."},{"location":"planning/behavior_velocity_crosswalk_module/#parameters-for-object-filtering","title":"Parameters for object filtering","text":"<p>As a countermeasure against pedestrians attempting to cross outside the crosswalk area, this module watches not only the crosswalk zebra area but also in front of and behind space of the crosswalk, and if there are pedestrians or bicycles attempting to pass through the watch area, this module judges whether ego should pass or stop.</p> <p> </p> crosswalk attention range <p>This module mainly looks the following objects as target objects. There are also optional flags that enables the pass/stop decision for <code>motorcycle</code> and <code>unknown</code> objects.</p> <ul> <li>pedestrian</li> <li>bicycle</li> </ul> Parameter Type Description <code>crosswalk_attention_range</code> double [m] the detection area is defined as -X meters before the crosswalk to +X meters behind the crosswalk <code>target/unknown</code> bool whether to look and stop by UNKNOWN objects <code>target/bicycle</code> bool whether to look and stop by BICYCLE objects <code>target/motorcycle</code> bool whether to look and stop MOTORCYCLE objects <code>target/pedestrian</code> bool whether to look and stop PEDESTRIAN objects"},{"location":"planning/behavior_velocity_crosswalk_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/behavior_velocity_crosswalk_module/#stop-position","title":"Stop position","text":"<p>The stop position is determined by the existence of the stop line defined by the HDMap, the positional relationship between the stop line and the pedestrians and bicycles, and each parameter.</p> <p></p>"},{"location":"planning/behavior_velocity_crosswalk_module/#pass-judge-logic","title":"Pass judge logic","text":"<p>At first, this module determines whether the pedestrians or bicycles are likely to cross the crosswalk based on the color of the pedestrian traffic light signal related to the crosswalk. Only when the pedestrian traffic signal is RED, this module judges that the objects will not cross the crosswalk and skip the pass judge logic.</p> <p>Secondly, this module makes a decision as to whether ego should stop in front of the crosswalk or pass through based on the relative relationship between TTC(Time-To-Collision) and TTV(Time-To-Vehicle). The TTC is the time it takes for ego to reach the virtual collision point, and the TTV is the time it takes for the object to reach the virtual collision point.</p> <p> </p> virtual collision point <p>Depending on the relative relationship between TTC and TTV, the ego's behavior at crosswalks can be classified into three categories.</p> <ol> <li>TTC &gt;&gt; TTV: The objects have enough time to cross first before ego reaches the crosswalk. (Type-A)</li> <li>TTC \u2252 TTV: There is a risk of a near miss and collision between ego and objects at the virtual collision point. (Type-B)</li> <li>TTC &lt;&lt; TTV: Ego has enough time to path through the crosswalk before the objects reach the virtual collision point. (Type-C)</li> </ol> <p>This module judges that ego is able to pass through the crosswalk without collision risk when the relative relationship between TTC and TTV is Type-A and Type-C. On the other hand, this module judges that ego needs to stop in front of the crosswalk prevent collision with objects in Type-B condition. The time margin can be set by parameters <code>ego_pass_first_margin</code> and <code>ego_pass_later_margin</code>. This logic is designed based on [1].</p> <p> </p> time-to-collision vs time-to-vehicle <p>This module uses the larger value of estimated object velocity and <code>min_object_velocity</code> in calculating TTV in order to avoid division by zero.</p> <p></p>"},{"location":"planning/behavior_velocity_crosswalk_module/#dead-lock-prevention","title":"Dead lock prevention","text":"<p>If there are objects stop within a radius of <code>min_object_velocity * ego_pass_later_margin</code> meters from virtual collision point, this module judges that ego should stop based on the pass judge logic described above at all times. In such a situation, even if the pedestrian has no intention of crossing, ego continues the stop decision on the spot. So, this module has another logic for dead lock prevention, and if the object continues to stop for more than <code>timeout_no_intention_to_walk</code> seconds after ego stops in front of the crosswalk, this module judges that the object has no intention of crossing and switches from STOP state to PASS state. The parameter <code>stop_object_velocity_threshold</code> is used to judge whether the objects are stopped or not. In addition, if the object starts to move after the module judges that the object has no intention of crossing, this module judges whether ego should stop or not once again.</p> <p> </p> dead lock situation"},{"location":"planning/behavior_velocity_crosswalk_module/#safety-slow-down-behavior","title":"Safety Slow Down Behavior","text":"<p>In current autoware implementation if there are no target objects around a crosswalk, ego vehicle will not slow down for the crosswalk. However, if ego vehicle to slow down to a certain speed in such cases is wanted then it is possible by adding some tags to the related crosswalk definition as it is instructed in lanelet2_format_extension.md document.</p>"},{"location":"planning/behavior_velocity_crosswalk_module/#limitations","title":"Limitations","text":"<p>When multiple crosswalks are nearby (such as intersection), this module may make a stop decision even at crosswalks where the object has no intention of crossing.</p> <p> </p> design limits"},{"location":"planning/behavior_velocity_crosswalk_module/#known-issues","title":"Known Issues","text":""},{"location":"planning/behavior_velocity_crosswalk_module/#debugging","title":"Debugging","text":"<p>By <code>ros2 run behavior_velocity_crosswalk_module time_to_collision_plotter.py</code>, you can visualize the following figure of the ego and pedestrian's time to collision. The label of each plot is <code>&lt;crosswalk module id&gt;-&lt;pedestrian uuid&gt;</code>.</p> <p> </p> Plot of time to collision"},{"location":"planning/behavior_velocity_crosswalk_module/#referencesexternal-links","title":"References/External links","text":"<p>[1] \u4f50\u85e4 \u307f\u306a\u307f, \u65e9\u5742 \u7965\u4e00, \u6e05\u6c34 \u653f\u884c, \u6751\u91ce \u9686\u5f66, \u6a2a\u65ad\u6b69\u884c\u8005\u306b\u5bfe\u3059\u308b\u30c9\u30e9\u30a4\u30d0\u306e\u30ea\u30b9\u30af\u56de\u907f\u884c\u52d5\u306e\u30e2\u30c7\u30eb\u5316, \u81ea\u52d5\u8eca\u6280\u8853\u4f1a\u8ad6\u6587\u96c6, 2013, 44 \u5dfb, 3 \u53f7, p. 931-936.</p>"},{"location":"planning/behavior_velocity_detection_area_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_detection_area_module/#detection-area","title":"Detection Area","text":""},{"location":"planning/behavior_velocity_detection_area_module/#role","title":"Role","text":"<p>If pointcloud is detected in a detection area defined on a map, the stop planning will be executed at the predetermined point.</p> <p></p>"},{"location":"planning/behavior_velocity_detection_area_module/#activation-timing","title":"Activation Timing","text":"<p>This module is activated when there is a detection area on the target lane.</p>"},{"location":"planning/behavior_velocity_detection_area_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>use_dead_line</code> bool [-] weather to use dead line or not <code>use_pass_judge_line</code> bool [-] weather to use pass judge line or not <code>state_clear_time</code> double [s] when the vehicle is stopping for certain time without incoming obstacle, move to STOPPED state <code>stop_margin</code> double [m] a margin that the vehicle tries to stop before stop_line <code>dead_line_margin</code> double [m] ignore threshold that vehicle behind is collide with ego vehicle or not <code>hold_stop_margin_distance</code> double [m] parameter for restart prevention (See Algorithm section) <code>distance_to_judge_over_stop_line</code> double [m] parameter for judging that the stop line has been crossed"},{"location":"planning/behavior_velocity_detection_area_module/#inner-workings-algorithm","title":"Inner-workings / Algorithm","text":"<ol> <li>Gets a detection area and stop line from map information and confirms if there is pointcloud in the detection area</li> <li>Inserts stop point l[m] in front of the stop line</li> <li>Inserts a pass judge point to a point where the vehicle can stop with a max deceleration</li> <li>Sets velocity as zero behind the stop line when the ego-vehicle is in front of the pass judge point</li> <li>If the ego vehicle has passed the pass judge point already, it doesn\u2019t stop and pass through.</li> </ol>"},{"location":"planning/behavior_velocity_detection_area_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_velocity_detection_area_module/#restart-prevention","title":"Restart prevention","text":"<p>If it needs X meters (e.g. 0.5 meters) to stop once the vehicle starts moving due to the poor vehicle control performance, the vehicle goes over the stopping position that should be strictly observed when the vehicle starts to moving in order to approach the near stop point (e.g. 0.3 meters away).</p> <p>This module has parameter <code>hold_stop_margin_distance</code> in order to prevent from these redundant restart. If the vehicle is stopped within <code>hold_stop_margin_distance</code> meters from stop point of the module (_front_to_stop_line &lt; hold_stop_margin_distance), the module judges that the vehicle has already stopped for the module's stop point and plans to keep stopping current position even if the vehicle is stopped due to other factors.</p> <p> </p> parameters <p> </p> outside the hold_stop_margin_distance <p> </p> inside the hold_stop_margin_distance"},{"location":"planning/behavior_velocity_intersection_module/","title":"Intersection","text":""},{"location":"planning/behavior_velocity_intersection_module/#intersection","title":"Intersection","text":""},{"location":"planning/behavior_velocity_intersection_module/#role","title":"Role","text":"<p>The intersection module is responsible for safely passing urban intersections by:</p> <ol> <li>checking collisions with upcoming vehicles</li> <li>recognizing the occluded area in the intersection</li> <li>reacting to each color/shape of associated traffic lights</li> </ol> <p>This module is designed to be agnostic to left-hand/right-hand traffic rules and work for crossroads, T-shape junctions, etc. Roundabout is not formally supported in this module.</p> <p></p>"},{"location":"planning/behavior_velocity_intersection_module/#activation-condition","title":"Activation condition","text":"<p>This module is activated when the path contains the lanes with turn_direction tag. More precisely, if the lane_ids of the path contain the ids of those lanes, corresponding instances of intersection module are activated on each lane respectively.</p>"},{"location":"planning/behavior_velocity_intersection_module/#requirementslimitations","title":"Requirements/Limitations","text":"<ul> <li>The HDMap needs to have the information of turn_direction tag (which should be one of straight, left, right) for all the lanes in intersections and right_of_way tag for specific lanes (refer to RightOfWay section for more details). See lanelet2_extension document for more detail.</li> <li>WIP(perception requirements/limitations)</li> <li>WIP(sensor visibility requirements/limitations)</li> </ul>"},{"location":"planning/behavior_velocity_intersection_module/#attention-area","title":"Attention area","text":"<p>The attention area in the intersection is defined as the set of lanes that are conflicting with ego path and their preceding lanes up to <code>common.attention_area_length</code> meters. By default RightOfWay tag is not set, so the attention area covers all the conflicting lanes and its preceding lanes as shown in the first row. RightOfWay tag is used to rule out the lanes that each lane has priority given the traffic light relation and turn_direction priority. In the second row, purple lanes are set as the yield_lane of the ego_lane in the RightOfWay tag.</p> <p></p> <p>intersection_area, which is supposed to be defined on the HDMap, is an area converting the entire intersection.</p>"},{"location":"planning/behavior_velocity_intersection_module/#in-phaseanti-phase-signal-group","title":"In-phase/Anti-phase signal group","text":"<p>The terms \"in-phase signal group\" and \"anti-phase signal group\" are introduced to distinguish the lanes by the timing of traffic light regulation as shown in below figure.</p> <p></p> <p>The set of intersection lanes whose color is in sync with lane L1 is called the in-phase signal group of L1, and the set of remaining lanes is called the anti-phase signal group.</p>"},{"location":"planning/behavior_velocity_intersection_module/#how-towhy-set-rightofway-tag","title":"How-to/Why set RightOfWay tag","text":"<p>Ideally RightOfWay tag is unnecessary if ego has perfect knowledge of all traffic signal information because:</p> <ul> <li>it can distinguish which conflicting lanes should be checked because they are GREEN currently and possible collision occur with the vehicles on those lanes</li> <li>it can distinguish which conflicting lanes can be ignored because they are RED currently and there is no chance of collision with the vehicles on those lanes unless they violate the traffic rule</li> </ul> <p>That allows ego to generate the attention area dynamically using the real time traffic signal information. However this ideal condition rarely holds unless the traffic signal information is provided through the infrastructure. Also there maybe be very complicated/bad intersection maps where multiple lanes overlap in a complex manner.</p> <ul> <li>If there is an perfect access to entire traffic light signal, then you can set <code>common.use_map_right_of_way</code> to false and there is no need to set RightOfWay tag on the map. The intersection module will generate the attention area by checking traffic signal and corresponding conflicting lanes. This feature is not implemented yet.</li> <li>If traffic signal information is not perfect, then set <code>common.use_map_right_of_way</code> to true. If you do not want to detect vehicles on the anti-phase signal group lanes, set them as yield_lane for ego lane.</li> <li>Even if there are no traffic lights if the intersection lanes are overlapped in a ugly manner, you may need to set RightOfWay tag. For example if adjacent intersection lanes of the same in-phase group are not sharing the boundary line and overlapped a little bit, you may need to set RightOfWay to each other for them in order to avoid unnecessary stop for vehicle on such unrelated lane.</li> </ul> <p>To help the intersection module care only a set of limited lanes, RightOfWay tag needs to be properly set.</p> <p>Following table shows an example of how to set yield_lanes to each lane in a intersection w/o traffic lights. Since it is not apparent how to uniquely determine signal phase group for a set of intersection lanes in geometric/topological manner, yield_lane needs to be set manually. Straight lanes with traffic lights are exceptionally handled to detect no lanes because commonly it has priority over all the other lanes, so no RightOfWay setting is required.</p> turn direction of right_of_way yield_lane(with traffic light) yield_lane(without traffic light) straight not need to set yield_lane(this case is special) left/right conflicting lanes of in-phase group left(Left hand traffic) all conflicting lanes of the anti-phase group and right conflicting lanes of in-phase group right conflicting lanes of in-phase group right(Left hand traffic) all conflicting lanes of the anti-phase group no yield_lane left(Right hand traffic) all conflicting lanes of the anti-phase group no yield_lane right(Right hand traffic) all conflicting lanes of the anti-phase group and right conflicting lanes of in-phase group left conflicting lanes of in-phase group <p>This setting gives the following <code>attention_area</code> configurations.</p> <p> </p> <p>For complex/bad intersection map like the one illustrated below, additional RightOfWay setting maybe necessary.</p> <p></p> <p>The bad points are:</p> <ol> <li>ego lane is overlapped with adjacent lane of the in-phase group. In this case you need to set this lane as yield_lane additionally because otherwise attention area is generated for its preceding lanes as well, which may cause unwanted stop.</li> <li>ego lane is overlapped with unrelated lane. In this case the lane is right-turn only and there is no chance of collision in theory. But you need to set this lane as yield_lane additionally for the same reason as (1).</li> </ol>"},{"location":"planning/behavior_velocity_intersection_module/#possible-stop-lines","title":"Possible stop lines","text":"<p>Following figure illustrates important positions used in the intersection module. Note that each solid line represents ego front line position and the corresponding dot represents the actual inserted stop point position for the vehicle frame, namely the center of the rear wheel.</p> <p></p> <p>To precisely calculate stop positions, the path is interpolated at the certain interval of <code>common.path_interpolation_ds</code>.</p> <ul> <li>closest_idx denotes the path point index which is closest to ego position.</li> <li>first_attention_stopline denotes the first path point where ego footprint intersects with the attention_area.</li> <li>If a stopline is associated with the intersection lane on the map, that line is used as the default_stopline for collision detection. Otherwise the point which is <code>common.default_stopline_margin</code> meters behind first_attention_stopline is defined as the default_stopline instead.</li> <li>occlusion_peeking_stopline is a bit ahead of first_attention_stopline as described later.</li> <li>occlusion_wo_tl_pass_judge_line is the first position where ego footprint intersects with the centerline of the first attention_area lane.</li> </ul>"},{"location":"planning/behavior_velocity_intersection_module/#target-objects","title":"Target objects","text":"<p>For stuck vehicle detection and collision detection, this module checks car, bus, truck, trailer, motor cycle, and bicycle type objects.</p> <p>Objects that satisfy all of the following conditions are considered as target objects (possible collision objects):</p> <ul> <li>The center of the object is within a certain distance from the attention lane (threshold = <code>common.attention_area_margin</code>) .<ul> <li>(Optional condition) The center of the object is in the intersection area.<ul> <li>To deal with objects that is in the area not covered by the lanelets in the intersection.</li> </ul> </li> </ul> </li> <li>The posture of object is the same direction as the attention lane (threshold = <code>common.attention_area_angle_threshold</code>).</li> <li>Not being in the adjacent lanes of ego.</li> </ul>"},{"location":"planning/behavior_velocity_intersection_module/#overview-of-decision-process","title":"Overview of decision process","text":"<p>There are several behaviors depending on the scene.</p> behavior scene action Safe Ego detected no occlusion and collision Ego passes the intersection StuckStop The exit of the intersection is blocked by traffic jam Ego stops before the intersection or the boundary of attention area YieldStuck Another vehicle stops to yield ego Ego stops before the intersection or the boundary of attention area NonOccludedCollisionStop Ego detects no occlusion but detects collision Ego stops at the default_stop_line FirstWaitBeforeOcclusion Ego detected occlusion when entering the intersection Ego stops at the default_stop_line at first PeekingTowardOcclusion Ego detected occlusion and but no collision within the FOV (after FirstWaitBeforeOcclusion) Ego approaches the boundary of the attention area slowly OccludedCollisionStop Ego detected both occlusion and collision (after FirstWaitBeforeOcclusion) Ego stops immediately FullyPrioritized Ego is fully prioritized by the RED/Arrow signal Ego only cares vehicles still running inside the intersection. Occlusion is ignored OverPassJudgeLine Ego is already inside the attention area and/or cannot stop before the boundary of attention area Ego does not detect collision/occlusion anymore and passes the intersection <p></p>"},{"location":"planning/behavior_velocity_intersection_module/#stuck-vehicle-detection","title":"Stuck Vehicle Detection","text":"<p>If there is any object on the path inside the intersection and at the exit of the intersection (up to <code>stuck_vehicle.stuck_vehicle_detect_dist</code>) lane and its velocity is less than the threshold (<code>stuck_vehicle.stuck_vehicle_velocity_threshold</code>), the object is regarded as a stuck vehicle. If stuck vehicles exist, this module inserts a stopline a certain distance (=<code>default_stopline_margin</code>) before the overlapped region with other lanes. The stuck vehicle detection area is generated based on the planned path, so the stuck vehicle stopline is not inserted if the upstream module generated an avoidance path.</p> <p></p>"},{"location":"planning/behavior_velocity_intersection_module/#collision-detection","title":"Collision detection","text":"<p>The following process is performed for the targets objects to determine whether ego can pass the intersection safely. If it is judged that ego cannot pass the intersection with enough margin, this module inserts a stopline on the path.</p> <ol> <li>predict the time \\(t\\) when the object intersects with ego path for the first time from the predicted path time step. Only the predicted whose confidence is greater than <code>collision_detection.min_predicted_path_confidence</code> is used.</li> <li>detect collision between the predicted path and ego's predicted path in the following process<ol> <li>calculate the collision interval of [\\(t\\) - <code>collision_detection.collision_start_margin_time</code>, \\(t\\) + <code>collision_detection.collision_end_margin_time</code>]</li> <li>calculate the passing area of ego during the collision interval from the array of (time, distance) obtained by smoothed velocity profile</li> <li>check if ego passing area and object predicted path interval collides</li> </ol> </li> <li>if collision is detected, the module inserts a stopline</li> <li>if ego is over the pass_judge_line, collision checking is skipped to avoid sudden braking and/or unnecessary stop in the middle of the intersection</li> </ol> <p>The parameters <code>collision_detection.collision_start_margin_time</code> and <code>collision_detection.collision_end_margin_time</code> can be interpreted as follows:</p> <ul> <li>If ego was to pass the intersection earlier than the target object, collision would be detected if the time difference between the two was less than <code>collision_detection.collision_start_margin_time</code>.</li> <li>If ego was to pass the intersection later than the target object, collision would be detected if the time difference between the two was less than <code>collision_detection.collision_end_margin_time</code>.</li> </ul> <p>If collision is detected, the state transits to \"STOP\" immediately. On the other hand, the state does not transit to \"GO\" unless safe judgement continues for a certain period <code>collision_detection.collision_detection_hold_time</code> to prevent the chattering of decisions.</p> <p>Currently, the intersection module uses <code>motion_velocity_smoother</code> feature to precisely calculate ego velocity profile along the intersection lane under longitudinal/lateral constraints. If the flag <code>collision_detection.velocity_profile.use_upstream</code> is true, the target velocity profile of the original path is used. Otherwise the target velocity is set to <code>collision.velocity_profile.default_velocity</code>. In the trajectory smoothing process the target velocity at/before ego trajectory points are set to ego current velocity. The smoothed trajectory is then converted to an array of (time, distance) which indicates the arrival time to each trajectory point on the path from current ego position. You can visualize this array by adding the lane id to <code>debug.ttc</code> and running</p> <pre><code>ros2 run behavior_velocity_intersection_module ttc.py --lane_id &lt;lane_id&gt;\n</code></pre> <p></p>"},{"location":"planning/behavior_velocity_intersection_module/#occlusion-detection","title":"Occlusion detection","text":"<p>If the flag <code>occlusion.enable</code> is true this module checks if there is sufficient field of view (FOV) on the attention area up to <code>occlusion.occlusion_attention_area_length</code>. If FOV is not clear enough ego first makes a brief stop at the default stop line for <code>occlusion.temporal_stop_time_before_peeking</code>, and then slowly creeps toward occlusion_peeking_stop_line. If <code>occlusion.creep_during_peeking.enable</code> is true <code>occlusion.creep_during_peeking.creep_velocity</code> is inserted up to occlusion_peeking_stop_line. Otherwise only stop line is inserted.</p> <p>During the creeping if collision is detected this module inserts a stop line in front of ego immediately, and if the FOV gets sufficiently clear the intersection_occlusion wall will disappear. If occlusion is cleared and no collision is detected ego will pass the intersection.</p> <p>The occlusion is detected as the common area of occlusion attention area(which is partially the same as the normal attention area) and the unknown cells of the occupancy grid map. The occupancy grid map is denoised using morphology with the window size of <code>occlusion.denoise_kernel</code>. The occlusion attention area lanes are discretized to line strings and they are used to generate a grid whose each cell represents the distance from ego path along the lane as shown below.</p> <p></p> <p>If the nearest occlusion cell value is below the threshold <code>occlusion.occlusion_required_clearance_distance</code>, it means that the FOV of ego is not clear. It is expected that the occlusion gets cleared as the vehicle approaches the occlusion peeking stop line.</p>"},{"location":"planning/behavior_velocity_intersection_module/#occlusion-source-estimation-at-intersection-with-traffic-light","title":"Occlusion source estimation at intersection with traffic light","text":"<p>At intersection with traffic light, the whereabout of occlusion is estimated by checking if there are any objects between ego and the nearest occlusion cell. While the occlusion is estimated to be caused by some object (DYNAMICALLY occluded), intersection_wall appears at all times. If no objects are found between ego and the nearest occlusion cell (STATICALLY occluded), after ego stopped for the duration of <code>occlusion.static_occlusion_with_traffic_light_timeout</code> plus <code>occlusion.occlusion_detection_hold_time</code>, occlusion is intentionally ignored to avoid stuck.</p> <p></p> <p>The remaining time is visualized on the intersection_occlusion virtual wall.</p> <p></p>"},{"location":"planning/behavior_velocity_intersection_module/#occlusion-handling-at-intersection-without-traffic-light","title":"Occlusion handling at intersection without traffic light","text":"<p>At intersection without traffic light, if occlusion is detected, ego makes a brief stop at the default_stopline and first_attention_stopline respectively. After stopping at the first_attention_area_stopline this module inserts <code>occlusion.absence_traffic_light.creep_velocity</code> velocity between ego and occlusion_wo_tl_pass_judge_line while occlusion is not cleared. If collision is detected, ego immediately stops. Once the occlusion is cleared or ego has passed occlusion_wo_tl_pass_judge_line this module does not detect collision and occlusion because ego footprint is already inside the intersection.</p> <p></p> <p>While ego is creeping, yellow intersection_wall appears in front ego.</p> <p></p>"},{"location":"planning/behavior_velocity_intersection_module/#traffic-signal-specific-behavior","title":"Traffic signal specific behavior","text":""},{"location":"planning/behavior_velocity_intersection_module/#collision-detection_1","title":"Collision detection","text":"<p>TTC parameter varies depending on the traffic light color/shape as follows.</p> traffic light color ttc(start) ttc(end) GREEN <code>collision_detection.not_prioritized.collision_start_margin</code> <code>collision_detection.not_prioritized.collision_end_margin</code> AMBER <code>collision_detection.partially_prioritized.collision_start_end_margin</code> <code>collision_detection.partially_prioritized.collision_start_end_margin</code> RED / Arrow <code>collision_detection.fully_prioritized.collision_start_end_margin</code> <code>collision_detection.fully_prioritized.collision_start_end_margin</code>"},{"location":"planning/behavior_velocity_intersection_module/#yield-on-green","title":"yield on GREEN","text":"<p>If the traffic light color changed to GREEN and ego approached the entry of the intersection lane within the distance <code>collision_detection.yield_on_green_traffic_light.distance_to_assigned_lanelet_start</code> and there is any object whose distance to its stopline is less than <code>collision_detection.yield_on_green_traffic_light.object_dist_to_stopline</code>, this module commands to stop for the duration of <code>collision_detection.yield_on_green_traffic_light.duration</code> at the default_stopline.</p>"},{"location":"planning/behavior_velocity_intersection_module/#skip-on-amber","title":"skip on AMBER","text":"<p>If the traffic light color is AMBER but the object is expected to stop before its stopline under the deceleration of <code>collision_detection.ignore_on_amber_traffic_light.object_expected_deceleration</code>, collision checking is skipped.</p>"},{"location":"planning/behavior_velocity_intersection_module/#skip-on-red","title":"skip on RED","text":"<p>If the traffic light color is RED or Arrow signal is turned on, the attention lanes which are not conflicting with ego lane are not used for detection. And even if the object stops with a certain overshoot from its stopline, but its expected stop position under the deceleration of <code>collision_detection.ignore_on_amber_traffic_light.object_expected_deceleration</code> is more than the distance <code>collision_detection.ignore_on_red_traffic_light.object_margin_to_path</code> from collision point, the object is ignored.</p>"},{"location":"planning/behavior_velocity_intersection_module/#occlusion-detection_1","title":"Occlusion detection","text":"<p>When the traffic light color/shape is RED/Arrow, occlusion detection is skipped.</p> <p></p>"},{"location":"planning/behavior_velocity_intersection_module/#pass-judge-line","title":"Pass Judge Line","text":"<p>To avoid sudden braking, if deceleration and jerk more than the threshold (<code>common.max_accel</code> and <code>common.max_jerk</code>) is required to stop at first_attention_stopline, this module does not command to stop once it passed the default_stopline position.</p> <p>If ego passed pass_judge_line, then ego does not stop anymore. If ego passed pass_judge_line while ego is stopping for dangerous decision, then ego stops while the situation is judged as dangerous. Once the judgement turned safe, ego restarts and does not stop anymore.</p> <p>The position of the pass judge line depends on the occlusion detection configuration and the existence of the associated traffic light of the intersection lane.</p> <ul> <li>If <code>occlusion.enable</code> is false, the pass judge line before the <code>first_attention_stopline</code> by the braking distance \\(v_{ego}^{2} / 2a_{max}\\).</li> <li>If <code>occlusion.enable</code> is true and:<ul> <li>if there are associated traffic lights, the pass judge line is at the <code>occlusion_peeking_stopline</code> in order to continue peeking/collision detection while occlusion is detected.</li> <li>if there are no associated traffic lights and:<ul> <li>if occlusion is detected, pass judge line is at the <code>occlusion_wo_tl_pass_judge_line</code> to continue peeking.</li> <li>if occlusion is not detected, pass judge line is at the same place at the case where <code>occlusion.enable</code> is false.</li> </ul> </li> </ul> </li> </ul>"},{"location":"planning/behavior_velocity_intersection_module/#data-structure","title":"Data Structure","text":"<p>Each data structure is defined in <code>util_type.hpp</code>.</p> <p></p>"},{"location":"planning/behavior_velocity_intersection_module/#intersectionlanelets","title":"<code>IntersectionLanelets</code>","text":""},{"location":"planning/behavior_velocity_intersection_module/#intersectionstoplines","title":"<code>IntersectionStopLines</code>","text":"<p>Each stop lines are generated from interpolated path points to obtain precise positions.</p> <p></p>"},{"location":"planning/behavior_velocity_intersection_module/#targetobject","title":"<code>TargetObject</code>","text":"<p><code>TargetObject</code> holds the object, its belonging lane and corresponding stopline information.</p> <p></p>"},{"location":"planning/behavior_velocity_intersection_module/#module-parameters","title":"Module Parameters","text":""},{"location":"planning/behavior_velocity_intersection_module/#common","title":"common","text":"Parameter Type Description <code>.attention_area_length</code> double [m] range for object detection <code>.attention_area_margin</code> double [m] margin for expanding attention area width <code>.attention_area_angle_threshold</code> double [rad] threshold of angle difference between the detected object and lane <code>.use_intersection_area</code> bool [-] flag to use intersection_area for collision detection <code>.default_stopline_margin</code> double [m] margin before_stop_line <code>.stopline_overshoot_margin</code> double [m] margin for the overshoot from stopline <code>.max_accel</code> double [m/ss] max acceleration for stop <code>.max_jerk</code> double [m/sss] max jerk for stop <code>.delay_response_time</code> double [s] action delay before stop"},{"location":"planning/behavior_velocity_intersection_module/#stuck_vehicleyield_stuck","title":"stuck_vehicle/yield_stuck","text":"Parameter Type Description <code>stuck_vehicle.turn_direction</code> - [-] turn_direction specifier for stuck vehicle detection <code>stuck_vehicle.stuck_vehicle_detect_dist</code> double [m] length toward from the exit of intersection for stuck vehicle detection <code>stuck_vehicle.stuck_vehicle_velocity_threshold</code> double [m/s] velocity threshold for stuck vehicle detection <code>yield_stuck.distance_threshold</code> double [m/s] distance threshold of yield stuck vehicle from ego path along the lane"},{"location":"planning/behavior_velocity_intersection_module/#collision_detection","title":"collision_detection","text":"Parameter Type Description <code>.consider_wrong_direction_vehicle</code> bool [-] flag to detect objects in the wrong direction <code>.collision_detection_hold_time</code> double [s] hold time of collision detection <code>.min_predicted_path_confidence</code> double [-] minimum confidence value of predicted path to use for collision detection <code>.keep_detection_velocity_threshold</code> double [s] ego velocity threshold for continuing collision detection before pass judge line <code>.velocity_profile.use_upstream</code> bool [-] flag to use velocity profile planned by upstream modules <code>.velocity_profile.minimum_upstream_velocity</code> double [m/s] minimum velocity of upstream velocity profile to avoid zero division <code>.velocity_profile.default_velocity</code> double [m/s] constant velocity profile when use_upstream is false <code>.velocity_profile.minimum_default_velocity</code> double [m/s] minimum velocity of default velocity profile to avoid zero division <code>.yield_on_green_traffic_light</code> - [-] description <code>.ignore_amber_traffic_light</code> - [-] description <code>.ignore_on_red_traffic_light</code> - [-] description"},{"location":"planning/behavior_velocity_intersection_module/#occlusion","title":"occlusion","text":"Parameter Type Description <code>.enable</code> bool [-] flag to calculate occlusion detection <code>.occlusion_attention_area_length</code> double [m] the length of attention are for occlusion detection <code>.free_space_max</code> int [-] maximum value of occupancy grid cell to treat at occluded <code>.occupied_min</code> int [-] minimum value of occupancy grid cell to treat at occluded <code>.denoise_kernel</code> double [m] morphology window size for preprocessing raw occupancy grid <code>.attention_lane_crop_curvature_threshold</code> double [m] curvature threshold for trimming curved part of the lane <code>.attention_lane_crop_curvature_ds</code> double [m] discretization interval of centerline for lane curvature calculation <code>.creep_during_peeking.enable</code> bool [-] flag to insert <code>creep_velocity</code> while peeking to intersection occlusion stopline <code>.creep_during_peeking.creep_velocity</code> double [m/s] the command velocity while peeking to intersection occlusion stopline <code>.peeking_offset</code> double [m] the offset of the front of the vehicle into the attention area for peeking to occlusion <code>.occlusion_required_clearance_distance</code> double [m] threshold for the distance to nearest occlusion cell from ego path <code>.possible_object_bbox</code> [double] [m] minimum bounding box size for checking if occlusion polygon is small enough <code>.ignore_parked_vehicle_speed_threshold</code> double [m/s] velocity threshold for checking parked vehicle <code>.occlusion_detection_hold_time</code> double [s] hold time of occlusion detection <code>.temporal_stop_time_before_peeking</code> double [s] temporal stop duration at the default_stop_line before starting peeking <code>.temporal_stop_before_attention_area</code> bool [-] flag to temporarily stop at first_attention_stopline before peeking into attention_area <code>.creep_velocity_without_traffic_light</code> double [m/s] creep velocity to occlusion_wo_tl_pass_judge_line <code>.static_occlusion_with_traffic_light_timeout</code> double [s] the timeout duration for ignoring static occlusion at intersection with traffic light"},{"location":"planning/behavior_velocity_intersection_module/#trouble-shooting","title":"Trouble shooting","text":""},{"location":"planning/behavior_velocity_intersection_module/#intersection-module-stops-against-unrelated-vehicles","title":"Intersection module stops against unrelated vehicles","text":"<p>In this case, first visualize <code>/planning/scenario_planning/lane_driving/behavior_planning/behavior_velocity_planner/debug/intersection</code> topic and check the <code>attention_area</code> polygon. Intersection module performs collision checking for vehicles running on this polygon, so if it extends to unintended lanes, it needs to have RightOfWay tag.</p> <p>By lowering <code>common.attention_area_length</code> you can check which lanes are conflicting with the intersection lane. Then set part of the conflicting lanes as the yield_lane.</p>"},{"location":"planning/behavior_velocity_intersection_module/#the-stop-line-of-intersection-is-chattering","title":"The stop line of intersection is chattering","text":"<p>The parameter <code>collision_detection.collision_detection_hold_time</code> suppresses the chattering by keeping UNSAFE decision for this duration until SAFE decision is finally made. The role of this parameter is to account for unstable detection/tracking of objects. By increasing this value you can suppress the chattering. However it could elongate the stopping duration excessively.</p> <p>If the chattering arises from the acceleration/deceleration of target vehicles, increase <code>collision_detection.collision_detection.collision_end_margin_time</code> and/or <code>collision_detection.collision_detection.collision_end_margin_time</code>.</p>"},{"location":"planning/behavior_velocity_intersection_module/#the-stop-line-is-released-too-fastslow","title":"The stop line is released too fast/slow","text":"<p>If the intersection wall appears too fast, or ego tends to stop too conservatively for upcoming vehicles, lower the parameter <code>collision_detection.collision_detection.collision_start_margin_time</code>. If it lasts too long after the target vehicle passed, then lower the parameter <code>collision_detection.collision_detection.collision_end_margin_time</code>.</p>"},{"location":"planning/behavior_velocity_intersection_module/#ego-suddenly-stops-at-intersection-with-traffic-light","title":"Ego suddenly stops at intersection with traffic light","text":"<p>If the traffic light color changed from AMBER/RED to UNKNOWN, the intersection module works in the GREEN color mode. So collision and occlusion are likely to be detected again.</p>"},{"location":"planning/behavior_velocity_intersection_module/#occlusion-is-detected-overly","title":"Occlusion is detected overly","text":"<p>You can check which areas are detected as occlusion by visualizing <code>/planning/scenario_planning/lane_driving/behavior_planning/behavior_velocity_planner/debug/intersection/occlusion_polygons</code>.</p> <p>If you do not want to detect / do want to ignore occlusion far from ego or lower the computational cost of occlusion detection, <code>occlusion.occlusion_attention_area_length</code> should be set to lower value.</p> <p>If you want to care the occlusion nearby ego more cautiously, set <code>occlusion.occlusion_required_clearance_distance</code> to a larger value. Then ego will approach the occlusion_peeking_stopline more closely to assure more clear FOV.</p> <p><code>occlusion.possible_object_bbox</code> is used for checking if detected occlusion area is small enough that no vehicles larger than this size can exist inside. By decreasing this size ego will ignore small occluded area.</p>"},{"location":"planning/behavior_velocity_intersection_module/#occupancy-grid-map-tuning","title":"occupancy grid map tuning","text":"<p>Refer to the document of probabilistic_occupancy_grid_map for details. If occlusion tends to be detected at apparently free space, increase <code>occlusion.free_space_max</code> to ignore them.</p>"},{"location":"planning/behavior_velocity_intersection_module/#in-simple_planning_simulator","title":"in simple_planning_simulator","text":"<p>intersection_occlusion feature is not recommended for use in planning_simulator because the laserscan_based_occupancy_grid_map generates unnatural UNKNOWN cells in 2D manner:</p> <ul> <li>all the cells behind pedestrians are UNKNOWN</li> <li>no ground point clouds are generated</li> </ul> <p>Also many users do not set traffic light information frequently although it is very critical for intersection_occlusion (and in real traffic environment too).</p> <p>For these reasons, <code>occlusion.enable</code> is false by default.</p>"},{"location":"planning/behavior_velocity_intersection_module/#on-real-vehicle-in-end-to-end-simulator","title":"on real vehicle / in end-to-end simulator","text":"<p>On real vehicle or in end-to-end simulator like AWSIM the following pointcloud_based_occupancy_grid_map configuration is highly recommended:</p> <pre><code>scan_origin_frame: \"velodyne_top\"\n\ngrid_map_type: \"OccupancyGridMapProjectiveBlindSpot\"\nOccupancyGridMapProjectiveBlindSpot:\nprojection_dz_threshold: 0.01 # [m] for avoiding null division\nobstacle_separation_threshold: 1.0 # [m] fill the interval between obstacles with unknown for this length\n</code></pre> <p>You should set the top lidar link as the <code>scan_origin_frame</code>. In the example it is <code>velodyne_top</code>. The method <code>OccupancyGridMapProjectiveBlindSpot</code> estimates the FOV by running projective ray-tracing from <code>scan_origin</code> to obstacle or up to the ground and filling the cells on the \"shadow\" of the object as UNKNOWN.</p>"},{"location":"planning/behavior_velocity_intersection_module/#flowchart","title":"Flowchart","text":"<p>WIP</p> <p></p>"},{"location":"planning/behavior_velocity_intersection_module/#merge-from-private","title":"Merge From Private","text":""},{"location":"planning/behavior_velocity_intersection_module/#role_1","title":"Role","text":"<p>When an ego enters a public road from a private road (e.g. a parking lot), it needs to face and stop before entering the public road to make sure it is safe.</p> <p>This module is activated when there is an intersection at the private area from which the vehicle enters the public road. The stop line is generated both when the goal is in the intersection lane and when the path goes beyond the intersection lane. The basic behavior is the same as the intersection module, but ego must stop once at the stop line.</p> <p></p>"},{"location":"planning/behavior_velocity_intersection_module/#activation-timing","title":"Activation Timing","text":"<p>This module is activated when the following conditions are met:</p> <ul> <li>ego-lane has a <code>private</code> tag</li> <li>ego-lane has a conflict with other no-private lanelets</li> </ul>"},{"location":"planning/behavior_velocity_intersection_module/#module-parameters_1","title":"Module Parameters","text":"Parameter Type Description <code>merge_from_private_road/stop_duration_sec</code> double [m] time margin to change state"},{"location":"planning/behavior_velocity_intersection_module/#known-issue","title":"Known Issue","text":"<p>If ego go over the stop line for a certain distance, then it will not transit from STOP.</p>"},{"location":"planning/behavior_velocity_no_drivable_lane_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_no_drivable_lane_module/#no-drivable-lane","title":"No Drivable Lane","text":""},{"location":"planning/behavior_velocity_no_drivable_lane_module/#role","title":"Role","text":"<p>This module plans the velocity of the related part of the path in case there is a no drivable lane referring to it.</p> <p>A no drivable lane is a lanelet or more that are out of operation design domain (ODD), i.e., the vehicle must not drive autonomously in this lanelet. A lanelet can be no drivable (out of ODD) due to many reasons, either technical limitations of the SW and/or HW, business requirements, safety considerations, .... etc, or even a combination of those.</p> <p>Some examples of No Drivable Lanes</p> <ul> <li>Closed road intentionally, due to construction work for example</li> <li>Underpass road that goes under a railway, for safety reasons</li> <li>Road with slope/inclination that the vehicle is not be able to drive autonomously due to technical limitations. And lots of other examples.</li> </ul> <p></p> <p>A lanelet becomes invalid by adding a new tag under the relevant lanelet in the map file <code>&lt;tag k=\"no_drivable_lane\" v=\"yes\"/&gt;</code>.</p> <p>The target of this module is to stop the vehicle before entering the no drivable lane (with configurable stop margin) or keep the vehicle stationary if autonomous mode started inside a no drivable lane. Then ask the human driver to take the responsibility of the driving task (Takeover Request / Request to Intervene)</p>"},{"location":"planning/behavior_velocity_no_drivable_lane_module/#activation-timing","title":"Activation Timing","text":"<p>This function is activated when the lane id of the target path has an no drivable lane label (i.e. the <code>no_drivable_lane</code> attribute is <code>yes</code>).</p>"},{"location":"planning/behavior_velocity_no_drivable_lane_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>stop_margin</code> double [m] margin for ego vehicle to stop before speed_bump <code>print_debug_info</code> bool whether debug info will be printed or not"},{"location":"planning/behavior_velocity_no_drivable_lane_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ul> <li>Get no_drivable_lane attribute on the path from lanelet2 map</li> <li>The no drivable lane state machine starts in <code>INIT</code> state</li> <li>Get the intersection points between path and no drivable lane polygon</li> <li>Assign the state to <code>APPROACHING</code> toward a no drivable lane if:<ul> <li>the distance from front of the ego vehicle till the first intersection point between the ego path and the no drivable lane polygon is more than the <code>stop_margin</code></li> </ul> </li> <li>Assign the state to <code>INSIDE_NO_DRIVABLE_LANE</code> if:<ul> <li>the first point of the ego path is inside the no drivable lane polygon, or</li> <li>the distance from front of the ego vehicle till the first intersection point between the ego path and the no drivable lane polygon is less than the <code>stop_margin</code></li> </ul> </li> <li>Assign the state to <code>STOPPED</code> when the vehicle is completely stopped</li> </ul>"},{"location":"planning/behavior_velocity_no_drivable_lane_module/#future-work","title":"Future Work","text":"<ul> <li>As Request to Intervene API is not implemented yet, this will be handled to notify the driver to takeover the driving task responsibility after the vehicle stops due to <code>no_drivable_lane</code></li> <li>Handle the case when the vehicle stops before a no drivable lane but part of its footprint intersects with the no drivable lane polygon.</li> </ul>"},{"location":"planning/behavior_velocity_no_stopping_area_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_no_stopping_area_module/#no-stopping-area","title":"No Stopping Area","text":""},{"location":"planning/behavior_velocity_no_stopping_area_module/#role","title":"Role","text":"<p>This module plans to avoid stop in 'no stopping area`.</p> <p></p> <ul> <li>PassThrough case<ul> <li>if ego vehicle go through pass judge point, then ego vehicle can't stop with maximum jerk and acceleration, so this module doesn't insert stop velocity. In this case override or external operation is necessary.</li> </ul> </li> <li>STOP case<ul> <li>If there is a stuck vehicle or stop velocity around <code>no_stopping_area</code>, then vehicle stops inside <code>no_stopping_area</code> so this module makes stop velocity in front of <code>no_stopping_area</code></li> </ul> </li> <li>GO case<ul> <li>else</li> </ul> </li> </ul>"},{"location":"planning/behavior_velocity_no_stopping_area_module/#limitation","title":"Limitation","text":"<p>This module allows developers to design vehicle velocity in <code>no_stopping_area</code> module using specific rules. Once ego vehicle go through pass through point, ego vehicle does't insert stop velocity and does't change decision from GO. Also this module only considers dynamic object in order to avoid unnecessarily stop.</p>"},{"location":"planning/behavior_velocity_no_stopping_area_module/#modelparameter","title":"ModelParameter","text":"Parameter Type Description <code>state_clear_time</code> double [s] time to clear stop state <code>stuck_vehicle_vel_thr</code> double [m/s] vehicles below this velocity are considered as stuck vehicle. <code>stop_margin</code> double [m] margin to stop line at no stopping area <code>dead_line_margin</code> double [m] if ego pass this position GO <code>stop_line_margin</code> double [m] margin to auto-gen stop line at no stopping area <code>detection_area_length</code> double [m] length of searching polygon <code>stuck_vehicle_front_margin</code> double [m] obstacle stop max distance"},{"location":"planning/behavior_velocity_no_stopping_area_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_velocity_occlusion_spot_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_occlusion_spot_module/#occlusion-spot","title":"Occlusion Spot","text":""},{"location":"planning/behavior_velocity_occlusion_spot_module/#role","title":"Role","text":"<p>This module plans safe velocity to slow down before reaching collision point that hidden object is darting out from <code>occlusion spot</code> where driver can't see clearly because of obstacles.</p> <p></p>"},{"location":"planning/behavior_velocity_occlusion_spot_module/#activation-timing","title":"Activation Timing","text":"<p>This module is activated if <code>launch_occlusion_spot</code> becomes true. To make pedestrian first zone map tag is one of the TODOs.</p>"},{"location":"planning/behavior_velocity_occlusion_spot_module/#limitation-and-todos","title":"Limitation and TODOs","text":"<p>This module is prototype implementation to care occlusion spot. To solve the excessive deceleration due to false positive of the perception, the logic of detection method can be selectable. This point has not been discussed in detail and needs to be improved.</p> <ul> <li>Make occupancy grid for planning.</li> <li>Make map tag for occlusion spot.</li> <li>About the best safe motion.</li> </ul> <p>TODOs are written in each Inner-workings / Algorithms (see the description below).</p>"},{"location":"planning/behavior_velocity_occlusion_spot_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/behavior_velocity_occlusion_spot_module/#logics-working","title":"Logics Working","text":"<p>There are several types of occlusions, such as \"occlusions generated by parked vehicles\" and \"occlusions caused by obstructions\". In situations such as driving on road with obstacles, where people jump out of the way frequently, all possible occlusion spots must be taken into account. This module considers all occlusion spots calculated from the occupancy grid, but it is not reasonable to take into account all occlusion spots for example, people jumping out from behind a guardrail, or behind cruising vehicle. Therefore currently detection area will be limited to to use predicted object information.</p> <p>Note that this decision logic is still under development and needs to be improved.</p>"},{"location":"planning/behavior_velocity_occlusion_spot_module/#detectionarea-polygon","title":"DetectionArea Polygon","text":"<p>This module considers TTV from pedestrian velocity and lateral distance to occlusion spot. TTC is calculated from ego velocity and acceleration and longitudinal distance until collision point using motion velocity smoother. To compute fast this module only consider occlusion spot whose TTV is less than TTC and only consider area within \"max lateral distance\".</p> <p></p>"},{"location":"planning/behavior_velocity_occlusion_spot_module/#occlusion-spot-occupancy-grid-base","title":"Occlusion Spot Occupancy Grid Base","text":"<p>This module considers any occlusion spot around ego path computed from the occupancy grid. Due to the computational cost occupancy grid is not high resolution and this will make occupancy grid noisy so this module add information of occupancy to occupancy grid map.</p> <p>TODO: consider hight of obstacle point cloud to generate occupancy grid.</p>"},{"location":"planning/behavior_velocity_occlusion_spot_module/#collision-free-judgement","title":"Collision Free Judgement","text":"<p>obstacle that can run out from occlusion should have free space until intersection from ego vehicle</p> <p></p>"},{"location":"planning/behavior_velocity_occlusion_spot_module/#partition-lanelet","title":"Partition Lanelet","text":"<p>By using lanelet information of \"guard_rail\", \"fence\", \"wall\" tag, it's possible to remove unwanted occlusion spot.</p> <p>By using static object information, it is possible to make occupancy grid more accurate.</p> <p>To make occupancy grid for planning is one of the TODOs.</p> <p></p>"},{"location":"planning/behavior_velocity_occlusion_spot_module/#possible-collision","title":"Possible Collision","text":"<p>obstacle that can run out from occlusion is interrupted by moving vehicle.</p> <p></p>"},{"location":"planning/behavior_velocity_occlusion_spot_module/#about-safe-motion","title":"About safe motion","text":""},{"location":"planning/behavior_velocity_occlusion_spot_module/#the-concept-of-safe-velocity-and-margin","title":"The Concept of Safe Velocity and Margin","text":"<p>The safe slowdown velocity is calculated from the below parameters of ego emergency braking system and time to collision. Below calculation is included but change velocity dynamically is not recommended for planner.</p> <ul> <li>jerk limit[m/s^3]</li> <li>deceleration limit[m/s2]</li> <li>delay response time[s]</li> <li> <p>time to collision of pedestrian[s]   with these parameters we can briefly define safe motion before occlusion spot for ideal environment.</p> <p></p> </li> </ul> <p>This module defines safe margin to consider ego distance to stop and collision path point geometrically. While ego is cruising from safe margin to collision path point, ego vehicle keeps the same velocity as occlusion spot safe velocity.</p> <p></p> <p>Note: This logic assumes high-precision vehicle speed tracking and margin for decel point might not be the best solution, and override with manual driver is considered if pedestrian really run out from occlusion spot.</p> <p>TODO: consider one of the best choices</p> <ol> <li>stop in front of occlusion spot</li> <li>insert 1km/h velocity in front of occlusion spot</li> <li>slowdown this way</li> <li>etc... .</li> </ol>"},{"location":"planning/behavior_velocity_occlusion_spot_module/#maximum-slowdown-velocity","title":"Maximum Slowdown Velocity","text":"<p>The maximum slowdown velocity is calculated from the below parameters of ego current velocity and acceleration with maximum slowdown jerk and maximum slowdown acceleration in order not to slowdown too much.</p> <ul> <li>\\(j_{max}\\) slowdown jerk limit[m/s^3]</li> <li>\\(a_{max}\\) slowdown deceleration limit[m/s2]</li> <li>\\(v_{0}\\) current velocity[m/s]</li> <li>\\(a_{0}\\) current acceleration[m/s]</li> </ul> <p></p>"},{"location":"planning/behavior_velocity_occlusion_spot_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>pedestrian_vel</code> double [m/s] maximum velocity assumed pedestrian coming out from occlusion point. <code>pedestrian_radius</code> double [m] assumed pedestrian radius which fits in occlusion spot. Parameter Type Description <code>use_object_info</code> bool [-] whether to reflect object info to occupancy grid map or not. <code>use_partition_lanelet</code> bool [-] whether to use partition lanelet map data. Parameter /debug Type Description <code>is_show_occlusion</code> bool [-] whether to show occlusion point markers.\u3000 <code>is_show_cv_window</code> bool [-] whether to show open_cv debug window. <code>is_show_processing_time</code> bool [-] whether to show processing time. Parameter /threshold Type Description <code>detection_area_length</code> double [m] the length of path to consider occlusion spot <code>stuck_vehicle_vel</code> double [m/s] velocity below this value is assumed to stop <code>lateral_distance</code> double [m] maximum lateral distance to consider hidden collision Parameter /motion Type Description <code>safety_ratio</code> double [-] safety ratio for jerk and acceleration <code>max_slow_down_jerk</code> double [m/s^3] jerk for safe brake <code>max_slow_down_accel</code> double [m/s^2] deceleration for safe brake <code>non_effective_jerk</code> double [m/s^3] weak jerk for velocity planning. <code>non_effective_acceleration</code> double [m/s^2] weak deceleration for velocity planning. <code>min_allowed_velocity</code> double [m/s] minimum velocity allowed <code>safe_margin</code> double [m] maximum error to stop with emergency braking system. Parameter /detection_area Type Description <code>min_occlusion_spot_size</code> double [m] the length of path to consider occlusion spot <code>slice_length</code> double [m] the distance of divided detection area <code>max_lateral_distance</code> double [m] buffer around the ego path used to build the detection_area area. Parameter /grid Type Description <code>free_space_max</code> double [-] maximum value of a free space cell in the occupancy grid <code>occupied_min</code> double [-] buffer around the ego path used to build the detection_area area."},{"location":"planning/behavior_velocity_occlusion_spot_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_velocity_occlusion_spot_module/#rough-overview-of-the-whole-process","title":"Rough overview of the whole process","text":""},{"location":"planning/behavior_velocity_occlusion_spot_module/#detail-process-for-predicted-objectnot-updated","title":"Detail process for predicted object(not updated)","text":""},{"location":"planning/behavior_velocity_occlusion_spot_module/#detail-process-for-occupancy-grid-base","title":"Detail process for Occupancy grid base","text":""},{"location":"planning/behavior_velocity_out_of_lane_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_out_of_lane_module/#out-of-lane","title":"Out of Lane","text":""},{"location":"planning/behavior_velocity_out_of_lane_module/#role","title":"Role","text":"<p><code>out_of_lane</code> is the module that decelerates and stops to prevent the ego vehicle from entering another lane with incoming dynamic objects.</p>"},{"location":"planning/behavior_velocity_out_of_lane_module/#activation-timing","title":"Activation Timing","text":"<p>This module is activated if <code>launch_out_of_lane</code> is set to true.</p>"},{"location":"planning/behavior_velocity_out_of_lane_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The algorithm is made of the following steps.</p> <ol> <li>Calculate the ego path footprints (red).</li> <li>Calculate the other lanes (magenta).</li> <li>Calculate the overlapping ranges between the ego path footprints and the other lanes (green).</li> <li>For each overlapping range, decide if a stop or slow down action must be taken.</li> <li>For each action, insert the corresponding stop or slow down point in the path.</li> </ol> <p></p>"},{"location":"planning/behavior_velocity_out_of_lane_module/#1-ego-path-footprints","title":"1. Ego Path Footprints","text":"<p>In this first step, the ego footprint is projected at each path point and are eventually inflated based on the <code>extra_..._offset</code> parameters.</p>"},{"location":"planning/behavior_velocity_out_of_lane_module/#2-other-lanes","title":"2. Other lanes","text":"<p>In the second step, the set of lanes to consider for overlaps is generated. This set is built by selecting all lanelets within some distance from the ego vehicle, and then removing non-relevant lanelets. The selection distance is chosen as the maximum between the <code>slowdown.distance_threshold</code> and the <code>stop.distance_threshold</code>.</p> <p>A lanelet is deemed non-relevant if it meets one of the following conditions.</p> <ul> <li>It is part of the lanelets followed by the ego path.</li> <li>It contains the rear point of the ego footprint.</li> <li>It follows one of the ego path lanelets.</li> </ul>"},{"location":"planning/behavior_velocity_out_of_lane_module/#3-overlapping-ranges","title":"3. Overlapping ranges","text":"<p>In the third step, overlaps between the ego path footprints and the other lanes are calculated. For each pair of other lane \\(l\\) and ego path footprint \\(f\\), we calculate the overlapping polygons using <code>boost::geometry::intersection</code>. For each overlapping polygon found, if the distance inside the other lane \\(l\\) is above the <code>overlap.minimum_distance</code> threshold, then the overlap is ignored. Otherwise, the arc length range (relative to the ego path) and corresponding points of the overlapping polygons are stored. Ultimately, for each other lane \\(l\\), overlapping ranges of successive overlaps are built with the following information:</p> <ul> <li>overlapped other lane \\(l\\).</li> <li>start and end ego path indexes.</li> <li>start and end ego path arc lengths.</li> <li>start and end overlap points.</li> </ul>"},{"location":"planning/behavior_velocity_out_of_lane_module/#4-decisions","title":"4. Decisions","text":"<p>In the fourth step, a decision to either slow down or stop before each overlapping range is taken based on the dynamic objects. The conditions for the decision depend on the value of the <code>mode</code> parameter.</p> <p>Whether it is decided to slow down or stop is determined by the distance between the ego vehicle and the start of the overlapping range (in arc length along the ego path). If this distance is bellow the <code>actions.slowdown.threshold</code>, a velocity of <code>actions.slowdown.velocity</code> will be used. If the distance is bellow the <code>actions.stop.threshold</code>, a velocity of <code>0</code>m/s will be used.</p>"},{"location":"planning/behavior_velocity_out_of_lane_module/#threshold","title":"Threshold","text":"<p>With the <code>mode</code> set to <code>\"threshold\"</code>, a decision to stop or slow down before a range is made if an incoming dynamic object is estimated to reach the overlap within <code>threshold.time_threshold</code>.</p>"},{"location":"planning/behavior_velocity_out_of_lane_module/#ttc-time-to-collision","title":"TTC (time to collision)","text":"<p>With the <code>mode</code> set to <code>\"ttc\"</code>, estimates for the times when ego and the dynamic objects reach the start and end of the overlapping range are calculated. This is then used to calculate the time to collision over the period where ego crosses the overlap. If the time to collision is predicted to go bellow the <code>ttc.threshold</code>, the decision to stop or slow down is made.</p>"},{"location":"planning/behavior_velocity_out_of_lane_module/#intervals","title":"Intervals","text":"<p>With the <code>mode</code> set to <code>\"intervals\"</code>, the estimated times when ego and the dynamic objects reach the start and end points of the overlapping range are used to create time intervals. These intervals can be made shorter or longer using the <code>intervals.ego_time_buffer</code> and <code>intervals.objects_time_buffer</code> parameters. If the time interval of ego overlaps with the time interval of an object, the decision to stop or slow down is made.</p>"},{"location":"planning/behavior_velocity_out_of_lane_module/#time-estimates","title":"Time estimates","text":""},{"location":"planning/behavior_velocity_out_of_lane_module/#ego","title":"Ego","text":"<p>To estimate the times when ego will reach an overlap, it is assumed that ego travels along its path at its current velocity or at half the velocity of the path points, whichever is higher.</p>"},{"location":"planning/behavior_velocity_out_of_lane_module/#dynamic-objects","title":"Dynamic objects","text":"<p>Two methods are used to estimate the time when a dynamic objects with reach some point. If <code>objects.use_predicted_paths</code> is set to <code>true</code>, the predicted paths of the dynamic object are used if their confidence value is higher than the value set by the <code>objects.predicted_path_min_confidence</code> parameter. Otherwise, the lanelet map is used to estimate the distance between the object and the point and the time is calculated assuming the object keeps its current velocity.</p>"},{"location":"planning/behavior_velocity_out_of_lane_module/#5-path-update","title":"5. Path update","text":"<p>Finally, for each decision to stop or slow down before an overlapping range, a point is inserted in the path. For a decision taken for an overlapping range with a lane \\(l\\) starting at ego path point index \\(i\\), a point is inserted in the path between index \\(i\\) and \\(i-1\\) such that the ego footprint projected at the inserted point does not overlap \\(l\\). Such point with no overlap must exist since, by definition of the overlapping range, we know that there is no overlap at \\(i-1\\).</p> <p>If the point would cause a higher deceleration than allowed by the <code>max_accel</code> parameter (node parameter), it is skipped.</p> <p>Moreover, parameter <code>action.distance_buffer</code> adds an extra distance between the ego footprint and the overlap when possible.</p>"},{"location":"planning/behavior_velocity_out_of_lane_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>mode</code> string [-] mode used to consider a dynamic object. Candidates: threshold, intervals, ttc <code>skip_if_already_overlapping</code> bool [-] if true, do not run this module when ego already overlaps another lane Parameter /threshold Type Description <code>time_threshold</code> double [s] consider objects that will reach an overlap within this time Parameter /intervals Type Description <code>ego_time_buffer</code> double [s] extend the ego time interval by this buffer <code>objects_time_buffer</code> double [s] extend the time intervals of objects by this buffer Parameter /ttc Type Description <code>threshold</code> double [s] consider objects with an estimated time to collision bellow this value while ego is on the overlap Parameter /objects Type Description <code>minimum_velocity</code> double [m/s] ignore objects with a velocity lower than this value <code>predicted_path_min_confidence</code> double [-] minimum confidence required for a predicted path to be considered <code>use_predicted_paths</code> bool [-] if true, use the predicted paths to estimate future positions; if false, assume the object moves at constant velocity along all lanelets it currently is located in Parameter /overlap Type Description <code>minimum_distance</code> double [m] minimum distance inside a lanelet for an overlap to be considered <code>extra_length</code> double [m] extra arc length to add to the front and back of an overlap (used to calculate enter/exit times) Parameter /action Type Description <code>skip_if_over_max_decel</code> bool [-] if true, do not take an action that would cause more deceleration than the maximum allowed <code>distance_buffer</code> double [m] buffer distance to try to keep between the ego footprint and lane <code>slowdown.distance_threshold</code> double [m] insert a slow down when closer than this distance from an overlap <code>slowdown.velocity</code> double [m] slow down velocity <code>stop.distance_threshold</code> double [m] insert a stop when closer than this distance from an overlap Parameter /ego Type Description <code>extra_front_offset</code> double [m] extra front distance to add to the ego footprint <code>extra_rear_offset</code> double [m] extra rear distance to add to the ego footprint <code>extra_left_offset</code> double [m] extra left distance to add to the ego footprint <code>extra_right_offset</code> double [m] extra right distance to add to the ego footprint"},{"location":"planning/behavior_velocity_planner/","title":"Behavior Velocity Planner","text":""},{"location":"planning/behavior_velocity_planner/#behavior-velocity-planner","title":"Behavior Velocity Planner","text":""},{"location":"planning/behavior_velocity_planner/#overview","title":"Overview","text":"<p><code>behavior_velocity_planner</code> is a planner that adjust velocity based on the traffic rules. It loads modules as plugins. Please refer to the links listed below for detail on each module.</p> <p></p> <ul> <li>Blind Spot</li> <li>Crosswalk</li> <li>Walkway</li> <li>Detection Area</li> <li>Intersection</li> <li>MergeFromPrivate</li> <li>Stop Line</li> <li>Virtual Traffic Light</li> <li>Traffic Light</li> <li>Occlusion Spot</li> <li>No Stopping Area</li> <li>Run Out</li> <li>Speed Bump</li> <li>Out of Lane</li> </ul> <p>When each module plans velocity, it considers based on <code>base_link</code>(center of rear-wheel axis) pose. So for example, in order to stop at a stop line with the vehicles' front on the stop line, it calculates <code>base_link</code> position from the distance between <code>base_link</code> to front and modifies path velocity from the <code>base_link</code> position.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/#input-topics","title":"Input topics","text":"Name Type Description <code>~input/path_with_lane_id</code> autoware_auto_planning_msgs::msg::PathWithLaneId path with lane_id <code>~input/vector_map</code> autoware_auto_mapping_msgs::msg::HADMapBin vector map <code>~input/vehicle_odometry</code> nav_msgs::msg::Odometry vehicle velocity <code>~input/dynamic_objects</code> autoware_auto_perception_msgs::msg::PredictedObjects dynamic objects <code>~input/no_ground_pointcloud</code> sensor_msgs::msg::PointCloud2 obstacle pointcloud <code>~/input/compare_map_filtered_pointcloud</code> sensor_msgs::msg::PointCloud2 obstacle pointcloud filtered by compare map. Note that this is used only when the detection method of run out module is Points. <code>~input/traffic_signals</code> autoware_perception_msgs::msg::TrafficSignalArray traffic light states"},{"location":"planning/behavior_velocity_planner/#output-topics","title":"Output topics","text":"Name Type Description <code>~output/path</code> autoware_auto_planning_msgs::msg::Path path to be followed <code>~output/stop_reasons</code> tier4_planning_msgs::msg::StopReasonArray reasons that cause the vehicle to stop"},{"location":"planning/behavior_velocity_planner/#node-parameters","title":"Node parameters","text":"Parameter Type Description <code>launch_modules</code> vector&lt;string&gt; module names to launch <code>forward_path_length</code> double forward path length <code>backward_path_length</code> double backward path length <code>max_accel</code> double (to be a global parameter) max acceleration of the vehicle <code>system_delay</code> double (to be a global parameter) delay time until output control command <code>delay_response_time</code> double (to be a global parameter) delay time of the vehicle's response to control commands"},{"location":"planning/behavior_velocity_planner_common/","title":"Behavior Velocity Planner Common","text":""},{"location":"planning/behavior_velocity_planner_common/#behavior-velocity-planner-common","title":"Behavior Velocity Planner Common","text":"<p>This package provides common functions as a library, which are used in the <code>behavior_velocity_planner</code> node and modules.</p>"},{"location":"planning/behavior_velocity_run_out_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_run_out_module/#run-out","title":"Run Out","text":""},{"location":"planning/behavior_velocity_run_out_module/#role","title":"Role","text":"<p><code>run_out</code> is the module that decelerates and stops for dynamic obstacles such as pedestrians and bicycles.</p> <p></p>"},{"location":"planning/behavior_velocity_run_out_module/#activation-timing","title":"Activation Timing","text":"<p>This module is activated if <code>launch_run_out</code> becomes true</p>"},{"location":"planning/behavior_velocity_run_out_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/behavior_velocity_run_out_module/#flow-chart","title":"Flow chart","text":""},{"location":"planning/behavior_velocity_run_out_module/#preprocess-path","title":"Preprocess path","text":""},{"location":"planning/behavior_velocity_run_out_module/#calculate-the-expected-target-velocity-for-ego-vehicle","title":"Calculate the expected target velocity for ego vehicle","text":"<p>Calculate the expected target velocity for the ego vehicle path to calculate time to collision with obstacles more precisely. The expected target velocity is calculated with motion velocity smoother module by using current velocity, current acceleration and velocity limits directed by the map and external API.</p> <p></p>"},{"location":"planning/behavior_velocity_run_out_module/#extend-the-path","title":"Extend the path","text":"<p>The path is extended by the length of base link to front to consider obstacles after the goal.</p>"},{"location":"planning/behavior_velocity_run_out_module/#trim-path-from-ego-position","title":"Trim path from ego position","text":"<p>The path is trimmed from ego position to a certain distance to reduce calculation time. Trimmed distance is specified by parameter of <code>detection_distance</code>.</p>"},{"location":"planning/behavior_velocity_run_out_module/#preprocess-obstacles","title":"Preprocess obstacles","text":""},{"location":"planning/behavior_velocity_run_out_module/#create-data-of-abstracted-dynamic-obstacle","title":"Create data of abstracted dynamic obstacle","text":"<p>This module can handle multiple types of obstacles by creating abstracted dynamic obstacle data layer. Currently we have 3 types of detection method (Object, ObjectWithoutPath, Points) to create abstracted obstacle data.</p>"},{"location":"planning/behavior_velocity_run_out_module/#abstracted-dynamic-obstacle","title":"Abstracted dynamic obstacle","text":"<p>Abstracted obstacle data has following information.</p> Name Type Description pose <code>geometry_msgs::msg::Pose</code> pose of the obstacle classifications <code>std::vector&lt;autoware_auto_perception_msgs::msg::ObjectClassification&gt;</code> classifications with probability shape <code>autoware_auto_perception_msgs::msg::Shape</code> shape of the obstacle predicted_paths <code>std::vector&lt;DynamicObstacle::PredictedPath&gt;</code> predicted paths with confidence. this data doesn't have time step because we use minimum and maximum velocity instead. min_velocity_mps <code>float</code> minimum velocity of the obstacle. specified by parameter of <code>dynamic_obstacle.min_vel_kmph</code> max_velocity_mps <code>float</code> maximum velocity of the obstacle. specified by parameter of <code>dynamic_obstacle.max_vel_kmph</code> <p>Enter the maximum/minimum velocity of the object as a parameter, adding enough margin to the expected velocity. This parameter is used to create polygons for collision detection.</p> <p>Future work: Determine the maximum/minimum velocity from the estimated velocity with covariance of the object</p>"},{"location":"planning/behavior_velocity_run_out_module/#3-types-of-detection-method","title":"3 types of detection method","text":"<p>We have 3 types of detection method to meet different safety and availability requirements. The characteristics of them are shown in the table below. Method of <code>Object</code> has high availability (less false positive) because it detects only objects whose predicted path is on the lane. However, sometimes it is not safe because perception may fail to detect obstacles or generate incorrect predicted paths. On the other hand, method of <code>Points</code> has high safety (less false negative) because it uses pointcloud as input. Since points don't have a predicted path, the path that moves in the direction normal to the path of ego vehicle is considered to be the predicted path of abstracted dynamic obstacle data. However, without proper adjustment of filter of points, it may detect a lot of points and it will result in very low availability. Method of <code>ObjectWithoutPath</code> has the characteristics of an intermediate of <code>Object</code> and <code>Points</code>.</p> Method Description Object use an object with the predicted path for collision detection. ObjectWithoutPath use an object but not use the predicted path for collision detection. replace the path assuming that an object jumps out to the lane at specified velocity. Points use filtered points for collision detection. the path is created assuming that points jump out to the lane. points are regarded as an small circular shaped obstacle. <p></p>"},{"location":"planning/behavior_velocity_run_out_module/#exclude-obstacles-outside-of-partition","title":"Exclude obstacles outside of partition","text":"<p>This module can exclude the obstacles outside of partition such as guardrail, fence, and wall. We need lanelet map that has the information of partition to use this feature. By this feature, we can reduce unnecessary deceleration by obstacles that are unlikely to jump out to the lane. You can choose whether to use this feature by parameter of <code>use_partition_lanelet</code>.</p> <p></p>"},{"location":"planning/behavior_velocity_run_out_module/#collision-detection","title":"Collision detection","text":""},{"location":"planning/behavior_velocity_run_out_module/#detect-collision-with-dynamic-obstacles","title":"Detect collision with dynamic obstacles","text":"<p>Along the ego vehicle path, determine the points where collision detection is to be performed for each <code>detection_span</code>.</p> <p>The travel times to the each points are calculated from the expected target velocity.</p> <p></p> <p>For the each points, collision detection is performed using the footprint polygon of the ego vehicle and the polygon of the predicted location of the obstacles. The predicted location of the obstacles is described as rectangle or polygon that has the range calculated by min velocity, max velocity and the ego vehicle's travel time to the point. If the input type of the dynamic obstacle is <code>Points</code>, the obstacle shape is defined as a small cylinder.</p> <p></p> <p>Multiple points are detected as collision points because collision detection is calculated between two polygons. So we select the point that is on the same side as the obstacle and close to ego vehicle as the collision point.</p> <p></p>"},{"location":"planning/behavior_velocity_run_out_module/#insert-velocity","title":"Insert velocity","text":""},{"location":"planning/behavior_velocity_run_out_module/#insert-velocity-to-decelerate-for-obstacles","title":"Insert velocity to decelerate for obstacles","text":"<p>If the collision is detected, stop point is inserted on distance of base link to front + stop margin from the selected collision point. The base link to front means the distance between base_link (center of rear-wheel axis) and front of the car. Stop margin is determined by the parameter of <code>stop_margin</code>.</p> <p></p>"},{"location":"planning/behavior_velocity_run_out_module/#insert-velocity-to-approach-the-obstacles","title":"Insert velocity to approach the obstacles","text":"<p>If you select the method of <code>Points</code> or <code>ObjectWithoutPath</code>, sometimes ego keeps stopping in front of the obstacle. To avoid this problem, This feature has option to approach the obstacle with slow velocity after stopping. If the parameter of <code>approaching.enable</code> is set to true, ego will approach the obstacle after ego stopped for <code>state.stop_time_thresh</code> seconds. The maximum velocity of approaching can be specified by the parameter of <code>approaching.limit_vel_kmph</code>. The decision to approach the obstacle is determined by a simple state transition as following image.</p> <p></p> <p></p>"},{"location":"planning/behavior_velocity_run_out_module/#limit-velocity-with-specified-jerk-and-acc-limit","title":"Limit velocity with specified jerk and acc limit","text":"<p>The maximum slowdown velocity is calculated in order not to slowdown too much. See the Occlusion Spot document for more details. You can choose whether to use this feature by parameter of <code>slow_down_limit.enable</code>.</p>"},{"location":"planning/behavior_velocity_run_out_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>detection_method</code> string [-] candidate: Object, ObjectWithoutPath, Points <code>use_partition_lanelet</code> bool [-] whether to use partition lanelet map data <code>specify_decel_jerk</code> bool [-] whether to specify jerk when ego decelerates <code>stop_margin</code> double [m] the vehicle decelerates to be able to stop with this margin <code>passing_margin</code> double [m] the vehicle begins to accelerate if the vehicle's front in predicted position is ahead of the obstacle + this margin <code>deceleration_jerk</code> double [m/s^3] ego decelerates with this jerk when stopping for obstacles <code>detection_distance</code> double [m] ahead distance from ego to detect the obstacles <code>detection_span</code> double [m] calculate collision with this span to reduce calculation time <code>min_vel_ego_kmph</code> double [km/h] min velocity to calculate time to collision Parameter /detection_area Type Description <code>margin_ahead</code> double [m] ahead margin for detection area polygon <code>margin_behind</code> double [m] behind margin for detection area polygon Parameter /dynamic_obstacle Type Description <code>use_mandatory_area</code> double [-] whether to use mandatory detection area <code>assume_fixed_velocity.enable</code> double [-] If enabled, the obstacle's velocity is assumed to be within the minimum and maximum velocity values specified below <code>assume_fixed_velocity.min_vel_kmph</code> double [km/h] minimum velocity for dynamic obstacles <code>assume_fixed_velocity.max_vel_kmph</code> double [km/h] maximum velocity for dynamic obstacles <code>diameter</code> double [m] diameter of obstacles. used for creating dynamic obstacles from points <code>height</code> double [m] height of obstacles. used for creating dynamic obstacles from points <code>max_prediction_time</code> double [sec] create predicted path until this time <code>time_step</code> double [sec] time step for each path step. used for creating dynamic obstacles from points or objects without path <code>points_interval</code> double [m] divide obstacle points into groups with this interval, and detect only lateral nearest point. used only for Points method Parameter /approaching Type Description <code>enable</code> bool [-] whether to enable approaching after stopping <code>margin</code> double [m] distance on how close ego approaches the obstacle <code>limit_vel_kmph</code> double [km/h] limit velocity for approaching after stopping Parameter /state Type Description <code>stop_thresh</code> double [m/s] threshold to decide if ego is stopping <code>stop_time_thresh</code> double [sec] threshold for stopping time to transit to approaching state <code>disable_approach_dist</code> double [m] end the approaching state if distance to the obstacle is longer than this value <code>keep_approach_duration</code> double [sec] keep approach state for this duration to avoid chattering of state transition Parameter /slow_down_limit Type Description <code>enable</code> bool [-] whether to enable to limit velocity with max jerk and acc <code>max_jerk</code> double [m/s^3] minimum jerk deceleration for safe brake. <code>max_acc</code> double [m/s^2] minimum accel deceleration for safe brake. Parameter /ignore_momentary_detection Type Description <code>enable</code> bool [-] whether to ignore momentary detection <code>time_threshold</code> double [sec] ignores detections that persist for less than this duration"},{"location":"planning/behavior_velocity_run_out_module/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":"<ul> <li>Calculate obstacle's min velocity and max velocity from covariance</li> <li>Detect collisions with polygon object</li> <li>Handle the case when the predicted path of obstacles are not straight line<ul> <li>Currently collision check is calculated based on the assumption that the predicted path of the obstacle is a straight line</li> </ul> </li> </ul>"},{"location":"planning/behavior_velocity_speed_bump_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_speed_bump_module/#speed-bump","title":"Speed Bump","text":""},{"location":"planning/behavior_velocity_speed_bump_module/#role","title":"Role","text":"<p>This module plans the velocity of the related part of the path in case there is speed bump regulatory element referring to it.</p> <p></p>"},{"location":"planning/behavior_velocity_speed_bump_module/#activation-timing","title":"Activation Timing","text":"<p>The manager launch speed bump scene module when there is speed bump regulatory element referring to the reference path.</p>"},{"location":"planning/behavior_velocity_speed_bump_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>slow_start_margin</code> double [m] margin for ego vehicle to slow down before speed_bump <code>slow_end_margin</code> double [m] margin for ego vehicle to accelerate after speed_bump <code>print_debug_info</code> bool whether debug info will be printed or not"},{"location":"planning/behavior_velocity_speed_bump_module/#speed-calculation","title":"Speed Calculation","text":"<ul> <li>limits for speed bump height and slow down speed to create a linear equation</li> </ul> Parameter Type Description <code>min_height</code> double [m] minimum height assumption of the speed bump <code>max_height</code> double [m] maximum height assumption of the speed bump <code>min_speed</code> double [m/s] minimum speed assumption of slow down speed <code>max_speed</code> double [m/s] maximum speed assumption of slow down speed"},{"location":"planning/behavior_velocity_speed_bump_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ul> <li>Get speed bump regulatory element on the path from lanelet2 map</li> <li>Calculate <code>slow_down_speed</code> wrt to <code>speed_bump_height</code> specified in regulatory element or   read <code>slow_down_speed</code> tag from speed bump annotation if available</li> </ul> <p>Note: If in speed bump annotation <code>slow_down_speed</code> tag is used then calculating the speed wrt the speed bump height will be ignored. In such case, specified <code>slow_down_speed</code> value in [kph] is being used.</p> <ul> <li>Get the intersection points between path and speed bump polygon</li> <li>Calculate <code>slow_start_point</code> &amp; <code>slow_end_point</code> wrt the intersection points and insert them to   path</li> <li>If <code>slow_start_point</code> or <code>slow_end_point</code> can not be inserted with given/calculated offset values   check if any path point can be virtually assigned as <code>slow_start_point</code> or <code>slow_end_point</code></li> </ul> <p></p> <ul> <li>Assign <code>slow_down_speed</code> to the path points between <code>slow_start_point</code> or <code>slow_end_point</code></li> </ul>"},{"location":"planning/behavior_velocity_speed_bump_module/#future-work","title":"Future Work","text":"<ul> <li>In an article here, a bump modeling method   is proposed. Simply it is based on fitting the bump in a circle and a radius calculation is done   with it. Although the velocity calculation is based on just the height of the bump in the recent   implementation, applying this method is intended in the future which will yield more realistic   results.</li> </ul>"},{"location":"planning/behavior_velocity_stop_line_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_stop_line_module/#stop-line","title":"Stop Line","text":""},{"location":"planning/behavior_velocity_stop_line_module/#role","title":"Role","text":"<p>This module plans velocity so that the vehicle can stop right before stop lines and restart driving after stopped.</p> <p></p>"},{"location":"planning/behavior_velocity_stop_line_module/#activation-timing","title":"Activation Timing","text":"<p>This module is activated when there is a stop line in a target lane.</p>"},{"location":"planning/behavior_velocity_stop_line_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>stop_margin</code> double a margin that the vehicle tries to stop before stop_line <code>stop_duration_sec</code> double [s] time parameter for the ego vehicle to stop in front of a stop line <code>hold_stop_margin_distance</code> double [m] parameter for restart prevention (See Algorithm section). Also, when the ego vehicle is within this distance from a stop line, the ego state becomes STOPPED from APPROACHING <code>use_initialization_stop_state</code> bool A flag to determine whether to return to the approaching state when the vehicle moves away from a stop line. <code>show_stop_line_collision_check</code> bool A flag to determine whether to show the debug information of collision check with a stop line"},{"location":"planning/behavior_velocity_stop_line_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ul> <li>Gets a stop line from map information.</li> <li>insert a stop point on the path from the stop line defined in the map and the ego vehicle length.</li> <li>Sets velocities of the path after the stop point to 0[m/s].</li> <li>Release the inserted stop velocity when the vehicle stops at the stop point for <code>stop_duration_sec</code> seconds.</li> </ul>"},{"location":"planning/behavior_velocity_stop_line_module/#flowchart","title":"Flowchart","text":"<p>This algorithm is based on <code>segment</code>. <code>segment</code> consists of two node points. It's useful for removing boundary conditions because if <code>segment(i)</code> exists we can assume <code>node(i)</code> and <code>node(i+1)</code> exist.</p> <p></p> <p>First, this algorithm finds a collision between reference path and stop line. Then, we can get <code>collision segment</code> and <code>collision point</code>.</p> <p></p> <p>Next, based on <code>collision point</code>, it finds <code>offset segment</code> by iterating backward points up to a specific offset length. The offset length is <code>stop_margin</code>(parameter) + <code>base_link to front</code>(to adjust head pose to stop line). Then, we can get <code>offset segment</code> and <code>offset from segment start</code>.</p> <p></p> <p>After that, we can calculate a offset point from <code>offset segment</code> and <code>offset</code>. This will be <code>stop_pose</code>.</p> <p></p>"},{"location":"planning/behavior_velocity_stop_line_module/#restart-prevention","title":"Restart prevention","text":"<p>If it needs X meters (e.g. 0.5 meters) to stop once the vehicle starts moving due to the poor vehicle control performance, the vehicle goes over the stopping position that should be strictly observed when the vehicle starts to moving in order to approach the near stop point (e.g. 0.3 meters away).</p> <p>This module has parameter <code>hold_stop_margin_distance</code> in order to prevent from these redundant restart. If the vehicle is stopped within <code>hold_stop_margin_distance</code> meters from stop point of the module (_front_to_stop_line &lt; hold_stop_margin_distance), the module judges that the vehicle has already stopped for the module's stop point and plans to keep stopping current position even if the vehicle is stopped due to other factors.</p> <p> </p> parameters <p> </p> outside the hold_stop_margin_distance <p> </p> inside the hold_stop_margin_distance"},{"location":"planning/behavior_velocity_template_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_template_module/#template","title":"Template","text":"<p>A template for behavior velocity modules based on the behavior_velocity_speed_bump_module.</p>"},{"location":"planning/behavior_velocity_template_module/#autoware-behavior-velocity-module-template","title":"Autoware Behavior Velocity Module Template","text":""},{"location":"planning/behavior_velocity_template_module/#scene","title":"<code>Scene</code>","text":""},{"location":"planning/behavior_velocity_template_module/#templatemodule-class","title":"<code>TemplateModule</code> Class","text":"<p>The <code>TemplateModule</code> class serves as a foundation for creating a scene module within the Autoware behavior velocity planner. It defines the core methods and functionality needed for the module's behavior. You should replace the placeholder code with actual implementations tailored to your specific behavior velocity module.</p>"},{"location":"planning/behavior_velocity_template_module/#constructor","title":"Constructor","text":"<ul> <li>The constructor for <code>TemplateModule</code> takes the essential parameters to create a module: <code>const int64_t module_id</code>, <code>const rclcpp::Logger &amp; logger</code>, and <code>const rclcpp::Clock::SharedPtr clock</code>. These parameters are supplied by the <code>TemplateModuleManager</code> when registering a new module. Other parameters can be added to the constructor, if required by your specific module implementation.</li> </ul>"},{"location":"planning/behavior_velocity_template_module/#modifypathvelocity-method","title":"<code>modifyPathVelocity</code> Method","text":"<ul> <li>This method, defined in the <code>TemplateModule</code> class, is expected to modify the velocity of the input path based on certain conditions. In the provided code, it logs an informational message once when the template module is executing.</li> <li>The specific logic for velocity modification should be implemented in this method based on the module's requirements.</li> </ul>"},{"location":"planning/behavior_velocity_template_module/#createdebugmarkerarray-method","title":"<code>createDebugMarkerArray</code> Method","text":"<ul> <li>This method, also defined in the <code>TemplateModule</code> class, is responsible for creating a visualization of debug markers and returning them as a <code>visualization_msgs::msg::MarkerArray</code>. In the provided code, it returns an empty <code>MarkerArray</code>.</li> <li>You should implement the logic to generate debug markers specific to your module's functionality.</li> </ul>"},{"location":"planning/behavior_velocity_template_module/#createvirtualwalls-method","title":"<code>createVirtualWalls</code> Method","text":"<ul> <li>The <code>createVirtualWalls</code> method creates virtual walls for the scene and returns them as <code>motion_utils::VirtualWalls</code>. In the provided code, it returns an empty <code>VirtualWalls</code> object.</li> <li>You should implement the logic to create virtual walls based on your module's requirements.</li> </ul>"},{"location":"planning/behavior_velocity_template_module/#manager","title":"<code>Manager</code>","text":"<p>The managing of your modules is defined in manager.hpp and manager.cpp. The managing is handled by two classes:</p> <ul> <li>The <code>TemplateModuleManager</code> class defines the core logic for managing and launching the behavior_velocity_template scenes (defined in behavior_velocity_template_module/src/scene.cpp/hpp). It inherits essential manager attributes from its parent class <code>SceneModuleManagerInterface</code>.</li> <li>The <code>TemplateModulePlugin</code> class provides a way to integrate the <code>TemplateModuleManager</code> into the logic of the Behavior Velocity Planner.</li> </ul>"},{"location":"planning/behavior_velocity_template_module/#templatemodulemanager-class","title":"<code>TemplateModuleManager</code> Class","text":""},{"location":"planning/behavior_velocity_template_module/#constructor-templatemodulemanager","title":"Constructor <code>TemplateModuleManager</code>","text":"<ul> <li>This is the constructor of the <code>TemplateModuleManager</code> class, and it takes an <code>rclcpp::Node</code> reference as a parameter.</li> <li>It initializes a member variable <code>dummy_parameter</code> to 0.0.</li> </ul>"},{"location":"planning/behavior_velocity_template_module/#getmodulename-method","title":"<code>getModuleName()</code> Method","text":"<ul> <li>This method is an override of a virtual method from the <code>SceneModuleManagerInterface</code> class.</li> <li>It returns a pointer to a constant character string, which is the name of the module. In this case, it returns \"template\" as the module name.</li> </ul>"},{"location":"planning/behavior_velocity_template_module/#launchnewmodules-method","title":"<code>launchNewModules()</code> Method","text":"<ul> <li>This is a private method that takes an argument of type <code>autoware_auto_planning_msgs::msg::PathWithLaneId</code>.</li> <li>It is responsible for launching new modules based on the provided path information (PathWithLaneId). The implementation of this method involves initializing and configuring modules specific to your behavior velocity planner by using the <code>TemplateModule</code> class.</li> <li>In the provided source code, it initializes a <code>module_id</code> to 0 and checks if a module with the same ID is already registered. If not, it registers a new <code>TemplateModule</code> with the module ID. Note that each module managed by the <code>TemplateModuleManager</code> should have a unique ID. The template code registers a single module, so the <code>module_id</code> is set as 0 for simplicity.</li> </ul>"},{"location":"planning/behavior_velocity_template_module/#getmoduleexpiredfunction-method","title":"<code>getModuleExpiredFunction()</code> Method","text":"<ul> <li>This is a private method that takes an argument of type <code>autoware_auto_planning_msgs::msg::PathWithLaneId</code>.</li> <li>It returns a <code>std::function&lt;bool(const std::shared_ptr&lt;SceneModuleInterface&gt;&amp;)&gt;</code>. This function is used by the behavior velocity planner to determine whether a particular module has expired or not based on the given path.</li> <li>The implementation of this method is expected to return a function that can be used to check the expiration status of modules.</li> </ul> <p>Please note that the specific functionality of the methods <code>launchNewModules()</code> and <code>getModuleExpiredFunction()</code> would depend on the details of your behavior velocity modules and how they are intended to be managed within the Autoware system. You would need to implement these methods according to your module's requirements.</p>"},{"location":"planning/behavior_velocity_template_module/#templatemoduleplugin-class","title":"<code>TemplateModulePlugin</code> Class","text":""},{"location":"planning/behavior_velocity_template_module/#templatemoduleplugin-class_1","title":"<code>TemplateModulePlugin</code> Class","text":"<ul> <li>This class inherits from <code>PluginWrapper&lt;TemplateModuleManager&gt;</code>. It essentially wraps your <code>TemplateModuleManager</code> class within a plugin, which can be loaded and managed dynamically.</li> </ul>"},{"location":"planning/behavior_velocity_template_module/#example-usage","title":"<code>Example Usage</code>","text":"<p>In the following example, we take each point of the path, and multiply it by 2. Essentially duplicating the speed. Note that the velocity smoother will further modify the path speed after all the behavior velocity modules are executed.</p> <pre><code>bool TemplateModule::modifyPathVelocity(\n[[maybe_unused]] PathWithLaneId * path, [[maybe_unused]] StopReason * stop_reason)\n{\nfor (auto &amp; p : path-&gt;points) {\np.point.longitudinal_velocity_mps *= 2.0;\n}\n\nreturn false;\n}\n</code></pre>"},{"location":"planning/behavior_velocity_traffic_light_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_traffic_light_module/#traffic-light","title":"Traffic Light","text":""},{"location":"planning/behavior_velocity_traffic_light_module/#role","title":"Role","text":"<p>Judgement whether a vehicle can go into an intersection or not by traffic light status, and planning a velocity of the stop if necessary. This module is designed for rule-based velocity decision that is easy for developers to design its behavior. It generates proper velocity for traffic light scene.</p> <p></p>"},{"location":"planning/behavior_velocity_traffic_light_module/#limitations","title":"Limitations","text":"<p>This module allows developers to design STOP/GO in traffic light module using specific rules. Due to the property of rule-based planning, the algorithm is greatly depends on object detection and perception accuracy considering traffic light. Also, this module only handles STOP/Go at traffic light scene, so rushing or quick decision according to traffic condition is future work.</p>"},{"location":"planning/behavior_velocity_traffic_light_module/#activation-timing","title":"Activation Timing","text":"<p>This module is activated when there is traffic light in ego lane.</p>"},{"location":"planning/behavior_velocity_traffic_light_module/#algorithm","title":"Algorithm","text":"<ol> <li> <p>Obtains a traffic light mapped to the route and a stop line correspond to the traffic light from a map information.</p> <ul> <li>If a corresponding traffic light signal have never been found, it treats as a signal to pass.</li> </ul> <ul> <li>If a corresponding traffic light signal is found but timed out, it treats as a signal to stop.</li> </ul> </li> <li> <p>Uses the highest reliability one of the traffic light recognition result and if the color of that was not green or corresponding arrow signal, generates a stop point.</p> <ul> <li>If an elapsed time to receive stop signal is less than <code>stop_time_hysteresis</code>, it treats as a signal to pass. This feature is to prevent chattering.</li> </ul> </li> <li> <p>When vehicle current velocity is</p> <ul> <li>higher than 2.0m/s \u21d2 pass judge(using next slide formula)</li> </ul> <ul> <li>lower than 2.0m/s \u21d2 stop</li> </ul> </li> <li> <p>When it to be judged that vehicle can\u2019t stop before stop line, autoware chooses one of the following behaviors</p> <ul> <li>\"can pass through\" stop line during yellow lamp =&gt; pass</li> </ul> <ul> <li>\"can\u2019t pass through\" stop line during yellow lamp =&gt; emergency stop</li> </ul> </li> </ol>"},{"location":"planning/behavior_velocity_traffic_light_module/#dilemma-zone","title":"Dilemma Zone","text":"<ul> <li> <p>yellow lamp line</p> <p>It\u2019s called \u201cyellow lamp line\u201d which shows the distance traveled by the vehicle during yellow lamp.</p> </li> </ul> <ul> <li> <p>dilemma zone</p> <p>It\u2019s called \u201cdilemma zone\u201d which satisfies following conditions:</p> <ul> <li>vehicle can\u2019t pass through stop line during yellow lamp.(right side of the yellow lamp line)</li> </ul> <ul> <li> <p>vehicle can\u2019t stop under deceleration and jerk limit.(left side of the pass judge curve)</p> <p>\u21d2emergency stop(relax deceleration and jerk limitation in order to observe the traffic regulation)</p> </li> </ul> </li> </ul> <ul> <li> <p>optional zone</p> <p>It\u2019s called \u201coptional zone\u201d which satisfies following conditions:</p> <ul> <li>vehicle can pass through stop line during yellow lamp.(left side of the yellow lamp line)</li> </ul> <ul> <li> <p>vehicle can stop under deceleration and jerk limit.(right side of the pass judge curve)</p> <p>\u21d2 stop(autoware selects the safety choice)</p> </li> </ul> </li> </ul>"},{"location":"planning/behavior_velocity_traffic_light_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>stop_margin</code> double [m] margin before stop point <code>tl_state_timeout</code> double [s] time out for detected traffic light result. <code>stop_time_hysteresis</code> double [s] time threshold to decide stop planning for chattering prevention <code>yellow_lamp_period</code> double [s] time for yellow lamp <code>enable_pass_judge</code> bool [-] whether to use pass judge"},{"location":"planning/behavior_velocity_traffic_light_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_velocity_traffic_light_module/#known-limits","title":"Known Limits","text":"<ul> <li>tbd.</li> </ul>"},{"location":"planning/behavior_velocity_virtual_traffic_light_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_virtual_traffic_light_module/#virtual-traffic-light","title":"Virtual Traffic Light","text":""},{"location":"planning/behavior_velocity_virtual_traffic_light_module/#role","title":"Role","text":"<p>Autonomous vehicles have to cooperate with the infrastructures such as:</p> <ul> <li>Warehouse shutters</li> <li>Traffic lights with V2X support</li> <li>Communication devices at intersections</li> <li>Fleet Management Systems (FMS)</li> </ul> <p>The following items are example cases:</p> <ol> <li> <p>Traffic control by traffic lights with V2X support    </p> </li> <li> <p>Intersection coordination of multiple vehicles by FMS.    </p> </li> </ol> <p>It's possible to make each function individually, however, the use cases can be generalized with these three elements.</p> <ol> <li><code>start</code>: Start a cooperation procedure after the vehicle enters a certain zone.</li> <li><code>stop</code>: Stop at a defined stop line according to the status received from infrastructures.</li> <li><code>end</code>: Finalize the cooperation procedure after the vehicle reaches the exit zone. This should be done within the range of stable communication.</li> </ol> <p>This module sends/receives status from infrastructures and plans the velocity of the cooperation result.</p>"},{"location":"planning/behavior_velocity_virtual_traffic_light_module/#system-configuration-diagram","title":"System Configuration Diagram","text":"<p>Planner and each infrastructure communicate with each other using common abstracted messages.</p> <ul> <li>Special handling for each infrastructure is not scalable. The interface is defined as an Autoware API.</li> <li>The requirements for each infrastructure are slightly different, but will be handled flexibly.</li> </ul> <p>FMS: Intersection coordination when multiple vehicles are in operation and the relevant lane is occupied</p> <ul> <li>Automatic shutter: Open the shutter when approaching/close it when leaving</li> <li>Manual shutter: Have the driver open and close the shutter.</li> <li>Remote control signal: Have the driver change the signal status to match the direction of travel.</li> <li>Warning light: Activate the warning light</li> </ul> <p>Support different communication methods for different infrastructures</p> <ul> <li>HTTP</li> <li>Bluetooth</li> <li>ZigBee</li> </ul> <p>Have different meta-information for each geographic location</p> <ul> <li>Associated lane ID</li> <li>Hardware ID</li> <li>Communication method</li> </ul> <p>FMS: Fleet Management System</p> <p></p>"},{"location":"planning/behavior_velocity_virtual_traffic_light_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>max_delay_sec</code> double [s] maximum allowed delay for command <code>near_line_distance</code> double [m] threshold distance to stop line to check ego stop. <code>dead_line_margin</code> double [m] threshold distance that this module continue to insert stop line. <code>hold_stop_margin_distance</code> double [m] parameter for restart prevention (See following section) <code>check_timeout_after_stop_line</code> bool [-] check timeout to stop when linkage is disconnected"},{"location":"planning/behavior_velocity_virtual_traffic_light_module/#restart-prevention","title":"Restart prevention","text":"<p>If it needs X meters (e.g. 0.5 meters) to stop once the vehicle starts moving due to the poor vehicle control performance, the vehicle goes over the stopping position that should be strictly observed when the vehicle starts to moving in order to approach the near stop point (e.g. 0.3 meters away).</p> <p>This module has parameter <code>hold_stop_margin_distance</code> in order to prevent from these redundant restart. If the vehicle is stopped within <code>hold_stop_margin_distance</code> meters from stop point of the module (_front_to_stop_line &lt; hold_stop_margin_distance), the module judges that the vehicle has already stopped for the module's stop point and plans to keep stopping current position even if the vehicle is stopped due to other factors.</p> <p> </p> parameters <p> </p> outside the hold_stop_margin_distance <p> </p> inside the hold_stop_margin_distance"},{"location":"planning/behavior_velocity_virtual_traffic_light_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_velocity_virtual_traffic_light_module/#map-format","title":"Map Format","text":"<ul> <li>To avoid sudden braking, the length between the start line and stop line of a virtual traffic light must be longer than \\(l_{\\mathrm{min}}\\) calculated as follows, assuming that \\(v_0\\) is the velocity when passing the start line and \\(a_{\\mathrm{min}}\\) is minimum acceleration defined in Autoware.</li> </ul> \\[ \\begin{align} l_{\\mathrm{min}} = -\\frac{v_0^2}{2 a_{\\mathrm{min}}} \\end{align} \\]"},{"location":"planning/behavior_velocity_virtual_traffic_light_module/#known-limits","title":"Known Limits","text":"<ul> <li>tbd.</li> </ul>"},{"location":"planning/behavior_velocity_walkway_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_walkway_module/#walkway","title":"Walkway","text":""},{"location":"planning/behavior_velocity_walkway_module/#role","title":"Role","text":"<p>This module decide to stop before the ego will cross the walkway including crosswalk to enter or exit the private area.</p>"},{"location":"planning/costmap_generator/","title":"costmap_generator","text":""},{"location":"planning/costmap_generator/#costmap_generator","title":"costmap_generator","text":""},{"location":"planning/costmap_generator/#costmap_generator_node","title":"costmap_generator_node","text":"<p>This node reads <code>PointCloud</code> and/or <code>DynamicObjectArray</code> and creates an <code>OccupancyGrid</code> and <code>GridMap</code>. <code>VectorMap(Lanelet2)</code> is optional.</p>"},{"location":"planning/costmap_generator/#input-topics","title":"Input topics","text":"Name Type Description <code>~input/objects</code> autoware_auto_perception_msgs::PredictedObjects predicted objects, for obstacles areas <code>~input/points_no_ground</code> sensor_msgs::PointCloud2 ground-removed points, for obstacle areas which can't be detected as objects <code>~input/vector_map</code> autoware_auto_mapping_msgs::HADMapBin vector map, for drivable areas <code>~input/scenario</code> tier4_planning_msgs::Scenario scenarios to be activated, for node activation"},{"location":"planning/costmap_generator/#output-topics","title":"Output topics","text":"Name Type Description <code>~output/grid_map</code> grid_map_msgs::GridMap costmap as GridMap, values are from 0.0 to 1.0 <code>~output/occupancy_grid</code> nav_msgs::OccupancyGrid costmap as OccupancyGrid, values are from 0 to 100"},{"location":"planning/costmap_generator/#output-tfs","title":"Output TFs","text":"<p>None</p>"},{"location":"planning/costmap_generator/#how-to-launch","title":"How to launch","text":"<ol> <li> <p>Execute the command <code>source install/setup.bash</code> to setup the environment</p> </li> <li> <p>Run <code>ros2 launch costmap_generator costmap_generator.launch.xml</code> to launch the node</p> </li> </ol>"},{"location":"planning/costmap_generator/#parameters","title":"Parameters","text":"Name Type Description <code>update_rate</code> double timer's update rate <code>activate_by_scenario</code> bool if true, activate by scenario = parking. Otherwise, activate if vehicle is inside parking lot. <code>use_objects</code> bool whether using <code>~input/objects</code> or not <code>use_points</code> bool whether using <code>~input/points_no_ground</code> or not <code>use_wayarea</code> bool whether using <code>wayarea</code> from <code>~input/vector_map</code> or not <code>use_parkinglot</code> bool whether using <code>parkinglot</code> from <code>~input/vector_map</code> or not <code>costmap_frame</code> string created costmap's coordinate <code>vehicle_frame</code> string vehicle's coordinate <code>map_frame</code> string map's coordinate <code>grid_min_value</code> double minimum cost for gridmap <code>grid_max_value</code> double maximum cost for gridmap <code>grid_resolution</code> double resolution for gridmap <code>grid_length_x</code> int size of gridmap for x direction <code>grid_length_y</code> int size of gridmap for y direction <code>grid_position_x</code> int offset from coordinate in x direction <code>grid_position_y</code> int offset from coordinate in y direction <code>maximum_lidar_height_thres</code> double maximum height threshold for pointcloud data <code>minimum_lidar_height_thres</code> double minimum height threshold for pointcloud data <code>expand_rectangle_size</code> double expand object's rectangle with this value <code>size_of_expansion_kernel</code> int kernel size for blurring effect on object's costmap"},{"location":"planning/costmap_generator/#flowchart","title":"Flowchart","text":""},{"location":"planning/external_velocity_limit_selector/","title":"External Velocity Limit Selector","text":""},{"location":"planning/external_velocity_limit_selector/#external-velocity-limit-selector","title":"External Velocity Limit Selector","text":""},{"location":"planning/external_velocity_limit_selector/#purpose","title":"Purpose","text":"<p>The <code>external_velocity_limit_selector_node</code> is a node that keeps consistency of external velocity limits. This module subscribes</p> <ol> <li>velocity limit command sent by API,</li> <li>velocity limit command sent by Autoware internal modules.</li> </ol> <p>VelocityLimit.msg contains not only max velocity but also information about the acceleration/jerk constraints on deceleration. The <code>external_velocity_limit_selector_node</code> integrates the lowest velocity limit and the highest jerk constraint to calculate the hardest velocity limit that protects all the deceleration points and max velocities sent by API and Autoware internal modules.</p> <p></p>"},{"location":"planning/external_velocity_limit_selector/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>WIP</p>"},{"location":"planning/external_velocity_limit_selector/#inputs","title":"Inputs","text":"Name Type Description <code>~input/velocity_limit_from_api</code> tier4_planning_msgs::VelocityLimit velocity limit from api <code>~input/velocity_limit_from_internal</code> tier4_planning_msgs::VelocityLimit velocity limit from autoware internal modules <code>~input/velocity_limit_clear_command_from_internal</code> tier4_planning_msgs::VelocityLimitClearCommand velocity limit clear command"},{"location":"planning/external_velocity_limit_selector/#outputs","title":"Outputs","text":"Name Type Description <code>~output/max_velocity</code> tier4_planning_msgs::VelocityLimit current information of the hardest velocity limit"},{"location":"planning/external_velocity_limit_selector/#parameters","title":"Parameters","text":"Name Type Description Default Range max_velocity float max velocity limit [m/s] 20 N/A normal.min_acc float min deceleration [m/ss] -0.5 N/A normal.max_acc float max acceleration [m/ss] 1 N/A normal.min_jerk float min jerk [m/sss] -0.5 N/A normal.max_jerk float max jerk [m/sss] 1 N/A limit.min_acc float min deceleration to be observed [m/ss] -2.5 N/A limit.max_acc float max acceleration to be observed [m/ss] 1 N/A limit.min_jerk float min jerk to be observed [m/sss] -1.5 N/A limit.max_jerk float max jerk to be observed [m/sss] 1.5 N/A"},{"location":"planning/external_velocity_limit_selector/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"planning/external_velocity_limit_selector/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"planning/external_velocity_limit_selector/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"planning/external_velocity_limit_selector/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"planning/external_velocity_limit_selector/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"planning/freespace_planner/","title":"The `freespace_planner`","text":""},{"location":"planning/freespace_planner/#the-freespace_planner","title":"The <code>freespace_planner</code>","text":""},{"location":"planning/freespace_planner/#freespace_planner_node","title":"freespace_planner_node","text":"<p><code>freespace_planner_node</code> is a global path planner node that plans trajectory in the space having static/dynamic obstacles. This node is currently based on Hybrid A* search algorithm in <code>freespace_planning_algorithms</code> package. Other algorithms such as rrt* will be also added and selectable in the future.</p> <p>Note Due to the constraint of trajectory following, the output trajectory will be split to include only the single direction path. In other words, the output trajectory doesn't include both forward and backward trajectories at once.</p>"},{"location":"planning/freespace_planner/#input-topics","title":"Input topics","text":"Name Type Description <code>~input/route</code> autoware_auto_planning_msgs::Route route and goal pose <code>~input/occupancy_grid</code> nav_msgs::OccupancyGrid costmap, for drivable areas <code>~input/odometry</code> nav_msgs::Odometry vehicle velocity, for checking whether vehicle is stopped <code>~input/scenario</code> tier4_planning_msgs::Scenario scenarios to be activated, for node activation"},{"location":"planning/freespace_planner/#output-topics","title":"Output topics","text":"Name Type Description <code>~output/trajectory</code> autoware_auto_planning_msgs::Trajectory trajectory to be followed <code>is_completed</code> bool (implemented as rosparam) whether all split trajectory are published"},{"location":"planning/freespace_planner/#output-tfs","title":"Output TFs","text":"<p>None</p>"},{"location":"planning/freespace_planner/#how-to-launch","title":"How to launch","text":"<ol> <li>Write your remapping info in <code>freespace_planner.launch</code> or add args when executing <code>roslaunch</code></li> <li><code>roslaunch freespace_planner freespace_planner.launch</code></li> </ol>"},{"location":"planning/freespace_planner/#parameters","title":"Parameters","text":""},{"location":"planning/freespace_planner/#node-parameters","title":"Node parameters","text":"Parameter Type Description <code>planning_algorithms</code> string algorithms used in the node <code>vehicle_shape_margin_m</code> float collision margin in planning algorithm <code>update_rate</code> double timer's update rate <code>waypoints_velocity</code> double velocity in output trajectory (currently, only constant velocity is supported) <code>th_arrived_distance_m</code> double threshold distance to check if vehicle has arrived at the trajectory's endpoint <code>th_stopped_time_sec</code> double threshold time to check if vehicle is stopped <code>th_stopped_velocity_mps</code> double threshold velocity to check if vehicle is stopped <code>th_course_out_distance_m</code> double threshold distance to check if vehicle is out of course <code>vehicle_shape_margin_m</code> double vehicle margin <code>replan_when_obstacle_found</code> bool whether replanning when obstacle has found on the trajectory <code>replan_when_course_out</code> bool whether replanning when vehicle is out of course"},{"location":"planning/freespace_planner/#planner-common-parameters","title":"Planner common parameters","text":"Parameter Type Description <code>time_limit</code> double time limit of planning <code>minimum_turning_radius</code> double minimum turning radius of robot <code>maximum_turning_radius</code> double maximum turning radius of robot <code>theta_size</code> double the number of angle's discretization <code>lateral_goal_range</code> double goal range of lateral position <code>longitudinal_goal_range</code> double goal range of longitudinal position <code>angle_goal_range</code> double goal range of angle <code>curve_weight</code> double additional cost factor for curve actions <code>reverse_weight</code> double additional cost factor for reverse actions <code>obstacle_threshold</code> double threshold for regarding a certain grid as obstacle"},{"location":"planning/freespace_planner/#a-search-parameters","title":"A* search parameters","text":"Parameter Type Description <code>only_behind_solutions</code> bool whether restricting the solutions to be behind the goal <code>use_back</code> bool whether using backward trajectory <code>distance_heuristic_weight</code> double heuristic weight for estimating node's cost"},{"location":"planning/freespace_planner/#rrt-search-parameters","title":"RRT* search parameters","text":"Parameter Type Description <code>max planning time</code> double maximum planning time [msec] (used only when <code>enable_update</code> is set <code>true</code>) <code>enable_update</code> bool whether update after feasible solution found until <code>max_planning time</code> elapse <code>use_informed_sampling</code> bool Use informed RRT* (of Gammell et al.) <code>neighbor_radius</code> double neighbor radius of RRT* algorithm <code>margin</code> double safety margin ensured in path's collision checking in RRT* algorithm"},{"location":"planning/freespace_planner/#flowchart","title":"Flowchart","text":""},{"location":"planning/freespace_planning_algorithms/","title":"freespace planning algorithms","text":""},{"location":"planning/freespace_planning_algorithms/#freespace-planning-algorithms","title":"freespace planning algorithms","text":""},{"location":"planning/freespace_planning_algorithms/#role","title":"Role","text":"<p>This package is for development of path planning algorithms in free space.</p>"},{"location":"planning/freespace_planning_algorithms/#implemented-algorithms","title":"Implemented algorithms","text":"<ul> <li>Hybrid A* and RRT* (includes RRT and informed RRT*)</li> </ul> <p>Please see rrtstar.md for a note on the implementation for informed-RRT*.</p> <p>NOTE: As for RRT*, one can choose whether update after feasible solution found in RRT*. If not doing so, the algorithm is the almost (but exactly because of rewiring procedure) same as vanilla RRT. If you choose update, then you have option if the sampling after feasible solution found is \"informed\". If set true, then the algorithm is equivalent to <code>informed RRT\\* of Gammell et al. 2014</code>.</p>"},{"location":"planning/freespace_planning_algorithms/#algorithm-selection","title":"Algorithm selection","text":"<p>There is a trade-off between algorithm speed and resulting solution quality. When we sort the algorithms by the spectrum of (high quality solution/ slow) -&gt; (low quality solution / fast) it would be A* -&gt; informed RRT* -&gt; RRT. Note that in almost all case informed RRT* is better than RRT* for solution quality given the same computational time budget. So, RRT* is omitted in the comparison.</p> <p>Some selection criteria would be:</p> <ul> <li>If obstacle geometry is complex: -&gt; avoid RRT and RRT*. The resulting path could be too messy.</li> <li>If goal location is far from the start: -&gt; avoid A*. Take too long time because it based on grid discretization.</li> </ul>"},{"location":"planning/freespace_planning_algorithms/#guide-to-implement-a-new-algorithm","title":"Guide to implement a new algorithm","text":"<ul> <li>All planning algorithm class in this package must inherit <code>AbstractPlanningAlgorithm</code>   class. If necessary, please overwrite the virtual functions.</li> <li>All algorithms must use <code>nav_msgs::OccupancyGrid</code>-typed costmap.   Thus, <code>AbstractPlanningAlgorithm</code> class mainly implements the collision checking   using the costmap, grid-based indexing, and coordinate transformation related to   costmap.</li> <li>All algorithms must take both <code>PlannerCommonParam</code>-typed and algorithm-specific-   type structs as inputs of the constructor. For example, <code>AstarSearch</code> class's   constructor takes both <code>PlannerCommonParam</code> and <code>AstarParam</code>.</li> </ul>"},{"location":"planning/freespace_planning_algorithms/#running-the-standalone-tests-and-visualization","title":"Running the standalone tests and visualization","text":"<p>Building the package with ros-test and run tests:</p> <pre><code>colcon build --packages-select freespace_planning_algorithms\ncolcon test --packages-select freespace_planning_algorithms\n</code></pre> <p>Inside the test, simulation results are stored in <code>/tmp/fpalgos-{algorithm_type}-case{scenario_number}</code> as a rosbag. Loading these resulting files, by using test/debug_plot.py, one can create plots visualizing the path and obstacles as shown in the figures below. The created figures are then again saved in <code>/tmp</code>.</p>"},{"location":"planning/freespace_planning_algorithms/#a-single-curvature-case","title":"A* (single curvature case)","text":""},{"location":"planning/freespace_planning_algorithms/#informed-rrt-with-200-msec-time-budget","title":"informed RRT* with 200 msec time budget","text":""},{"location":"planning/freespace_planning_algorithms/#rrt-without-update-almost-same-as-rrt","title":"RRT* without update (almost same as RRT)","text":"<p>The black cells, green box, and red box, respectively, indicate obstacles, start configuration, and goal configuration. The sequence of the blue boxes indicate the solution path.</p>"},{"location":"planning/freespace_planning_algorithms/#license-notice","title":"License notice","text":"<p>Files <code>src/reeds_shepp.cpp</code> and <code>include/astar_search/reeds_shepp.h</code> are fetched from pyReedsShepp. Note that the implementation in <code>pyReedsShepp</code> is also heavily based on the code in ompl. Both <code>pyReedsShepp</code> and <code>ompl</code> are distributed under 3-clause BSD license.</p>"},{"location":"planning/freespace_planning_algorithms/rrtstar/","title":"Rrtstar","text":""},{"location":"planning/freespace_planning_algorithms/rrtstar/#note-on-implementation-of-informed-rrt","title":"Note on implementation of informed RRT*","text":""},{"location":"planning/freespace_planning_algorithms/rrtstar/#preliminary-knowledge-on-informed-rrt","title":"Preliminary knowledge on informed-RRT*","text":"<p>Let us define \\(f(x)\\) as minimum cost of the path when path is constrained to pass through \\(x\\) (so path will be \\(x_{\\mathrm{start}} \\to \\mathrm{x} \\to \\mathrm{x_{\\mathrm{goal}}}\\)). Also, let us define \\(c_{\\mathrm{best}}\\) as the current minimum cost of the feasible paths. Let us define a set $ X(f) = \\left{ x \\in X | f(x) &lt; c*{\\mathrm{best}} \\right} $. If we could sample a new point from \\(X_f\\) instead of \\(X\\) as in vanilla RRT*, chance that \\(c*{\\mathrm{best}}\\) is updated is increased, thus the convergence rate is improved.</p> <p>In most case, \\(f(x)\\) is unknown, thus it is straightforward to approximate the function \\(f\\) by a heuristic function \\(\\hat{f}\\). A heuristic function is admissible if \\(\\forall x \\in X, \\hat{f}(x) &lt; f(x)\\), which is sufficient condition of conversion to optimal path. The good heuristic function \\(\\hat{f}\\) has two properties: 1) it is an admissible tight lower bound of \\(f\\) and 2) sampling from \\(X(\\hat{f})\\) is easy.</p> <p>According to Gammell et al [1], a good heuristic function when path is always straight is \\(\\hat{f}(x) = ||x_{\\mathrm{start}} - x|| + ||x - x_{\\mathrm{goal}}||\\). If we don't assume any obstacle information the heuristic is tightest. Also, \\(X(\\hat{f})\\) is hyper-ellipsoid, and hence sampling from it can be done analytically.</p>"},{"location":"planning/freespace_planning_algorithms/rrtstar/#modification-to-fit-reeds-sheep-path-case","title":"Modification to fit reeds-sheep path case","text":"<p>In the vehicle case, state is \\(x = (x_{1}, x_{2}, \\theta)\\). Unlike normal informed-RRT* where we can connect path by a straight line, here we connect the vehicle path by a reeds-sheep path. So, we need some modification of the original algorithm a bit. To this end, one might first consider a heuristic function \\(\\hat{f}_{\\mathrm{RS}}(x) = \\mathrm{RS}(x_{\\mathrm{start}}, x) + \\mathrm{RS}(x, x_{\\mathrm{goal}}) &lt; f(x)\\) where \\(\\mathrm{RS}\\) computes reeds-sheep distance. Though it is good in the sense of tightness, however, sampling from \\(X(\\hat{f}_{RS})\\) is really difficult. Therefore, we use \\(\\hat{f}_{euc} = ||\\mathrm{pos}(x_{\\mathrm{start}}) - \\mathrm{pos}(x)|| + ||\\mathrm{pos}(x)- \\mathrm{pos}(x_{\\mathrm{goal}})||\\), which is admissible because \\(\\forall x \\in X, \\hat{f}_{euc}(x) &lt; \\hat{f}_{\\mathrm{RS}}(x) &lt; f(x)\\). Here, \\(\\mathrm{pos}\\) function returns position \\((x_{1}, x_{2})\\) of the vehicle.</p> <p>Sampling from \\(X(\\hat{f}_{\\mathrm{euc}})\\) is easy because \\(X(\\hat{f}_{\\mathrm{euc}}) = \\mathrm{Ellipse} \\times (-\\pi, \\pi]\\). Here \\(\\mathrm{Ellipse}\\)'s focal points are \\(x_{\\mathrm{start}}\\) and \\(x_{\\mathrm{goal}}\\) and conjugate diameters is $\\sqrt{c^{2}{\\mathrm{best}} - ||\\mathrm{pos}(x}) - \\mathrm{pos}(x_{\\mathrm{goal}}))|| } $ (similar to normal informed-rrtstar's ellipsoid). Please notice that \\(\\theta\\) can be arbitrary because \\(\\hat{f}_{\\mathrm{euc}}\\) is independent of \\(\\theta\\).</p> <p>[1] Gammell et al., \"Informed RRT*: Optimal sampling-based path planning focused via direct sampling of an admissible ellipsoidal heuristic.\" IROS (2014)</p>"},{"location":"planning/mission_planner/","title":"Mission Planner","text":""},{"location":"planning/mission_planner/#mission-planner","title":"Mission Planner","text":""},{"location":"planning/mission_planner/#purpose","title":"Purpose","text":"<p><code>Mission Planner</code> calculates a route that navigates from the current ego pose to the goal pose following the given check points. The route is made of a sequence of lanes on a static map. Dynamic objects (e.g. pedestrians and other vehicles) and dynamic map information (e.g. road construction which blocks some lanes) are not considered during route planning. Therefore, the output topic is only published when the goal pose or check points are given and will be latched until the new goal pose or check points are given.</p> <p>The core implementation does not depend on a map format. In current Autoware.universe, only Lanelet2 map format is supported.</p>"},{"location":"planning/mission_planner/#interfaces","title":"Interfaces","text":""},{"location":"planning/mission_planner/#parameters","title":"Parameters","text":"Name Type Description <code>map_frame</code> string The frame name for map <code>arrival_check_angle_deg</code> double Angle threshold for goal check <code>arrival_check_distance</code> double Distance threshold for goal check <code>arrival_check_duration</code> double Duration threshold for goal check <code>goal_angle_threshold</code> double Max goal pose angle for goal approve <code>enable_correct_goal_pose</code> bool Enabling correction of goal pose according to the closest lanelet orientation <code>reroute_time_threshold</code> double If the time to the rerouting point at the current velocity is greater than this threshold, rerouting is possible <code>minimum_reroute_length</code> double Minimum Length for publishing a new route <code>consider_no_drivable_lanes</code> bool This flag is for considering no_drivable_lanes in planning or not."},{"location":"planning/mission_planner/#services","title":"Services","text":"Name Type Description <code>/planning/mission_planning/clear_route</code> autoware_adapi_v1_msgs/srv/ClearRoute route clear request <code>/planning/mission_planning/set_route_points</code> autoware_adapi_v1_msgs/srv/SetRoutePoints route request with pose waypoints. Assumed the vehicle is stopped. <code>/planning/mission_planning/set_route</code> autoware_adapi_v1_msgs/srv/SetRoute route request with lanelet waypoints. Assumed the vehicle is stopped. <code>/planning/mission_planning/change_route_points</code> autoware_adapi_v1_msgs/srv/SetRoutePoints route change request with pose waypoints. This can be called when the vehicle is moving. <code>/planning/mission_planning/change_route</code> autoware_adapi_v1_msgs/srv/SetRoute route change request with lanelet waypoints. This can be called when the vehicle is moving. <code>~/srv/set_mrm_route</code> autoware_adapi_v1_msgs/srv/SetRoutePoints set emergency route. This can be called when the vehicle is moving. <code>~/srv/clear_mrm_route</code> std_srvs/srv/Trigger clear emergency route."},{"location":"planning/mission_planner/#subscriptions","title":"Subscriptions","text":"Name Type Description <code>input/vector_map</code> autoware_auto_mapping_msgs/HADMapBin vector map of Lanelet2 <code>input/modified_goal</code> geometry_msgs/PoseWithUuidStamped modified goal pose"},{"location":"planning/mission_planner/#publications","title":"Publications","text":"Name Type Description <code>/planning/mission_planning/route_state</code> autoware_adapi_v1_msgs/msg/RouteState route state <code>/planning/mission_planning/route</code> autoware_planning_msgs/LaneletRoute route <code>debug/route_marker</code> visualization_msgs/msg/MarkerArray route marker for debug <code>debug/goal_footprint</code> visualization_msgs/msg/MarkerArray goal footprint for debug"},{"location":"planning/mission_planner/#route-section","title":"Route section","text":"<p>Route section, whose type is <code>autoware_planning_msgs/LaneletSegment</code>, is a \"slice\" of a road that bundles lane changeable lanes. Note that the most atomic unit of route is <code>autoware_auto_mapping_msgs/LaneletPrimitive</code>, which has the unique id of a lane in a vector map and its type. Therefore, route message does not contain geometric information about the lane since we did not want to have planning module\u2019s message to have dependency on map data structure.</p> <p>The ROS message of route section contains following three elements for each route section.</p> <ul> <li><code>preferred_primitive</code>: Preferred lane to follow towards the goal.</li> <li><code>primitives</code>: All neighbor lanes in the same direction including the preferred lane.</li> </ul>"},{"location":"planning/mission_planner/#goal-validation","title":"Goal Validation","text":"<p>The mission planner has control mechanism to validate the given goal pose and create a route. If goal pose angle between goal pose lanelet and goal pose' yaw is greater than <code>goal_angle_threshold</code> parameter, the goal is rejected. Another control mechanism is the creation of a footprint of the goal pose according to the dimensions of the vehicle and checking whether this footprint is within the lanelets. If goal footprint exceeds lanelets, then the goal is rejected.</p> <p>At the image below, there are sample goal pose validation cases.</p> <p></p>"},{"location":"planning/mission_planner/#implementation","title":"Implementation","text":""},{"location":"planning/mission_planner/#mission-planner_1","title":"Mission Planner","text":"<p>Two callbacks (goal and check points) are a trigger for route planning. Routing graph, which plans route in Lanelet2, must be created before those callbacks, and this routing graph is created in vector map callback.</p> <p><code>plan route</code> is explained in detail in the following section.</p> <p></p>"},{"location":"planning/mission_planner/#route-planner","title":"Route Planner","text":"<p><code>plan route</code> is executed with check points including current ego pose and goal pose.</p> <p></p> <p><code>plan path between each check points</code> firstly calculates closest lanes to start and goal pose. Then routing graph of Lanelet2 plans the shortest path from start and goal pose.</p> <p><code>initialize route lanelets</code> initializes route handler, and calculates <code>route_lanelets</code>. <code>route_lanelets</code>, all of which will be registered in route sections, are lanelets next to the lanelets in the planned path, and used when planning lane change. To calculate <code>route_lanelets</code>,</p> <ol> <li>All the neighbor (right and left) lanes for the planned path which is lane-changeable is memorized as <code>route_lanelets</code>.</li> <li>All the neighbor (right and left) lanes for the planned path which is not lane-changeable is memorized as <code>candidate_lanelets</code>.</li> <li>If the following and previous lanelets of each <code>candidate_lanelets</code> are <code>route_lanelets</code>, the <code>candidate_lanelet</code> is registered as <code>route_lanelets</code><ul> <li>This is because even though <code>candidate_lanelet</code> (an adjacent lane) is not lane-changeable, we can pass the <code>candidate_lanelet</code> without lane change if the following and previous lanelets of the <code>candidate_lanelet</code> are <code>route_lanelets</code></li> </ul> </li> </ol> <p><code>get preferred lanelets</code> extracts <code>preferred_primitive</code> from <code>route_lanelets</code> with the route handler.</p> <p><code>create route sections</code> extracts <code>primitives</code> from <code>route_lanelets</code> for each route section with the route handler, and creates route sections.</p>"},{"location":"planning/mission_planner/#rerouting","title":"Rerouting","text":"<p>Reroute here means changing the route while driving. Unlike route setting, it is required to keep a certain distance from vehicle to the point where the route is changed.</p> <p></p> <p>And there are three use cases that require reroute.</p> <ul> <li>Route change API</li> <li>Emergency route</li> <li>Goal modification</li> </ul> <p></p>"},{"location":"planning/mission_planner/#route-change-api","title":"Route change API","text":"<ul> <li><code>change_route_points</code></li> <li><code>change_route</code></li> </ul> <p>This is route change that the application makes using the API. It is used when changing the destination while driving or when driving a divided loop route. When the vehicle is driving on a MRM route, normal rerouting by this interface is not allowed.</p>"},{"location":"planning/mission_planner/#emergency-route","title":"Emergency route","text":"<ul> <li><code>set_mrm_route</code></li> <li><code>clear_mrm_route</code></li> </ul> <p>This interface for the MRM that pulls over the road shoulder. It has to be stopped as soon as possible, so a reroute is required. The MRM route has priority over the normal route. And if MRM route is cleared, try to return to the normal route also with a rerouting safety check.</p>"},{"location":"planning/mission_planner/#goal-modification","title":"Goal modification","text":"<ul> <li><code>modified_goal</code></li> </ul> <p>This is a goal change to pull over, avoid parked vehicles, and so on by a planning component. If the modified goal is outside the calculated route, a reroute is required. This goal modification is executed by checking the local environment and path safety as the vehicle actually approaches the destination. And this modification is allowed for both normal_route and mrm_route. The new route generated here is sent to the AD API so that it can also be referenced by the application. Note, however, that the specifications here are subject to change in the future.</p>"},{"location":"planning/mission_planner/#rerouting-limitations","title":"Rerouting Limitations","text":"<ul> <li>The safety judgment of rerouting is not guaranteed to the level of trajectory or control. Therefore, the distance to the reroute change must be large for the safety.</li> <li>The validity of the <code>modified_goal</code> needs to be guaranteed by the behavior_path_planner, e.g., that it is not placed in the wrong lane, that it can be safely rerouted, etc.</li> </ul>"},{"location":"planning/mission_planner/#limitations","title":"Limitations","text":"<ul> <li>Dynamic objects (e.g. pedestrians and other vehicles) and dynamic map information (e.g. road construction which blocks some lanes) are not considered during route planning.</li> <li>Looped route is not supported.</li> </ul>"},{"location":"planning/motion_velocity_smoother/","title":"Motion Velocity Smoother","text":""},{"location":"planning/motion_velocity_smoother/#motion-velocity-smoother","title":"Motion Velocity Smoother","text":""},{"location":"planning/motion_velocity_smoother/#purpose","title":"Purpose","text":"<p><code>motion_velocity_smoother</code> outputs a desired velocity profile on a reference trajectory. This module plans a velocity profile within the limitations of the velocity, the acceleration and the jerk to realize both the maximization of velocity and the ride quality. We call this module <code>motion_velocity_smoother</code> because the limitations of the acceleration and the jerk means the smoothness of the velocity profile.</p>"},{"location":"planning/motion_velocity_smoother/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/motion_velocity_smoother/#flow-chart","title":"Flow chart","text":""},{"location":"planning/motion_velocity_smoother/#extract-trajectory","title":"Extract trajectory","text":"<p>For the point on the reference trajectory closest to the center of the rear wheel axle of the vehicle, it extracts the reference path between <code>extract_behind_dist</code> behind and <code>extract_ahead_dist</code> ahead.</p>"},{"location":"planning/motion_velocity_smoother/#apply-external-velocity-limit","title":"Apply external velocity limit","text":"<p>It applies the velocity limit input from the external of <code>motion_velocity_smoother</code>. Remark that the external velocity limit is different from the velocity limit already set on the map and the reference trajectory. The external velocity is applied at the position that it is able to reach the velocity limit with the deceleration and the jerk constraints set as the parameter.</p>"},{"location":"planning/motion_velocity_smoother/#apply-stop-approaching-velocity","title":"Apply stop approaching velocity","text":"<p>It applies the velocity limit near the stopping point. This function is used to approach near the obstacle or improve the accuracy of stopping.</p>"},{"location":"planning/motion_velocity_smoother/#apply-lateral-acceleration-limit","title":"Apply lateral acceleration limit","text":"<p>It applies the velocity limit to decelerate at the curve. It calculates the velocity limit from the curvature of the reference trajectory and the maximum lateral acceleration <code>max_lateral_accel</code>. The velocity limit is set as not to fall under <code>min_curve_velocity</code>.</p> <p>Note: velocity limit that requests larger than <code>nominal.jerk</code> is not applied. In other words, even if a sharp curve is planned just in front of the ego, no deceleration is performed.</p>"},{"location":"planning/motion_velocity_smoother/#apply-steering-rate-limit","title":"Apply steering rate limit","text":"<p>It calculates the desired steering angles of trajectory points. and it applies the steering rate limit. If the (<code>steering_angle_rate</code> &gt; <code>max_steering_angle_rate</code>), it decreases the velocity of the trajectory point to acceptable velocity.</p>"},{"location":"planning/motion_velocity_smoother/#resample-trajectory","title":"Resample trajectory","text":"<p>It resamples the points on the reference trajectory with designated time interval. Note that the range of the length of the trajectory is set between <code>min_trajectory_length</code> and <code>max_trajectory_length</code>, and the distance between two points is longer than <code>min_trajectory_interval_distance</code>. It samples densely up to the distance traveled between <code>resample_time</code> with the current velocity, then samples sparsely after that. By sampling according to the velocity, both calculation load and accuracy are achieved since it samples finely at low velocity and coarsely at high velocity.</p>"},{"location":"planning/motion_velocity_smoother/#calculate-initial-state","title":"Calculate initial state","text":"<p>Calculate initial values for velocity planning. The initial values are calculated according to the situation as shown in the following table.</p> Situation Initial velocity Initial acceleration First calculation Current velocity 0.0 Engaging <code>engage_velocity</code> <code>engage_acceleration</code> Deviate between the planned velocity and the current velocity Current velocity Previous planned value Normal Previous planned value Previous planned value"},{"location":"planning/motion_velocity_smoother/#smooth-velocity","title":"Smooth velocity","text":"<p>It plans the velocity. The algorithm of velocity planning is chosen from <code>JerkFiltered</code>, <code>L2</code> and <code>Linf</code>, and it is set in the launch file. In these algorithms, they use OSQP[1] as the solver of the optimization.</p>"},{"location":"planning/motion_velocity_smoother/#jerkfiltered","title":"JerkFiltered","text":"<p>It minimizes the sum of the minus of the square of the velocity and the square of the violation of the velocity limit, the acceleration limit and the jerk limit.</p>"},{"location":"planning/motion_velocity_smoother/#l2","title":"L2","text":"<p>It minimizes the sum of the minus of the square of the velocity, the square of the the pseudo-jerk[2] and the square of the violation of the velocity limit and the acceleration limit.</p>"},{"location":"planning/motion_velocity_smoother/#linf","title":"Linf","text":"<p>It minimizes the sum of the minus of the square of the velocity, the maximum absolute value of the the pseudo-jerk[2] and the square of the violation of the velocity limit and the acceleration limit.</p>"},{"location":"planning/motion_velocity_smoother/#post-process","title":"Post process","text":"<p>It performs the post-process of the planned velocity.</p> <ul> <li>Set zero velocity ahead of the stopping point</li> <li>Set maximum velocity given in the config named <code>max_velocity</code></li> <li>Set velocity behind the current pose</li> <li>Resample trajectory (<code>post resampling</code>)</li> <li>Output debug data</li> </ul> <p>After the optimization, a resampling called <code>post resampling</code> is performed before passing the optimized trajectory to the next node. Since the required path interval from optimization may be different from the one for the next module, <code>post resampling</code> helps to fill this gap. Therefore, in <code>post resampling</code>, it is necessary to check the path specification of the following module to determine the parameters. Note that if the computational load of the optimization algorithm is high and the path interval is sparser than the path specification of the following module in the first resampling, <code>post resampling</code> would resample the trajectory densely. On the other hand, if the computational load of the optimization algorithm is small and the path interval is denser than the path specification of the following module in the first resampling, the path is sparsely resampled according to the specification of the following module.</p>"},{"location":"planning/motion_velocity_smoother/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/motion_velocity_smoother/#input","title":"Input","text":"Name Type Description <code>~/input/trajectory</code> <code>autoware_auto_planning_msgs/Trajectory</code> Reference trajectory <code>/planning/scenario_planning/max_velocity</code> <code>std_msgs/Float32</code> External velocity limit [m/s] <code>/localization/kinematic_state</code> <code>nav_msgs/Odometry</code> Current odometry <code>/tf</code> <code>tf2_msgs/TFMessage</code> TF <code>/tf_static</code> <code>tf2_msgs/TFMessage</code> TF static"},{"location":"planning/motion_velocity_smoother/#output","title":"Output","text":"Name Type Description <code>~/output/trajectory</code> <code>autoware_auto_planning_msgs/Trajectory</code> Modified trajectory <code>/planning/scenario_planning/current_max_velocity</code> <code>std_msgs/Float32</code> Current external velocity limit [m/s] <code>~/closest_velocity</code> <code>std_msgs/Float32</code> Planned velocity closest to ego base_link (for debug) <code>~/closest_acceleration</code> <code>std_msgs/Float32</code> Planned acceleration closest to ego base_link (for debug) <code>~/closest_jerk</code> <code>std_msgs/Float32</code> Planned jerk closest to ego base_link (for debug) <code>~/debug/trajectory_raw</code> <code>autoware_auto_planning_msgs/Trajectory</code> Extracted trajectory (for debug) <code>~/debug/trajectory_external_velocity_limited</code> <code>autoware_auto_planning_msgs/Trajectory</code> External velocity limited trajectory (for debug) <code>~/debug/trajectory_lateral_acc_filtered</code> <code>autoware_auto_planning_msgs/Trajectory</code> Lateral acceleration limit filtered trajectory (for debug) <code>~/debug/trajectory_steering_rate_limited</code> <code>autoware_auto_planning_msgs/Trajectory</code> Steering angle rate limit filtered trajectory (for debug) <code>~/debug/trajectory_time_resampled</code> <code>autoware_auto_planning_msgs/Trajectory</code> Time resampled trajectory (for debug) <code>~/distance_to_stopline</code> <code>std_msgs/Float32</code> Distance to stop line from current ego pose (max 50 m) (for debug) <code>~/stop_speed_exceeded</code> <code>std_msgs/Bool</code> It publishes <code>true</code> if planned velocity on the point which the maximum velocity is zero is over threshold"},{"location":"planning/motion_velocity_smoother/#parameters","title":"Parameters","text":""},{"location":"planning/motion_velocity_smoother/#constraint-parameters","title":"Constraint parameters","text":"Name Type Description Default value <code>max_velocity</code> <code>double</code> Max velocity limit [m/s] 20.0 <code>max_accel</code> <code>double</code> Max acceleration limit [m/ss] 1.0 <code>min_decel</code> <code>double</code> Min deceleration limit [m/ss] -0.5 <code>stop_decel</code> <code>double</code> Stop deceleration value at a stop point [m/ss] 0.0 <code>max_jerk</code> <code>double</code> Max jerk limit [m/sss] 1.0 <code>min_jerk</code> <code>double</code> Min jerk limit [m/sss] -0.5"},{"location":"planning/motion_velocity_smoother/#external-velocity-limit-parameter","title":"External velocity limit parameter","text":"Name Type Description Default value <code>margin_to_insert_external_velocity_limit</code> <code>double</code> margin distance to insert external velocity limit [m] 0.3"},{"location":"planning/motion_velocity_smoother/#curve-parameters","title":"Curve parameters","text":"Name Type Description Default value <code>enable_lateral_acc_limit</code> <code>bool</code> To toggle the lateral acceleration filter on and off. You can switch it dynamically at runtime. true <code>max_lateral_accel</code> <code>double</code> Max lateral acceleration limit [m/ss] 0.5 <code>min_curve_velocity</code> <code>double</code> Min velocity at lateral acceleration limit [m/ss] 2.74 <code>decel_distance_before_curve</code> <code>double</code> Distance to slowdown before a curve for lateral acceleration limit [m] 3.5 <code>decel_distance_after_curve</code> <code>double</code> Distance to slowdown after a curve for lateral acceleration limit [m] 2.0 <code>min_decel_for_lateral_acc_lim_filter</code> <code>double</code> Deceleration limit to avoid sudden braking by the lateral acceleration filter [m/ss]. Strong limitation degrades the deceleration response to the appearance of sharp curves due to obstacle avoidance, etc. -2.5"},{"location":"planning/motion_velocity_smoother/#engage-replan-parameters","title":"Engage &amp; replan parameters","text":"Name Type Description Default value <code>replan_vel_deviation</code> <code>double</code> Velocity deviation to replan initial velocity [m/s] 5.53 <code>engage_velocity</code> <code>double</code> Engage velocity threshold [m/s] (if the trajectory velocity is higher than this value, use this velocity for engage vehicle speed) 0.25 <code>engage_acceleration</code> <code>double</code> Engage acceleration [m/ss] (use this acceleration when engagement) 0.1 <code>engage_exit_ratio</code> <code>double</code> Exit engage sequence to normal velocity planning when the velocity exceeds engage_exit_ratio x engage_velocity. 0.5 <code>stop_dist_to_prohibit_engage</code> <code>double</code> If the stop point is in this distance, the speed is set to 0 not to move the vehicle [m] 0.5"},{"location":"planning/motion_velocity_smoother/#stopping-velocity-parameters","title":"Stopping velocity parameters","text":"Name Type Description Default value <code>stopping_velocity</code> <code>double</code> change target velocity to this value before v=0 point [m/s] 2.778 <code>stopping_distance</code> <code>double</code> distance for the stopping_velocity [m]. 0 means the stopping velocity is not applied. 0.0"},{"location":"planning/motion_velocity_smoother/#extraction-parameters","title":"Extraction parameters","text":"Name Type Description Default value <code>extract_ahead_dist</code> <code>double</code> Forward trajectory distance used for planning [m] 200.0 <code>extract_behind_dist</code> <code>double</code> backward trajectory distance used for planning [m] 5.0 <code>delta_yaw_threshold</code> <code>double</code> Allowed delta yaw between ego pose and trajectory pose [radian] 1.0472"},{"location":"planning/motion_velocity_smoother/#resampling-parameters","title":"Resampling parameters","text":"Name Type Description Default value <code>max_trajectory_length</code> <code>double</code> Max trajectory length for resampling [m] 200.0 <code>min_trajectory_length</code> <code>double</code> Min trajectory length for resampling [m] 30.0 <code>resample_time</code> <code>double</code> Resample total time [s] 10.0 <code>dense_dt</code> <code>double</code> resample time interval for dense sampling [s] 0.1 <code>dense_min_interval_distance</code> <code>double</code> minimum points-interval length for dense sampling [m] 0.1 <code>sparse_dt</code> <code>double</code> resample time interval for sparse sampling [s] 0.5 <code>sparse_min_interval_distance</code> <code>double</code> minimum points-interval length for sparse sampling [m] 4.0"},{"location":"planning/motion_velocity_smoother/#resampling-parameters-for-post-process","title":"Resampling parameters for post process","text":"Name Type Description Default value <code>post_max_trajectory_length</code> <code>double</code> max trajectory length for resampling [m] 300.0 <code>post_min_trajectory_length</code> <code>double</code> min trajectory length for resampling [m] 30.0 <code>post_resample_time</code> <code>double</code> resample total time for dense sampling [s] 10.0 <code>post_dense_dt</code> <code>double</code> resample time interval for dense sampling [s] 0.1 <code>post_dense_min_interval_distance</code> <code>double</code> minimum points-interval length for dense sampling [m] 0.1 <code>post_sparse_dt</code> <code>double</code> resample time interval for sparse sampling [s] 0.1 <code>post_sparse_min_interval_distance</code> <code>double</code> minimum points-interval length for sparse sampling [m] 1.0"},{"location":"planning/motion_velocity_smoother/#limit-steering-angle-rate-parameters","title":"Limit steering angle rate parameters","text":"Name Type Description Default value <code>enable_steering_rate_limit</code> <code>bool</code> To toggle the steer rate filter on and off. You can switch it dynamically at runtime. true <code>max_steering_angle_rate</code> <code>double</code> Maximum steering angle rate [degree/s] 40.0 <code>resample_ds</code> <code>double</code> Distance between trajectory points [m] 0.1 <code>curvature_threshold</code> <code>double</code> If curvature &gt; curvature_threshold, steeringRateLimit is triggered [1/m] 0.02 <code>curvature_calculation_distance</code> <code>double</code> Distance of points while curvature is calculating [m] 1.0"},{"location":"planning/motion_velocity_smoother/#weights-for-optimization","title":"Weights for optimization","text":""},{"location":"planning/motion_velocity_smoother/#jerkfiltered_1","title":"JerkFiltered","text":"Name Type Description Default value <code>jerk_weight</code> <code>double</code> Weight for \"smoothness\" cost for jerk 10.0 <code>over_v_weight</code> <code>double</code> Weight for \"over speed limit\" cost 100000.0 <code>over_a_weight</code> <code>double</code> Weight for \"over accel limit\" cost 5000.0 <code>over_j_weight</code> <code>double</code> Weight for \"over jerk limit\" cost 1000.0"},{"location":"planning/motion_velocity_smoother/#l2_1","title":"L2","text":"Name Type Description Default value <code>pseudo_jerk_weight</code> <code>double</code> Weight for \"smoothness\" cost 100.0 <code>over_v_weight</code> <code>double</code> Weight for \"over speed limit\" cost 100000.0 <code>over_a_weight</code> <code>double</code> Weight for \"over accel limit\" cost 1000.0"},{"location":"planning/motion_velocity_smoother/#linf_1","title":"Linf","text":"Name Type Description Default value <code>pseudo_jerk_weight</code> <code>double</code> Weight for \"smoothness\" cost 100.0 <code>over_v_weight</code> <code>double</code> Weight for \"over speed limit\" cost 100000.0 <code>over_a_weight</code> <code>double</code> Weight for \"over accel limit\" cost 1000.0"},{"location":"planning/motion_velocity_smoother/#others","title":"Others","text":"Name Type Description Default value <code>over_stop_velocity_warn_thr</code> <code>double</code> Threshold to judge that the optimized velocity exceeds the input velocity on the stop point [m/s] 1.389"},{"location":"planning/motion_velocity_smoother/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>Assume that the velocity limit or the stopping point is properly set at the point on the reference trajectory</li> <li>If the velocity limit set in the reference path cannot be achieved by the designated constraints of the deceleration and the jerk, decelerate while suppressing the velocity, the acceleration and the jerk deviation as much as possible</li> <li>The importance of the deviations is set in the config file</li> </ul>"},{"location":"planning/motion_velocity_smoother/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"planning/motion_velocity_smoother/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"planning/motion_velocity_smoother/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>[1] B. Stellato, et al., \"OSQP: an operator splitting solver for quadratic programs\", Mathematical Programming Computation, 2020, 10.1007/s12532-020-00179-2.</p> <p>[2] Y. Zhang, et al., \"Toward a More Complete, Flexible, and Safer Speed Planning for Autonomous Driving via Convex Optimization\", Sensors, vol. 18, no. 7, p. 2185, 2018, 10.3390/s18072185</p>"},{"location":"planning/motion_velocity_smoother/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"planning/motion_velocity_smoother/README.ja/","title":"Motion Velocity Smoother","text":""},{"location":"planning/motion_velocity_smoother/README.ja/#motion-velocity-smoother","title":"Motion Velocity Smoother","text":""},{"location":"planning/motion_velocity_smoother/README.ja/#purpose","title":"Purpose","text":"<p><code>motion_velocity_smoother</code>\u306f\u76ee\u6a19\u8ecc\u9053\u4e0a\u306e\u5404\u70b9\u306b\u304a\u3051\u308b\u671b\u307e\u3057\u3044\u8eca\u901f\u3092\u8a08\u753b\u3057\u3066\u51fa\u529b\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3042\u308b\u3002 \u3053\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u306f\u3001\u901f\u5ea6\u306e\u6700\u5927\u5316\u3068\u4e57\u308a\u5fc3\u5730\u306e\u826f\u3055\u3092\u4e21\u7acb\u3059\u308b\u305f\u3081\u306b\u3001\u4e8b\u524d\u306b\u6307\u5b9a\u3055\u308c\u305f\u5236\u9650\u901f\u5ea6\u3001\u5236\u9650\u52a0\u901f\u5ea6\u304a\u3088\u3073\u5236\u9650\u8e8d\u5ea6\u306e\u7bc4\u56f2\u3067\u8eca\u901f\u3092\u8a08\u753b\u3059\u308b\u3002 \u52a0\u901f\u5ea6\u3084\u8e8d\u5ea6\u306e\u5236\u9650\u3092\u4e0e\u3048\u308b\u3053\u3068\u306f\u8eca\u901f\u306e\u5909\u5316\u3092\u6ed1\u3089\u304b\u306b\u3059\u308b\u3053\u3068\u306b\u5bfe\u5fdc\u3059\u308b\u305f\u3081\u3001\u3053\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u3092<code>motion_velocity_smoother</code>\u3068\u547c\u3093\u3067\u3044\u308b\u3002</p>"},{"location":"planning/motion_velocity_smoother/README.ja/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/motion_velocity_smoother/README.ja/#flow-chart","title":"Flow chart","text":""},{"location":"planning/motion_velocity_smoother/README.ja/#extract-trajectory","title":"Extract trajectory","text":"<p>\u81ea\u8eca\u5f8c\u8f2a\u8ef8\u4e2d\u5fc3\u4f4d\u7f6e\u306b\u6700\u3082\u8fd1\u3044\u53c2\u7167\u7d4c\u8def\u4e0a\u306e\u70b9\u306b\u5bfe\u3057\u3001<code>extract_behind_dist</code>\u3060\u3051\u623b\u3063\u305f\u70b9\u304b\u3089<code>extract_ahead_dist</code>\u3060\u3051\u9032\u3093\u3060\u70b9\u307e\u3067\u306e\u53c2\u7167\u7d4c\u8def\u3092\u629c\u304d\u51fa\u3059\u3002</p>"},{"location":"planning/motion_velocity_smoother/README.ja/#apply-external-velocity-limit","title":"Apply external velocity limit","text":"<p>\u30e2\u30b8\u30e5\u30fc\u30eb\u5916\u90e8\u304b\u3089\u6307\u5b9a\u3055\u308c\u305f\u901f\u5ea6\u5236\u9650\u3092\u9069\u7528\u3059\u308b\u3002 \u3053\u3053\u3067\u6271\u3046\u5916\u90e8\u306e\u901f\u5ea6\u5236\u9650\u306f<code>/planning/scenario_planning/max_velocity</code>\u306e topic \u3067\u6e21\u3055\u308c\u308b\u3082\u306e\u3067\u3001\u5730\u56f3\u4e0a\u3067\u8a2d\u5b9a\u3055\u308c\u305f\u901f\u5ea6\u5236\u9650\u306a\u3069\u3001\u53c2\u7167\u7d4c\u8def\u306b\u3059\u3067\u306b\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u5236\u9650\u901f\u5ea6\u3068\u306f\u5225\u3067\u3042\u308b\u3002 \u5916\u90e8\u304b\u3089\u6307\u5b9a\u3055\u308c\u308b\u901f\u5ea6\u5236\u9650\u306f\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u6e1b\u901f\u5ea6\u304a\u3088\u3073\u8e8d\u5ea6\u306e\u5236\u9650\u306e\u7bc4\u56f2\u3067\u6e1b\u901f\u53ef\u80fd\u306a\u4f4d\u7f6e\u304b\u3089\u901f\u5ea6\u5236\u9650\u3092\u9069\u7528\u3059\u308b\u3002</p>"},{"location":"planning/motion_velocity_smoother/README.ja/#apply-stop-approaching-velocity","title":"Apply stop approaching velocity","text":"<p>\u505c\u6b62\u70b9\u306b\u8fd1\u3065\u3044\u305f\u3068\u304d\u306e\u901f\u5ea6\u3092\u8a2d\u5b9a\u3059\u308b\u3002\u969c\u5bb3\u7269\u8fd1\u508d\u307e\u3067\u8fd1\u3065\u304f\u5834\u5408\u3084\u3001\u6b63\u7740\u7cbe\u5ea6\u5411\u4e0a\u306a\u3069\u306e\u76ee\u7684\u306b\u7528\u3044\u308b\u3002</p>"},{"location":"planning/motion_velocity_smoother/README.ja/#apply-lateral-acceleration-limit","title":"Apply lateral acceleration limit","text":"<p>\u7d4c\u8def\u306e\u66f2\u7387\u306b\u5fdc\u3058\u3066\u3001\u6307\u5b9a\u3055\u308c\u305f\u6700\u5927\u6a2a\u52a0\u901f\u5ea6<code>max_lateral_accel</code>\u3092\u8d85\u3048\u306a\u3044\u901f\u5ea6\u3092\u5236\u9650\u901f\u5ea6\u3068\u3057\u3066\u8a2d\u5b9a\u3059\u308b\u3002\u305f\u3060\u3057\u3001\u5236\u9650\u901f\u5ea6\u306f<code>min_curve_velocity</code>\u3092\u4e0b\u56de\u3089\u306a\u3044\u3088\u3046\u306b\u8a2d\u5b9a\u3059\u308b\u3002</p>"},{"location":"planning/motion_velocity_smoother/README.ja/#resample-trajectory","title":"Resample trajectory","text":"<p>\u6307\u5b9a\u3055\u308c\u305f\u6642\u9593\u9593\u9694\u3067\u7d4c\u8def\u306e\u70b9\u3092\u518d\u30b5\u30f3\u30d7\u30eb\u3059\u308b\u3002\u305f\u3060\u3057\u3001\u7d4c\u8def\u5168\u4f53\u306e\u9577\u3055\u306f<code>min_trajectory_length</code>\u304b\u3089<code>max_trajectory_length</code>\u306e\u9593\u3068\u306a\u308b\u3088\u3046\u306b\u518d\u30b5\u30f3\u30d7\u30eb\u3092\u884c\u3044\u3001\u70b9\u306e\u9593\u9694\u306f<code>min_trajectory_interval_distance</code>\u3088\u308a\u5c0f\u3055\u304f\u306a\u3089\u306a\u3044\u3088\u3046\u306b\u3059\u308b\u3002 \u73fe\u5728\u8eca\u901f\u3067<code>resample_time</code>\u306e\u9593\u9032\u3080\u8ddd\u96e2\u307e\u3067\u306f\u5bc6\u306b\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3001\u305d\u308c\u4ee5\u964d\u306f\u758e\u306b\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3002 \u3053\u306e\u65b9\u6cd5\u3067\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u3001\u4f4e\u901f\u6642\u306f\u5bc6\u306b\u3001\u9ad8\u901f\u6642\u306f\u758e\u306b\u30b5\u30f3\u30d7\u30eb\u3055\u308c\u308b\u305f\u3081\u3001\u505c\u6b62\u7cbe\u5ea6\u3068\u8a08\u7b97\u8ca0\u8377\u8efd\u6e1b\u306e\u4e21\u7acb\u3092\u56f3\u3063\u3066\u3044\u308b\u3002</p>"},{"location":"planning/motion_velocity_smoother/README.ja/#calculate-initial-state","title":"Calculate initial state","text":"<p>\u901f\u5ea6\u8a08\u753b\u306e\u305f\u3081\u306e\u521d\u671f\u5024\u3092\u8a08\u7b97\u3059\u308b\u3002\u521d\u671f\u5024\u306f\u72b6\u6cc1\u306b\u5fdc\u3058\u3066\u4e0b\u8868\u306e\u3088\u3046\u306b\u8a08\u7b97\u3059\u308b\u3002</p> \u72b6\u6cc1 \u521d\u671f\u901f\u5ea6 \u521d\u671f\u52a0\u901f\u5ea6 \u6700\u521d\u306e\u8a08\u7b97\u6642 \u73fe\u5728\u8eca\u901f 0.0 \u767a\u9032\u6642 <code>engage_velocity</code> <code>engage_acceleration</code> \u73fe\u5728\u8eca\u901f\u3068\u8a08\u753b\u8eca\u901f\u304c\u4e56\u96e2 \u73fe\u5728\u8eca\u901f \u524d\u56de\u8a08\u753b\u5024 \u901a\u5e38\u6642 \u524d\u56de\u8a08\u753b\u5024 \u524d\u56de\u8a08\u753b\u5024"},{"location":"planning/motion_velocity_smoother/README.ja/#smooth-velocity","title":"Smooth velocity","text":"<p>\u901f\u5ea6\u306e\u8a08\u753b\u3092\u884c\u3046\u3002\u901f\u5ea6\u8a08\u753b\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f<code>JerkFiltered</code>, <code>L2</code>, <code>Linf</code>\u306e 3 \u7a2e\u985e\u306e\u3046\u3061\u304b\u3089\u30b3\u30f3\u30d5\u30a3\u30b0\u3067\u6307\u5b9a\u3059\u308b\u3002 \u6700\u9069\u5316\u306e\u30bd\u30eb\u30d0\u306f OSQP[1]\u3092\u5229\u7528\u3059\u308b\u3002</p>"},{"location":"planning/motion_velocity_smoother/README.ja/#jerkfiltered","title":"JerkFiltered","text":"<p>\u901f\u5ea6\u306e 2 \u4e57\uff08\u6700\u5c0f\u5316\u3067\u8868\u3059\u305f\u3081\u8ca0\u5024\u3067\u8868\u73fe\uff09\u3001\u5236\u9650\u901f\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u5236\u9650\u52a0\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u5236\u9650\u30b8\u30e3\u30fc\u30af\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u30b8\u30e3\u30fc\u30af\u306e 2 \u4e57\u306e\u7dcf\u548c\u3092\u6700\u5c0f\u5316\u3059\u308b\u3002</p>"},{"location":"planning/motion_velocity_smoother/README.ja/#l2","title":"L2","text":"<p>\u901f\u5ea6\u306e 2 \u4e57\uff08\u6700\u5c0f\u5316\u3067\u8868\u3059\u305f\u3081\u8ca0\u5024\u3067\u8868\u73fe\uff09\u3001\u5236\u9650\u901f\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u5236\u9650\u52a0\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u7591\u4f3c\u30b8\u30e3\u30fc\u30af[2]\u306e 2 \u4e57\u306e\u7dcf\u548c\u3092\u6700\u5c0f\u5316\u3059\u308b\u3002</p>"},{"location":"planning/motion_velocity_smoother/README.ja/#linf","title":"Linf","text":"<p>\u901f\u5ea6\u306e 2 \u4e57\uff08\u6700\u5c0f\u5316\u3067\u8868\u3059\u305f\u3081\u8ca0\u5024\u3067\u8868\u73fe\uff09\u3001\u5236\u9650\u901f\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u5236\u9650\u52a0\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u306e\u7dcf\u548c\u3068\u7591\u4f3c\u30b8\u30e3\u30fc\u30af[2]\u306e\u7d76\u5bfe\u6700\u5927\u5024\u306e\u548c\u306e\u6700\u5c0f\u5316\u3059\u308b\u3002</p>"},{"location":"planning/motion_velocity_smoother/README.ja/#post-process","title":"Post process","text":"<p>\u8a08\u753b\u3055\u308c\u305f\u8ecc\u9053\u306e\u5f8c\u51e6\u7406\u3092\u884c\u3046\u3002</p> <ul> <li>\u505c\u6b62\u70b9\u3088\u308a\u5148\u306e\u901f\u5ea6\u3092 0 \u306b\u8a2d\u5b9a</li> <li>\u901f\u5ea6\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u4e0e\u3048\u3089\u308c\u308b<code>max_velocity</code>\u4ee5\u4e0b\u3068\u306a\u308b\u3088\u3046\u306b\u8a2d\u5b9a</li> <li>\u81ea\u8eca\u4f4d\u7f6e\u3088\u308a\u624b\u524d\u306e\u70b9\u306b\u304a\u3051\u308b\u901f\u5ea6\u3092\u8a2d\u5b9a</li> <li>Trajectory \u306e\u4ed5\u69d8\u306b\u5408\u308f\u305b\u3066\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0(<code>post resampling</code>)</li> <li>\u30c7\u30d0\u30c3\u30b0\u30c7\u30fc\u30bf\u306e\u51fa\u529b</li> </ul> <p>\u6700\u9069\u5316\u306e\u8a08\u7b97\u304c\u7d42\u308f\u3063\u305f\u3042\u3068\u3001\u6b21\u306e\u30ce\u30fc\u30c9\u306b\u7d4c\u8def\u3092\u6e21\u3059\u524d\u306b<code>post resampling</code>\u3068\u547c\u3070\u308c\u308b\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3092\u884c\u3046\u3002\u3053\u3053\u3067\u518d\u5ea6\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3092\u884c\u3063\u3066\u3044\u308b\u7406\u7531\u3068\u3057\u3066\u306f\u3001\u6700\u9069\u5316\u524d\u3067\u5fc5\u8981\u306a\u7d4c\u8def\u9593\u9694\u3068\u5f8c\u6bb5\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u306b\u6e21\u3059\u7d4c\u8def\u9593\u9694\u304c\u5fc5\u305a\u3057\u3082\u4e00\u81f4\u3057\u3066\u3044\u306a\u3044\u304b\u3089\u3067\u3042\u308a\u3001\u305d\u306e\u5dee\u3092\u57cb\u3081\u308b\u305f\u3081\u306b\u518d\u5ea6\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3092\u884c\u3063\u3066\u3044\u308b\u3002\u305d\u306e\u305f\u3081\u3001<code>post resampling</code>\u3067\u306f\u5f8c\u6bb5\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u7d4c\u8def\u4ed5\u69d8\u3092\u78ba\u8a8d\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6c7a\u3081\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\u306a\u304a\u3001\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u8a08\u7b97\u8ca0\u8377\u304c\u9ad8\u304f\u3001\u6700\u521d\u306e\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3067\u7d4c\u8def\u9593\u9694\u304c\u5f8c\u6bb5\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u7d4c\u8def\u4ed5\u69d8\u3088\u308a\u758e\u306b\u306a\u3063\u3066\u3044\u308b\u5834\u5408\u3001<code>post resampling</code>\u3067\u7d4c\u8def\u3092\u871c\u306b\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3002\u9006\u306b\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u8a08\u7b97\u8ca0\u8377\u304c\u5c0f\u3055\u304f\u3001\u6700\u521d\u306e\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3067\u7d4c\u8def\u9593\u9694\u304c\u5f8c\u6bb5\u306e\u7d4c\u8def\u4ed5\u69d8\u3088\u308a\u871c\u306b\u306a\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u3001<code>post resampling</code>\u3067\u7d4c\u8def\u3092\u305d\u306e\u4ed5\u69d8\u306b\u5408\u308f\u305b\u3066\u758e\u306b\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3002</p>"},{"location":"planning/motion_velocity_smoother/README.ja/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/motion_velocity_smoother/README.ja/#input","title":"Input","text":"Name Type Description <code>~/input/trajectory</code> <code>autoware_auto_planning_msgs/Trajectory</code> Reference trajectory <code>/planning/scenario_planning/max_velocity</code> <code>std_msgs/Float32</code> External velocity limit [m/s] <code>/localization/kinematic_state</code> <code>nav_msgs/Odometry</code> Current odometry <code>/tf</code> <code>tf2_msgs/TFMessage</code> TF <code>/tf_static</code> <code>tf2_msgs/TFMessage</code> TF static"},{"location":"planning/motion_velocity_smoother/README.ja/#output","title":"Output","text":"Name Type Description <code>~/output/trajectory</code> <code>autoware_auto_planning_msgs/Trajectory</code> Modified trajectory <code>/planning/scenario_planning/current_max_velocity</code> <code>std_msgs/Float32</code> Current external velocity limit [m/s] <code>~/closest_velocity</code> <code>std_msgs/Float32</code> Planned velocity closest to ego base_link (for debug) <code>~/closest_acceleration</code> <code>std_msgs/Float32</code> Planned acceleration closest to ego base_link (for debug) <code>~/closest_jerk</code> <code>std_msgs/Float32</code> Planned jerk closest to ego base_link (for debug) <code>~/debug/trajectory_raw</code> <code>autoware_auto_planning_msgs/Trajectory</code> Extracted trajectory (for debug) <code>~/debug/trajectory_external_velocity_limited</code> <code>autoware_auto_planning_msgs/Trajectory</code> External velocity limited trajectory (for debug) <code>~/debug/trajectory_lateral_acc_filtered</code> <code>autoware_auto_planning_msgs/Trajectory</code> Lateral acceleration limit filtered trajectory (for debug) <code>~/debug/trajectory_time_resampled</code> <code>autoware_auto_planning_msgs/Trajectory</code> Time resampled trajectory (for debug) <code>~/distance_to_stopline</code> <code>std_msgs/Float32</code> Distance to stop line from current ego pose (max 50 m) (for debug) <code>~/stop_speed_exceeded</code> <code>std_msgs/Bool</code> It publishes <code>true</code> if planned velocity on the point which the maximum velocity is zero is over threshold"},{"location":"planning/motion_velocity_smoother/README.ja/#parameters","title":"Parameters","text":""},{"location":"planning/motion_velocity_smoother/README.ja/#constraint-parameters","title":"Constraint parameters","text":"Name Type Description Default value <code>max_velocity</code> <code>double</code> Max velocity limit [m/s] 20.0 <code>max_accel</code> <code>double</code> Max acceleration limit [m/ss] 1.0 <code>min_decel</code> <code>double</code> Min deceleration limit [m/ss] -0.5 <code>stop_decel</code> <code>double</code> Stop deceleration value at a stop point [m/ss] 0.0 <code>max_jerk</code> <code>double</code> Max jerk limit [m/sss] 1.0 <code>min_jerk</code> <code>double</code> Min jerk limit [m/sss] -0.5"},{"location":"planning/motion_velocity_smoother/README.ja/#external-velocity-limit-parameter","title":"External velocity limit parameter","text":"Name Type Description Default value <code>margin_to_insert_external_velocity_limit</code> <code>double</code> margin distance to insert external velocity limit [m] 0.3"},{"location":"planning/motion_velocity_smoother/README.ja/#curve-parameters","title":"Curve parameters","text":"Name Type Description Default value <code>max_lateral_accel</code> <code>double</code> Max lateral acceleration limit [m/ss] 0.5 <code>min_curve_velocity</code> <code>double</code> Min velocity at lateral acceleration limit [m/ss] 2.74 <code>decel_distance_before_curve</code> <code>double</code> Distance to slowdown before a curve for lateral acceleration limit [m] 3.5 <code>decel_distance_after_curve</code> <code>double</code> Distance to slowdown after a curve for lateral acceleration limit [m] 2.0"},{"location":"planning/motion_velocity_smoother/README.ja/#engage-replan-parameters","title":"Engage &amp; replan parameters","text":"Name Type Description Default value <code>replan_vel_deviation</code> <code>double</code> Velocity deviation to replan initial velocity [m/s] 5.53 <code>engage_velocity</code> <code>double</code> Engage velocity threshold [m/s] (if the trajectory velocity is higher than this value, use this velocity for engage vehicle speed) 0.25 <code>engage_acceleration</code> <code>double</code> Engage acceleration [m/ss] (use this acceleration when engagement) 0.1 <code>engage_exit_ratio</code> <code>double</code> Exit engage sequence to normal velocity planning when the velocity exceeds engage_exit_ratio x engage_velocity. 0.5 <code>stop_dist_to_prohibit_engage</code> <code>double</code> If the stop point is in this distance, the speed is set to 0 not to move the vehicle [m] 0.5"},{"location":"planning/motion_velocity_smoother/README.ja/#stopping-velocity-parameters","title":"Stopping velocity parameters","text":"Name Type Description Default value <code>stopping_velocity</code> <code>double</code> change target velocity to this value before v=0 point [m/s] 2.778 <code>stopping_distance</code> <code>double</code> distance for the stopping_velocity [m]. 0 means the stopping velocity is not applied. 0.0"},{"location":"planning/motion_velocity_smoother/README.ja/#extraction-parameters","title":"Extraction parameters","text":"Name Type Description Default value <code>extract_ahead_dist</code> <code>double</code> Forward trajectory distance used for planning [m] 200.0 <code>extract_behind_dist</code> <code>double</code> backward trajectory distance used for planning [m] 5.0 <code>delta_yaw_threshold</code> <code>double</code> Allowed delta yaw between ego pose and trajectory pose [radian] 1.0472"},{"location":"planning/motion_velocity_smoother/README.ja/#resampling-parameters","title":"Resampling parameters","text":"Name Type Description Default value <code>max_trajectory_length</code> <code>double</code> Max trajectory length for resampling [m] 200.0 <code>min_trajectory_length</code> <code>double</code> Min trajectory length for resampling [m] 30.0 <code>resample_time</code> <code>double</code> Resample total time [s] 10.0 <code>dense_resample_dt</code> <code>double</code> resample time interval for dense sampling [s] 0.1 <code>dense_min_interval_distance</code> <code>double</code> minimum points-interval length for dense sampling [m] 0.1 <code>sparse_resample_dt</code> <code>double</code> resample time interval for sparse sampling [s] 0.5 <code>sparse_min_interval_distance</code> <code>double</code> minimum points-interval length for sparse sampling [m] 4.0"},{"location":"planning/motion_velocity_smoother/README.ja/#resampling-parameters-for-post-process","title":"Resampling parameters for post process","text":"Name Type Description Default value <code>post_max_trajectory_length</code> <code>double</code> max trajectory length for resampling [m] 300.0 <code>post_min_trajectory_length</code> <code>double</code> min trajectory length for resampling [m] 30.0 <code>post_resample_time</code> <code>double</code> resample total time for dense sampling [s] 10.0 <code>post_dense_resample_dt</code> <code>double</code> resample time interval for dense sampling [s] 0.1 <code>post_dense_min_interval_distance</code> <code>double</code> minimum points-interval length for dense sampling [m] 0.1 <code>post_sparse_resample_dt</code> <code>double</code> resample time interval for sparse sampling [s] 0.1 <code>post_sparse_min_interval_distance</code> <code>double</code> minimum points-interval length for sparse sampling [m] 1.0"},{"location":"planning/motion_velocity_smoother/README.ja/#weights-for-optimization","title":"Weights for optimization","text":""},{"location":"planning/motion_velocity_smoother/README.ja/#jerkfiltered_1","title":"JerkFiltered","text":"Name Type Description Default value <code>jerk_weight</code> <code>double</code> Weight for \"smoothness\" cost for jerk 10.0 <code>over_v_weight</code> <code>double</code> Weight for \"over speed limit\" cost 100000.0 <code>over_a_weight</code> <code>double</code> Weight for \"over accel limit\" cost 5000.0 <code>over_j_weight</code> <code>double</code> Weight for \"over jerk limit\" cost 1000.0"},{"location":"planning/motion_velocity_smoother/README.ja/#l2_1","title":"L2","text":"Name Type Description Default value <code>pseudo_jerk_weight</code> <code>double</code> Weight for \"smoothness\" cost 100.0 <code>over_v_weight</code> <code>double</code> Weight for \"over speed limit\" cost 100000.0 <code>over_a_weight</code> <code>double</code> Weight for \"over accel limit\" cost 1000.0"},{"location":"planning/motion_velocity_smoother/README.ja/#linf_1","title":"Linf","text":"Name Type Description Default value <code>pseudo_jerk_weight</code> <code>double</code> Weight for \"smoothness\" cost 100.0 <code>over_v_weight</code> <code>double</code> Weight for \"over speed limit\" cost 100000.0 <code>over_a_weight</code> <code>double</code> Weight for \"over accel limit\" cost 1000.0"},{"location":"planning/motion_velocity_smoother/README.ja/#others","title":"Others","text":"Name Type Description Default value <code>over_stop_velocity_warn_thr</code> <code>double</code> Threshold to judge that the optimized velocity exceeds the input velocity on the stop point [m/s] 1.389"},{"location":"planning/motion_velocity_smoother/README.ja/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>\u53c2\u7167\u7d4c\u8def\u4e0a\u306e\u70b9\u306b\u306f\u5236\u9650\u901f\u5ea6\uff08\u505c\u6b62\u70b9\uff09\u304c\u6b63\u3057\u304f\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u4eee\u5b9a</li> <li>\u53c2\u7167\u7d4c\u8def\u306b\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u5236\u9650\u901f\u5ea6\u3092\u6307\u5b9a\u3057\u305f\u6e1b\u901f\u5ea6\u3084\u30b8\u30e3\u30fc\u30af\u3067\u9054\u6210\u4e0d\u53ef\u80fd\u306a\u5834\u5408\u3001\u53ef\u80fd\u306a\u7bc4\u56f2\u3067\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u3001\u30b8\u30e3\u30fc\u30af\u306e\u9038\u8131\u91cf\u3092\u6291\u3048\u306a\u304c\u3089\u6e1b\u901f</li> <li>\u5404\u9038\u8131\u91cf\u306e\u91cd\u8996\u306e\u5ea6\u5408\u3044\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3088\u308a\u6307\u5b9a</li> </ul>"},{"location":"planning/motion_velocity_smoother/README.ja/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"planning/motion_velocity_smoother/README.ja/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"planning/motion_velocity_smoother/README.ja/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>[1] B. Stellato, et al., \"OSQP: an operator splitting solver for quadratic programs\", Mathematical Programming Computation, 2020, 10.1007/s12532-020-00179-2.</p> <p>[2] Y. Zhang, et al., \"Toward a More Complete, Flexible, and Safer Speed Planning for Autonomous Driving via Convex Optimization\", Sensors, vol. 18, no. 7, p. 2185, 2018, 10.3390/s18072185</p>"},{"location":"planning/motion_velocity_smoother/README.ja/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"planning/objects_of_interest_marker_interface/","title":"Objects Of Interest Marker Interface","text":""},{"location":"planning/objects_of_interest_marker_interface/#objects-of-interest-marker-interface","title":"Objects Of Interest Marker Interface","text":"<p>Warning</p> <p>Under Construction</p>"},{"location":"planning/objects_of_interest_marker_interface/#purpose","title":"Purpose","text":""},{"location":"planning/objects_of_interest_marker_interface/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/objects_of_interest_marker_interface/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/objects_of_interest_marker_interface/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"planning/objects_of_interest_marker_interface/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"planning/obstacle_avoidance_planner/","title":"Obstacle Avoidance Planner","text":""},{"location":"planning/obstacle_avoidance_planner/#obstacle-avoidance-planner","title":"Obstacle Avoidance Planner","text":""},{"location":"planning/obstacle_avoidance_planner/#purpose","title":"Purpose","text":"<p>This package generates a trajectory that is kinematically-feasible to drive and collision-free based on the input path, drivable area. Only position and orientation of trajectory are updated in this module, and velocity is just taken over from the one in the input path.</p>"},{"location":"planning/obstacle_avoidance_planner/#feature","title":"Feature","text":"<p>This package is able to</p> <ul> <li>make the trajectory inside the drivable area as much as possible<ul> <li>NOTE: Static obstacles to avoid can be removed from the drivable area.</li> </ul> </li> <li>insert stop point before the planned footprint will be outside the drivable area</li> </ul> <p>Note that the velocity is just taken over from the input path.</p>"},{"location":"planning/obstacle_avoidance_planner/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/obstacle_avoidance_planner/#input","title":"input","text":"Name Type Description <code>~/input/path</code> autoware_auto_planning_msgs/msg/Path Reference path and the corresponding drivable area <code>~/input/odometry</code> nav_msgs/msg/Odometry Current Velocity of ego vehicle"},{"location":"planning/obstacle_avoidance_planner/#output","title":"output","text":"Name Type Description <code>~/output/trajectory</code> autoware_auto_planning_msgs/msg/Trajectory Optimized trajectory that is feasible to drive and collision-free"},{"location":"planning/obstacle_avoidance_planner/#flowchart","title":"Flowchart","text":"<p>Flowchart of functions is explained here.</p> <p></p>"},{"location":"planning/obstacle_avoidance_planner/#createplannerdata","title":"createPlannerData","text":"<p>The following data for planning is created.</p> <pre><code>struct PlannerData\n{\n// input\nHeader header;\nstd::vector&lt;TrajectoryPoint&gt; traj_points; // converted from the input path\nstd::vector&lt;geometry_msgs::msg::Point&gt; left_bound;\nstd::vector&lt;geometry_msgs::msg::Point&gt; right_bound;\n\n// ego\ngeometry_msgs::msg::Pose ego_pose;\ndouble ego_vel;\n};\n</code></pre>"},{"location":"planning/obstacle_avoidance_planner/#check-replan","title":"check replan","text":"<p>When one of the following conditions are met, trajectory optimization will be executed. Otherwise, previously optimized trajectory is used with updating the velocity from the latest input path.</p> <p>max_path_shape_around_ego_lat_dist</p> <ul> <li>Ego moves longer than <code>replan.max_ego_moving_dist</code> in one cycle. (default: 3.0 [m])<ul> <li>This is for when the ego pose is set again in the simulation.</li> </ul> </li> <li>Trajectory's end, which is considered as the goal pose, moves longer than <code>replan.max_goal_moving_dist</code> in one cycle. (default: 15.0 [ms])<ul> <li>When the goal pose is set again, the planning should be reset.</li> </ul> </li> <li>Time passes. (default: 1.0 [s])<ul> <li>The optimization is skipped for a while sine the optimization is sometimes heavy.</li> </ul> </li> <li>The input path changes laterally longer than <code>replan.max_path_shape_around_ego_lat_dist</code> in one cycle. (default: 2.0)</li> </ul>"},{"location":"planning/obstacle_avoidance_planner/#getmodelpredictivetrajectory","title":"getModelPredictiveTrajectory","text":"<p>This module makes the trajectory kinematically-feasible and collision-free. We define vehicle pose in the frenet coordinate, and minimize tracking errors by optimization. This optimization considers vehicle kinematics and collision checking with road boundary and obstacles. To decrease the computation cost, the optimization is applied to the shorter trajectory (default: 50 [m]) than the whole trajectory, and concatenate the remained trajectory with the optimized one at last.</p> <p>The trajectory just in front of the ego must not be changed a lot so that the steering wheel will be stable. Therefore, we use the previously generated trajectory in front of the ego.</p> <p>Optimization center on the vehicle, that tries to locate just on the trajectory, can be tuned along side the vehicle vertical axis. This parameter <code>mpt.kinematics.optimization center offset</code> is defined as the signed length from the back-wheel center to the optimization center. Some examples are shown in the following figure, and it is shown that the trajectory of vehicle shape differs according to the optimization center even if the reference trajectory (green one) is the same.</p> <p></p> <p>More details can be seen here.</p>"},{"location":"planning/obstacle_avoidance_planner/#applyinputvelocity","title":"applyInputVelocity","text":"<p>Velocity is assigned in the optimized trajectory from the velocity in the behavior path. The shapes of the optimized trajectory and the path are different, therefore the each nearest trajectory point to the path is searched and the velocity is interpolated with zero-order hold.</p>"},{"location":"planning/obstacle_avoidance_planner/#insertzerovelocityoutsidedrivablearea","title":"insertZeroVelocityOutsideDrivableArea","text":"<p>Optimized trajectory is too short for velocity planning, therefore extend the trajectory by concatenating the optimized trajectory and the behavior path considering drivability. Generated trajectory is checked if it is inside the drivable area or not, and if outside drivable area, output a trajectory inside drivable area with the behavior path or the previously generated trajectory.</p> <p>As described above, the behavior path is separated into two paths: one is for optimization and the other is the remain. The first path becomes optimized trajectory, and the second path just is transformed to a trajectory. Then a trajectory inside the drivable area is calculated as follows.</p> <ul> <li>If optimized trajectory is inside the drivable area, and the remained trajectory is inside/outside the drivable area,<ul> <li>the output trajectory will be just concatenation of those two trajectories.</li> <li>In this case, we do not care if the remained trajectory is inside or outside the drivable area since generally it is outside the drivable area (especially in a narrow road), but we want to pass a trajectory as long as possible to the latter module.</li> </ul> </li> <li>If optimized trajectory is outside the drivable area, and the remained trajectory is inside/outside the drivable area,<ul> <li>and if the previously generated trajectory is memorized,<ul> <li>the output trajectory will be the previously generated trajectory, where zero velocity is inserted to the point firstly going outside the drivable area.</li> </ul> </li> <li>and if the previously generated trajectory is not memorized,<ul> <li>the output trajectory will be a part of trajectory just transformed from the behavior path, where zero velocity is inserted to the point firstly going outside the drivable area.</li> </ul> </li> </ul> </li> </ul> <p>Optimization failure is dealt with the same as if the optimized trajectory is outside the drivable area. The output trajectory is memorized as a previously generated trajectory for the next cycle.</p> <p>Rationale In the current design, since there are some modelling errors, the constraints are considered to be soft constraints. Therefore, we have to make sure that the optimized trajectory is inside the drivable area or not after optimization.</p>"},{"location":"planning/obstacle_avoidance_planner/#limitation","title":"Limitation","text":"<ul> <li>Computation cost is sometimes high.</li> <li>Because of the approximation such as linearization, some narrow roads cannot be run by the planner.</li> <li>Roles of planning for <code>behavior_path_planner</code> and <code>obstacle_avoidance_planner</code> are not decided clearly. Both can avoid obstacles.</li> </ul>"},{"location":"planning/obstacle_avoidance_planner/#comparison-to-other-methods","title":"Comparison to other methods","text":"<p>Trajectory planning problem that satisfies kinematically-feasibility and collision-free has two main characteristics that makes hard to be solved: one is non-convex and the other is high dimension. Based on the characteristics, we investigate pros/cons of the typical planning methods: optimization-based, sampling-based, and learning-based method.</p>"},{"location":"planning/obstacle_avoidance_planner/#optimization-based-method","title":"Optimization-based method","text":"<ul> <li>pros: comparatively fast against high dimension by leveraging the gradient descent</li> <li>cons: often converge to the local minima in the non-convex problem</li> </ul>"},{"location":"planning/obstacle_avoidance_planner/#sampling-based-method","title":"Sampling-based method","text":"<ul> <li>pros: realize global optimization</li> <li>cons: high computation cost especially in the complex case</li> </ul>"},{"location":"planning/obstacle_avoidance_planner/#learning-based-method","title":"Learning-based method","text":"<ul> <li>under research yet</li> </ul> <p>Based on these pros/cons, we chose the optimization-based planner first. Although it has a cons to converge to the local minima, it can get a good solution by the preprocessing to approximate the problem to convex that almost equals to the original non-convex problem.</p>"},{"location":"planning/obstacle_avoidance_planner/#how-to-tune-parameters","title":"How to Tune Parameters","text":""},{"location":"planning/obstacle_avoidance_planner/#drivability-in-narrow-roads","title":"Drivability in narrow roads","text":"<ul> <li>modify <code>mpt.clearance.soft_clearance_from_road</code><ul> <li>This parameter describes how much margin to make between the trajectory and road boundaries.</li> <li>Due to the model error for optimization, the constraint such as collision-free is not fully met.<ul> <li>By making this parameter larger, the is for narrow-road driving may be resolved. 12180</li> </ul> </li> </ul> </li> <li> <p>modify <code>mpt.kinematics.optimization_center_offset</code></p> <ul> <li>The point on the vehicle, offset forward with this parameter from the base link` tries to follow the reference path.</li> </ul> </li> </ul> <ul> <li>change or tune the method to approximate footprints with a set of circles.<ul> <li>See here</li> <li>Tuning means changing the ratio of circle's radius.</li> </ul> </li> </ul>"},{"location":"planning/obstacle_avoidance_planner/#computation-time","title":"Computation time","text":"<ul> <li>under construction</li> </ul>"},{"location":"planning/obstacle_avoidance_planner/#robustness","title":"Robustness","text":"<ul> <li>Check if the trajectory before or after MPT is not robust<ul> <li>if the trajectory before MPT is not robust</li> <li>if the trajectory after MPT is not robust<ul> <li>make <code>mpt.weight.steer_input_weight</code> or <code>mpt.weight.steer_rate_weight</code> larger, which are stability of steering wheel along the trajectory.</li> </ul> </li> </ul> </li> </ul>"},{"location":"planning/obstacle_avoidance_planner/#other-options","title":"Other options","text":"<ul> <li><code>option.enable_skip_optimization</code> skips MPT optimization.</li> <li><code>option.enable_calculation_time_info</code> enables showing each calculation time for functions and total calculation time on the terminal.</li> <li><code>option.enable_outside_drivable_area_stop</code> enables stopping just before the generated trajectory point will be outside the drivable area.</li> </ul>"},{"location":"planning/obstacle_avoidance_planner/#how-to-debug","title":"How To Debug","text":"<p>How to debug can be seen here.</p>"},{"location":"planning/obstacle_avoidance_planner/docs/debug/","title":"Debug","text":""},{"location":"planning/obstacle_avoidance_planner/docs/debug/#debug","title":"Debug","text":""},{"location":"planning/obstacle_avoidance_planner/docs/debug/#debug-visualization","title":"Debug visualization","text":"<p>The visualization markers of the planning flow (Input, Model Predictive Trajectory, and Output) are explained here.</p> <p>All the following markers can be visualized by</p> <pre><code>ros2 launch obstacle_avoidance_planner launch_visualiation.launch.xml vehilce_model:=sample_vehicle\n</code></pre> <p>The <code>vehicle_model</code> must be specified to make footprints with vehicle's size.</p>"},{"location":"planning/obstacle_avoidance_planner/docs/debug/#input","title":"Input","text":"<ul> <li>Path<ul> <li>The path generated in the <code>behavior</code> planner.</li> <li>The semitransparent and thick, green and red band, that is visualized by default.</li> </ul> </li> </ul> <ul> <li>Path Footprint<ul> <li>The path generated in the <code>behavior</code> planner is converted to footprints.</li> <li>NOTE:<ul> <li>Check if there is no high curvature.</li> <li>The path may be outside the drivable area in some cases, but it is okay to ignore it since the <code>behavior</code> planner does not support it.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Drivable Area<ul> <li>The Drivable area generated in the <code>behavior</code> planner.</li> <li>The skyblue left and right line strings, that is visualized by default.</li> <li>NOTE:<ul> <li>Check if the path is almost inside the drivable area.<ul> <li>Then, the <code>obstacle_avoidance_planner</code> will try to make the trajectory fully inside the drivable area.</li> </ul> </li> <li>During avoidance or lane change by the <code>behavior</code> planner, please make sure that the drivable area is expanded correctly.</li> </ul> </li> </ul> </li> </ul>"},{"location":"planning/obstacle_avoidance_planner/docs/debug/#model-predictive-trajectory-mpt","title":"Model Predictive Trajectory (MPT)","text":"<ul> <li>MPT Reference Trajectory<ul> <li>The reference trajectory points of model predictive trajectory.</li> </ul> </li> </ul> <ul> <li>MPT Fixed Trajectory<ul> <li>The fixed trajectory points as a constraint of model predictive trajectory.</li> </ul> </li> </ul> <ul> <li>Boundaries' Width<ul> <li>The boundaries' width is calculated from the drivable area line strings.</li> </ul> </li> </ul> <ul> <li>Vehicle Circles<ul> <li>The vehicle's shape is represented by a set of circles.</li> <li>The <code>obstacle_avoidance_planner</code> will try to make the these circles inside the above boundaries' width.</li> </ul> </li> </ul> <ul> <li>Vehicle Circles on Trajectory<ul> <li>The vehicle's circles on the MPT trajectory.</li> <li>Check if the circles are not so big compared to the road's width.</li> </ul> </li> </ul> <ul> <li>MPT Trajectory<ul> <li>The optimized trajectory points by model predictive trajectory.</li> <li>The footprints are supposed to be fully inside the drivable area.</li> </ul> </li> </ul>"},{"location":"planning/obstacle_avoidance_planner/docs/debug/#output","title":"Output","text":"<ul> <li>Trajectory<ul> <li>The output trajectory.</li> <li>The dark and thin, green and red band, that is visualized by default.</li> </ul> </li> </ul> <ul> <li>Trajectory Footprint<ul> <li>The output trajectory is converted to footprints.</li> </ul> </li> </ul>"},{"location":"planning/obstacle_avoidance_planner/docs/debug/#calculation-time","title":"Calculation time","text":"<p>The <code>obstacle_avoidance_planner</code> consists of many functions such as boundaries' width calculation, collision-free planning, etc. We can see the calculation time for each function as follows.</p>"},{"location":"planning/obstacle_avoidance_planner/docs/debug/#raw-data","title":"Raw data","text":"<p>Enable <code>option.enable_calculation_time_info</code> or echo the topic as follows.</p> <pre><code>$ ros2 topic echo /planning/scenario_planning/lane_driving/motion_planning/obstacle_avoidance_planner/debug/calculation_time --field data\n---\n        insertFixedPoint:= 0.008 [ms]\ngetPaddedTrajectoryPoints:= 0.002 [ms]\nupdateConstraint:= 0.741 [ms]\noptimizeTrajectory:= 0.101 [ms]\nconvertOptimizedPointsToTrajectory:= 0.014 [ms]\ngetEBTrajectory:= 0.991 [ms]\nresampleReferencePoints:= 0.058 [ms]\nupdateFixedPoint:= 0.237 [ms]\nupdateBounds:= 0.22 [ms]\nupdateVehicleBounds:= 0.509 [ms]\ncalcReferencePoints:= 1.649 [ms]\ncalcMatrix:= 0.209 [ms]\ncalcValueMatrix:= 0.015 [ms]\ncalcObjectiveMatrix:= 0.305 [ms]\ncalcConstraintMatrix:= 0.641 [ms]\ninitOsqp:= 6.896 [ms]\nsolveOsqp:= 2.796 [ms]\ncalcOptimizedSteerAngles:= 9.856 [ms]\ncalcMPTPoints:= 0.04 [ms]\ngetModelPredictiveTrajectory:= 12.782 [ms]\noptimizeTrajectory:= 12.981 [ms]\napplyInputVelocity:= 0.577 [ms]\ninsertZeroVelocityOutsideDrivableArea:= 0.81 [ms]\ngetDebugMarker:= 0.684 [ms]\npublishDebugMarker:= 4.354 [ms]\npublishDebugMarkerOfOptimization:= 5.047 [ms]\ngenerateOptimizedTrajectory:= 20.374 [ms]\nextendTrajectory:= 0.326 [ms]\npublishDebugData:= 0.008 [ms]\nonPath:= 20.737 [ms]\n</code></pre>"},{"location":"planning/obstacle_avoidance_planner/docs/debug/#plot","title":"Plot","text":"<p>With the following script, any calculation time of the above functions can be plot.</p> <pre><code>ros2 run obstacle_avoidance_planner calculation_time_plotter.py\n</code></pre> <p></p> <p>You can specify functions to plot with the <code>-f</code> option.</p> <pre><code>ros2 run obstacle_avoidance_planner calculation_time_plotter.py -f \"onPath, generateOptimizedTrajectory, calcReferencePoints\"\n</code></pre>"},{"location":"planning/obstacle_avoidance_planner/docs/debug/#qa-for-debug","title":"Q&amp;A for Debug","text":""},{"location":"planning/obstacle_avoidance_planner/docs/debug/#the-output-frequency-is-low","title":"The output frequency is low","text":"<p>Check the function which is comparatively heavy according to this information.</p> <p>For your information, the following functions for optimization and its initialization may be heavy in some complicated cases.</p> <ul> <li>MPT<ul> <li><code>initOsqp</code></li> <li><code>solveOsqp</code></li> </ul> </li> </ul>"},{"location":"planning/obstacle_avoidance_planner/docs/debug/#when-a-part-of-the-trajectory-has-high-curvature","title":"When a part of the trajectory has high curvature","text":"<p>Some of the following may have an issue. Please check if there is something weird by the visualization.</p> <ul> <li>Input Path</li> <li>Drivable Area</li> <li>Boundaries' Width</li> </ul>"},{"location":"planning/obstacle_avoidance_planner/docs/debug/#when-the-trajectorys-shape-is-zigzag","title":"When the trajectory's shape is zigzag","text":"<p>Some of the following may have an issue. Please check if there is something weird by the visualization.</p> <ul> <li>Vehicle Circles on Trajectory</li> </ul>"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/","title":"Model predictive trajectory","text":""},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#model-predictive-trajectory","title":"Model predictive trajectory","text":""},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#abstract","title":"Abstract","text":"<p>Model Predictive Trajectory (MPT) calculates the trajectory that meets the following conditions.</p> <ul> <li>Kinematically feasible for linear vehicle kinematics model</li> <li>Collision free with obstacles and road boundaries</li> </ul> <p>Conditions for collision free is considered to be not hard constraints but soft constraints. When the optimization failed or the optimized trajectory is not collision free, the output trajectory will be previously generated trajectory.</p> <p>Trajectory near the ego must be stable, therefore the condition where trajectory points near the ego are the same as previously generated trajectory is considered, and this is the only hard constraints in MPT.</p>"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#flowchart","title":"Flowchart","text":""},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#vehicle-kinematics","title":"Vehicle kinematics","text":"<p>As the following figure, we consider the bicycle kinematics model in the frenet frame to track the reference path. At time step \\(k\\), we define lateral distance to the reference path, heading angle against the reference path, and steer angle as \\(y_k\\), \\(\\theta_k\\), and \\(\\delta_k\\) respectively.</p> <p></p> <p>Assuming that the commanded steer angle is \\(\\delta_{des, k}\\), the kinematics model in the frenet frame is formulated as follows. We also assume that the steer angle \\(\\delta_k\\) is first-order lag to the commanded one.</p> \\[ \\begin{align} y_{k+1} &amp; = y_{k} + v \\sin \\theta_k dt \\\\ \\theta_{k+1} &amp; = \\theta_k + \\frac{v \\tan \\delta_k}{L}dt - \\kappa_k v \\cos \\theta_k dt \\\\ \\delta_{k+1} &amp; = \\delta_k - \\frac{\\delta_k - \\delta_{des,k}}{\\tau}dt \\end{align} \\]"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#linearization","title":"Linearization","text":"<p>Then we linearize these equations. \\(y_k\\) and \\(\\theta_k\\) are tracking errors, so we assume that those are small enough. Therefore \\(\\sin \\theta_k \\approx \\theta_k\\).</p> <p>Since \\(\\delta_k\\) is a steer angle, it is not always small. By using a reference steer angle \\(\\delta_{\\mathrm{ref}, k}\\) calculated by the reference path curvature \\(\\kappa_k\\), we express \\(\\delta_k\\) with a small value \\(\\Delta \\delta_k\\).</p> <p>Note that the steer angle \\(\\delta_k\\) is within the steer angle limitation \\(\\delta_{\\max}\\). When the reference steer angle \\(\\delta_{\\mathrm{ref}, k}\\) is larger than the steer angle limitation \\(\\delta_{\\max}\\), and \\(\\delta_{\\mathrm{ref}, k}\\) is used to linearize the steer angle, \\(\\Delta \\delta_k\\) is \\(\\Delta \\delta_k = \\delta - \\delta_{\\mathrm{ref}, k} = \\delta_{\\max} - \\delta_{\\mathrm{ref}, k}\\), and the absolute \\(\\Delta \\delta_k\\) gets larger. Therefore, we have to apply the steer angle limitation to \\(\\delta_{\\mathrm{ref}, k}\\) as well.</p> \\[ \\begin{align} \\delta_{\\mathrm{ref}, k} &amp; = \\mathrm{clamp}(\\arctan(L \\kappa_k), -\\delta_{\\max}, \\delta_{\\max}) \\\\ \\delta_k &amp; = \\delta_{\\mathrm{ref}, k} + \\Delta \\delta_k, \\ \\Delta \\delta_k \\ll 1 \\\\ \\end{align} \\] <p>\\(\\mathrm{clamp}(v, v_{\\min}, v_{\\max})\\) is a function to convert \\(v\\) to be larger than \\(v_{\\min}\\) and smaller than \\(v_{\\max}\\).</p> <p>Using this \\(\\delta_{\\mathrm{ref}, k}\\), \\(\\tan \\delta_k\\) is linearized as follows.</p> \\[ \\begin{align} \\tan \\delta_k &amp; \\approx \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} \\Delta \\delta_k \\\\ &amp; = \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} (\\delta_{\\mathrm{ref}, k} - \\delta_k) \\\\ &amp; = \\tan \\delta_{\\mathrm{ref}, k} - \\frac{\\delta_{\\mathrm{ref}, k}}{\\cos^2 \\delta_{\\mathrm{ref}, k}} + \\frac{1}{\\cos^2 \\delta_{\\mathrm{ref}, k}} \\delta_k \\end{align} \\]"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#one-step-state-equation","title":"One-step state equation","text":"<p>Based on the linearization, the error kinematics is formulated with the following linear equations,</p> \\[ \\begin{align}     \\begin{pmatrix}         y_{k+1} \\\\         \\theta_{k+1}     \\end{pmatrix}     =     \\begin{pmatrix}         1 &amp; v dt \\\\         0 &amp; 1 \\\\     \\end{pmatrix}     \\begin{pmatrix}         y_k \\\\         \\theta_k \\\\     \\end{pmatrix}     +     \\begin{pmatrix}         0 \\\\         \\frac{v dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} \\\\     \\end{pmatrix}     \\delta_{k}     +     \\begin{pmatrix}         0 \\\\         \\frac{v \\tan(\\delta_{\\mathrm{ref}, k}) dt}{L} - \\frac{v \\delta_{\\mathrm{ref}, k} dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} - \\kappa_k v dt\\\\     \\end{pmatrix} \\end{align} \\] <p>which can be formulated as follows with the state \\(\\boldsymbol{x}\\), control input \\(u\\) and some matrices, where \\(\\boldsymbol{x} = (y_k, \\theta_k)\\)</p> \\[ \\begin{align}   \\boldsymbol{x}_{k+1} = A_k \\boldsymbol{x}_k + \\boldsymbol{b}_k u_k + \\boldsymbol{w}_k \\end{align} \\]"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#time-series-state-equation","title":"Time-series state equation","text":"<p>Then, we formulate time-series state equation by concatenating states, control inputs and matrices respectively as</p> \\[ \\begin{align}   \\boldsymbol{x} = A \\boldsymbol{x}_0 + B \\boldsymbol{u} + \\boldsymbol{w} \\end{align} \\] <p>where</p> \\[ \\begin{align} \\boldsymbol{x} = (\\boldsymbol{x}^T_1, \\boldsymbol{x}^T_2, \\boldsymbol{x}^T_3, \\dots, \\boldsymbol{x}^T_{n-1})^T \\\\ \\boldsymbol{u} = (u_0, u_1, u_2, \\dots, u_{n-2})^T \\\\ \\boldsymbol{w} = (\\boldsymbol{w}^T_0, \\boldsymbol{w}^T_1, \\boldsymbol{w}^T_2, \\dots, \\boldsymbol{w}^T_{n-1})^T. \\\\ \\end{align} \\] <p>In detail, each matrices are constructed as follows.</p> \\[ \\begin{align}     \\begin{pmatrix}         \\boldsymbol{x}_1 \\\\         \\boldsymbol{x}_2 \\\\         \\boldsymbol{x}_3 \\\\         \\vdots \\\\         \\boldsymbol{x}_{n-1}     \\end{pmatrix}     =     \\begin{pmatrix}         A_0 \\\\         A_1 A_0 \\\\         A_2 A_1 A_0\\\\         \\vdots \\\\         \\prod\\limits_{k=0}^{n-1} A_{k}     \\end{pmatrix}     \\boldsymbol{x}_0     +     \\begin{pmatrix}       B_0 &amp; 0 &amp; &amp; \\dots &amp; 0 \\\\       A_0 B_0 &amp; B_1 &amp; 0 &amp; \\dots &amp; 0 \\\\       A_1 A_0 B_0 &amp; A_0 B_1 &amp; B_2 &amp; \\dots &amp; 0 \\\\       \\vdots &amp; \\vdots &amp; &amp; \\ddots &amp; 0 \\\\       \\prod\\limits_{k=0}^{n-3} A_k B_0 &amp; \\prod\\limits_{k=0}^{n-4} A_k B_1 &amp; \\dots &amp; A_0 B_{n-3} &amp; B_{n-2}     \\end{pmatrix}     \\begin{pmatrix}         u_0 \\\\         u_1 \\\\         u_2 \\\\         \\vdots \\\\         u_{n-2}     \\end{pmatrix}     +     \\begin{pmatrix}       I &amp; 0 &amp; &amp; \\dots &amp; 0 \\\\       A_0 &amp; I &amp; 0 &amp; \\dots &amp; 0 \\\\       A_1 A_0 &amp; A_0 &amp; I &amp; \\dots &amp; 0 \\\\       \\vdots &amp; \\vdots &amp; &amp; \\ddots &amp; 0 \\\\       \\prod\\limits_{k=0}^{n-3} A_k &amp; \\prod\\limits_{k=0}^{n-4} A_k &amp; \\dots &amp; A_0 &amp; I     \\end{pmatrix}     \\begin{pmatrix}         \\boldsymbol{w}_0 \\\\         \\boldsymbol{w}_1 \\\\         \\boldsymbol{w}_2 \\\\         \\vdots \\\\         \\boldsymbol{w}_{n-2}     \\end{pmatrix} \\end{align} \\]"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#free-boundary-conditioned-time-series-state-equation","title":"Free-boundary-conditioned time-series state equation","text":"<p>For path planning which does not start from the current ego pose, \\(\\boldsymbol{x}_0\\) should be the design variable of optimization. Therefore, we make \\(\\boldsymbol{u}'\\) by concatenating \\(\\boldsymbol{x}_0\\) and \\(\\boldsymbol{u}\\), and redefine \\(\\boldsymbol{x}\\) as follows.</p> \\[ \\begin{align}   \\boldsymbol{u}' &amp; = (\\boldsymbol{x}^T_0, \\boldsymbol{u}^T)^T \\\\   \\boldsymbol{x} &amp; = (\\boldsymbol{x}^T_0, \\boldsymbol{x}^T_1, \\boldsymbol{x}^T_2, \\dots, \\boldsymbol{x}^T_{n-1})^T \\end{align} \\] <p>Then we get the following state equation</p> \\[ \\begin{align}   \\boldsymbol{x}' = B \\boldsymbol{u}' + \\boldsymbol{w}, \\end{align} \\] <p>which is in detail</p> \\[ \\begin{align}     \\begin{pmatrix}         \\boldsymbol{x}_0 \\\\         \\boldsymbol{x}_1 \\\\         \\boldsymbol{x}_2 \\\\         \\boldsymbol{x}_3 \\\\         \\vdots \\\\         \\boldsymbol{x}_{n-1}     \\end{pmatrix}     =     \\begin{pmatrix}       I &amp; 0 &amp; \\dots &amp; &amp; &amp; 0 \\\\       A_0 &amp; B_0 &amp; 0 &amp; &amp; \\dots &amp; 0 \\\\       A_1 A_0 &amp; A_0 B_0 &amp; B_1 &amp; 0 &amp; \\dots &amp; 0 \\\\       A_2 A_1 A_0 &amp; A_1 A_0 B_0 &amp; A_0 B_1 &amp; B_2 &amp; \\dots &amp; 0 \\\\       \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\ddots &amp; 0 \\\\       \\prod\\limits_{k=0}^{n-1} A_k &amp; \\prod\\limits_{k=0}^{n-3} A_k B_0 &amp; \\prod\\limits_{k=0}^{n-4} A_k B_1 &amp; \\dots &amp; A_0 B_{n-3} &amp; B_{n-2}     \\end{pmatrix}     \\begin{pmatrix}         \\boldsymbol{x}_0 \\\\         u_0 \\\\         u_1 \\\\         u_2 \\\\         \\vdots \\\\         u_{n-2}     \\end{pmatrix}     +     \\begin{pmatrix}       0 &amp; \\dots &amp; &amp; &amp; 0 \\\\       I &amp; 0 &amp; &amp; \\dots &amp; 0 \\\\       A_0 &amp; I &amp; 0 &amp; \\dots &amp; 0 \\\\       A_1 A_0 &amp; A_0 &amp; I &amp; \\dots &amp; 0 \\\\       \\vdots &amp; \\vdots &amp; &amp; \\ddots &amp; 0 \\\\       \\prod\\limits_{k=0}^{n-3} A_k &amp; \\prod\\limits_{k=0}^{n-4} A_k &amp; \\dots &amp; A_0 &amp; I     \\end{pmatrix}     \\begin{pmatrix}         \\boldsymbol{w}_0 \\\\         \\boldsymbol{w}_1 \\\\         \\boldsymbol{w}_2 \\\\         \\vdots \\\\         \\boldsymbol{w}_{n-2}     \\end{pmatrix}. \\end{align} \\]"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#objective-function","title":"Objective function","text":"<p>The objective function for smoothing and tracking is shown as follows, which can be formulated with value function matrices \\(Q, R\\).</p> \\[ \\begin{align} J_1 (\\boldsymbol{x}', \\boldsymbol{u}') &amp; = w_y \\sum_{k} y_k^2 + w_{\\theta} \\sum_{k} \\theta_k^2 + w_{\\delta} \\sum_k \\delta_k^2 + w_{\\dot{\\delta}} \\sum_k \\dot{\\delta}_k^2 + w_{\\ddot{\\delta}} \\sum_k \\ddot{\\delta}_k^2 \\\\ &amp; = \\boldsymbol{x}'^T Q \\boldsymbol{x}' + \\boldsymbol{u}'^T R \\boldsymbol{u}' \\\\ &amp; = \\boldsymbol{u}'^T H \\boldsymbol{u}' + \\boldsymbol{u}'^T \\boldsymbol{f} \\end{align} \\] <p>As mentioned before, the constraints to be collision free with obstacles and road boundaries are formulated to be soft constraints. Assuming that the lateral distance to the road boundaries or obstacles from the back wheel center, front wheel center, and the point between them are \\(y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k}\\) respectively, and slack variables for each point are \\(\\lambda_{\\mathrm{base}}, \\lambda_{\\mathrm{top}}, \\lambda_{\\mathrm{mid}}\\), the soft constraints can be formulated as follows.</p> \\[ y_{\\mathrm{base}, k, \\min} - \\lambda_{\\mathrm{base}, k} \\leq y_{\\mathrm{base}, k} (y_k)  \\leq y_{\\mathrm{base}, k, \\max} + \\lambda_{\\mathrm{base}, k}\\\\ y_{\\mathrm{top}, k, \\min} - \\lambda_{\\mathrm{top}, k} \\leq y_{\\mathrm{top}, k} (y_k) \\leq y_{\\mathrm{top}, k, \\max} + \\lambda_{\\mathrm{top}, k}\\\\ y_{\\mathrm{mid}, k, \\min} - \\lambda_{\\mathrm{mid}, k} \\leq y_{\\mathrm{mid}, k} (y_k) \\leq y_{\\mathrm{mid}, k, \\max} + \\lambda_{\\mathrm{mid}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{base}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{top}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{mid}, k} \\] <p>Since \\(y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k}\\) is formulated as a linear function of \\(y_k\\), the objective function for soft constraints is formulated as follows.</p> \\[ \\begin{align} J_2 &amp; (\\boldsymbol{\\lambda}_\\mathrm{base}, \\boldsymbol{\\lambda}_\\mathrm{top}, \\boldsymbol {\\lambda}_\\mathrm{mid})\\\\ &amp; = w_{\\mathrm{base}} \\sum_{k} \\lambda_{\\mathrm{base}, k} + w_{\\mathrm{mid}} \\sum_k \\lambda_{\\mathrm{mid}, k} + w_{\\mathrm{top}} \\sum_k \\lambda_{\\mathrm{top}, k} \\end{align} \\] <p>Slack variables are also design variables for optimization. We define a vector \\(\\boldsymbol{v}\\), that concatenates all the design variables.</p> \\[ \\begin{align} \\boldsymbol{v} = \\begin{pmatrix}   \\boldsymbol{u}'^T &amp; \\boldsymbol{\\lambda}_\\mathrm{base}^T &amp; \\boldsymbol{\\lambda}_\\mathrm{top}^T &amp; \\boldsymbol{\\lambda}_\\mathrm{mid}^T \\end{pmatrix}^T \\end{align} \\] <p>The summation of these two objective functions is the objective function for the optimization problem.</p> \\[ \\begin{align} \\min_{\\boldsymbol{v}} J (\\boldsymbol{v}) = \\min_{\\boldsymbol{v}} J_1 (\\boldsymbol{u}') + J_2 (\\boldsymbol{\\lambda}_\\mathrm{base}, \\boldsymbol{\\lambda}_\\mathrm{top}, \\boldsymbol{\\lambda}_\\mathrm{mid}) \\end{align} \\] <p>As mentioned before, we use hard constraints where some trajectory points in front of the ego are the same as the previously generated trajectory points. This hard constraints is formulated as follows.</p> \\[ \\begin{align} \\delta_k = \\delta_{k}^{\\mathrm{prev}} (0 \\leq i \\leq N_{\\mathrm{fix}}) \\end{align} \\] <p>Finally we transform those objective functions to the following QP problem, and solve it.</p> \\[ \\begin{align} \\min_{\\boldsymbol{v}} \\ &amp; \\frac{1}{2} \\boldsymbol{v}^T \\boldsymbol{H} \\boldsymbol{v} + \\boldsymbol{f} \\boldsymbol{v} \\\\ \\mathrm{s.t.} \\ &amp; \\boldsymbol{b}_{lower} \\leq \\boldsymbol{A} \\boldsymbol{v} \\leq \\boldsymbol{b}_{upper} \\end{align} \\]"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#constraints","title":"Constraints","text":""},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#steer-angle-limitation","title":"Steer angle limitation","text":"<p>Steer angle has a limitation \\(\\delta_{max}\\) and \\(\\delta_{min}\\). Therefore we add linear inequality equations.</p> \\[ \\begin{align} \\delta_{min} \\leq \\delta_i \\leq \\delta_{max} \\end{align} \\]"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#collision-free","title":"Collision free","text":"<p>To realize collision-free trajectory planning, we have to formulate constraints that the vehicle is inside the road and also does not collide with obstacles in linear equations. For linearity, we implemented some methods to approximate the vehicle shape with a set of circles, that is reliable and easy to implement.</p> <ul> <li> <ol> <li>Bicycle Model</li> </ol> </li> <li> <ol> <li>Uniform Circles</li> </ol> </li> <li> <ol> <li>Fitting Uniform Circles</li> </ol> </li> </ul> <p></p> <p>Now we formulate the linear constraints where a set of circles on each trajectory point is collision-free. By using the drivable area, we calculate upper and lower boundaries along reference points, which will be interpolated on any position on the trajectory. NOTE that upper and lower boundary is left and right, respectively.</p> <p>Assuming that upper and lower boundaries are \\(b_l\\), \\(b_u\\) respectively, and \\(r\\) is a radius of a circle, lateral deviation of the circle center \\(y'\\) has to be</p> \\[ b_l + r \\leq y' \\leq b_u - r. \\] <p></p> <p>Based on the following figure, \\(y'\\) can be formulated as follows.</p> \\[ \\begin{align} y' &amp; = L \\sin(\\theta + \\beta) + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\\\ &amp; = L \\sin \\theta \\cos \\beta + L \\cos \\theta \\sin \\beta + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\\\ &amp; \\approx L \\theta \\cos \\beta + L \\sin \\beta + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\end{align} \\] \\[ b_l + r - \\lambda \\leq y' \\leq b_u - r + \\lambda. \\] \\[ \\begin{align} y' &amp; = C_1 \\boldsymbol{x} + C_2 \\\\ &amp; = C_1 (B \\boldsymbol{v} + \\boldsymbol{w}) + C_2 \\\\ &amp; = C_1 B \\boldsymbol{v} + \\boldsymbol{w} + C_2 \\end{align} \\] <p>Note that longitudinal position of the circle center and the trajectory point to calculate boundaries are different. But each boundaries are vertical against the trajectory, resulting in less distortion by the longitudinal position difference since road boundaries does not change so much. For example, if the boundaries are not vertical against the trajectory and there is a certain difference of longitudinal position between the circe center and the trajectory point, we can easily guess that there is much more distortion when comparing lateral deviation and boundaries.</p> \\[ \\begin{align}     A_{blk} &amp; =     \\begin{pmatrix}         C_1 B &amp; O &amp; \\dots &amp; O &amp; I_{N_{ref} \\times N_{ref}} &amp; O \\dots &amp; O\\\\         -C_1 B &amp; O &amp; \\dots &amp; O &amp; I &amp; O \\dots &amp; O\\\\         O &amp; O &amp; \\dots &amp; O &amp; I &amp; O \\dots &amp; O     \\end{pmatrix}     \\in \\boldsymbol{R}^{3 N_{ref} \\times D_v + N_{circle} N_{ref}} \\\\     \\boldsymbol{b}_{lower, blk} &amp; =     \\begin{pmatrix}         \\boldsymbol{b}_{lower} - C_1 \\boldsymbol{w} - C_2 \\\\         -\\boldsymbol{b}_{upper} + C_1 \\boldsymbol{w} + C_2 \\\\         O     \\end{pmatrix}     \\in \\boldsymbol{R}^{3 N_{ref}} \\\\     \\boldsymbol{b}_{upper, blk} &amp; = \\boldsymbol{\\infty}     \\in \\boldsymbol{R}^{3 N_{ref}} \\end{align} \\] <p>We will explain options for optimization.</p>"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#l-infinity-optimization","title":"L-infinity optimization","text":"<p>The above formulation is called L2 norm for slack variables. Instead, if we use L-infinity norm where slack variables are shared by enabling <code>l_inf_norm</code>.</p> \\[ \\begin{align}     A_{blk} =     \\begin{pmatrix}         C_1 B &amp; I_{N_{ref} \\times N_{ref}} \\\\         -C_1 B &amp; I \\\\         O &amp; I     \\end{pmatrix} \\in \\boldsymbol{R}^{3N_{ref} \\times D_v + N_{ref}} \\end{align} \\]"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#tips-for-stable-trajectory-planning","title":"Tips for stable trajectory planning","text":"<p>In order to make the trajectory optimization problem stabler to solve, the boundary constraint which the trajectory footprints should be inside and optimization weights are modified.</p>"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#keep-minimum-boundary-width","title":"Keep minimum boundary width","text":"<p>The drivable area's width is sometimes smaller than the vehicle width since the behavior module does not consider the width. To realize the stable trajectory optimization, the drivable area's width is guaranteed to be larger than the vehicle width and an additional margin in a rule-based way.</p> <p>We cannot distinguish the boundary by roads from the boundary by obstacles for avoidance in the motion planner, the drivable area is modified in the following multi steps assuming that \\(l_{width}\\) is the vehicle width and \\(l_{margin}\\) is an additional margin.</p> <p></p>"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#extend-violated-boundary","title":"Extend violated boundary","text":""},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#avoid-sudden-steering","title":"Avoid sudden steering","text":"<p>When the obstacle suddenly appears which is determined to avoid by the behavior module, the drivable area's shape just in front of the ego will change, resulting in the sudden steering. To prevent this, the drivable area's shape close to the ego is fixed as previous drivable area's shape.</p> <p>Assume that \\(v_{ego}\\) is the ego velocity, and \\(t_{fix}\\) is the time to fix the forward drivable area's shape.</p> <p></p>"},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#calculate-avoidance-cost","title":"Calculate avoidance cost","text":""},{"location":"planning/obstacle_avoidance_planner/docs/mpt/#change-optimization-weights","title":"Change optimization weights","text":"\\[ \\begin{align} r &amp; = \\mathrm{lerp}(w^{\\mathrm{steer}}_{\\mathrm{normal}}, w^{\\mathrm{steer}}_{\\mathrm{avoidance}}, c) \\\\ w^{\\mathrm{lat}} &amp; = \\mathrm{lerp}(w^{\\mathrm{lat}}_{\\mathrm{normal}}, w^{\\mathrm{lat}}_{\\mathrm{avoidance}}, r) \\\\ w^{\\mathrm{yaw}} &amp; = \\mathrm{lerp}(w^{\\mathrm{yaw}}_{\\mathrm{normal}}, w^{\\mathrm{yaw}}_{\\mathrm{avoidance}}, r) \\end{align} \\] <p>Assume that \\(c\\) is the normalized avoidance cost, \\(w^{\\mathrm{lat}}\\) is the weight for lateral error, \\(w^{\\mathrm{yaw}}\\) is the weight for yaw error, and other variables are as follows.</p> Parameter Type Description \\(w^{\\mathrm{steer}}_{\\mathrm{normal}}\\) double weight for steering minimization in normal cases \\(w^{\\mathrm{steer}}_{\\mathrm{avoidance}}\\) double weight for steering minimization in avoidance cases \\(w^{\\mathrm{lat}}_{\\mathrm{normal}}\\) double weight for lateral error minimization in normal cases \\(w^{\\mathrm{lat}}_{\\mathrm{avoidance}}\\) double weight for lateral error minimization in avoidance cases \\(w^{\\mathrm{yaw}}_{\\mathrm{normal}}\\) double weight for yaw error minimization in normal cases \\(w^{\\mathrm{yaw}}_{\\mathrm{avoidance}}\\) double weight for yaw error minimization in avoidance cases"},{"location":"planning/obstacle_cruise_planner/","title":"Obstacle Cruise Planner","text":""},{"location":"planning/obstacle_cruise_planner/#obstacle-cruise-planner","title":"Obstacle Cruise Planner","text":""},{"location":"planning/obstacle_cruise_planner/#overview","title":"Overview","text":"<p>The <code>obstacle_cruise_planner</code> package has following modules.</p> <ul> <li>Stop planning<ul> <li>stop when there is a static obstacle near the trajectory.</li> </ul> </li> <li>Cruise planning<ul> <li>cruise a dynamic obstacle in front of the ego.</li> </ul> </li> <li>Slow down planning<ul> <li>slow down when there is a static/dynamic obstacle near the trajectory.</li> </ul> </li> </ul>"},{"location":"planning/obstacle_cruise_planner/#interfaces","title":"Interfaces","text":""},{"location":"planning/obstacle_cruise_planner/#input-topics","title":"Input topics","text":"Name Type Description <code>~/input/trajectory</code> autoware_auto_planning_msgs::Trajectory input trajectory <code>~/input/objects</code> autoware_auto_perception_msgs::PredictedObjects dynamic objects <code>~/input/odometry</code> nav_msgs::msg::Odometry ego odometry"},{"location":"planning/obstacle_cruise_planner/#output-topics","title":"Output topics","text":"Name Type Description <code>~/output/trajectory</code> autoware_auto_planning_msgs::Trajectory output trajectory <code>~/output/velocity_limit</code> tier4_planning_msgs::VelocityLimit velocity limit for cruising <code>~/output/clear_velocity_limit</code> tier4_planning_msgs::VelocityLimitClearCommand clear command for velocity limit <code>~/output/stop_reasons</code> tier4_planning_msgs::StopReasonArray reasons that make the vehicle to stop"},{"location":"planning/obstacle_cruise_planner/#design","title":"Design","text":"<p>Design for the following functions is defined here.</p> <ul> <li>Behavior determination against obstacles</li> <li>Stop planning</li> <li>Cruise planning</li> <li>Slow down planning</li> </ul> <p>A data structure for cruise and stop planning is as follows. This planner data is created first, and then sent to the planning algorithm.</p> <pre><code>struct PlannerData\n{\nrclcpp::Time current_time;\nautoware_auto_planning_msgs::msg::Trajectory traj;\ngeometry_msgs::msg::Pose current_pose;\ndouble ego_vel;\ndouble current_acc;\nstd::vector&lt;Obstacle&gt; target_obstacles;\n};\n</code></pre> <pre><code>struct Obstacle\n{\nrclcpp::Time stamp;  // This is not the current stamp, but when the object was observed.\ngeometry_msgs::msg::Pose pose;  // interpolated with the current stamp\nbool orientation_reliable;\nTwist twist;\nbool twist_reliable;\nObjectClassification classification;\nstd::string uuid;\nShape shape;\nstd::vector&lt;PredictedPath&gt; predicted_paths;\n};\n</code></pre>"},{"location":"planning/obstacle_cruise_planner/#behavior-determination-against-obstacles","title":"Behavior determination against obstacles","text":"<p>Obstacles for cruising, stopping and slowing down are selected in this order based on their pose and velocity. The obstacles not in front of the ego will be ignored.</p> <p></p>"},{"location":"planning/obstacle_cruise_planner/#determine-cruise-vehicles","title":"Determine cruise vehicles","text":"<p>The obstacles meeting the following condition are determined as obstacles for cruising.</p> <ul> <li>The lateral distance from the object to the ego's trajectory is smaller than <code>behavior_determination.cruise.max_lat_margin</code>.</li> </ul> <ul> <li>The object type is for cruising according to <code>common.cruise_obstacle_type.*</code>.</li> <li>The object is not crossing the ego's trajectory (*1).</li> <li>If the object is inside the trajectory.<ul> <li>The object type is for inside cruising according to <code>common.cruise_obstacle_type.inside.*</code>.</li> <li>The object velocity is larger than <code>behavior_determination.obstacle_velocity_threshold_from_cruise_to_stop</code>.</li> </ul> </li> <li>If the object is outside the trajectory.<ul> <li>The object type is for outside cruising according to <code>common.cruise_obstacle_type.outside.*</code>.</li> <li>The object velocity is larger than <code>behavior_determination.cruise.outside_obstacle.obstacle_velocity_threshold</code>.</li> <li>The highest confident predicted path collides with the ego's trajectory.</li> <li>Its collision's period is larger than <code>behavior_determination.cruise.outside_obstacle.ego_obstacle_overlap_time_threshold</code>.</li> </ul> </li> </ul> Parameter Type Description <code>common.cruise_obstacle_type.inside.unknown</code> bool flag to consider unknown objects for cruising <code>common.cruise_obstacle_type.inside.car</code> bool flag to consider unknown objects for cruising <code>common.cruise_obstacle_type.inside.truck</code> bool flag to consider unknown objects for cruising ... bool ... <code>common.cruise_obstacle_type.outside.unknown</code> bool flag to consider unknown objects for cruising <code>common.cruise_obstacle_type.outside.car</code> bool flag to consider unknown objects for cruising <code>common.cruise_obstacle_type.outside.truck</code> bool flag to consider unknown objects for cruising ... bool ... <code>behavior_determination.cruise.max_lat_margin</code> double maximum lateral margin for cruise obstacles <code>behavior_determination.obstacle_velocity_threshold_from_cruise_to_stop</code> double maximum obstacle velocity for cruise obstacle inside the trajectory <code>behavior_determination.cruise.outside_obstacle.obstacle_velocity_threshold</code> double maximum obstacle velocity for cruise obstacle outside the trajectory <code>behavior_determination.cruise.outside_obstacle.ego_obstacle_overlap_time_threshold</code> double maximum overlap time of the collision between the ego and obstacle"},{"location":"planning/obstacle_cruise_planner/#determine-stop-vehicles","title":"Determine stop vehicles","text":"<p>Among obstacles which are not for cruising, the obstacles meeting the following condition are determined as obstacles for stopping.</p> <ul> <li>The object type is for stopping according to <code>common.stop_obstacle_type.*</code>.</li> <li>The lateral distance from the object to the ego's trajectory is smaller than <code>behavior_determination.stop.max_lat_margin</code>.</li> <li>The object velocity along the ego's trajectory is smaller than <code>behavior_determination.obstacle_velocity_threshold_from_stop_to_cruise</code>.</li> <li>The object<ul> <li>does not cross the ego's trajectory (*1)</li> <li>with the velocity smaller than <code>behavior_determination.crossing_obstacle.obstacle_velocity_threshold</code></li> <li>and its collision time margin is large enough (*2).</li> </ul> </li> </ul> Parameter Type Description <code>common.stop_obstacle_type.unknown</code> bool flag to consider unknown objects for stopping <code>common.stop_obstacle_type.car</code> bool flag to consider unknown objects for stopping <code>common.stop_obstacle_type.truck</code> bool flag to consider unknown objects for stopping ... bool ... <code>behavior_determination.stop.max_lat_margin</code> double maximum lateral margin for stop obstacles <code>behavior_determination.crossing_obstacle.obstacle_velocity_threshold</code> double maximum crossing obstacle velocity to ignore <code>behavior_determination.obstacle_velocity_threshold_from_stop_to_cruise</code> double maximum obstacle velocity for stop"},{"location":"planning/obstacle_cruise_planner/#determine-slow-down-vehicles","title":"Determine slow down vehicles","text":"<p>Among obstacles which are not for cruising and stopping, the obstacles meeting the following condition are determined as obstacles for slowing down.</p> <ul> <li>The object type is for slowing down according to <code>common.slow_down_obstacle_type.*</code>.</li> <li>The lateral distance from the object to the ego's trajectory is smaller than <code>behavior_determination.slow_down.max_lat_margin</code>.</li> </ul> Parameter Type Description <code>common.slow_down_obstacle_type.unknown</code> bool flag to consider unknown objects for slowing down <code>common.slow_down_obstacle_type.car</code> bool flag to consider unknown objects for slowing down <code>common.slow_down_obstacle_type.truck</code> bool flag to consider unknown objects for slowing down ... bool ... <code>behavior_determination.slow_down.max_lat_margin</code> double maximum lateral margin for slow down obstacles"},{"location":"planning/obstacle_cruise_planner/#note","title":"NOTE","text":""},{"location":"planning/obstacle_cruise_planner/#1-crossing-obstacles","title":"*1: Crossing obstacles","text":"<p>Crossing obstacle is the object whose orientation's yaw angle against the ego's trajectory is smaller than <code>behavior_determination.crossing_obstacle.obstacle_traj_angle_threshold</code>.</p> Parameter Type Description <code>behavior_determination.crossing_obstacle.obstacle_traj_angle_threshold</code> double maximum angle against the ego's trajectory to judge the obstacle is crossing the trajectory [rad]"},{"location":"planning/obstacle_cruise_planner/#2-enough-collision-time-margin","title":"*2: Enough collision time margin","text":"<p>We predict the collision area and its time by the ego with a constant velocity motion and the obstacle with its predicted path. Then, we calculate a collision time margin which is the difference of the time when the ego will be inside the collision area and the obstacle will be inside the collision area. When this time margin is smaller than <code>behavior_determination.stop.crossing_obstacle.collision_time_margin</code>, the margin is not enough.</p> Parameter Type Description <code>behavior_determination.stop.crossing_obstacle.collision_time_margin</code> double maximum collision time margin of the ego and obstacle"},{"location":"planning/obstacle_cruise_planner/#stop-planning","title":"Stop planning","text":"Parameter Type Description <code>common.min_strong_accel</code> double ego's minimum acceleration to stop [m/ss] <code>common.safe_distance_margin</code> double distance with obstacles for stop [m] <code>common.terminal_safe_distance_margin</code> double terminal_distance with obstacles for stop, which cannot be exceed safe distance margin [m] <p>The role of the stop planning is keeping a safe distance with static vehicle objects or dynamic/static non vehicle objects.</p> <p>The stop planning just inserts the stop point in the trajectory to keep a distance with obstacles. The safe distance is parameterized as <code>common.safe_distance_margin</code>. When it stops at the end of the trajectory, and obstacle is on the same point, the safe distance becomes <code>terminal_safe_distance_margin</code>.</p> <p>When inserting the stop point, the required acceleration for the ego to stop in front of the stop point is calculated. If the acceleration is less than <code>common.min_strong_accel</code>, the stop planning will be cancelled since this package does not assume a strong sudden brake for emergency.</p>"},{"location":"planning/obstacle_cruise_planner/#cruise-planning","title":"Cruise planning","text":"Parameter Type Description <code>common.safe_distance_margin</code> double minimum distance with obstacles for cruise [m] <p>The role of the cruise planning is keeping a safe distance with dynamic vehicle objects with smoothed velocity transition. This includes not only cruising a front vehicle, but also reacting a cut-in and cut-out vehicle.</p> <p>The safe distance is calculated dynamically based on the Responsibility-Sensitive Safety (RSS) by the following equation.</p> \\[ d_{rss} = v_{ego} t_{idling} + \\frac{1}{2} a_{ego} t_{idling}^2 + \\frac{v_{ego}^2}{2 a_{ego}} - \\frac{v_{obstacle}^2}{2 a_{obstacle}}, \\] <p>assuming that \\(d_{rss}\\) is the calculated safe distance, \\(t_{idling}\\) is the idling time for the ego to detect the front vehicle's deceleration, \\(v_{ego}\\) is the ego's current velocity, \\(v_{obstacle}\\) is the front obstacle's current velocity, \\(a_{ego}\\) is the ego's acceleration, and \\(a_{obstacle}\\) is the obstacle's acceleration. These values are parameterized as follows. Other common values such as ego's minimum acceleration is defined in <code>common.param.yaml</code>.</p> Parameter Type Description <code>common.idling_time</code> double idling time for the ego to detect the front vehicle starting deceleration [s] <code>common.min_ego_accel_for_rss</code> double ego's acceleration for RSS [m/ss] <code>common.min_object_accel_for_rss</code> double front obstacle's acceleration for RSS [m/ss] <p>The detailed formulation is as follows.</p> \\[ \\begin{align} d_{error} &amp; = d - d_{rss} \\\\ d_{normalized} &amp; = lpf(d_{error} / d_{obstacle}) \\\\ d_{quad, normalized} &amp; = sign(d_{normalized}) *d_{normalized}*d_{normalized} \\\\ v_{pid} &amp; = pid(d_{quad, normalized}) \\\\ v_{add} &amp; = v_{pid} &gt; 0 ? v_{pid}* w_{acc} : v_{pid} \\\\ v_{target} &amp; = max(v_{ego} + v_{add}, v_{min, cruise}) \\end{align} \\] Variable Description <code>d</code> actual distance to obstacle <code>d_{rss}</code> ideal distance to obstacle based on RSS <code>v_{min, cruise}</code> <code>min_cruise_target_vel</code> <code>w_{acc}</code> <code>output_ratio_during_accel</code> <code>lpf(val)</code> apply low-pass filter to <code>val</code> <code>pid(val)</code> apply pid to <code>val</code>"},{"location":"planning/obstacle_cruise_planner/#slow-down-planning","title":"Slow down planning","text":"Parameter Type Description <code>slow_down.labels</code> vector(string) A vector of labels for customizing obstacle-label-based slow down behavior. Each label represents an obstacle type that will be treated differently when applying slow down. The possible labels are (\"default\" (Mandatory), \"unknown\",\"car\",\"truck\",\"bus\",\"trailer\",\"motorcycle\",\"bicycle\" or \"pedestrian\") <code>slow_down.default.static.min_lat_velocity</code> double minimum velocity to linearly calculate slow down velocity [m]. Note: This default value will be used when the detected obstacle label does not match any of the slow_down.labels and the obstacle is considered to be static, or not moving <code>slow_down.default.static.max_lat_velocity</code> double maximum velocity to linearly calculate slow down velocity [m]. Note: This default value will be used when the detected obstacle label does not match any of the slow_down.labels and the obstacle is considered to be static, or not moving <code>slow_down.default.static.min_lat_margin</code> double minimum lateral margin to linearly calculate slow down velocity [m]. Note: This default value will be used when the detected obstacle label does not match any of the slow_down.labels and the obstacle is considered to be static, or not moving <code>slow_down.default.static.max_lat_margin</code> double maximum lateral margin to linearly calculate slow down velocity [m]. Note: This default value will be used when the detected obstacle label does not match any of the slow_down.labels and the obstacle is considered to be static, or not moving <code>slow_down.default.moving.min_lat_velocity</code> double minimum velocity to linearly calculate slow down velocity [m]. Note: This default value will be used when the detected obstacle label does not match any of the slow_down.labels and the obstacle is considered to be moving <code>slow_down.default.moving.max_lat_velocity</code> double maximum velocity to linearly calculate slow down velocity [m]. Note: This default value will be used when the detected obstacle label does not match any of the slow_down.labels and the obstacle is considered to be moving <code>slow_down.default.moving.min_lat_margin</code> double minimum lateral margin to linearly calculate slow down velocity [m]. Note: This default value will be used when the detected obstacle label does not match any of the slow_down.labels and the obstacle is considered to be moving <code>slow_down.default.moving.max_lat_margin</code> double maximum lateral margin to linearly calculate slow down velocity [m]. Note: This default value will be used when the detected obstacle label does not match any of the slow_down.labels and the obstacle is considered to be moving <code>(optional) slow_down.\"label\".(static &amp; moving).min_lat_velocity</code> double minimum velocity to linearly calculate slow down velocity [m]. Note: only for obstacles specified in <code>slow_down.labels</code>. Requires a <code>static</code> and a <code>moving</code> value <code>(optional) slow_down.\"label\".(static &amp; moving).max_lat_velocity</code> double maximum velocity to linearly calculate slow down velocity [m]. Note: only for obstacles specified in <code>slow_down.labels</code>. Requires a <code>static</code> and a <code>moving</code> value <code>(optional) slow_down.\"label\".(static &amp; moving).min_lat_margin</code> double minimum lateral margin to linearly calculate slow down velocity [m]. Note: only for obstacles specified in <code>slow_down.labels</code>. Requires a <code>static</code> and a <code>moving</code> value <code>(optional) slow_down.\"label\".(static &amp; moving).max_lat_margin</code> double maximum lateral margin to linearly calculate slow down velocity [m]. Note: only for obstacles specified in <code>slow_down.labels</code>. Requires a <code>static</code> and a <code>moving</code> value <p>The role of the slow down planning is inserting slow down velocity in the trajectory where the trajectory points are close to the obstacles. The parameters can be customized depending on the obstacle type (see <code>slow_down.labels</code>), making it possible to adjust the slow down behavior depending if the obstacle is a pedestrian, bicycle, car, etc. Each obstacle type has a <code>static</code> and a <code>moving</code> parameter set, so it is possible to customize the slow down response of the ego vehicle according to the obstacle type and if it is moving or not. If an obstacle is determined to be moving, the corresponding <code>moving</code> set of parameters will be used to compute the vehicle slow down, otherwise, the <code>static</code> parameters will be used. The <code>static</code> and <code>moving</code> separation is useful for customizing the ego vehicle slow down behavior to, for example, slow down more significantly when passing stopped vehicles that might cause occlusion or that might suddenly open its doors.</p> <p>An obstacle is classified as <code>static</code> if its total speed is less than the <code>moving_object_speed_threshold</code> parameter. Furthermore, a hysteresis based approach is used to avoid chattering, it uses the <code>moving_object_hysteresis_range</code> parameter range and the obstacle's previous state (<code>moving</code> or <code>static</code>) to determine if the obstacle is moving or not. In other words, if an obstacle was previously classified as <code>static</code>, it will not change its classification to <code>moving</code> unless its total speed is greater than <code>moving_object_speed_threshold</code> + <code>moving_object_hysteresis_range</code>. Likewise, an obstacle previously classified as <code>moving</code>, will only change to <code>static</code> if its speed is lower than <code>moving_object_speed_threshold</code> - <code>moving_object_hysteresis_range</code>.</p> <p>The closest point on the obstacle to the ego's trajectory is calculated. Then, the slow down velocity is calculated by linear interpolation with the distance between the point and trajectory as follows.</p> <p></p> Variable Description <code>v_{out}</code> calculated velocity for slow down <code>v_{min}</code> <code>slow_down.min_lat_velocity</code> <code>v_{max}</code> <code>slow_down.max_lat_velocity</code> <code>l_{min}</code> <code>slow_down.min_lat_margin</code> <code>l_{max}</code> <code>slow_down.max_lat_margin</code> <code>l'_{max}</code> <code>behavior_determination.slow_down.max_lat_margin</code> <p>The calculated velocity is inserted in the trajectory where the obstacle is inside the area with <code>behavior_determination.slow_down.max_lat_margin</code>.</p> <p></p>"},{"location":"planning/obstacle_cruise_planner/#implementation","title":"Implementation","text":""},{"location":"planning/obstacle_cruise_planner/#flowchart","title":"Flowchart","text":"<p>Successive functions consist of <code>obstacle_cruise_planner</code> as follows.</p> <p>Various algorithms for stop and cruise planning will be implemented, and one of them is designated depending on the use cases. The core algorithm implementation <code>generateTrajectory</code> depends on the designated algorithm.</p> <p></p>"},{"location":"planning/obstacle_cruise_planner/#algorithm-selection-for-cruise-planner","title":"Algorithm selection for cruise planner","text":"<p>Currently, only a PID-based planner is supported. Each planner will be explained in the following.</p> Parameter Type Description <code>common.planning_method</code> string cruise and stop planning algorithm, selected from \"pid_base\""},{"location":"planning/obstacle_cruise_planner/#pid-based-planner","title":"PID-based planner","text":""},{"location":"planning/obstacle_cruise_planner/#stop-planning_1","title":"Stop planning","text":"<p>In the <code>pid_based_planner</code> namespace,</p> Parameter Type Description <code>obstacle_velocity_threshold_from_cruise_to_stop</code> double obstacle velocity threshold to be stopped from cruised [m/s] <p>Only one obstacle is targeted for the stop planning. It is the obstacle among obstacle candidates whose velocity is less than <code>obstacle_velocity_threshold_from_cruise_to_stop</code>, and which is the nearest to the ego along the trajectory. A stop point is inserted keeping<code>common.safe_distance_margin</code> distance between the ego and obstacle.</p> <p>Note that, as explained in the stop planning design, a stop planning which requires a strong acceleration (less than <code>common.min_strong_accel</code>) will be canceled.</p>"},{"location":"planning/obstacle_cruise_planner/#cruise-planning_1","title":"Cruise planning","text":"<p>In the <code>pid_based_planner</code> namespace,</p> Parameter Type Description <code>kp</code> double p gain for pid control [-] <code>ki</code> double i gain for pid control [-] <code>kd</code> double d gain for pid control [-] <code>output_ratio_during_accel</code> double The output velocity will be multiplied by the ratio during acceleration to follow the front vehicle. [-] <code>vel_to_acc_weight</code> double target acceleration is target velocity * <code>vel_to_acc_weight</code> [-] <code>min_cruise_target_vel</code> double minimum target velocity during cruise [m/s] <p>In order to keep the safe distance, the target velocity and acceleration is calculated and sent as an external velocity limit to the velocity smoothing package (<code>motion_velocity_smoother</code> by default). The target velocity and acceleration is respectively calculated with the PID controller according to the error between the reference safe distance and the actual distance.</p>"},{"location":"planning/obstacle_cruise_planner/#optimization-based-planner","title":"Optimization-based planner","text":"<p>under construction</p>"},{"location":"planning/obstacle_cruise_planner/#minor-functions","title":"Minor functions","text":""},{"location":"planning/obstacle_cruise_planner/#prioritization-of-behavior-modules-stop-point","title":"Prioritization of behavior module's stop point","text":"<p>When stopping for a pedestrian walking on the crosswalk, the behavior module inserts the zero velocity in the trajectory in front of the crosswalk. Also <code>obstacle_cruise_planner</code>'s stop planning also works, and the ego may not reach the behavior module's stop point since the safe distance defined in <code>obstacle_cruise_planner</code> may be longer than the behavior module's safe distance. To resolve this non-alignment of the stop point between the behavior module and <code>obstacle_cruise_planner</code>, <code>common.min_behavior_stop_margin</code> is defined. In the case of the crosswalk described above, <code>obstacle_cruise_planner</code> inserts the stop point with a distance <code>common.min_behavior_stop_margin</code> at minimum between the ego and obstacle.</p> Parameter Type Description <code>common.min_behavior_stop_margin</code> double minimum stop margin when stopping with the behavior module enabled [m]"},{"location":"planning/obstacle_cruise_planner/#a-function-to-keep-the-closest-stop-obstacle-in-target-obstacles","title":"A function to keep the closest stop obstacle in target obstacles","text":"<p>In order to keep the closest stop obstacle in the target obstacles, we check whether it is disappeared or not from the target obstacles in the <code>checkConsistency</code> function. If the previous closest stop obstacle is remove from the lists, we keep it in the lists for <code>stop_obstacle_hold_time_threshold</code> seconds. Note that if a new stop obstacle appears and the previous closest obstacle removes from the lists, we do not add it to the target obstacles again.</p> Parameter Type Description <code>behavior_determination.stop_obstacle_hold_time_threshold</code> double maximum time for holding closest stop obstacle [s]"},{"location":"planning/obstacle_cruise_planner/#how-to-debug","title":"How To Debug","text":"<p>How to debug can be seen here.</p>"},{"location":"planning/obstacle_cruise_planner/#known-limits","title":"Known Limits","text":"<ul> <li>Common<ul> <li>When the obstacle pose or velocity estimation has a delay, the ego sometimes will go close to the front vehicle keeping deceleration.</li> <li>Current implementation only uses predicted objects message for static/dynamic obstacles and does not use pointcloud. Therefore, if object recognition is lost, the ego cannot deal with the lost obstacle.</li> <li>The current predicted paths for obstacle's lane change does not have enough precision for obstacle_cruise_planner. Therefore, we set <code>rough_detection_area</code> a small value.</li> </ul> </li> <li>PID-based planner<ul> <li>The algorithm strongly depends on the velocity smoothing package (<code>motion_velocity_smoother</code> by default) whether or not the ego realizes the designated target speed. If the velocity smoothing package is updated, please take care of the vehicle's behavior as much as possible.</li> </ul> </li> </ul>"},{"location":"planning/obstacle_cruise_planner/docs/debug/","title":"Debug","text":""},{"location":"planning/obstacle_cruise_planner/docs/debug/#debug","title":"Debug","text":""},{"location":"planning/obstacle_cruise_planner/docs/debug/#debug-visualization","title":"Debug visualization","text":""},{"location":"planning/obstacle_cruise_planner/docs/debug/#detection-area","title":"Detection area","text":"<p>Green polygons which is a detection area is visualized by <code>detection_polygons</code> in the <code>~/debug/marker</code> topic. To determine each behavior (cruise, stop, and slow down), if <code>behavior_determination.*.max_lat_margin</code> is not zero, the polygons are expanded with the additional width.</p> <p></p>"},{"location":"planning/obstacle_cruise_planner/docs/debug/#collision-points","title":"Collision points","text":"<p>Red points which are collision points with obstacle are visualized by <code>*_collision_points</code> for each behavior in the <code>~/debug/marker</code> topic.</p> <p></p>"},{"location":"planning/obstacle_cruise_planner/docs/debug/#obstacle-for-cruise","title":"Obstacle for cruise","text":"<p>Orange sphere which is an obstacle for cruise is visualized by <code>obstacles_to_cruise</code> in the <code>~/debug/marker</code> topic.</p> <p>Orange wall which means a safe distance to cruise if the ego's front meets the wall is visualized in the <code>~/debug/cruise/virtual_wall</code> topic.</p> <p></p>"},{"location":"planning/obstacle_cruise_planner/docs/debug/#obstacle-for-stop","title":"Obstacle for stop","text":"<p>Red sphere which is an obstacle for stop is visualized by <code>obstacles_to_stop</code> in the <code>~/debug/marker</code> topic.</p> <p>Red wall which means a safe distance to stop if the ego's front meets the wall is visualized in the <code>~/virtual_wall</code> topic.</p> <p></p>"},{"location":"planning/obstacle_cruise_planner/docs/debug/#obstacle-for-slow-down","title":"Obstacle for slow down","text":"<p>Yellow sphere which is an obstacle for slow_down is visualized by <code>obstacles_to_slow_down</code> in the <code>~/debug/marker</code> topic.</p> <p>Yellow wall which means a safe distance to slow_down if the ego's front meets the wall is visualized in the <code>~/debug/slow_down/virtual_wall</code> topic.</p> <p></p>"},{"location":"planning/obstacle_stop_planner/","title":"Obstacle Stop Planner","text":""},{"location":"planning/obstacle_stop_planner/#obstacle-stop-planner","title":"Obstacle Stop Planner","text":""},{"location":"planning/obstacle_stop_planner/#overview","title":"Overview","text":"<p><code>obstacle_stop_planner</code> has following modules</p> <ul> <li>Obstacle Stop Planner<ul> <li>inserts a stop point in trajectory when there is a static point cloud on the trajectory.</li> </ul> </li> <li>Slow Down Planner<ul> <li>inserts a deceleration section in trajectory when there is a point cloud near the trajectory.</li> </ul> </li> <li>Adaptive Cruise Controller (ACC)<ul> <li>embeds target velocity in trajectory when there is a dynamic point cloud on the trajectory.</li> </ul> </li> </ul>"},{"location":"planning/obstacle_stop_planner/#input-topics","title":"Input topics","text":"Name Type Description <code>~/input/pointcloud</code> sensor_msgs::PointCloud2 obstacle pointcloud <code>~/input/trajectory</code> autoware_auto_planning_msgs::Trajectory trajectory <code>~/input/vector_map</code> autoware_auto_mapping_msgs::HADMapBin vector map <code>~/input/odometry</code> nav_msgs::Odometry vehicle velocity <code>~/input/dynamic_objects</code> autoware_auto_perception_msgs::PredictedObjects dynamic objects <code>~/input/expand_stop_range</code> tier4_planning_msgs::msg::ExpandStopRange expand stop range"},{"location":"planning/obstacle_stop_planner/#output-topics","title":"Output topics","text":"Name Type Description <code>~output/trajectory</code> autoware_auto_planning_msgs::Trajectory trajectory to be followed <code>~output/stop_reasons</code> tier4_planning_msgs::StopReasonArray reasons that cause the vehicle to stop"},{"location":"planning/obstacle_stop_planner/#common-parameter","title":"Common Parameter","text":"Parameter Type Description <code>enable_slow_down</code> bool enable slow down planner [-] <code>max_velocity</code> double max velocity [m/s] <code>chattering_threshold</code> double even if the obstacle disappears, the stop judgment continues for chattering_threshold [s] <code>enable_z_axis_obstacle_filtering</code> bool filter obstacles in z axis (height) [-] <code>z_axis_filtering_buffer</code> double additional buffer for z axis filtering [m] <code>use_predicted_objects</code> bool whether to use predicted objects for collision and slowdown detection [-] <code>predicted_object_filtering_threshold</code> double threshold for filtering predicted objects [valid only publish_obstacle_polygon true] [m] <code>publish_obstacle_polygon</code> bool if use_predicted_objects is true, node publishes collision polygon [-]"},{"location":"planning/obstacle_stop_planner/#obstacle-stop-planner_1","title":"Obstacle Stop Planner","text":""},{"location":"planning/obstacle_stop_planner/#role","title":"Role","text":"<p>This module inserts the stop point before the obstacle with margin. In nominal case, the margin is the sum of <code>baselink_to_front</code> and <code>max_longitudinal_margin</code>. The <code>baselink_to_front</code> means the distance between <code>baselink</code>( center of rear-wheel axis) and front of the car. The detection area is generated along the processed trajectory as following figure. (This module cut off the input trajectory behind the ego position and decimates the trajectory points for reducing computational costs.)</p> <p> </p> parameters for obstacle stop planner <p> </p> target for obstacle stop planner <p>If another stop point has already been inserted by other modules within <code>max_longitudinal_margin</code>, the margin is the sum of <code>baselink_to_front</code> and <code>min_longitudinal_margin</code>. This feature exists to avoid stopping unnaturally position. (For example, the ego stops unnaturally far away from stop line of crosswalk that pedestrians cross to without this feature.)</p> <p> </p> minimum longitudinal margin <p>The module searches the obstacle pointcloud within detection area. When the pointcloud is found, <code>Adaptive Cruise Controller</code> modules starts to work. only when <code>Adaptive Cruise Controller</code> modules does not insert target velocity, the stop point is inserted to the trajectory. The stop point means the point with 0 velocity.</p>"},{"location":"planning/obstacle_stop_planner/#restart-prevention","title":"Restart prevention","text":"<p>If it needs X meters (e.g. 0.5 meters) to stop once the vehicle starts moving due to the poor vehicle control performance, the vehicle goes over the stopping position that should be strictly observed when the vehicle starts to moving in order to approach the near stop point (e.g. 0.3 meters away).</p> <p>This module has parameter <code>hold_stop_margin_distance</code> in order to prevent from these redundant restart. If the vehicle is stopped within <code>hold_stop_margin_distance</code> meters from stop point of the module, the module judges that the vehicle has already stopped for the module's stop point and plans to keep stopping current position even if the vehicle is stopped due to other factors.</p> <p> </p> parameters <p> </p> outside the hold_stop_margin_distance <p> </p> inside the hold_stop_margin_distance"},{"location":"planning/obstacle_stop_planner/#parameters","title":"Parameters","text":""},{"location":"planning/obstacle_stop_planner/#stop-position","title":"Stop position","text":"Parameter Type Description <code>max_longitudinal_margin</code> double margin between obstacle and the ego's front [m] <code>max_longitudinal_margin_behind_goal</code> double margin between obstacle and the ego's front when the stop point is behind the goal[m] <code>min_longitudinal_margin</code> double if any obstacle exists within <code>max_longitudinal_margin</code>, this module set margin as the value of stop margin to <code>min_longitudinal_margin</code> [m] <code>hold_stop_margin_distance</code> double parameter for restart prevention (See above section) [m]"},{"location":"planning/obstacle_stop_planner/#obstacle-detection-area","title":"Obstacle detection area","text":"Parameter Type Description <code>lateral_margin</code> double lateral margin from the vehicle footprint for collision obstacle detection area [m] <code>step_length</code> double step length for pointcloud search range [m] <code>enable_stop_behind_goal_for_obstacle</code> bool enabling extend trajectory after goal lane for obstacle detection"},{"location":"planning/obstacle_stop_planner/#flowchart","title":"Flowchart","text":""},{"location":"planning/obstacle_stop_planner/#slow-down-planner","title":"Slow Down Planner","text":""},{"location":"planning/obstacle_stop_planner/#role_1","title":"Role","text":"<p>This module inserts the slow down section before the obstacle with forward margin and backward margin. The forward margin is the sum of <code>baselink_to_front</code> and <code>longitudinal_forward_margin</code>, and the backward margin is the sum of <code>baselink_to_front</code> and <code>longitudinal_backward_margin</code>. The ego keeps slow down velocity in slow down section. The velocity is calculated the following equation.</p> <p>\\(v_{target} = v_{min} + \\frac{l_{ld} - l_{vw}/2}{l_{margin}} (v_{max} - v_{min} )\\)</p> <ul> <li>\\(v_{target}\\) : slow down target velocity [m/s]</li> <li>\\(v_{min}\\) : <code>min_slow_down_velocity</code> [m/s]</li> <li>\\(v_{max}\\) : <code>max_slow_down_velocity</code> [m/s]</li> <li>\\(l_{ld}\\) : lateral deviation between the obstacle and the ego footprint [m]</li> <li>\\(l_{margin}\\) : <code>lateral_margin</code> [m]</li> <li>\\(l_{vw}\\) : width of the ego footprint [m]</li> </ul> <p>The above equation means that the smaller the lateral deviation of the pointcloud, the lower the velocity of the slow down section.</p> <p> </p> parameters for slow down planner <p> </p> target for slow down planner"},{"location":"planning/obstacle_stop_planner/#parameters_1","title":"Parameters","text":""},{"location":"planning/obstacle_stop_planner/#slow-down-section","title":"Slow down section","text":"Parameter Type Description <code>longitudinal_forward_margin</code> double margin between obstacle and the ego's front [m] <code>longitudinal_backward_margin</code> double margin between obstacle and the ego's rear [m]"},{"location":"planning/obstacle_stop_planner/#obstacle-detection-area_1","title":"Obstacle detection area","text":"Parameter Type Description <code>lateral_margin</code> double lateral margin from the vehicle footprint for slow down obstacle detection area [m]"},{"location":"planning/obstacle_stop_planner/#slow-down-target-velocity","title":"Slow down target velocity","text":"Parameter Type Description <code>max_slow_down_velocity</code> double max slow down velocity [m/s] <code>min_slow_down_velocity</code> double min slow down velocity [m/s]"},{"location":"planning/obstacle_stop_planner/#flowchart_1","title":"Flowchart","text":""},{"location":"planning/obstacle_stop_planner/#adaptive-cruise-controller","title":"Adaptive Cruise Controller","text":""},{"location":"planning/obstacle_stop_planner/#role_2","title":"Role","text":"<p><code>Adaptive Cruise Controller</code> module embeds maximum velocity in trajectory when there is a dynamic point cloud on the trajectory. The value of maximum velocity depends on the own velocity, the velocity of the point cloud ( = velocity of the front car), and the distance to the point cloud (= distance to the front car).</p> Parameter Type Description <code>adaptive_cruise_control.use_object_to_estimate_vel</code> bool use dynamic objects for estimating object velocity or not (valid only if osp.use_predicted_objects false) <code>adaptive_cruise_control.use_pcl_to_estimate_vel</code> bool use raw pointclouds for estimating object velocity or not (valid only if osp.use_predicted_objects false) <code>adaptive_cruise_control.consider_obj_velocity</code> bool consider forward vehicle velocity to calculate target velocity in adaptive cruise or not <code>adaptive_cruise_control.obstacle_velocity_thresh_to_start_acc</code> double start adaptive cruise control when the velocity of the forward obstacle exceeds this value [m/s] <code>adaptive_cruise_control.obstacle_velocity_thresh_to_stop_acc</code> double stop acc when the velocity of the forward obstacle falls below this value [m/s] <code>adaptive_cruise_control.emergency_stop_acceleration</code> double supposed minimum acceleration (deceleration) in emergency stop [m/ss] <code>adaptive_cruise_control.emergency_stop_idling_time</code> double supposed idling time to start emergency stop [s] <code>adaptive_cruise_control.min_dist_stop</code> double minimum distance of emergency stop [m] <code>adaptive_cruise_control.obstacle_emergency_stop_acceleration</code> double supposed minimum acceleration (deceleration) in emergency stop [m/ss] <code>adaptive_cruise_control.max_standard_acceleration</code> double supposed maximum acceleration in active cruise control [m/ss] <code>adaptive_cruise_control.min_standard_acceleration</code> double supposed minimum acceleration (deceleration) in active cruise control [m/ss] <code>adaptive_cruise_control.standard_idling_time</code> double supposed idling time to react object in active cruise control [s] <code>adaptive_cruise_control.min_dist_standard</code> double minimum distance in active cruise control [m] <code>adaptive_cruise_control.obstacle_min_standard_acceleration</code> double supposed minimum acceleration of forward obstacle [m/ss] <code>adaptive_cruise_control.margin_rate_to_change_vel</code> double rate of margin distance to insert target velocity [-] <code>adaptive_cruise_control.use_time_compensation_to_calc_distance</code> bool use time-compensation to calculate distance to forward vehicle <code>adaptive_cruise_control.p_coefficient_positive</code> double coefficient P in PID control (used when target dist -current_dist &gt;=0) [-] <code>adaptive_cruise_control.p_coefficient_negative</code> double coefficient P in PID control (used when target dist -current_dist &lt;0) [-] <code>adaptive_cruise_control.d_coefficient_positive</code> double coefficient D in PID control (used when delta_dist &gt;=0) [-] <code>adaptive_cruise_control.d_coefficient_negative</code> double coefficient D in PID control (used when delta_dist &lt;0) [-] <code>adaptive_cruise_control.object_polygon_length_margin</code> double The distance to extend the polygon length the object in pointcloud-object matching [m] <code>adaptive_cruise_control.object_polygon_width_margin</code> double The distance to extend the polygon width the object in pointcloud-object matching [m] <code>adaptive_cruise_control.valid_estimated_vel_diff_time</code> double Maximum time difference treated as continuous points in speed estimation using a point cloud [s] <code>adaptive_cruise_control.valid_vel_que_time</code> double Time width of information used for speed estimation in speed estimation using a point cloud [s] <code>adaptive_cruise_control.valid_estimated_vel_max</code> double Maximum value of valid speed estimation results in speed estimation using a point cloud [m/s] <code>adaptive_cruise_control.valid_estimated_vel_min</code> double Minimum value of valid speed estimation results in speed estimation using a point cloud [m/s] <code>adaptive_cruise_control.thresh_vel_to_stop</code> double Embed a stop line if the maximum speed calculated by ACC is lower than this speed [m/s] <code>adaptive_cruise_control.lowpass_gain_of_upper_velocity</code> double Lowpass-gain of target velocity <code>adaptive_cruise_control.use_rough_velocity_estimation:</code> bool Use rough estimated velocity if the velocity estimation is failed (valid only if osp.use_predicted_objects false) <code>adaptive_cruise_control.rough_velocity_rate</code> double In the rough velocity estimation, the velocity of front car is estimated as self current velocity * this value"},{"location":"planning/obstacle_stop_planner/#flowchart_2","title":"Flowchart","text":"<p>(*1) The target vehicle point is calculated as a closest obstacle PointCloud from ego along the trajectory.</p> <p>(*2) The sources of velocity estimation can be changed by the following ROS parameters.</p> <ul> <li><code>adaptive_cruise_control.use_object_to_estimate_vel</code></li> <li><code>adaptive_cruise_control.use_pcl_to_estimate_vel</code></li> </ul> <p>This module works only when the target point is found in the detection area of the <code>Obstacle stop planner</code> module.</p> <p>The first process of this module is to estimate the velocity of the target vehicle point. The velocity estimation uses the velocity information of dynamic objects or the travel distance of the target vehicle point from the previous step. The dynamic object information is primal, and the travel distance estimation is used as a backup in case of the perception failure. If the target vehicle point is contained in the bounding box of a dynamic object geometrically, the velocity of the dynamic object is used as the target point velocity. Otherwise, the target point velocity is calculated by the travel distance of the target point from the previous step; that is <code>(current_position - previous_position) / dt</code>. Note that this travel distance based estimation fails when the target point is detected in the first time (it mainly happens in the cut-in situation). To improve the stability of the estimation, the median of the calculation result for several steps is used.</p> <p>If the calculated velocity is within the threshold range, it is used as the target point velocity.</p> <p>Only when the estimation is succeeded and the estimated velocity exceeds the value of <code>obstacle_stop_velocity_thresh_*</code>, the distance to the pointcloud from self-position is calculated. For prevent chattering in the mode transition, <code>obstacle_velocity_thresh_to_start_acc</code> is used for the threshold to start adaptive cruise, and <code>obstacle_velocity_thresh_to_stop_acc</code> is used for the threshold to stop adaptive cruise. When the calculated distance value exceeds the emergency distance \\(d\\_{emergency}\\) calculated by emergency_stop parameters, target velocity to insert is calculated.</p> <p>The emergency distance \\(d\\_{emergency}\\) is calculated as follows.</p> <p>\\(d_{emergency} = d_{margin_{emergency}} + t_{idling_{emergency}} \\cdot v_{ego} + (-\\frac{v_{ego}^2}{2 \\cdot a_{ego_ {emergency}}}) - (-\\frac{v_{obj}^2}{2 \\cdot a_{obj_{emergency}}})\\)</p> <ul> <li>\\(d_{margin_{emergency}}\\) is a minimum margin to the obstacle pointcloud. The value of \\(d_{margin_{emergency}}\\) depends   on the parameter <code>min_dist_stop</code></li> <li>\\(t_{idling_{emergency}}\\) is a supposed idling time. The value of \\(t_{idling_{emergency}}\\) depends on the   parameter <code>emergency_stop_idling_time</code></li> <li>\\(v_{ego}\\) is a current velocity of own vehicle</li> <li>\\(a_{ego_{_{emergency}}}\\) is a minimum acceleration (maximum deceleration) of own vehicle. The value of \\(a_{ego_{_   {emergency}}}\\) depends on the parameter <code>emergency_stop_acceleration</code></li> <li>\\(v_{obj}\\) is a current velocity of obstacle pointcloud.</li> <li>\\(a_{obj_{_{emergency}}}\\) is a supposed minimum acceleration of obstacle pointcloud. The value of \\(a_{obj_{_   {emergency}}}\\) depends on the parameter <code>obstacle_emergency_stop_acceleration</code></li> <li>*Above \\(X_{_{emergency}}\\) parameters are used only in emergency situation.</li> </ul> <p>The target velocity is determined to keep the distance to the obstacle pointcloud from own vehicle at the standard distance \\(d\\_{standard}\\) calculated as following. Therefore, if the distance to the obstacle pointcloud is longer than standard distance, The target velocity becomes higher than the current velocity, and vice versa. For keeping the distance, a PID controller is used.</p> <p>\\(d_{standard} = d_{margin_{standard}} + t_{idling_{standard}} \\cdot v_{ego} + (-\\frac{v_{ego}^2}{2 \\cdot a_{ego_ {standard}}}) - (-\\frac{v_{obj}^2}{2 \\cdot a_{obj_{standard}}})\\)</p> <ul> <li>\\(d_{margin_{standard}}\\) is a minimum margin to the obstacle pointcloud. The value of \\(d_{margin_{standard}}\\) depends   on the parameter <code>min_dist_stop</code></li> <li>\\(t_{idling_{standard}}\\) is a supposed idling time. The value of \\(t_{idling_{standard}}\\) depends on the   parameter <code>standard_stop_idling_time</code></li> <li>\\(v_{ego}\\) is a current velocity of own vehicle</li> <li>\\(a_{ego_{_{standard}}}\\) is a minimum acceleration (maximum deceleration) of own vehicle. The value of \\(a_{ego_{_   {standard}}}\\) depends on the parameter <code>min_standard_acceleration</code></li> <li>\\(v_{obj}\\) is a current velocity of obstacle pointcloud.</li> <li>\\(a_{obj_{_{standard}}}\\) is a supposed minimum acceleration of obstacle pointcloud. The value of \\(a_{obj_{_   {standard}}}\\) depends on the parameter <code>obstacle_min_standard_acceleration</code></li> <li>*Above \\(X_{_{standard}}\\) parameters are used only in non-emergency situation.</li> </ul> <p></p> <p>If the target velocity exceeds the value of <code>thresh_vel_to_stop</code>, the target velocity is embedded in the trajectory.</p>"},{"location":"planning/obstacle_stop_planner/#known-limits","title":"Known Limits","text":"<ul> <li>It is strongly depends on velocity planning module whether or not it moves according to the target speed embedded   by <code>Adaptive Cruise Controller</code> module. If the velocity planning module is updated, please take care of the vehicle's   behavior as much as possible and always be ready for overriding.</li> </ul> <ul> <li>The velocity estimation algorithm in <code>Adaptive Cruise Controller</code> is depend on object tracking module. Please note   that if the object-tracking fails or the tracking result is incorrect, it the possibility that the vehicle behaves   dangerously.</li> </ul> <ul> <li>It does not work for backward driving, but publishes the path of the input as it is. Please   use obstacle_cruise_planner if you want to stop against an obstacle when   backward driving.</li> </ul>"},{"location":"planning/obstacle_velocity_limiter/","title":"Obstacle Velocity Limiter","text":""},{"location":"planning/obstacle_velocity_limiter/#obstacle-velocity-limiter","title":"Obstacle Velocity Limiter","text":""},{"location":"planning/obstacle_velocity_limiter/#purpose","title":"Purpose","text":"<p>This node limits the velocity when driving in the direction of an obstacle. For example, it allows to reduce the velocity when driving close to a guard rail in a curve.</p> Without this node With this node"},{"location":"planning/obstacle_velocity_limiter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Using a parameter <code>min_ttc</code> (minimum time to collision), the node set velocity limits such that no collision with an obstacle would occur, even without new control inputs for a duration of <code>min_ttc</code>.</p> <p>To achieve this, the motion of the ego vehicle is simulated forward in time at each point of the trajectory to create a corresponding footprint. If the footprint collides with some obstacle, the velocity at the trajectory point is reduced such that the new simulated footprint do not have any collision.</p>"},{"location":"planning/obstacle_velocity_limiter/#simulated-motion-footprint-and-collision-distance","title":"Simulated Motion, Footprint, and Collision Distance","text":"<p>The motion of the ego vehicle is simulated at each trajectory point using the <code>heading</code>, <code>velocity</code>, and <code>steering</code> defined at the point. Footprints are then constructed from these simulations and checked for collision. If a collision is found, the distance from the trajectory point is used to calculate the adjusted velocity that would produce a collision-free footprint. Parameter <code>simulation.distance_method</code> allow to switch between an exact distance calculation and a less expensive approximation using a simple euclidean distance.</p> <p>Two models can be selected with parameter <code>simulation.model</code> for simulating the motion of the vehicle: a simple particle model and a more complicated bicycle model.</p>"},{"location":"planning/obstacle_velocity_limiter/#particle-model","title":"Particle Model","text":"<p>The particle model uses the constant heading and velocity of the vehicle at a trajectory point to simulate the future motion. The simulated forward motion corresponds to a straight line and the footprint to a rectangle.</p>"},{"location":"planning/obstacle_velocity_limiter/#footprint","title":"Footprint","text":"<p>The rectangle footprint is built from 2 lines parallel to the simulated forward motion and at a distance of half the vehicle width.</p> <p></p>"},{"location":"planning/obstacle_velocity_limiter/#distance","title":"Distance","text":"<p>When a collision point is found within the footprint, the distance is calculated as described in the following figure.</p> <p></p>"},{"location":"planning/obstacle_velocity_limiter/#bicycle-model","title":"Bicycle Model","text":"<p>The bicycle model uses the constant heading, velocity, and steering of the vehicle at a trajectory point to simulate the future motion. The simulated forward motion corresponds to an arc around the circle of curvature associated with the steering. Uncertainty in the steering can be introduced with the <code>simulation.steering_offset</code> parameter which will generate a range of motion from a left-most to a right-most steering. This results in 3 curved lines starting from the same trajectory point. A parameter <code>simulation.nb_points</code> is used to adjust the precision of these lines, with a minimum of <code>2</code> resulting in straight lines and higher values increasing the precision of the curves.</p> <p>By default, the steering values contained in the trajectory message are used. Parameter <code>trajectory_preprocessing.calculate_steering_angles</code> allows to recalculate these values when set to <code>true</code>.</p>"},{"location":"planning/obstacle_velocity_limiter/#footprint_1","title":"Footprint","text":"<p>The footprint of the bicycle model is created from lines parallel to the left and right simulated motion at a distance of half the vehicle width. In addition, the two points on the left and right of the end point of the central simulated motion are used to complete the polygon.</p> <p></p>"},{"location":"planning/obstacle_velocity_limiter/#distance_1","title":"Distance","text":"<p>The distance to a collision point is calculated by finding the curvature circle passing through the trajectory point and the collision point.</p> <p></p>"},{"location":"planning/obstacle_velocity_limiter/#obstacle-detection","title":"Obstacle Detection","text":"<p>Obstacles are represented as points or linestrings (i.e., sequence of points) around the obstacles and are constructed from an occupancy grid, a pointcloud, or the lanelet map. The lanelet map is always checked for obstacles but the other source is switched using parameter <code>obstacles.dynamic_source</code>.</p> <p>To efficiently find obstacles intersecting with a footprint, they are stored in a R-tree. Two trees are used, one for the obstacle points, and one for the obstacle linestrings (which are decomposed into segments to simplify the R-tree).</p>"},{"location":"planning/obstacle_velocity_limiter/#obstacle-masks","title":"Obstacle masks","text":""},{"location":"planning/obstacle_velocity_limiter/#dynamic-obstacles","title":"Dynamic obstacles","text":"<p>Moving obstacles such as other cars should not be considered by this module. These obstacles are detected by the perception modules and represented as polygons. Obstacles inside these polygons are ignored.</p> <p>Only dynamic obstacles with a velocity above parameter <code>obstacles.dynamic_obstacles_min_vel</code> are removed.</p> <p>To deal with delays and precision errors, the polygons can be enlarged with parameter <code>obstacles.dynamic_obstacles_buffer</code>.</p>"},{"location":"planning/obstacle_velocity_limiter/#obstacles-outside-of-the-safety-envelope","title":"Obstacles outside of the safety envelope","text":"<p>Obstacles that are not inside any forward simulated footprint are ignored if parameter <code>obstacles.filter_envelope</code> is set to true. The safety envelope polygon is built from all the footprints and used as a positive mask on the occupancy grid or pointcloud.</p> <p>This option can reduce the total number of obstacles which reduces the cost of collision detection. However, the cost of masking the envelope is usually too high to be interesting.</p>"},{"location":"planning/obstacle_velocity_limiter/#obstacles-on-the-ego-path","title":"Obstacles on the ego path","text":"<p>If parameter <code>obstacles.ignore_obstacles_on_path</code> is set to <code>true</code>, a polygon mask is built from the trajectory and the vehicle dimension. Any obstacle in this polygon is ignored.</p> <p>The size of the polygon can be increased using parameter <code>obstacles.ignore_extra_distance</code> which is added to the vehicle lateral offset.</p> <p>This option is a bit expensive and should only be used in case of noisy dynamic obstacles where obstacles are wrongly detected on the ego path, causing unwanted velocity limits.</p>"},{"location":"planning/obstacle_velocity_limiter/#lanelet-map","title":"Lanelet Map","text":"<p>Information about static obstacles can be stored in the Lanelet map using the value of the <code>type</code> tag of linestrings. If any linestring has a <code>type</code> with one of the value from parameter <code>obstacles.static_map_tags</code>, then it will be used as an obstacle.</p> <p>Obstacles from the lanelet map are not impacted by the masks.</p>"},{"location":"planning/obstacle_velocity_limiter/#occupancy-grid","title":"Occupancy Grid","text":"<p>Masking is performed by iterating through the cells inside each polygon mask using the <code>grid_map_utils::PolygonIterator</code> function. A threshold is then applied to only keep cells with an occupancy value above parameter <code>obstacles.occupancy_grid_threshold</code>. Finally, the image is converted to an image and obstacle linestrings are extracted using the opencv function <code>findContour</code>.</p>"},{"location":"planning/obstacle_velocity_limiter/#pointcloud","title":"Pointcloud","text":"<p>Masking is performed using the <code>pcl::CropHull</code> function. Points from the pointcloud are then directly used as obstacles.</p>"},{"location":"planning/obstacle_velocity_limiter/#velocity-adjustment","title":"Velocity Adjustment","text":"<p>If a collision is found, the velocity at the trajectory point is adjusted such that the resulting footprint would no longer collide with an obstacle: \\(velocity = \\frac{dist\\_to\\_collision}{min\\_ttc}\\)</p> <p>To prevent sudden deceleration of the ego vehicle, the parameter <code>max_deceleration</code> limits the deceleration relative to the current ego velocity. For a trajectory point occurring at a duration <code>t</code> in the future (calculated from the original velocity profile), the adjusted velocity cannot be set lower than \\(v_{current} - t * max\\_deceleration\\).</p> <p>Furthermore, a parameter <code>min_adjusted_velocity</code> provides a lower bound on the modified velocity.</p>"},{"location":"planning/obstacle_velocity_limiter/#trajectory-preprocessing","title":"Trajectory preprocessing","text":"<p>The node only modifies part of the input trajectory, starting from the current ego position. Parameter <code>trajectory_preprocessing.start_distance</code> is used to adjust how far ahead of the ego position the velocities will start being modified. Parameters <code>trajectory_preprocessing.max_length</code> and <code>trajectory_preprocessing.max_duration</code> are used to control how much of the trajectory will see its velocity adjusted.</p> <p>To reduce computation cost at the cost of precision, the trajectory can be downsampled using parameter <code>trajectory_preprocessing.downsample_factor</code>. For example a value of <code>1</code> means all trajectory points will be evaluated while a value of <code>10</code> means only 1/10th of the points will be evaluated.</p>"},{"location":"planning/obstacle_velocity_limiter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/obstacle_velocity_limiter/#inputs","title":"Inputs","text":"Name Type Description <code>~/input/trajectory</code> <code>autoware_auto_planning_msgs/Trajectory</code> Reference trajectory <code>~/input/occupancy_grid</code> <code>nav_msgs/OccupancyGrid</code> Occupancy grid with obstacle information <code>~/input/obstacle_pointcloud</code> <code>sensor_msgs/PointCloud2</code> Pointcloud containing only obstacle points <code>~/input/dynamic_obstacles</code> <code>autoware_auto_perception_msgs/PredictedObjects</code> Dynamic objects <code>~/input/odometry</code> <code>nav_msgs/Odometry</code> Odometry used to retrieve the current ego velocity <code>~/input/map</code> <code>autoware_auto_mapping_msgs/HADMapBin</code> Vector map used to retrieve static obstacles"},{"location":"planning/obstacle_velocity_limiter/#outputs","title":"Outputs","text":"Name Type Description <code>~/output/trajectory</code> <code>autoware_auto_planning_msgs/Trajectory</code> Trajectory with adjusted velocities <code>~/output/debug_markers</code> <code>visualization_msgs/MarkerArray</code> Debug markers (envelopes, obstacle polygons) <code>~/output/runtime_microseconds</code> <code>tier4_debug_msgs/Float64</code> Time taken to calculate the trajectory (in microseconds)"},{"location":"planning/obstacle_velocity_limiter/#parameters","title":"Parameters","text":"Name Type Description <code>min_ttc</code> float [s] required minimum time with no collision at each point of the trajectory assuming constant heading and velocity. <code>distance_buffer</code> float [m] required distance buffer with the obstacles. <code>min_adjusted_velocity</code> float [m/s] minimum adjusted velocity this node can set. <code>max_deceleration</code> float [m/s\u00b2] maximum deceleration an adjusted velocity can cause. <code>trajectory_preprocessing.start_distance</code> float [m] controls from which part of the trajectory (relative to the current ego pose) the velocity is adjusted. <code>trajectory_preprocessing.max_length</code> float [m] controls the maximum length (starting from the <code>start_distance</code>) where the velocity is adjusted. <code>trajectory_preprocessing.max_distance</code> float [s] controls the maximum duration (measured from the <code>start_distance</code>) where the velocity is adjusted. <code>trajectory_preprocessing.downsample_factor</code> int trajectory downsampling factor to allow tradeoff between precision and performance. <code>trajectory_preprocessing.calculate_steering_angle</code> bool if true, the steering angles of the trajectory message are not used but are recalculated. <code>simulation.model</code> string model to use for forward simulation. Either \"particle\" or \"bicycle\". <code>simulation.distance_method</code> string method to use for calculating distance to collision. Either \"exact\" or \"approximation\". <code>simulation.steering_offset</code> float offset around the steering used by the bicycle model. <code>simulation.nb_points</code> int number of points used to simulate motion with the bicycle model. <code>obstacles.dynamic_source</code> string source of dynamic obstacle used for collision checking. Can be \"occupancy_grid\", \"point_cloud\", or \"static_only\" (no dynamic obstacle). <code>obstacles.occupancy_grid_threshold</code> int value in the occupancy grid above which a cell is considered an obstacle. <code>obstacles.dynamic_obstacles_buffer</code> float buffer around dynamic obstacles used when masking an obstacle in order to prevent noise. <code>obstacles.dynamic_obstacles_min_vel</code> float velocity above which to mask a dynamic obstacle. <code>obstacles.static_map_tags</code> string list linestring of the lanelet map with this tags are used as obstacles. <code>obstacles.filter_envelope</code> bool wether to use the safety envelope to filter the dynamic obstacles source."},{"location":"planning/obstacle_velocity_limiter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The velocity profile produced by this node is not meant to be a realistic velocity profile and can contain sudden jumps of velocity with no regard for acceleration and jerk. This velocity profile is meant to be used as an upper bound on the actual velocity of the vehicle.</p>"},{"location":"planning/obstacle_velocity_limiter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":"<p>The critical case for this node is when an obstacle is falsely detected very close to the trajectory such that the corresponding velocity suddenly becomes very low. This can cause a sudden brake and two mechanisms can be used to mitigate these errors.</p> <p>Parameter <code>min_adjusted_velocity</code> allow to set a minimum to the adjusted velocity, preventing the node to slow down the vehicle too much. Parameter <code>max_deceleration</code> allow to set a maximum deceleration (relative to the current ego velocity) that the adjusted velocity would incur.</p>"},{"location":"planning/obstacle_velocity_limiter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"planning/obstacle_velocity_limiter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"planning/obstacle_velocity_limiter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"planning/path_smoother/","title":"Path Smoothing","text":""},{"location":"planning/path_smoother/#path-smoothing","title":"Path Smoothing","text":""},{"location":"planning/path_smoother/#purpose","title":"Purpose","text":"<p>This package contains code to smooth a path or trajectory.</p>"},{"location":"planning/path_smoother/#features","title":"Features","text":""},{"location":"planning/path_smoother/#elastic-band","title":"Elastic Band","text":"<p>More details about the elastic band can be found here.</p>"},{"location":"planning/path_smoother/docs/eb/","title":"Elastic band","text":""},{"location":"planning/path_smoother/docs/eb/#elastic-band","title":"Elastic band","text":""},{"location":"planning/path_smoother/docs/eb/#abstract","title":"Abstract","text":"<p>Elastic band smooths the input path. Since the latter optimization (model predictive trajectory) is calculated on the frenet frame, path smoothing is applied here so that the latter optimization will be stable.</p> <p>Note that this smoothing process does not consider collision checking. Therefore the output path may have a collision with road boundaries or obstacles.</p>"},{"location":"planning/path_smoother/docs/eb/#flowchart","title":"Flowchart","text":""},{"location":"planning/path_smoother/docs/eb/#general-parameters","title":"General parameters","text":"Parameter Type Description <code>eb.common.num_points</code> int points for elastic band optimization <code>eb.common.delta_arc_length</code> double delta arc length for elastic band optimization"},{"location":"planning/path_smoother/docs/eb/#parameters-for-optimization","title":"Parameters for optimization","text":"Parameter Type Description <code>eb.option.enable_warm_start</code> bool flag to use warm start <code>eb.weight.smooth_weight</code> double weight for smoothing <code>eb.weight.lat_error_weight</code> double weight for minimizing the lateral error"},{"location":"planning/path_smoother/docs/eb/#parameters-for-validation","title":"Parameters for validation","text":"Parameter Type Description <code>eb.option.enable_optimization_validation</code> bool flag to validate optimization <code>eb.validation.max_error</code> double max lateral error by optimization"},{"location":"planning/path_smoother/docs/eb/#formulation","title":"Formulation","text":""},{"location":"planning/path_smoother/docs/eb/#objective-function","title":"Objective function","text":"<p>We formulate a quadratic problem minimizing the diagonal length of the rhombus on each point generated by the current point and its previous and next points, shown as the red vector's length.</p> <p></p> <p>Assuming that \\(k\\)'th point is \\(\\boldsymbol{p}_k = (x_k, y_k)\\), the objective function is as follows.</p> \\[ \\begin{align} \\ J &amp; = \\min \\sum_{k=1}^{n-2} ||(\\boldsymbol{p}_{k+1} - \\boldsymbol{p}_{k}) - (\\boldsymbol{p}_{k} - \\boldsymbol{p}_{k-1})||^2 \\\\ \\ &amp; = \\min \\sum_{k=1}^{n-2} ||\\boldsymbol{p}_{k+1} - 2 \\boldsymbol{p}_{k} + \\boldsymbol{p}_{k-1}||^2 \\\\ \\ &amp; = \\min \\sum_{k=1}^{n-2} \\{(x_{k+1} - x_k + x_{k-1})^2 + (y_{k+1} - y_k + y_{k-1})^2\\} \\\\ \\ &amp; = \\min     \\begin{pmatrix}         \\ x_0 \\\\         \\ x_1 \\\\         \\ x_2 \\\\         \\vdots \\\\         \\ x_{n-3}\\\\         \\ x_{n-2} \\\\         \\ x_{n-1} \\\\         \\ y_0 \\\\         \\ y_1 \\\\         \\ y_2 \\\\         \\vdots \\\\         \\ y_{n-3}\\\\         \\ y_{n-2} \\\\         \\ y_{n-1} \\\\     \\end{pmatrix}^T     \\begin{pmatrix}       1 &amp; -2 &amp; 1 &amp; 0 &amp; \\dots&amp; \\\\       -2 &amp; 5 &amp; -4 &amp; 1 &amp; 0 &amp;\\dots   \\\\       1 &amp; -4 &amp; 6 &amp; -4 &amp; 1 &amp; \\\\       0 &amp; 1 &amp; -4 &amp; 6 &amp; -4 &amp;   \\\\       \\vdots &amp; 0 &amp; \\ddots&amp;\\ddots&amp; \\ddots   \\\\       &amp; \\vdots &amp; &amp; &amp; \\\\       &amp; &amp; &amp; 1 &amp; -4 &amp; 6 &amp; -4 &amp; 1 \\\\       &amp; &amp; &amp; &amp; 1 &amp; -4 &amp; 5 &amp; -2 \\\\       &amp; &amp; &amp; &amp; &amp; 1 &amp; -2 &amp;  1&amp; \\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;1 &amp; -2 &amp; 1 &amp; 0 &amp; \\dots&amp; \\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;-2 &amp; 5 &amp; -4 &amp; 1 &amp; 0 &amp;\\dots   \\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;1 &amp; -4 &amp; 6 &amp; -4 &amp; 1 &amp; \\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;0 &amp; 1 &amp; -4 &amp; 6 &amp; -4 &amp;   \\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;\\vdots &amp; 0 &amp; \\ddots&amp;\\ddots&amp; \\ddots   \\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\vdots &amp; &amp; &amp; \\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 1 &amp; -4 &amp; 6 &amp; -4 &amp; 1 \\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 1 &amp; -4 &amp; 5 &amp; -2 \\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 1 &amp; -2 &amp;  1&amp; \\\\     \\end{pmatrix}     \\begin{pmatrix}         \\ x_0 \\\\         \\ x_1 \\\\         \\ x_2 \\\\         \\vdots \\\\         \\ x_{n-3}\\\\         \\ x_{n-2} \\\\         \\ x_{n-1} \\\\         \\ y_0 \\\\         \\ y_1 \\\\         \\ y_2 \\\\         \\vdots \\\\         \\ y_{n-3}\\\\         \\ y_{n-2} \\\\         \\ y_{n-1} \\\\     \\end{pmatrix} \\end{align} \\]"},{"location":"planning/path_smoother/docs/eb/#constraint","title":"Constraint","text":"<p>The distance that each point can move is limited so that the path will not changed a lot but will be smoother. In detail, the longitudinal distance that each point can move is zero, and the lateral distance is parameterized as <code>eb.clearance.clearance_for_fix</code>, <code>eb.clearance.clearance_for_joint</code> and <code>eb.clearance.clearance_for_smooth</code>.</p> <p>The following figure describes how to constrain the lateral distance to move. The red line is where the point can move. The points for the upper and lower bound are described as \\((x_k^u, y_k^u)\\) and \\((x_k^l, y_k^l)\\), respectively.</p> <p></p> <p>Based on the line equation whose slope angle is \\(\\theta_k\\) and that passes through \\((x_k, y_k)\\), \\((x_k^u, y_k^u)\\) and \\((x_k^l, y_k^l)\\), the lateral constraint is formulated as follows.</p> \\[ C_k^l \\leq C_k \\leq C_k^u \\] <p>In addition, the beginning point is fixed and the end point as well if the end point is considered as the goal. This constraint can be applied with the upper equation by changing the distance that each point can move.</p>"},{"location":"planning/path_smoother/docs/eb/#debug","title":"Debug","text":"<ul> <li>EB Fixed Trajectory<ul> <li>The fixed trajectory points as a constraint of elastic band.</li> </ul> </li> </ul> <ul> <li>EB Trajectory<ul> <li>The optimized trajectory points by elastic band.</li> </ul> </li> </ul>"},{"location":"planning/planning_debug_tools/","title":"Planning Debug Tools","text":""},{"location":"planning/planning_debug_tools/#planning-debug-tools","title":"Planning Debug Tools","text":"<p>This package contains several planning-related debug tools.</p> <ul> <li>Trajectory analyzer: visualizes the information (speed, curvature, yaw, etc) along the trajectory</li> <li>Closest velocity checker: prints the velocity information indicated by each modules</li> <li>Perception reproducer: generates detected objects from rosbag data in planning simulator environment</li> <li>processing time checker: displays processing_time of modules on the terminal</li> <li>logging level updater: updates the logging level of the planning modules.</li> </ul>"},{"location":"planning/planning_debug_tools/#trajectory-analyzer","title":"Trajectory analyzer","text":"<p>The <code>trajectory_analyzer</code> visualizes the information (speed, curvature, yaw, etc) along the trajectory. This feature would be helpful for purposes such as \"investigating the reason why the vehicle decelerates here\". This feature employs the OSS PlotJuggler.</p> <p></p>"},{"location":"planning/planning_debug_tools/#stop-reason-visualizer","title":"Stop reason visualizer","text":"<p>This is to visualize stop factor and reason. see the details</p>"},{"location":"planning/planning_debug_tools/#how-to-use","title":"How to use","text":"<p>please launch the analyzer node</p> <pre><code>ros2 launch planning_debug_tools trajectory_analyzer.launch.xml\n</code></pre> <p>and visualize the analyzed data on the plot juggler following below.</p>"},{"location":"planning/planning_debug_tools/#setup-plotjuggler","title":"setup PlotJuggler","text":"<p>For the first time, please add the following code to reactive script and save it as the picture below! (Looking for the way to automatically load the configuration file...)</p> <p>You can customize what you plot by editing this code.</p> <p></p> <p>in Global code</p> <pre><code>behavior_path = '/planning/scenario_planning/lane_driving/behavior_planning/path_with_lane_id/debug_info'\nbehavior_velocity = '/planning/scenario_planning/lane_driving/behavior_planning/path/debug_info'\nmotion_avoid = '/planning/scenario_planning/lane_driving/motion_planning/obstacle_avoidance_planner/trajectory/debug_info'\nmotion_smoother_latacc = '/planning/scenario_planning/motion_velocity_smoother/debug/trajectory_lateral_acc_filtered/debug_info'\nmotion_smoother = '/planning/scenario_planning/trajectory/debug_info'\n</code></pre> <p>in function(tracker_time)</p> <pre><code>PlotCurvatureOverArclength('k_behavior_path', behavior_path, tracker_time)\nPlotCurvatureOverArclength('k_behavior_velocity', behavior_velocity, tracker_time)\nPlotCurvatureOverArclength('k_motion_avoid', motion_avoid, tracker_time)\nPlotCurvatureOverArclength('k_motion_smoother', motion_smoother, tracker_time)\n\nPlotVelocityOverArclength('v_behavior_path', behavior_path, tracker_time)\nPlotVelocityOverArclength('v_behavior_velocity', behavior_velocity, tracker_time)\nPlotVelocityOverArclength('v_motion_avoid', motion_avoid, tracker_time)\nPlotVelocityOverArclength('v_motion_smoother_latacc', motion_smoother_latacc, tracker_time)\nPlotVelocityOverArclength('v_motion_smoother', motion_smoother, tracker_time)\n\nPlotAccelerationOverArclength('a_behavior_path', behavior_path, tracker_time)\nPlotAccelerationOverArclength('a_behavior_velocity', behavior_velocity, tracker_time)\nPlotAccelerationOverArclength('a_motion_avoid', motion_avoid, tracker_time)\nPlotAccelerationOverArclength('a_motion_smoother_latacc', motion_smoother_latacc, tracker_time)\nPlotAccelerationOverArclength('a_motion_smoother', motion_smoother, tracker_time)\n\nPlotYawOverArclength('yaw_behavior_path', behavior_path, tracker_time)\nPlotYawOverArclength('yaw_behavior_velocity', behavior_velocity, tracker_time)\nPlotYawOverArclength('yaw_motion_avoid', motion_avoid, tracker_time)\nPlotYawOverArclength('yaw_motion_smoother_latacc', motion_smoother_latacc, tracker_time)\nPlotYawOverArclength('yaw_motion_smoother', motion_smoother, tracker_time)\n\nPlotCurrentVelocity('localization_kinematic_state', '/localization/kinematic_state', tracker_time)\n</code></pre> <p>in Function Library </p> <pre><code>function PlotValue(name, path, timestamp, value)\n  new_series = ScatterXY.new(name)\n  index = 0\n  while(true) do\n    series_k = TimeseriesView.find( string.format( \"%s/\"..value..\".%d\", path, index) )\n    series_s = TimeseriesView.find( string.format( \"%s/arclength.%d\", path, index) )\n    series_size = TimeseriesView.find( string.format( \"%s/size\", path) )\n\n    if series_k == nil or series_s == nil then break end\n\n    k = series_k:atTime(timestamp)\n    s = series_s:atTime(timestamp)\n    size = series_size:atTime(timestamp)\n\n    if index &gt;= size then break end\n\n    new_series:push_back(s,k)\n    index = index+1\n  end\nend\n\nfunction PlotCurvatureOverArclength(name, path, timestamp)\n  PlotValue(name, path, timestamp,\"curvature\")\nend\n\nfunction PlotVelocityOverArclength(name, path, timestamp)\n  PlotValue(name, path, timestamp,\"velocity\")\nend\n\nfunction PlotAccelerationOverArclength(name, path, timestamp)\n  PlotValue(name, path, timestamp,\"acceleration\")\nend\n\nfunction PlotYawOverArclength(name, path, timestamp)\n  PlotValue(name, path, timestamp,\"yaw\")\nend\n\nfunction PlotCurrentVelocity(name, kinematics_name, timestamp)\n  new_series = ScatterXY.new(name)\n  series_v = TimeseriesView.find( string.format( \"%s/twist/twist/linear/x\", kinematics_name))\n  if series_v == nil then\n    print(\"error\")\n    return\n  end\n  v = series_v:atTime(timestamp)\n  new_series:push_back(0.0, v)\nend\n</code></pre> <p>Then, run the plot juggler.</p>"},{"location":"planning/planning_debug_tools/#how-to-customize-the-plot","title":"How to customize the plot","text":"<p>Add Path/PathWithLaneIds/Trajectory topics you want to plot in the <code>trajectory_analyzer.launch.xml</code>, then the analyzed topics for these messages will be published with <code>TrajectoryDebugINfo.msg</code> type. You can then visualize these data by editing the reactive script on the PlotJuggler.</p>"},{"location":"planning/planning_debug_tools/#requirements","title":"Requirements","text":"<p>The version of the plotJuggler must be &gt; <code>3.5.0</code></p>"},{"location":"planning/planning_debug_tools/#closest-velocity-checker","title":"Closest velocity checker","text":"<p>This node prints the velocity information indicated by planning/control modules on a terminal. For trajectories calculated by planning modules, the target velocity on the trajectory point which is closest to the ego vehicle is printed. For control commands calculated by control modules, the target velocity and acceleration is directly printed. This feature would be helpful for purposes such as \"investigating the reason why the vehicle does not move\".</p> <p>You can launch by</p> <pre><code>ros2 run planning_debug_tools closest_velocity_checker.py\n</code></pre> <p></p>"},{"location":"planning/planning_debug_tools/#trajectory-visualizer","title":"Trajectory visualizer","text":"<p>The old version of the trajectory analyzer. It is written in Python and more flexible, but very slow.</p>"},{"location":"planning/planning_debug_tools/#for-other-use-case-experimental","title":"For other use case (experimental)","text":"<p>To see behavior velocity planner's internal plath with lane id add below example value to behavior velocity analyzer and set <code>is_publish_debug_path: true</code></p> <pre><code>crosswalk ='/planning/scenario_planning/lane_driving/behavior_planning/behavior_velocity_planner/debug/path_with_lane_id/crosswalk/debug_info'\nintersection ='/planning/scenario_planning/lane_driving/behavior_planning/behavior_velocity_planner/debug/path_with_lane_id/intersection/debug_info'\ntraffic_light ='/planning/scenario_planning/lane_driving/behavior_planning/behavior_velocity_planner/debug/path_with_lane_id/traffic_light/debug_info'\nmerge_from_private ='/planning/scenario_planning/lane_driving/behavior_planning/behavior_velocity_planner/debug/path_with_lane_id/merge_from_private/debug_info'\nocclusion_spot ='/planning/scenario_planning/lane_driving/behavior_planning/behavior_velocity_planner/debug/path_with_lane_id/occlusion_spot/debug_info'\n</code></pre> <pre><code>PlotVelocityOverArclength('v_crosswalk', crosswalk, tracker_time)\nPlotVelocityOverArclength('v_intersection', intersection, tracker_time)\nPlotVelocityOverArclength('v_merge_from_private', merge_from_private, tracker_time)\nPlotVelocityOverArclength('v_traffic_light', traffic_light, tracker_time)\nPlotVelocityOverArclength('v_occlusion', occlusion_spot, tracker_time)\n\nPlotYawOverArclength('yaw_crosswalk', crosswalk, tracker_time)\nPlotYawOverArclength('yaw_intersection', intersection, tracker_time)\nPlotYawOverArclength('yaw_merge_from_private', merge_from_private, tracker_time)\nPlotYawOverArclength('yaw_traffic_light', traffic_light, tracker_time)\nPlotYawOverArclength('yaw_occlusion', occlusion_spot, tracker_time)\n\nPlotCurrentVelocity('localization_kinematic_state', '/localization/kinematic_state', tracker_time)\n</code></pre>"},{"location":"planning/planning_debug_tools/#perception-reproducer","title":"Perception reproducer","text":"<p>This script can overlay the perception results from the rosbag on the planning simulator synchronized with the simulator's ego pose.</p> <p>In detail, the ego pose in the rosbag which is closest to the current ego pose in the simulator is calculated. The perception results at the timestamp of the closest ego pose is extracted, and published.</p>"},{"location":"planning/planning_debug_tools/#how-to-use_1","title":"How to use","text":"<p>First, launch the planning simulator, and put the ego pose. Then, run the script according to the following command.</p> <p>By designating a rosbag, perception reproducer can be launched.</p> <pre><code>ros2 run planning_debug_tools perception_reproducer.py -b &lt;bag-file&gt;\n</code></pre> <p>You can designate multiple rosbags in the directory.</p> <pre><code>ros2 run planning_debug_tools perception_reproducer.py -b &lt;dir-to-bag-files&gt;\n</code></pre> <p>Instead of publishing predicted objects, you can publish detected/tracked objects by designating <code>-d</code> or <code>-t</code>, respectively.</p>"},{"location":"planning/planning_debug_tools/#perception-replayer","title":"Perception replayer","text":"<p>A part of the feature is under development.</p> <p>This script can overlay the perception results from the rosbag on the planning simulator.</p> <p>In detail, this script publishes the data at a certain timestamp from the rosbag. The timestamp will increase according to the real time without any operation. By using the GUI, you can modify the timestamp by pausing, changing the rate or going back into the past.</p>"},{"location":"planning/planning_debug_tools/#how-to-use_2","title":"How to use","text":"<p>First, launch the planning simulator, and put the ego pose. Then, run the script according to the following command.</p> <p>By designating a rosbag, perception replayer can be launched. The GUI is launched as well with which a timestamp of rosbag can be managed.</p> <pre><code>ros2 run planning_debug_tools perception_replayer.py -b &lt;bag-file&gt;\n</code></pre> <p>You can designate multiple rosbags in the directory.</p> <pre><code>ros2 run planning_debug_tools perception_replayer.py -b &lt;dir-to-bag-files&gt;\n</code></pre> <p>Instead of publishing predicted objects, you can publish detected/tracked objects by designating <code>-d</code> or <code>-t</code>, respectively.</p>"},{"location":"planning/planning_debug_tools/#processing-time-checker","title":"Processing time checker","text":"<p>The purpose of the Processing Time Subscriber is to monitor and visualize the processing times of various ROS 2 topics in a system. By providing a real-time terminal-based visualization, users can easily confirm the processing time performance as in the picture below.</p> <p></p> <p>You can run the program by the following command.</p> <pre><code>ros2 run planning_debug_tools processing_time_checker.py -f &lt;update-hz&gt; -m &lt;max-bar-time&gt;\n</code></pre> <p>This program subscribes to ROS 2 topics that have a suffix of <code>processing_time_ms</code>.</p> <p>The program allows users to customize two parameters via command-line arguments:</p> <ul> <li>--max_display_time (or -m): This sets the maximum display time in milliseconds. The default value is 150ms.</li> <li>--display_frequency (or -f): This sets the frequency at which the terminal UI updates. The default value is 5Hz.</li> </ul> <p>By adjusting these parameters, users can tailor the display to their specific monitoring needs.</p>"},{"location":"planning/planning_debug_tools/#logging-level-updater","title":"Logging Level Updater","text":"<p>The purpose of the Logging Level Updater is to update the logging level of the planning modules via ROS 2 service. Users can easily update the logging level for debugging.</p> <pre><code>ros2 run planning_debug_tools update_logger_level.sh &lt;module-name&gt; &lt;logger-level&gt;\n</code></pre> <p><code>&lt;logger-level&gt;</code> will be <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code>, or <code>ERROR</code>.</p> <p></p> <p>When you have a typo of the planning module, the script will show the available modules.</p> <p></p>"},{"location":"planning/planning_debug_tools/doc-stop-reason-visualizer/","title":"Doc stop reason visualizer","text":""},{"location":"planning/planning_debug_tools/doc-stop-reason-visualizer/#stop_reason_visualizer","title":"stop_reason_visualizer","text":"<p>This module is to visualize stop factor quickly without selecting correct debug markers. This is supposed to use with virtual wall marker like below. </p>"},{"location":"planning/planning_debug_tools/doc-stop-reason-visualizer/#how-to-use","title":"How to use","text":"<p>Run this node.</p> <pre><code>ros2 run planning_debug_tools stop_reason_visualizer_exe\n</code></pre> <p>Add stop reason debug marker from rviz.</p> <p></p> <p>Note: ros2 process can be sometimes deleted only from <code>killall stop_reason_visualizer_exe</code></p> <p>Reference</p>"},{"location":"planning/planning_test_utils/","title":"Planning Interface Test Manager","text":""},{"location":"planning/planning_test_utils/#planning-interface-test-manager","title":"Planning Interface Test Manager","text":""},{"location":"planning/planning_test_utils/#background","title":"Background","text":"<p>In each node of the planning module, when exceptional input, such as unusual routes or significantly deviated ego-position, is given, the node may not be prepared for such input and could crash. As a result, debugging node crashes can be time-consuming. For example, if an empty trajectory is given as input and it was not anticipated during implementation, the node might crash due to the unaddressed exceptional input when changes are merged, during scenario testing or while the system is running on an actual vehicle.</p>"},{"location":"planning/planning_test_utils/#purpose","title":"Purpose","text":"<p>The purpose is to provide a utility for implementing tests to ensure that node operates correctly when receiving exceptional input. By utilizing this utility and implementing tests for exceptional input, the purpose is to reduce bugs that are only discovered when actually running the system, by requiring measures for exceptional input before merging PRs.</p>"},{"location":"planning/planning_test_utils/#features","title":"Features","text":""},{"location":"planning/planning_test_utils/#confirmation-of-normal-operation","title":"Confirmation of normal operation","text":"<p>For the test target node, confirm that the node operates correctly and publishes the required messages for subsequent nodes. To do this, test_node publish the necessary messages and confirm that the node's output is being published.</p>"},{"location":"planning/planning_test_utils/#robustness-confirmation-for-special-inputs","title":"Robustness confirmation for special inputs","text":"<p>After confirming normal operation, ensure that the test target node does not crash when given exceptional input. To do this, provide exceptional input from the test_node and confirm that the node does not crash.</p> <p>(WIP)</p>"},{"location":"planning/planning_test_utils/#usage","title":"Usage","text":"<pre><code>TEST(PlanningModuleInterfaceTest, NodeTestWithExceptionTrajectory)\n{\nrclcpp::init(0, nullptr);\n\n// instantiate test_manager with PlanningInterfaceTestManager type\nauto test_manager = std::make_shared&lt;planning_test_utils::PlanningInterfaceTestManager&gt;();\n\n// get package directories for necessary configuration files\nconst auto planning_test_utils_dir =\nament_index_cpp::get_package_share_directory(\"planning_test_utils\");\nconst auto target_node_dir =\nament_index_cpp::get_package_share_directory(\"target_node\");\n\n// set arguments to get the config file\nnode_options.arguments(\n{\"--ros-args\", \"--params-file\",\nplanning_test_utils_dir + \"/config/test_vehicle_info.param.yaml\", \"--params-file\",\nplanning_validator_dir + \"/config/planning_validator.param.yaml\"});\n\n// instantiate the TargetNode with node_options\nauto test_target_node = std::make_shared&lt;TargetNode&gt;(node_options);\n\n// publish the necessary topics from test_manager second argument is topic name\ntest_manager-&gt;publishOdometry(test_target_node, \"/localization/kinematic_state\");\ntest_manager-&gt;publishMaxVelocity(\ntest_target_node, \"motion_velocity_smoother/input/external_velocity_limit_mps\");\n\n// set scenario_selector's input topic name(this topic is changed to test node)\ntest_manager-&gt;setTrajectoryInputTopicName(\"input/parking/trajectory\");\n\n// test with normal trajectory\nASSERT_NO_THROW(test_manager-&gt;testWithNominalTrajectory(test_target_node));\n\n// make sure target_node is running\nEXPECT_GE(test_manager-&gt;getReceivedTopicNum(), 1);\n\n// test with trajectory input with empty/one point/overlapping point\nASSERT_NO_THROW(test_manager-&gt;testWithAbnormalTrajectory(test_target_node));\n\n// shutdown ROS context\nrclcpp::shutdown();\n}\n</code></pre>"},{"location":"planning/planning_test_utils/#implemented-tests","title":"Implemented tests","text":"Node Test name exceptional input output Exceptional input pattern planning_validator NodeTestWithExceptionTrajectory trajectory trajectory Empty, single point, path with duplicate points motion_velocity_smoother NodeTestWithExceptionTrajectory trajectory trajectory Empty, single point, path with duplicate points obstacle_cruise_planner NodeTestWithExceptionTrajectory trajectory trajectory Empty, single point, path with duplicate points obstacle_stop_planner NodeTestWithExceptionTrajectory trajectory trajectory Empty, single point, path with duplicate points obstacle_velocity_limiter NodeTestWithExceptionTrajectory trajectory trajectory Empty, single point, path with duplicate points obstacle_avoidance_planner NodeTestWithExceptionTrajectory trajectory trajectory Empty, single point, path with duplicate points scenario_selector NodeTestWithExceptionTrajectoryLaneDrivingMode NodeTestWithExceptionTrajectoryParkingMode trajectory scenario Empty, single point, path with duplicate points for scenarios:LANEDRIVING and PARKING freespace_planner NodeTestWithExceptionRoute route trajectory Empty route behavior_path_planner NodeTestWithExceptionRoute NodeTestWithOffTrackEgoPose route route odometry Empty route Off-lane ego-position behavior_velocity_planner NodeTestWithExceptionPathWithLaneID path_with_lane_id path Empty path"},{"location":"planning/planning_test_utils/#important-notes","title":"Important Notes","text":"<p>During test execution, when launching a node, parameters are loaded from the parameter file within each package. Therefore, when adding parameters, it is necessary to add the required parameters to the parameter file in the target node package. This is to prevent the node from being unable to launch if there are missing parameters when retrieving them from the parameter file during node launch.</p>"},{"location":"planning/planning_test_utils/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":"<p>(WIP)</p>"},{"location":"planning/planning_topic_converter/","title":"Planning Topic Converter","text":""},{"location":"planning/planning_topic_converter/#planning-topic-converter","title":"Planning Topic Converter","text":""},{"location":"planning/planning_topic_converter/#purpose","title":"Purpose","text":"<p>This package provides tools that convert topic type among types are defined in https://github.com/tier4/autoware_auto_msgs.</p>"},{"location":"planning/planning_topic_converter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/planning_topic_converter/#usage-example","title":"Usage example","text":"<p>The tools in this package are provided as composable ROS 2 component nodes, so that they can be spawned into an existing process, launched from launch files, or invoked from the command line.</p> <pre><code>&lt;load_composable_node target=\"container_name\"&gt;\n&lt;composable_node pkg=\"planning_topic_converter\" plugin=\"planning_topic_converter::PathToTrajectory\" name=\"path_to_trajectory_converter\" namespace=\"\"&gt;\n&lt;!-- params --&gt;\n&lt;param name=\"input_topic\" value=\"foo\"/&gt;\n&lt;param name=\"output_topic\" value=\"bar\"/&gt;\n&lt;!-- composable node config --&gt;\n&lt;extra_arg name=\"use_intra_process_comms\" value=\"false\"/&gt;\n&lt;/composable_node&gt;\n&lt;/load_composable_node&gt;\n</code></pre>"},{"location":"planning/planning_topic_converter/#parameters","title":"Parameters","text":"Name Type Description <code>input_topic</code> string input topic name. <code>output_topic</code> string output topic name."},{"location":"planning/planning_topic_converter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"planning/planning_topic_converter/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"planning/planning_validator/","title":"Planning Validator","text":""},{"location":"planning/planning_validator/#planning-validator","title":"Planning Validator","text":"<p>The <code>planning_validator</code> is a module that checks the validity of a trajectory before it is published. The status of the validation can be viewed in the <code>/diagnostics</code> and <code>/validation_status</code> topics. When an invalid trajectory is detected, the <code>planning_validator</code> will process the trajectory following the selected option: \"0. publish the trajectory as it is\", \"1. stop publishing the trajectory\", \"2. publish the last validated trajectory\".</p> <p></p>"},{"location":"planning/planning_validator/#supported-features","title":"Supported features","text":"<p>The following features are supported for trajectory validation and can have thresholds set by parameters:</p> <ul> <li>Invalid field : e.g. Inf, Nan</li> <li>Trajectory points interval : invalid if any of the distance of trajectory points is too large</li> <li>Curvature : invalid if the trajectory has too sharp turns that is not feasible for the given vehicle kinematics</li> <li>Relative angle : invalid if the yaw angle changes too fast in the sequence of trajectory points</li> <li>Lateral acceleration : invalid if the expected lateral acceleration/deceleration is too large</li> <li>Longitudinal acceleration/deceleration : invalid if the acceleration/deceleration in the trajectory point is too large</li> <li>Steering angle : invalid if the expected steering value is too large estimated from trajectory curvature</li> <li>Steering angle rate : invalid if the expected steering rate value is too large</li> <li>Velocity deviation : invalid if the planning velocity is too far from the ego velocity</li> <li>Distance deviation : invalid if the ego is too far from the trajectory</li> </ul> <p>The following features are to be implemented.</p> <ul> <li>(TODO) TTC calculation : invalid if the expected time-to-collision is too short on the trajectory</li> </ul>"},{"location":"planning/planning_validator/#inputsoutputs","title":"Inputs/Outputs","text":""},{"location":"planning/planning_validator/#inputs","title":"Inputs","text":"<p>The <code>planning_validator</code> takes in the following inputs:</p> Name Type Description <code>~/input/kinematics</code> nav_msgs/Odometry ego pose and twist <code>~/input/trajectory</code> autoware_auto_planning_msgs/Trajectory target trajectory to be validated in this node"},{"location":"planning/planning_validator/#outputs","title":"Outputs","text":"<p>It outputs the following:</p> Name Type Description <code>~/output/trajectory</code> autoware_auto_planning_msgs/Trajectory validated trajectory <code>~/output/validation_status</code> planning_validator/PlanningValidatorStatus validator status to inform the reason why the trajectory is valid/invalid <code>/diagnostics</code> diagnostic_msgs/DiagnosticStatus diagnostics to report errors"},{"location":"planning/planning_validator/#parameters","title":"Parameters","text":"<p>The following parameters can be set for the <code>planning_validator</code>:</p>"},{"location":"planning/planning_validator/#system-parameters","title":"System parameters","text":"Name Type Description Default value <code>invalid_trajectory_handling_type</code> int set the operation when the invalid trajectory is detected. 0: publish the trajectory even if it is invalid, 1: stop publishing the trajectory, 2: publish the last validated trajectory. 0 <code>publish_diag</code> bool the Diag will be set to ERROR when the number of consecutive invalid trajectory exceeds this threshold. (For example, threshold = 1 means, even if the trajectory is invalid, the Diag will not be ERROR if the next trajectory is valid.) true <code>diag_error_count_threshold</code> int if true, diagnostics msg is published. true <code>display_on_terminal</code> bool show error msg on terminal true"},{"location":"planning/planning_validator/#algorithm-parameters","title":"Algorithm parameters","text":""},{"location":"planning/planning_validator/#thresholds","title":"Thresholds","text":"<p>The input trajectory is detected as invalid if the index exceeds the following thresholds.</p> Name Type Description Default value <code>thresholds.interval</code> double invalid threshold of the distance of two neighboring trajectory points [m] 100.0 <code>thresholds.relative_angle</code> double invalid threshold of the relative angle of two neighboring trajectory points [rad] 2.0 <code>thresholds.curvature</code> double invalid threshold of the curvature in each trajectory point [1/m] 1.0 <code>thresholds.lateral_acc</code> double invalid threshold of the lateral acceleration in each trajectory point [m/ss] 9.8 <code>thresholds.longitudinal_max_acc</code> double invalid threshold of the maximum longitudinal acceleration in each trajectory point [m/ss] 9.8 <code>thresholds.longitudinal_min_acc</code> double invalid threshold of the minimum longitudinal deceleration in each trajectory point [m/ss] -9.8 <code>thresholds.steering</code> double invalid threshold of the steering angle in each trajectory point [rad] 1.414 <code>thresholds.steering_rate</code> double invalid threshold of the steering angle rate in each trajectory point [rad/s] 10.0 <code>thresholds.velocity_deviation</code> double invalid threshold of the velocity deviation between the ego velocity and the trajectory point closest to ego [m/s] 100.0 <code>thresholds.distance_deviation</code> double invalid threshold of the distance deviation between the ego position and the trajectory point closest to ego [m] 100.0"},{"location":"planning/route_handler/","title":"route handler","text":""},{"location":"planning/route_handler/#route-handler","title":"route handler","text":"<p><code>route_handler</code> is a library for calculating driving route on the lanelet map.</p>"},{"location":"planning/rtc_interface/","title":"RTC Interface","text":""},{"location":"planning/rtc_interface/#rtc-interface","title":"RTC Interface","text":""},{"location":"planning/rtc_interface/#purpose","title":"Purpose","text":"<p>RTC Interface is an interface to publish the decision status of behavior planning modules and receive execution command from external of an autonomous driving system.</p>"},{"location":"planning/rtc_interface/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/rtc_interface/#usage-example","title":"Usage example","text":"<pre><code>// Generate instance (in this example, \"intersection\" is selected)\nrtc_interface::RTCInterface rtc_interface(node, \"intersection\");\n\n// Generate UUID\nconst unique_identifier_msgs::msg::UUID uuid = generateUUID(getModuleId());\n\n// Repeat while module is running\nwhile (...) {\n// Get safety status of the module corresponding to the module id\nconst bool safe = ...\n\n// Get distance to the object corresponding to the module id\nconst double start_distance = ...\nconst double finish_distance = ...\n\n// Get time stamp\nconst rclcpp::Time stamp = ...\n\n// Update status\nrtc_interface.updateCooperateStatus(uuid, safe, start_distance, finish_distance, stamp);\n\nif (rtc_interface.isActivated(uuid)) {\n// Execute planning\n} else {\n// Stop planning\n}\n// Get time stamp\nconst rclcpp::Time stamp = ...\n\n// Publish status topic\nrtc_interface.publishCooperateStatus(stamp);\n}\n\n// Remove the status from array\nrtc_interface.removeCooperateStatus(uuid);\n</code></pre>"},{"location":"planning/rtc_interface/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/rtc_interface/#rtcinterface-constructor","title":"RTCInterface (Constructor)","text":"<pre><code>rtc_interface::RTCInterface(rclcpp::Node &amp; node, const std::string &amp; name);\n</code></pre>"},{"location":"planning/rtc_interface/#description","title":"Description","text":"<p>A constructor for <code>rtc_interface::RTCInterface</code>.</p>"},{"location":"planning/rtc_interface/#input","title":"Input","text":"<ul> <li><code>node</code> : Node calling this interface</li> <li><code>name</code> : Name of cooperate status array topic and cooperate commands service<ul> <li>Cooperate status array topic name : <code>~/{name}/cooperate_status</code></li> <li>Cooperate commands service name : <code>~/{name}/cooperate_commands</code></li> </ul> </li> </ul>"},{"location":"planning/rtc_interface/#output","title":"Output","text":"<p>An instance of <code>RTCInterface</code></p>"},{"location":"planning/rtc_interface/#publishcooperatestatus","title":"publishCooperateStatus","text":"<pre><code>rtc_interface::publishCooperateStatus(const rclcpp::Time &amp; stamp)\n</code></pre>"},{"location":"planning/rtc_interface/#description_1","title":"Description","text":"<p>Publish registered cooperate status.</p>"},{"location":"planning/rtc_interface/#input_1","title":"Input","text":"<ul> <li><code>stamp</code> : Time stamp</li> </ul>"},{"location":"planning/rtc_interface/#output_1","title":"Output","text":"<p>Nothing</p>"},{"location":"planning/rtc_interface/#updatecooperatestatus","title":"updateCooperateStatus","text":"<pre><code>rtc_interface::updateCooperateStatus(const unique_identifier_msgs::msg::UUID &amp; uuid, const bool safe, const double start_distance, const double finish_distance, const rclcpp::Time &amp; stamp)\n</code></pre>"},{"location":"planning/rtc_interface/#description_2","title":"Description","text":"<p>Update cooperate status corresponding to <code>uuid</code>. If cooperate status corresponding to <code>uuid</code> is not registered yet, add new cooperate status.</p>"},{"location":"planning/rtc_interface/#input_2","title":"Input","text":"<ul> <li><code>uuid</code> : UUID for requesting module</li> <li><code>safe</code> : Safety status of requesting module</li> <li><code>start_distance</code> : Distance to the start object from ego vehicle</li> <li><code>finish_distance</code> : Distance to the finish object from ego vehicle</li> <li><code>stamp</code> : Time stamp</li> </ul>"},{"location":"planning/rtc_interface/#output_2","title":"Output","text":"<p>Nothing</p>"},{"location":"planning/rtc_interface/#removecooperatestatus","title":"removeCooperateStatus","text":"<pre><code>rtc_interface::removeCooperateStatus(const unique_identifier_msgs::msg::UUID &amp; uuid)\n</code></pre>"},{"location":"planning/rtc_interface/#description_3","title":"Description","text":"<p>Remove cooperate status corresponding to <code>uuid</code> from registered statuses.</p>"},{"location":"planning/rtc_interface/#input_3","title":"Input","text":"<ul> <li><code>uuid</code> : UUID for expired module</li> </ul>"},{"location":"planning/rtc_interface/#output_3","title":"Output","text":"<p>Nothing</p>"},{"location":"planning/rtc_interface/#clearcooperatestatus","title":"clearCooperateStatus","text":"<pre><code>rtc_interface::clearCooperateStatus()\n</code></pre>"},{"location":"planning/rtc_interface/#description_4","title":"Description","text":"<p>Remove all cooperate statuses.</p>"},{"location":"planning/rtc_interface/#input_4","title":"Input","text":"<p>Nothing</p>"},{"location":"planning/rtc_interface/#output_4","title":"Output","text":"<p>Nothing</p>"},{"location":"planning/rtc_interface/#isactivated","title":"isActivated","text":"<pre><code>rtc_interface::isActivated(const unique_identifier_msgs::msg::UUID &amp; uuid)\n</code></pre>"},{"location":"planning/rtc_interface/#description_5","title":"Description","text":"<p>Return received command status corresponding to <code>uuid</code>.</p>"},{"location":"planning/rtc_interface/#input_5","title":"Input","text":"<ul> <li><code>uuid</code> : UUID for checking module</li> </ul>"},{"location":"planning/rtc_interface/#output_5","title":"Output","text":"<p>If auto mode is enabled, return based on the safety status. If not, if received command is <code>ACTIVATED</code>, return <code>true</code>. If not, return <code>false</code>.</p>"},{"location":"planning/rtc_interface/#isregistered","title":"isRegistered","text":"<pre><code>rtc_interface::isRegistered(const unique_identifier_msgs::msg::UUID &amp; uuid)\n</code></pre>"},{"location":"planning/rtc_interface/#description_6","title":"Description","text":"<p>Return <code>true</code> if <code>uuid</code> is registered.</p>"},{"location":"planning/rtc_interface/#input_6","title":"Input","text":"<ul> <li><code>uuid</code> : UUID for checking module</li> </ul>"},{"location":"planning/rtc_interface/#output_6","title":"Output","text":"<p>If <code>uuid</code> is registered, return <code>true</code>. If not, return <code>false</code>.</p>"},{"location":"planning/rtc_interface/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"planning/rtc_interface/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"planning/rtc_replayer/","title":"rtc_replayer","text":""},{"location":"planning/rtc_replayer/#rtc_replayer","title":"rtc_replayer","text":""},{"location":"planning/rtc_replayer/#purpose","title":"Purpose","text":"<p>The current issue for RTC commands is that service is not recorded to rosbag, so it's very hard to analyze what was happened exactly. So this package makes it possible to replay rtc commands service from rosbag rtc status topic to resolve that issue.</p>"},{"location":"planning/rtc_replayer/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/rtc_replayer/#input","title":"Input","text":"Name Type Description <code>/debug/rtc_status</code> tier4_rtc_msgs::msg::CooperateStatusArray CooperateStatusArray that is recorded in rosbag"},{"location":"planning/rtc_replayer/#output","title":"Output","text":"Name Type Description <code>/api/external/set/rtc_commands</code> tier4_rtc_msgs::msg::CooperateCommands CooperateCommands that is replayed by this package"},{"location":"planning/rtc_replayer/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/rtc_replayer/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>This package can't replay CooperateCommands correctly if CooperateStatusArray is not stable. And this replay is always later one step than actual however it will not affect much for behavior.</p>"},{"location":"planning/rtc_replayer/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":"<p>tbd.</p>"},{"location":"planning/sampling_based_planner/bezier_sampler/","title":"B\u00e9zier sampler","text":""},{"location":"planning/sampling_based_planner/bezier_sampler/#bezier-sampler","title":"B\u00e9zier sampler","text":"<p>Implementation of b\u00e9zier curves and their generation following the sampling strategy from https://ieeexplore.ieee.org/document/8932495</p>"},{"location":"planning/sampling_based_planner/frenet_planner/","title":"Frenet planner","text":""},{"location":"planning/sampling_based_planner/frenet_planner/#frenet-planner","title":"Frenet planner","text":"<p>Trajectory generation in Frenet frame.</p>"},{"location":"planning/sampling_based_planner/frenet_planner/#description","title":"Description","text":"<p>Original paper</p>"},{"location":"planning/sampling_based_planner/path_sampler/","title":"Path Sampler","text":""},{"location":"planning/sampling_based_planner/path_sampler/#path-sampler","title":"Path Sampler","text":""},{"location":"planning/sampling_based_planner/path_sampler/#purpose","title":"Purpose","text":"<p>This package implements a node that uses sampling based planning to generate a drivable trajectory.</p>"},{"location":"planning/sampling_based_planner/path_sampler/#feature","title":"Feature","text":"<p>This package is able to:</p> <ul> <li>make the trajectory smooth;</li> <li>keep the trajectory inside the drivable area;</li> <li>avoid static obstacles;</li> <li>stop if no valid trajectory can be generated.</li> </ul> <p>Note that the velocity is just taken over from the input path.</p>"},{"location":"planning/sampling_based_planner/path_sampler/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/sampling_based_planner/path_sampler/#input","title":"input","text":"Name Type Description <code>~/input/path</code> autoware_auto_planning_msgs/msg/Path Reference path and the corresponding drivable area <code>~/input/odometry</code> nav_msgs/msg/Odometry Current state of the ego vehicle <code>~/input/objects</code> autoware_auto_perception_msgs/msg/PredictedObjects objects to avoid"},{"location":"planning/sampling_based_planner/path_sampler/#output","title":"output","text":"Name Type Description <code>~/output/trajectory</code> autoware_auto_planning_msgs/msg/Trajectory generated trajectory that is feasible to drive and collision-free"},{"location":"planning/sampling_based_planner/path_sampler/#algorithm","title":"Algorithm","text":"<p>Sampling based planning is decomposed into 3 successive steps:</p> <ol> <li>Sampling: candidate trajectories are generated.</li> <li>Pruning: invalid candidates are discarded.</li> <li>Selection: the best remaining valid candidate is selected.</li> </ol>"},{"location":"planning/sampling_based_planner/path_sampler/#sampling","title":"Sampling","text":"<p>Candidate trajectories are generated based on the current ego state and some target state. 2 sampling algorithms are currently implemented: sampling with b\u00e9zier curves or with polynomials in the frenet frame.</p>"},{"location":"planning/sampling_based_planner/path_sampler/#pruning","title":"Pruning","text":"<p>The validity of each candidate trajectory is checked using a set of hard constraints.</p> <ul> <li>collision: ensure no collision with static obstacles;</li> <li>curvature: ensure smooth curvature;</li> <li>drivable area: ensure the trajectory stays within the drivable area.</li> </ul>"},{"location":"planning/sampling_based_planner/path_sampler/#selection","title":"Selection","text":"<p>Among the valid candidate trajectories, the best one is determined using a set of soft constraints (i.e., objective functions).</p> <ul> <li>curvature: prefer smoother trajectories;</li> <li>length: prefer longer trajectories;</li> <li>lateral deviation: prefer trajectories close to the reference path.</li> </ul> <p>Each soft constraint is associated with a weight to allow tuning of the preferences.</p>"},{"location":"planning/sampling_based_planner/path_sampler/#limitations","title":"Limitations","text":"<p>The quality of the candidates generated with polynomials in frenet frame greatly depend on the reference path. If the reference path is not smooth, the resulting candidates will probably be undriveable.</p> <p>Failure to find a valid trajectory current results in a suddenly stopping trajectory.</p>"},{"location":"planning/sampling_based_planner/path_sampler/#comparison-with-the-obstacle_avoidance_planner","title":"Comparison with the <code>obstacle_avoidance_planner</code>","text":"<p>The <code>obstacle_avoidance_planner</code> uses an optimization based approach, finding the optimal solution of a mathematical problem if it exists. When no solution can be found, it is often hard to identify the issue due to the intermediate mathematical representation of the problem.</p> <p>In comparison, the sampling based approach cannot guarantee an optimal solution but is much more straightforward, making it easier to debug and tune.</p>"},{"location":"planning/sampling_based_planner/path_sampler/#how-to-tune-parameters","title":"How to Tune Parameters","text":"<p>The sampling based planner mostly offers a trade-off between the consistent quality of the trajectory and the computation time. To guarantee that a good trajectory is found requires generating many candidates which linearly increases the computation time.</p> <p>TODO</p>"},{"location":"planning/sampling_based_planner/path_sampler/#drivability-in-narrow-roads","title":"Drivability in narrow roads","text":""},{"location":"planning/sampling_based_planner/path_sampler/#computation-time","title":"Computation time","text":""},{"location":"planning/sampling_based_planner/path_sampler/#robustness","title":"Robustness","text":""},{"location":"planning/sampling_based_planner/path_sampler/#other-options","title":"Other options","text":""},{"location":"planning/sampling_based_planner/path_sampler/#how-to-debug","title":"How To Debug","text":"<p>TODO</p>"},{"location":"planning/sampling_based_planner/sampler_common/","title":"Sampler Common","text":""},{"location":"planning/sampling_based_planner/sampler_common/#sampler-common","title":"Sampler Common","text":"<p>Common functions for sampling based planners. This includes classes for representing paths and trajectories, hard and soft constraints, conversion between cartesian and frenet frames, ...</p>"},{"location":"planning/scenario_selector/","title":"scenario_selector","text":""},{"location":"planning/scenario_selector/#scenario_selector","title":"scenario_selector","text":""},{"location":"planning/scenario_selector/#scenario_selector_node","title":"scenario_selector_node","text":"<p><code>scenario_selector_node</code> is a node that switches trajectories from each scenario.</p>"},{"location":"planning/scenario_selector/#input-topics","title":"Input topics","text":"Name Type Description <code>~input/lane_driving/trajectory</code> autoware_auto_planning_msgs::Trajectory trajectory of LaneDriving scenario <code>~input/parking/trajectory</code> autoware_auto_planning_msgs::Trajectory trajectory of Parking scenario <code>~input/lanelet_map</code> autoware_auto_mapping_msgs::HADMapBin <code>~input/route</code> autoware_planning_msgs::LaneletRoute route and goal pose <code>~input/odometry</code> nav_msgs::Odometry for checking whether vehicle is stopped <code>is_parking_completed</code> bool (implemented as rosparam) whether all split trajectory of Parking are published"},{"location":"planning/scenario_selector/#output-topics","title":"Output topics","text":"Name Type Description <code>~output/scenario</code> tier4_planning_msgs::Scenario current scenario and scenarios to be activated <code>~output/trajectory</code> autoware_auto_planning_msgs::Trajectory trajectory to be followed"},{"location":"planning/scenario_selector/#output-tfs","title":"Output TFs","text":"<p>None</p>"},{"location":"planning/scenario_selector/#how-to-launch","title":"How to launch","text":"<ol> <li>Write your remapping info in <code>scenario_selector.launch</code> or add args when executing <code>roslaunch</code></li> <li><code>roslaunch scenario_selector scenario_selector.launch</code><ul> <li>If you would like to use only a single scenario, <code>roslaunch scenario_selector dummy_scenario_selector_{scenario_name}.launch</code></li> </ul> </li> </ol>"},{"location":"planning/scenario_selector/#parameters","title":"Parameters","text":"Name Type Description Default Range update_rate float timer's update rate 10 \u22650.0 th_max_message_delay_sec float threshold time of input messages' maximum delay 1 \u22650.0 th_arrived_distance_m float threshold distance to check if vehicle has arrived at the trajectory's endpoint 1 \u22650.0 th_stopped_time_sec float threshold time to check if vehicle is stopped 1 \u22650.0 th_stopped_velocity_mps float threshold velocity to check if vehicle is stopped 0.01 \u22650.0"},{"location":"planning/scenario_selector/#flowchart","title":"Flowchart","text":""},{"location":"planning/static_centerline_optimizer/","title":"Static Centerline Optimizer","text":""},{"location":"planning/static_centerline_optimizer/#static-centerline-optimizer","title":"Static Centerline Optimizer","text":""},{"location":"planning/static_centerline_optimizer/#purpose","title":"Purpose","text":"<p>This package statically calculates the centerline satisfying path footprints inside the drivable area.</p> <p>On narrow-road driving, the default centerline, which is the middle line between lanelets' right and left boundaries, often causes path footprints outside the drivable area. To make path footprints inside the drivable area, we use online path shape optimization by the obstacle_avoidance_planner package.</p> <p>Instead of online path shape optimization, we introduce static centerline optimization. With this static centerline optimization, we have following advantages.</p> <ul> <li>We can see the optimized centerline shape in advance.<ul> <li>With the default autoware, path shape is not determined until the vehicle drives there.</li> <li>This enables offline path shape evaluation.</li> </ul> </li> <li>We do not have to calculate a heavy and sometimes unstable path optimization since the path footprints are already inside the drivable area.</li> </ul>"},{"location":"planning/static_centerline_optimizer/#use-cases","title":"Use cases","text":"<p>There are two interfaces to communicate with the centerline optimizer.</p>"},{"location":"planning/static_centerline_optimizer/#vector-map-builder-interface","title":"Vector Map Builder Interface","text":"<p>Note: This function of Vector Map Builder has not been released. Please wait for a while. Currently there is no documentation about Vector Map Builder's operation for this function.</p> <p>The optimized centerline can be generated from Vector Map Builder's operation.</p> <p>We can run</p> <ul> <li>path planning server</li> <li>http server to connect path planning server and Vector Map Builder</li> </ul> <p>with the following command by designating <code>&lt;vehicle_model&gt;</code></p> <pre><code>ros2 launch static_centerline_optimizer run_planning_server.launch.xml vehicle_model:=&lt;vehicle-model&gt;\n</code></pre> <p>FYI, port ID of the http server is 4010 by default.</p>"},{"location":"planning/static_centerline_optimizer/#command-line-interface","title":"Command Line Interface","text":"<p>The optimized centerline can be generated from the command line interface by designating</p> <ul> <li><code>&lt;input-osm-path&gt;</code></li> <li><code>&lt;output-osm-path&gt;</code> (not mandatory)</li> <li><code>&lt;start-lanelet-id&gt;</code></li> <li><code>&lt;end-lanelet-id&gt;</code></li> <li><code>&lt;vehicle-model&gt;</code></li> </ul> <pre><code>ros2 launch static_centerline_optimizer static_centerline_optimizer.launch.xml run_backgrond:=false lanelet2_input_file_path:=&lt;input-osm-path&gt; lanelet2_output_file_path:=&lt;output-osm-path&gt; start_lanelet_id:=&lt;start-lane-id&gt; end_lanelet_id:=&lt;end-lane-id&gt; vehicle_model:=&lt;vehicle-model&gt;\n</code></pre> <p>The default output map path containing the optimized centerline locates <code>/tmp/lanelet2_map.osm</code>. If you want to change the output map path, you can remap the path by designating <code>&lt;output-osm-path&gt;</code>.</p>"},{"location":"planning/static_centerline_optimizer/#visualization","title":"Visualization","text":"<p>When launching the path planning server, rviz is launched as well as follows. </p> <ul> <li>The yellow footprints are the original ones from the osm map file.<ul> <li>FYI: Footprints are generated based on the centerline and vehicle size.</li> </ul> </li> <li>The red footprints are the optimized ones.</li> <li>The gray area is the drivable area.</li> <li>You can see that the red footprints are inside the drivable area although the yellow ones are outside.</li> </ul>"},{"location":"planning/static_centerline_optimizer/#unsafe-footprints","title":"Unsafe footprints","text":"<p>Sometimes the optimized centerline footprints are close to the lanes' boundaries. We can check how close they are with <code>unsafe footprints</code> marker as follows.</p> <p>Footprints' color depends on its distance to the boundaries, and text expresses its distance.</p> <p></p> <p>By default, footprints' color is</p> <ul> <li>when the distance is less than 0.1 [m] : red</li> <li>when the distance is less than 0.2 [m] : green</li> <li>when the distance is less than 0.3 [m] : blue</li> </ul>"},{"location":"planning/surround_obstacle_checker/","title":"Surround Obstacle Checker","text":""},{"location":"planning/surround_obstacle_checker/#surround-obstacle-checker","title":"Surround Obstacle Checker","text":""},{"location":"planning/surround_obstacle_checker/#purpose","title":"Purpose","text":"<p>This module subscribes required data (ego-pose, obstacles, etc), and publishes zero velocity limit to keep stopping if any of stop conditions are satisfied.</p>"},{"location":"planning/surround_obstacle_checker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/surround_obstacle_checker/#flow-chart","title":"Flow chart","text":""},{"location":"planning/surround_obstacle_checker/#algorithms","title":"Algorithms","text":""},{"location":"planning/surround_obstacle_checker/#check-data","title":"Check data","text":"<p>Check that <code>surround_obstacle_checker</code> receives no ground pointcloud, dynamic objects and current velocity data.</p>"},{"location":"planning/surround_obstacle_checker/#get-distance-to-nearest-object","title":"Get distance to nearest object","text":"<p>Calculate distance between ego vehicle and the nearest object. In this function, it calculates the minimum distance between the polygon of ego vehicle and all points in pointclouds and the polygons of dynamic objects.</p>"},{"location":"planning/surround_obstacle_checker/#stop-requirement","title":"Stop requirement","text":"<p>If it satisfies all following conditions, it plans stopping.</p> <ul> <li>Ego vehicle is stopped</li> <li>It satisfies any following conditions<ol> <li>The distance to nearest obstacle satisfies following conditions<ul> <li>If state is <code>State::PASS</code>, the distance is less than <code>surround_check_distance</code></li> <li>If state is <code>State::STOP</code>, the distance is less than <code>surround_check_recover_distance</code></li> </ul> </li> <li>If it does not satisfies the condition in 1, elapsed time from the time it satisfies the condition in 1 is less than <code>state_clear_time</code></li> </ol> </li> </ul>"},{"location":"planning/surround_obstacle_checker/#states","title":"States","text":"<p>To prevent chattering, <code>surround_obstacle_checker</code> manages two states. As mentioned in stop condition section, it prevents chattering by changing threshold to find surround obstacle depending on the states.</p> <ul> <li><code>State::PASS</code> : Stop planning is released</li> <li><code>State::STOP</code> \uff1aWhile stop planning</li> </ul>"},{"location":"planning/surround_obstacle_checker/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/surround_obstacle_checker/#input","title":"Input","text":"Name Type Description <code>/perception/obstacle_segmentation/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Pointcloud of obstacles which the ego-vehicle should stop or avoid <code>/perception/object_recognition/objects</code> <code>autoware_auto_perception_msgs::msg::PredictedObjects</code> Dynamic objects <code>/localization/kinematic_state</code> <code>nav_msgs::msg::Odometry</code> Current twist <code>/tf</code> <code>tf2_msgs::msg::TFMessage</code> TF <code>/tf_static</code> <code>tf2_msgs::msg::TFMessage</code> TF static"},{"location":"planning/surround_obstacle_checker/#output","title":"Output","text":"Name Type Description <code>~/output/velocity_limit_clear_command</code> <code>tier4_planning_msgs::msg::VelocityLimitClearCommand</code> Velocity limit clear command <code>~/output/max_velocity</code> <code>tier4_planning_msgs::msg::VelocityLimit</code> Velocity limit command <code>~/output/no_start_reason</code> <code>diagnostic_msgs::msg::DiagnosticStatus</code> No start reason <code>~/output/stop_reasons</code> <code>tier4_planning_msgs::msg::StopReasonArray</code> Stop reasons <code>~/debug/marker</code> <code>visualization_msgs::msg::MarkerArray</code> Marker for visualization <code>~/debug/footprint</code> <code>geometry_msgs::msg::PolygonStamped</code> Ego vehicle base footprint for visualization <code>~/debug/footprint_offset</code> <code>geometry_msgs::msg::PolygonStamped</code> Ego vehicle footprint with <code>surround_check_distance</code> offset for visualization <code>~/debug/footprint_recover_offset</code> <code>geometry_msgs::msg::PolygonStamped</code> Ego vehicle footprint with <code>surround_check_recover_distance</code> offset for visualization"},{"location":"planning/surround_obstacle_checker/#parameters","title":"Parameters","text":"Name Type Description Default Range pointcloud.enable_check boolean enable to check surrounding pointcloud false N/A pointcloud.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 pointcloud.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 pointcloud.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 unknown.enable_check boolean enable to check surrounding unknown objects true N/A unknown.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 unknown.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 unknown.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 car.enable_check boolean enable to check surrounding car true N/A car.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 car.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 car.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 truck.enable_check boolean enable to check surrounding truck true N/A truck.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 truck.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 truck.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 bus.enable_check boolean enable to check surrounding bus true N/A bus.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 bus.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 bus.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 trailer.enable_check boolean enable to check surrounding trailer true N/A trailer.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 trailer.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 trailer.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 motorcycle.enable_check boolean enable to check surrounding motorcycle true N/A motorcycle.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 motorcycle.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 motorcycle.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 bicycle.enable_check boolean enable to check surrounding bicycle true N/A bicycle.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 bicycle.surround_check_side_distance float f objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 bicycle.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 pedestrian.enable_check boolean enable to check surrounding pedestrian true N/A pedestrian.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 pedestrian.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 pedestrian.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 surround_check_hysteresis_distance float If no object exists in this hysteresis distance added to the above distance, transit to \"non-surrounding-obstacle\" status [m] 0.3 \u22650.0 state_clear_time float Threshold to clear stop state [s] 2.0 \u22650.0 stop_state_ego_speed float Threshold to check ego vehicle stopped [m/s] 0.1 \u22650.0 publish_debug_footprints boolean Publish vehicle footprint &amp; footprints with surround_check_distance and surround_check_recover_distance offsets. true N/A debug_footprint_label string select the label for debug footprint car ['pointcloud', 'unknown', 'car', 'truck', 'bus', 'trailer', 'motorcycle', 'bicycle', 'pedestrian'] Name Type Description Default value <code>use_pointcloud</code> <code>bool</code> Use pointcloud as obstacle check <code>true</code> <code>use_dynamic_object</code> <code>bool</code> Use dynamic object as obstacle check <code>true</code> <code>surround_check_distance</code> <code>double</code> If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status [m] 0.5 <code>surround_check_recover_distance</code> <code>double</code> If no object exists in this distance, transit to \"non-surrounding-obstacle\" status [m] 0.8 <code>state_clear_time</code> <code>double</code> Threshold to clear stop state [s] 2.0 <code>stop_state_ego_speed</code> <code>double</code> Threshold to check ego vehicle stopped [m/s] 0.1 <code>stop_state_entry_duration_time</code> <code>double</code> Threshold to check ego vehicle stopped [s] 0.1 <code>publish_debug_footprints</code> <code>bool</code> Publish vehicle footprint with/without offsets <code>true</code>"},{"location":"planning/surround_obstacle_checker/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>To perform stop planning, it is necessary to get obstacle pointclouds data. Hence, it does not plan stopping if the obstacle is in blind spot.</p>"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/","title":"Surround Obstacle Checker","text":""},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#surround-obstacle-checker","title":"Surround Obstacle Checker","text":""},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#purpose","title":"Purpose","text":"<p><code>surround_obstacle_checker</code> \u306f\u3001\u81ea\u8eca\u304c\u505c\u8eca\u4e2d\u3001\u81ea\u8eca\u306e\u5468\u56f2\u306b\u969c\u5bb3\u7269\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u767a\u9032\u3057\u306a\u3044\u3088\u3046\u306b\u505c\u6b62\u8a08\u753b\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3042\u308b\u3002</p>"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#flow-chart","title":"Flow chart","text":""},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#algorithms","title":"Algorithms","text":""},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#check-data","title":"Check data","text":"<p>\u70b9\u7fa4\u3001\u52d5\u7684\u7269\u4f53\u3001\u81ea\u8eca\u901f\u5ea6\u306e\u30c7\u30fc\u30bf\u304c\u53d6\u5f97\u3067\u304d\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3059\u308b\u3002</p>"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#get-distance-to-nearest-object","title":"Get distance to nearest object","text":"<p>\u81ea\u8eca\u3068\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u3059\u308b\u3002 \u3053\u3053\u3067\u306f\u3001\u81ea\u8eca\u306e\u30dd\u30ea\u30b4\u30f3\u3092\u8a08\u7b97\u3057\u3001\u70b9\u7fa4\u306e\u5404\u70b9\u304a\u3088\u3073\u5404\u52d5\u7684\u7269\u4f53\u306e\u30dd\u30ea\u30b4\u30f3\u3068\u306e\u8ddd\u96e2\u3092\u305d\u308c\u305e\u308c\u8a08\u7b97\u3059\u308b\u3053\u3068\u3067\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u3092\u6c42\u3081\u308b\u3002</p>"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#stop-condition","title":"Stop condition","text":"<p>\u6b21\u306e\u6761\u4ef6\u3092\u3059\u3079\u3066\u6e80\u305f\u3059\u3068\u304d\u3001\u81ea\u8eca\u306f\u505c\u6b62\u8a08\u753b\u3092\u884c\u3046\u3002</p> <ul> <li>\u81ea\u8eca\u304c\u505c\u8eca\u3057\u3066\u3044\u308b\u3053\u3068</li> <li>\u6b21\u306e\u3046\u3061\u3044\u305a\u308c\u304b\u3092\u6e80\u305f\u3059\u3053\u3068<ol> <li>\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u304c\u6b21\u306e\u6761\u4ef6\u3092\u307f\u305f\u3059\u3053\u3068<ul> <li><code>State::PASS</code> \u306e\u3068\u304d\u3001<code>surround_check_distance</code> \u672a\u6e80\u3067\u3042\u308b</li> <li><code>State::STOP</code> \u306e\u3068\u304d\u3001<code>surround_check_recover_distance</code> \u4ee5\u4e0b\u3067\u3042\u308b</li> </ul> </li> <li>1 \u3092\u6e80\u305f\u3057\u3066\u3044\u306a\u3044\u3068\u304d\u30011 \u306e\u6761\u4ef6\u3092\u6e80\u305f\u3057\u305f\u6642\u523b\u304b\u3089\u306e\u7d4c\u904e\u6642\u9593\u304c <code>state_clear_time</code> \u4ee5\u4e0b\u3067\u3042\u308b\u3053\u3068</li> </ol> </li> </ul>"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#states","title":"States","text":"<p>\u30c1\u30e3\u30bf\u30ea\u30f3\u30b0\u9632\u6b62\u306e\u305f\u3081\u3001<code>surround_obstacle_checker</code> \u3067\u306f\u72b6\u614b\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u3002 Stop condition \u306e\u9805\u3067\u8ff0\u3079\u305f\u3088\u3046\u306b\u3001\u72b6\u614b\u306b\u3088\u3063\u3066\u969c\u5bb3\u7269\u5224\u5b9a\u306e\u3057\u304d\u3044\u5024\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u30c1\u30e3\u30bf\u30ea\u30f3\u30b0\u3092\u9632\u6b62\u3057\u3066\u3044\u308b\u3002</p> <ul> <li><code>State::PASS</code> \uff1a\u505c\u6b62\u8a08\u753b\u89e3\u9664\u4e2d</li> <li><code>State::STOP</code> \uff1a\u505c\u6b62\u8a08\u753b\u4e2d</li> </ul>"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#input","title":"Input","text":"Name Type Description <code>/perception/obstacle_segmentation/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Pointcloud of obstacles which the ego-vehicle should stop or avoid <code>/perception/object_recognition/objects</code> <code>autoware_auto_perception_msgs::msg::PredictedObjects</code> Dynamic objects <code>/localization/kinematic_state</code> <code>nav_msgs::msg::Odometry</code> Current twist <code>/tf</code> <code>tf2_msgs::msg::TFMessage</code> TF <code>/tf_static</code> <code>tf2_msgs::msg::TFMessage</code> TF static"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#output","title":"Output","text":"Name Type Description <code>~/output/velocity_limit_clear_command</code> <code>tier4_planning_msgs::msg::VelocityLimitClearCommand</code> Velocity limit clear command <code>~/output/max_velocity</code> <code>tier4_planning_msgs::msg::VelocityLimit</code> Velocity limit command <code>~/output/no_start_reason</code> <code>diagnostic_msgs::msg::DiagnosticStatus</code> No start reason <code>~/output/stop_reasons</code> <code>tier4_planning_msgs::msg::StopReasonArray</code> Stop reasons <code>~/debug/marker</code> <code>visualization_msgs::msg::MarkerArray</code> Marker for visualization"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#parameters","title":"Parameters","text":"Name Type Description Default value <code>use_pointcloud</code> <code>bool</code> Use pointcloud as obstacle check <code>true</code> <code>use_dynamic_object</code> <code>bool</code> Use dynamic object as obstacle check <code>true</code> <code>surround_check_distance</code> <code>double</code> If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status [m] 0.5 <code>surround_check_recover_distance</code> <code>double</code> If no object exists in this distance, transit to \"non-surrounding-obstacle\" status [m] 0.8 <code>state_clear_time</code> <code>double</code> Threshold to clear stop state [s] 2.0 <code>stop_state_ego_speed</code> <code>double</code> Threshold to check ego vehicle stopped [m/s] 0.1 <code>stop_state_entry_duration_time</code> <code>double</code> Threshold to check ego vehicle stopped [s] 0.1"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>\u3053\u306e\u6a5f\u80fd\u304c\u52d5\u4f5c\u3059\u308b\u305f\u3081\u306b\u306f\u969c\u5bb3\u7269\u70b9\u7fa4\u306e\u89b3\u6e2c\u304c\u5fc5\u8981\u306a\u305f\u3081\u3001\u969c\u5bb3\u7269\u304c\u6b7b\u89d2\u306b\u5165\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u505c\u6b62\u8a08\u753b\u3092\u884c\u308f\u306a\u3044\u3002</p>"},{"location":"sensing/gnss_poser/","title":"gnss_poser","text":""},{"location":"sensing/gnss_poser/#gnss_poser","title":"gnss_poser","text":""},{"location":"sensing/gnss_poser/#purpose","title":"Purpose","text":"<p>The <code>gnss_poser</code> is a node that subscribes gnss sensing messages and calculates vehicle pose with covariance.</p>"},{"location":"sensing/gnss_poser/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/gnss_poser/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/gnss_poser/#input","title":"Input","text":"Name Type Description <code>/map/map_projector_info</code> <code>tier4_map_msgs::msg::MapProjectorInfo</code> map projection info <code>~/input/fix</code> <code>sensor_msgs::msg::NavSatFix</code> gnss status message <code>~/input/autoware_orientation</code> <code>autoware_sensing_msgs::msg::GnssInsOrientationStamped</code> orientation click here for more details"},{"location":"sensing/gnss_poser/#output","title":"Output","text":"Name Type Description <code>~/output/pose</code> <code>geometry_msgs::msg::PoseStamped</code> vehicle pose calculated from gnss sensing data <code>~/output/gnss_pose_cov</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> vehicle pose with covariance calculated from gnss sensing data <code>~/output/gnss_fixed</code> <code>tier4_debug_msgs::msg::BoolStamped</code> gnss fix status"},{"location":"sensing/gnss_poser/#parameters","title":"Parameters","text":""},{"location":"sensing/gnss_poser/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range base_frame string frame id for base_frame base_link N/A gnss_frame string frame id for gnss_frame gnss N/A gnss_base_frame string frame id for gnss_base_frame gnss_base_link N/A map_frame string frame id for map_frame map N/A use_gnss_ins_orientation boolean use Gnss-Ins orientation true N/A gnss_pose_pub_method integer 0: Instant Value 1: Average Value 2: Median Value. If 0 is chosen buffer_epoch parameter loses affect. 0 \u22650\u22642 buff_epoch integer Buffer epoch 1 \u22650"},{"location":"sensing/gnss_poser/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/gnss_poser/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/gnss_poser/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/gnss_poser/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/gnss_poser/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/image_diagnostics/","title":"image_diagnostics","text":""},{"location":"sensing/image_diagnostics/#image_diagnostics","title":"image_diagnostics","text":""},{"location":"sensing/image_diagnostics/#purpose","title":"Purpose","text":"<p>The <code>image_diagnostics</code> is a node that check the status of the input raw image.</p>"},{"location":"sensing/image_diagnostics/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Below figure shows the flowchart of image diagnostics node. Each image is divided into small blocks for block state assessment.</p> <p></p> <p>Each small image block state is assessed as below figure.</p> <p></p> <p>After all image's blocks state are evaluated, the whole image status is summarized as below.</p> <p></p>"},{"location":"sensing/image_diagnostics/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/image_diagnostics/#input","title":"Input","text":"Name Type Description <code>input/raw_image</code> <code>sensor_msgs::msg::Image</code> raw image"},{"location":"sensing/image_diagnostics/#output","title":"Output","text":"Name Type Description <code>image_diag/debug/gray_image</code> <code>sensor_msgs::msg::Image</code> gray image <code>image_diag/debug/dft_image</code> <code>sensor_msgs::msg::Image</code> discrete Fourier transformation image <code>image_diag/debug/diag_block_image</code> <code>sensor_msgs::msg::Image</code> each block state colorization <code>image_diag/image_state_diag</code> <code>tier4_debug_msgs::msg::Int32Stamped</code> image diagnostics status value <code>/diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> diagnostics"},{"location":"sensing/image_diagnostics/#parameters","title":"Parameters","text":""},{"location":"sensing/image_diagnostics/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>This is proof of concept for image diagnostics and the algorithms still under further improvement.</li> </ul>"},{"location":"sensing/image_diagnostics/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/image_diagnostics/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/image_diagnostics/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/image_diagnostics/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<ul> <li>Consider more specific image distortion/occlusion type, for instance raindrop or dust.</li> </ul> <ul> <li>Consider degraded visibility under fog or rain condition from optical point of view</li> </ul>"},{"location":"sensing/image_transport_decompressor/","title":"image_transport_decompressor","text":""},{"location":"sensing/image_transport_decompressor/#image_transport_decompressor","title":"image_transport_decompressor","text":""},{"location":"sensing/image_transport_decompressor/#purpose","title":"Purpose","text":"<p>The <code>image_transport_decompressor</code> is a node that decompresses images.</p>"},{"location":"sensing/image_transport_decompressor/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/image_transport_decompressor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/image_transport_decompressor/#input","title":"Input","text":"Name Type Description <code>~/input/compressed_image</code> <code>sensor_msgs::msg::CompressedImage</code> compressed image"},{"location":"sensing/image_transport_decompressor/#output","title":"Output","text":"Name Type Description <code>~/output/raw_image</code> <code>sensor_msgs::msg::Image</code> decompressed image"},{"location":"sensing/image_transport_decompressor/#parameters","title":"Parameters","text":""},{"location":"sensing/image_transport_decompressor/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/image_transport_decompressor/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/image_transport_decompressor/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/image_transport_decompressor/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/image_transport_decompressor/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/imu_corrector/","title":"imu_corrector","text":""},{"location":"sensing/imu_corrector/#imu_corrector","title":"imu_corrector","text":""},{"location":"sensing/imu_corrector/#imu_corrector_1","title":"imu_corrector","text":"<p><code>imu_corrector_node</code> is a node that correct imu data.</p> <ol> <li>Correct yaw rate offset \\(b\\) by reading the parameter.</li> <li>Correct yaw rate standard deviation \\(\\sigma\\) by reading the parameter.</li> </ol> <p>Mathematically, we assume the following equation:</p> \\[ \\tilde{\\omega}(t) = \\omega(t) + b(t) + n(t) \\] <p>where \\(\\tilde{\\omega}\\) denotes observed angular velocity, \\(\\omega\\) denotes true angular velocity, \\(b\\) denotes an offset, and \\(n\\) denotes a gaussian noise. We also assume that \\(n\\sim\\mathcal{N}(0, \\sigma^2)\\).</p>"},{"location":"sensing/imu_corrector/#input","title":"Input","text":"Name Type Description <code>~input</code> <code>sensor_msgs::msg::Imu</code> raw imu data"},{"location":"sensing/imu_corrector/#output","title":"Output","text":"Name Type Description <code>~output</code> <code>sensor_msgs::msg::Imu</code> corrected imu data"},{"location":"sensing/imu_corrector/#parameters","title":"Parameters","text":"Name Type Description <code>angular_velocity_offset_x</code> double roll rate offset in imu_link [rad/s] <code>angular_velocity_offset_y</code> double pitch rate offset imu_link [rad/s] <code>angular_velocity_offset_z</code> double yaw rate offset imu_link [rad/s] <code>angular_velocity_stddev_xx</code> double roll rate standard deviation imu_link [rad/s] <code>angular_velocity_stddev_yy</code> double pitch rate standard deviation imu_link [rad/s] <code>angular_velocity_stddev_zz</code> double yaw rate standard deviation imu_link [rad/s] <code>acceleration_stddev</code> double acceleration standard deviation imu_link [m/s^2]"},{"location":"sensing/imu_corrector/#gyro_bias_estimator","title":"gyro_bias_estimator","text":"<p><code>gyro_bias_validator</code> is a node that validates the bias of the gyroscope. It subscribes to the <code>sensor_msgs::msg::Imu</code> topic and validate if the bias of the gyroscope is within the specified range.</p> <p>Note that the node calculates bias from the gyroscope data by averaging the data only when the vehicle is stopped.</p>"},{"location":"sensing/imu_corrector/#input_1","title":"Input","text":"Name Type Description <code>~/input/imu_raw</code> <code>sensor_msgs::msg::Imu</code> raw imu data <code>~/input/pose</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> ndt pose <p>Note that the input pose is assumed to be accurate enough. For example when using NDT, we assume that the NDT is appropriately converged.</p> <p>Currently, it is possible to use methods other than NDT as a <code>pose_source</code> for Autoware, but less accurate methods are not suitable for IMU bias estimation.</p> <p>In the future, with careful implementation for pose errors, the IMU bias estimated by NDT could potentially be used not only for validation but also for online calibration.</p>"},{"location":"sensing/imu_corrector/#output_1","title":"Output","text":"Name Type Description <code>~/output/gyro_bias</code> <code>geometry_msgs::msg::Vector3Stamped</code> bias of the gyroscope [rad/s]"},{"location":"sensing/imu_corrector/#parameters_1","title":"Parameters","text":"Name Type Description <code>angular_velocity_offset_x</code> double roll rate offset in imu_link [rad/s] <code>angular_velocity_offset_y</code> double pitch rate offset imu_link [rad/s] <code>angular_velocity_offset_z</code> double yaw rate offset imu_link [rad/s] <code>gyro_bias_threshold</code> double threshold of the bias of the gyroscope [rad/s] <code>timer_callback_interval_sec</code> double seconds about the timer callback function [sec] <code>straight_motion_ang_vel_upper_limit</code> double upper limit of yaw angular velocity, beyond which motion is not considered straight [rad/s]"},{"location":"sensing/livox/livox_tag_filter/","title":"livox_tag_filter","text":""},{"location":"sensing/livox/livox_tag_filter/#livox_tag_filter","title":"livox_tag_filter","text":""},{"location":"sensing/livox/livox_tag_filter/#purpose","title":"Purpose","text":"<p>The <code>livox_tag_filter</code> is a node that removes noise from pointcloud by using the following tags:</p> <ul> <li>Point property based on spatial position</li> <li>Point property based on intensity</li> <li>Return number</li> </ul>"},{"location":"sensing/livox/livox_tag_filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/livox/livox_tag_filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/livox/livox_tag_filter/#input","title":"Input","text":"Name Type Description <code>~/input</code> <code>sensor_msgs::msg::PointCloud2</code> reference points"},{"location":"sensing/livox/livox_tag_filter/#output","title":"Output","text":"Name Type Description <code>~/output</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"sensing/livox/livox_tag_filter/#parameters","title":"Parameters","text":""},{"location":"sensing/livox/livox_tag_filter/#node-parameters","title":"Node Parameters","text":"Name Type Description <code>ignore_tags</code> vector ignored tags (See the following table)"},{"location":"sensing/livox/livox_tag_filter/#tag-parameters","title":"Tag Parameters","text":"Bit Description Options 0~1 Point property based on spatial position 00: Normal 01: High confidence level of the noise 10: Moderate confidence level of the noise 11: Low confidence level of the noise 2~3 Point property based on intensity 00: Normal 01: High confidence level of the noise 10: Moderate confidence level of the noise 11: Reserved 4~5 Return number 00: return 0 01: return 1 10: return 2 11: return 3 6~7 Reserved <p>You can download more detail description about the livox from external link [1].</p>"},{"location":"sensing/livox/livox_tag_filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/livox/livox_tag_filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/livox/livox_tag_filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/livox/livox_tag_filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>[1] https://www.livoxtech.com/downloads</p>"},{"location":"sensing/livox/livox_tag_filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/pointcloud_preprocessor/","title":"pointcloud_preprocessor","text":""},{"location":"sensing/pointcloud_preprocessor/#pointcloud_preprocessor","title":"pointcloud_preprocessor","text":""},{"location":"sensing/pointcloud_preprocessor/#purpose","title":"Purpose","text":"<p>The <code>pointcloud_preprocessor</code> is a package that includes the following filters:</p> <ul> <li>removing outlier points</li> <li>cropping</li> <li>concatenating pointclouds</li> <li>correcting distortion</li> <li>downsampling</li> </ul>"},{"location":"sensing/pointcloud_preprocessor/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Detail description of each filter's algorithm is in the following links.</p> Filter Name Description Detail concatenate_data subscribe multiple pointclouds and concatenate them into a pointcloud link crop_box_filter remove points within a given box link distortion_corrector compensate pointcloud distortion caused by ego vehicle's movement during 1 scan link downsample_filter downsampling input pointcloud link outlier_filter remove points caused by hardware problems, rain drops and small insects as a noise link passthrough_filter remove points on the outside of a range in given field (e.g. x, y, z, intensity) link pointcloud_accumulator accumulate pointclouds for a given amount of time link vector_map_filter remove points on the outside of lane by using vector map link vector_map_inside_area_filter remove points inside of vector map area that has given type by parameter link"},{"location":"sensing/pointcloud_preprocessor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/pointcloud_preprocessor/#input","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points <code>~/input/indices</code> <code>pcl_msgs::msg::Indices</code> reference indices"},{"location":"sensing/pointcloud_preprocessor/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"sensing/pointcloud_preprocessor/#parameters","title":"Parameters","text":""},{"location":"sensing/pointcloud_preprocessor/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>input_frame</code> string \" \" input frame id <code>output_frame</code> string \" \" output frame id <code>max_queue_size</code> int 5 max queue size of input/output topics <code>use_indices</code> bool false flag to use pointcloud indices <code>latched_indices</code> bool false flag to latch pointcloud indices <code>approximate_sync</code> bool false flag to use approximate sync option"},{"location":"sensing/pointcloud_preprocessor/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p><code>pointcloud_preprocessor::Filter</code> is implemented based on pcl_perception [1] because of this issue.</p>"},{"location":"sensing/pointcloud_preprocessor/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/pointcloud_preprocessor/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/pointcloud_preprocessor/#referencesexternal-links","title":"References/External links","text":"<p>[1] https://github.com/ros-perception/perception_pcl/blob/ros2/pcl_ros/src/pcl_ros/filters/filter.cpp</p>"},{"location":"sensing/pointcloud_preprocessor/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/","title":"blockage_diag","text":""},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#blockage_diag","title":"blockage_diag","text":""},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#purpose","title":"Purpose","text":"<p>To ensure the performance of LiDAR and safety for autonomous driving, the abnormal condition diagnostics feature is needed. LiDAR blockage is abnormal condition of LiDAR when some unwanted objects stitch to and block the light pulses and return signal. This node's purpose is to detect the existing of blockage on LiDAR and its related size and location.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#inner-workings-algorithmsblockage-detection","title":"Inner-workings / Algorithms(Blockage detection)","text":"<p>This node bases on the no-return region and its location to decide if it is a blockage.</p> <p></p> <p>The logic is showed as below</p> <p></p>"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#inner-workings-algorithmsdust-detection","title":"Inner-workings /Algorithms(Dust detection)","text":"<p>About dust detection, morphological processing is implemented. If the lidar's ray cannot be acquired due to dust in the lidar area where the point cloud is considered to return from the ground, black pixels appear as noise in the depth image. The area of noise is found by erosion and dilation these black pixels.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#input","title":"Input","text":"Name Type Description <code>~/input/pointcloud_raw_ex</code> <code>sensor_msgs::msg::PointCloud2</code> The raw point cloud data is used to detect the no-return region"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#output","title":"Output","text":"Name Type Description <code>~/output/blockage_diag/debug/blockage_mask_image</code> <code>sensor_msgs::msg::Image</code> The mask image of detected blockage <code>~/output/blockage_diag/debug/ground_blockage_ratio</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> The area ratio of blockage region in ground region <code>~/output/blockage_diag/debug/sky_blockage_ratio</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> The area ratio of blockage region in sky region <code>~/output/blockage_diag/debug/lidar_depth_map</code> <code>sensor_msgs::msg::Image</code> The depth map image of input point cloud <code>~/output/blockage_diag/debug/single_frame_dust_mask</code> <code>sensor_msgs::msg::Image</code> The mask image of detected dusty area in latest single frame <code>~/output/blockage_diag/debug/multi_frame_dust_mask</code> <code>sensor_msgs::msg::Image</code> The mask image of continuous detected dusty area <code>~/output/blockage_diag/debug/blockage_dust_merged_image</code> <code>sensor_msgs::msg::Image</code> The merged image of blockage detection(red) and multi frame dusty area detection(yellow) results <code>~/output/blockage_diag/debug/ground_dust_ratio</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> The ratio of dusty area divided by area where ray usually returns from the ground."},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#parameters","title":"Parameters","text":"Name Type Description <code>blockage_ratio_threshold</code> float The threshold of blockage area ratio.If the blockage value exceeds this threshold, the diagnostic state will be set to ERROR. <code>blockage_count_threshold</code> float The threshold of number continuous blockage frames <code>horizontal_ring_id</code> int The id of horizontal ring of the LiDAR <code>angle_range</code> vector The effective range of LiDAR <code>vertical_bins</code> int The LiDAR channel number <code>model</code> string The LiDAR model <code>blockage_buffering_frames</code> int The number of buffering about blockage detection [range:1-200] <code>blockage_buffering_interval</code> int The interval of buffering about blockage detection <code>dust_ratio_threshold</code> float The threshold of dusty area ratio <code>dust_count_threshold</code> int The threshold of number continuous frames include dusty area <code>dust_kernel_size</code> int The kernel size of morphology processing in dusty area detection <code>dust_buffering_frames</code> int The number of buffering about dusty area detection [range:1-200] <code>dust_buffering_interval</code> int The interval of buffering about dusty area detection"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ol> <li>Only Hesai Pandar40P and Hesai PandarQT were tested. For a new LiDAR, it is necessary to check order of channel id in    vertical distribution manually and modify the code.</li> <li>About dusty area detection, False positives occur when there are water puddles on the road surface due to rain, etc.    Also, the area of the ray to the sky is currently undetectable.</li> </ol>"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#referencesexternal-links","title":"References/External links","text":""},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/","title":"concatenate_data","text":""},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#concatenate_data","title":"concatenate_data","text":""},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#purpose","title":"Purpose","text":"<p>Many self-driving cars combine multiple LiDARs to expand the sensing range. Therefore, a function to combine a plurality of point clouds is required.</p> <p>To combine multiple sensor data with a similar timestamp, the message_filters is often used in the ROS-based system, but this requires the assumption that all inputs can be received. Since safety must be strongly considered in autonomous driving, the point clouds concatenate node must be designed so that even if one sensor fails, the remaining sensor information can be output.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The figure below represents the reception time of each sensor data and how it is combined in the case.</p> <p></p>"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#input","title":"Input","text":"Name Type Description <code>~/input/twist</code> <code>geometry_msgs::msg::TwistWithCovarianceStamped</code> The vehicle odometry is used to interpolate the timestamp of each sensor data"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::Pointcloud2</code> concatenated point clouds"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#parameters","title":"Parameters","text":"Name Type Default Value Description <code>input/points</code> vector of string [] input topic names that type must be <code>sensor_msgs::msg::Pointcloud2</code> <code>input_frame</code> string \"\" input frame id <code>output_frame</code> string \"\" output frame id <code>max_queue_size</code> int 5 max queue size of input/output topics"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>timeout_sec</code> double 0.1 tolerance of time to publish next pointcloud [s]When this time limit is exceeded, the filter concatenates and publishes pointcloud, even if not all the point clouds are subscribed. <code>input_offset</code> vector of double [] This parameter can control waiting time for each input sensor pointcloud [s]. You must to set the same length of offsets with input pointclouds numbers.  For its tuning, please see actual usage page. <code>publish_synchronized_pointcloud</code> bool false If true, publish the time synchronized pointclouds. All input pointclouds are transformed and then re-published as message named <code>&lt;original_msg_name&gt;_synchronized</code>. <code>input_twist_topic_type</code> std::string twist Topic type for twist. Currently support <code>twist</code> or <code>odom</code>."},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#actual-usage","title":"Actual Usage","text":"<p>For the example of actual usage of this node, please refer to the preprocessor.launch.py file.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#how-to-tuning-timeout_sec-and-input_offset","title":"How to tuning timeout_sec and input_offset","text":"<p>The values in <code>timeout_sec</code> and <code>input_offset</code> are used in the timer_callback to control concatenation timings.</p> <ul> <li>Assumptions<ul> <li>when the timer runs out, we concatenate the pointclouds in the buffer</li> <li>when the first pointcloud comes to buffer, we reset the timer to <code>timeout_sec</code></li> <li>when the second and later pointclouds comes to buffer, we reset the timer to <code>timeout_sec</code> - <code>input_offset</code></li> <li>we assume all lidar has same frequency</li> </ul> </li> </ul> Name Description How to tune <code>timeout_sec</code> timeout sec for default timer To avoid mis-concatenation, at least this value must be shorter than sampling time. <code>input_offset</code> timeout extension when a pointcloud comes to buffer. The amount of waiting time will be <code>timeout_sec</code> - <code>input_offset</code>. So, you will need to set larger value for the last-coming pointcloud and smaller for fore-coming."},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#node-separation-options-for-future","title":"Node separation options for future","text":"<p>Since the pointcloud concatenation has two process, \"time synchronization\" and \"pointcloud concatenation\", it is possible to separate these processes.</p> <p>In the future, Nodes will be completely separated in order to achieve node loosely coupled nature, but currently both nodes can be selected for backward compatibility (See this PR).</p>"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>It is necessary to assume that the vehicle odometry value exists, the sensor data and odometry timestamp are correct, and the TF from <code>base_link</code> to <code>sensor_frame</code> is also correct.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/","title":"crop_box_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#crop_box_filter","title":"crop_box_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#purpose","title":"Purpose","text":"<p>The <code>crop_box_filter</code> is a node that removes points with in a given box region. This filter is used to remove the points that hit the vehicle itself.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p><code>pcl::CropBox</code> is used, which filters all points inside a given box.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherit <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherit <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>min_x</code> double -1.0 x-coordinate minimum value for crop range <code>max_x</code> double 1.0 x-coordinate maximum value for crop range <code>min_y</code> double -1.0 y-coordinate minimum value for crop range <code>max_y</code> double 1.0 y-coordinate maximum value for crop range <code>min_z</code> double -1.0 z-coordinate minimum value for crop range <code>max_z</code> double 1.0 z-coordinate maximum value for crop range"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/","title":"distortion_corrector","text":""},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#distortion_corrector","title":"distortion_corrector","text":""},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#purpose","title":"Purpose","text":"<p>The <code>distortion_corrector</code> is a node that compensates pointcloud distortion caused by ego vehicle's movement during 1 scan.</p> <p>Since the LiDAR sensor scans by rotating an internal laser, the resulting point cloud will be distorted if the ego-vehicle moves during a single scan (as shown by the figure below). The node corrects this by interpolating sensor data using odometry of ego-vehicle.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ul> <li>Use the equations below (specific to the Velodyne 32C sensor) to obtain an accurate timestamp for each scan data point.</li> <li>Use twist information to determine the distance the ego-vehicle has traveled between the time that the scan started and the corrected timestamp of each point, and then correct the position of the point.</li> </ul> <p>The offset equation is given by $ TimeOffset = (55.296 \\mu s SequenceIndex) + (2.304 \\mu s DataPointIndex) $</p> <p>To calculate the exact point time, add the TimeOffset to the timestamp. $ ExactPointTime = TimeStamp + TimeOffset $</p> <p></p>"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#input","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points <code>~/input/twist</code> <code>geometry_msgs::msg::TwistWithCovarianceStamped</code> twist <code>~/input/imu</code> <code>sensor_msgs::msg::Imu</code> imu data"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#parameters","title":"Parameters","text":""},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>timestamp_field_name</code> string \"time_stamp\" time stamp field name <code>use_imu</code> bool true use gyroscope for yaw rate if true, else use vehicle status"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/","title":"downsample_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#downsample_filter","title":"downsample_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#purpose","title":"Purpose","text":"<p>The <code>downsample_filter</code> is a node that reduces the number of points.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#approximate-downsample-filter","title":"Approximate Downsample Filter","text":"<p><code>pcl::VoxelGridNearestCentroid</code> is used. The algorithm is described in tier4_pcl_extensions</p>"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#random-downsample-filter","title":"Random Downsample Filter","text":"<p><code>pcl::RandomSample</code> is used, which points are sampled with uniform probability.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#voxel-grid-downsample-filter","title":"Voxel Grid Downsample Filter","text":"<p><code>pcl::VoxelGrid</code> is used, which points in each voxel are approximated with their centroid.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>These implementations inherit <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#note-parameters","title":"Note Parameters","text":"<p>These implementations inherit <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#core-parameters","title":"Core Parameters","text":""},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#approximate-downsample-filter_1","title":"Approximate Downsample Filter","text":"Name Type Default Value Description <code>voxel_size_x</code> double 0.3 voxel size x [m] <code>voxel_size_y</code> double 0.3 voxel size y [m] <code>voxel_size_z</code> double 0.1 voxel size z [m]"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#random-downsample-filter_1","title":"Random Downsample Filter","text":"Name Type Default Value Description <code>sample_num</code> int 1500 number of indices to be sampled"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#voxel-grid-downsample-filter_1","title":"Voxel Grid Downsample Filter","text":"Name Type Default Value Description <code>voxel_size_x</code> double 0.3 voxel size x [m] <code>voxel_size_y</code> double 0.3 voxel size y [m] <code>voxel_size_z</code> double 0.1 voxel size z [m]"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/","title":"dual_return_outlier_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#dual_return_outlier_filter","title":"dual_return_outlier_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#purpose","title":"Purpose","text":"<p>The purpose is to remove point cloud noise such as fog and rain and publish visibility as a diagnostic topic.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This node can remove rain and fog by considering the light reflected from the object in two stages according to the attenuation factor. The <code>dual_return_outlier_filter</code> is named because it removes noise using data that contains two types of return values separated by attenuation factor, as shown in the figure below.</p> <p></p> <p>Therefore, in order to use this node, the sensor driver must publish custom data including <code>return_type</code>. please refer to PointXYZIRADRT data structure.</p> <p>Another feature of this node is that it publishes visibility as a diagnostic topic. With this function, for example, in heavy rain, the sensing module can notify that the processing performance has reached its limit, which can lead to ensuring the safety of the vehicle.</p> <p>In some complicated road scenes where normal objects also reflect the light in two stages, for instance plants, leaves, some plastic net etc, the visibility faces some drop in fine weather condition. To deal with that, optional settings of a region of interest (ROI) are added.</p> <ol> <li><code>Fixed_xyz_ROI</code> mode: Visibility estimation based on the weak points in a fixed cuboid surrounding region of ego-vehicle, defined by x, y, z in base_link perspective.</li> <li><code>Fixed_azimuth_ROI</code> mode: Visibility estimation based on the weak points in a fixed surrounding region of ego-vehicle, defined by azimuth and distance of LiDAR perspective.</li> </ol> <p>When select 2 fixed ROI modes, due to the range of weak points is shrink, the sensitivity of visibility is decrease so that a trade of between <code>weak_first_local_noise_threshold</code> and <code>visibility_threshold</code> is needed.</p> <p></p> <p>The figure below describe how the node works. </p> <p>The below picture shows the ROI options.</p> <p></p>"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#output","title":"Output","text":"Name Type Description <code>/dual_return_outlier_filter/frequency_image</code> <code>sensor_msgs::msg::Image</code> The histogram image that represent visibility <code>/dual_return_outlier_filter/visibility</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> A representation of visibility with a value from 0 to 1 <code>/dual_return_outlier_filter/pointcloud_noise</code> <code>sensor_msgs::msg::Pointcloud2</code> The pointcloud removed as noise"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description <code>vertical_bins</code> int The number of vertical bin for visibility histogram <code>max_azimuth_diff</code> float Threshold for ring_outlier_filter <code>weak_first_distance_ratio</code> double Threshold for ring_outlier_filter <code>general_distance_ratio</code> double Threshold for ring_outlier_filter <code>weak_first_local_noise_threshold</code> int The parameter for determining whether it is noise <code>visibility_error_threshold</code> float When the percentage of white pixels in the binary histogram falls below this parameter the diagnostic status becomes ERR <code>visibility_warn_threshold</code> float When the percentage of white pixels in the binary histogram falls below this parameter the diagnostic status becomes WARN <code>roi_mode</code> string The name of ROI mode for switching <code>min_azimuth_deg</code> float The left limit of azimuth for <code>Fixed_azimuth_ROI</code> mode <code>max_azimuth_deg</code> float The right limit of azimuth for <code>Fixed_azimuth_ROI</code> mode <code>max_distance</code> float The limit distance for for <code>Fixed_azimuth_ROI</code> mode <code>x_max</code> float Maximum of x for <code>Fixed_xyz_ROI</code> mode <code>x_min</code> float Minimum of x for <code>Fixed_xyz_ROI</code> mode <code>y_max</code> float Maximum of y for <code>Fixed_xyz_ROI</code> mode <code>y_min</code> float Minimum of y for <code>Fixed_xyz_ROI</code> mode <code>z_max</code> float Maximum of z for <code>Fixed_xyz_ROI</code> mode <code>z_min</code> float Minimum of z for <code>Fixed_xyz_ROI</code> mode"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Not recommended for use as it is under development. Input data must be <code>PointXYZIRADRT</code> type data including <code>return_type</code>.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#referencesexternal-links","title":"References/External links","text":""},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/","title":"outlier_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#outlier_filter","title":"outlier_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#purpose","title":"Purpose","text":"<p>The <code>outlier_filter</code> is a package for filtering outlier of points.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"Filter Name Description Detail radius search 2d outlier filter A method of removing point cloud noise based on the number of points existing within a certain radius link ring outlier filter A method of operating scan in chronological order and removing noise based on the rate of change in the distance between points link voxel grid outlier filter A method of removing point cloud noise based on the number of points existing within a voxel link dual return outlier filter (under development) A method of removing rain and fog by considering the light reflected from the object in two stages according to the attenuation factor. link"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/","title":"passthrough_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#passthrough_filter","title":"passthrough_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#purpose","title":"Purpose","text":"<p>The <code>passthrough_filter</code> is a node that removes points on the outside of a range in a given field (e.g. x, y, z, intensity, ring, etc).</p>"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#input","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points <code>~/input/indices</code> <code>pcl_msgs::msg::Indices</code> reference indices"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>filter_limit_min</code> int 0 minimum allowed field value <code>filter_limit_max</code> int 127 maximum allowed field value <code>filter_field_name</code> string \"ring\" filtering field name <code>keep_organized</code> bool false flag to keep indices structure <code>filter_limit_negative</code> bool false flag to return whether the data is inside limit or not"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/","title":"pointcloud_accumulator","text":""},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#pointcloud_accumulator","title":"pointcloud_accumulator","text":""},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#purpose","title":"Purpose","text":"<p>The <code>pointcloud_accumulator</code> is a node that accumulates pointclouds for a given amount of time.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#input","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#parameters","title":"Parameters","text":""},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>accumulation_time_sec</code> double 2.0 accumulation period [s] <code>pointcloud_buffer_size</code> int 50 buffer size"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/","title":"radius_search_2d_outlier_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#radius_search_2d_outlier_filter","title":"radius_search_2d_outlier_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#purpose","title":"Purpose","text":"<p>The purpose is to remove point cloud noise such as insects and rain.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>RadiusOutlierRemoval filter which removes all indices in its input cloud that don\u2019t have at least some number of neighbors within a certain range.</p> <p>The description above is quoted from [1]. <code>pcl::search::KdTree</code> [2] is used to implement this package.</p> <p></p>"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description <code>min_neighbors</code> int If points in the circle centered on reference point is less than <code>min_neighbors</code>, a reference point is judged as outlier <code>search_radius</code> double Searching number of points included in <code>search_radius</code>"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Since the method is to count the number of points contained in the cylinder with the direction of gravity as the direction of the cylinder axis, it is a prerequisite that the ground has been removed.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#referencesexternal-links","title":"References/External links","text":"<p>[1] https://pcl.readthedocs.io/projects/tutorials/en/latest/remove_outliers.html</p> <p>[2] https://pcl.readthedocs.io/projects/tutorials/en/latest/kdtree_search.html#kdtree-search</p>"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/","title":"ring_outlier_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#ring_outlier_filter","title":"ring_outlier_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#purpose","title":"Purpose","text":"<p>The purpose is to remove point cloud noise such as insects and rain.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>A method of operating scan in chronological order and removing noise based on the rate of change in the distance between points</p> <p></p>"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>distance_ratio</code> double 1.03 <code>object_length_threshold</code> double 0.1 <code>num_points_threshold</code> int 4 <code>max_rings_num</code> uint_16 128"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>It is a prerequisite to input a scan point cloud in chronological order. In this repository it is defined as blow structure (please refer to PointXYZIRADT).</p> <ul> <li>X: x</li> <li>Y: y</li> <li>z: z</li> <li>I: intensity</li> <li>R: ring</li> <li>A :azimuth</li> <li>D: distance</li> <li>T: time_stamp</li> </ul>"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/","title":"vector_map_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#vector_map_filter","title":"vector_map_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#purpose","title":"Purpose","text":"<p>The <code>vector_map_filter</code> is a node that removes points on the outside of lane by using vector map.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#input","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points <code>~/input/vector_map</code> <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> vector map"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>voxel_size_x</code> double 0.04 voxel size <code>voxel_size_y</code> double 0.04 voxel size"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-inside-area-filter/","title":"vector_map_inside_area_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-inside-area-filter/#vector_map_inside_area_filter","title":"vector_map_inside_area_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-inside-area-filter/#purpose","title":"Purpose","text":"<p>The <code>vector_map_inside_area_filter</code> is a node that removes points inside the vector map area that has given type by parameter.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-inside-area-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ul> <li>Get the vector map area that has given type by parameter of <code>polygon_type</code></li> <li>Extract the vector map area that intersects with the bounding box of input points to reduce the calculation cost</li> <li>Create the 2D polygon from the extracted vector map area</li> <li>Remove input points inside the polygon</li> </ul>"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-inside-area-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, so please see also README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-inside-area-filter/#input","title":"Input","text":"Name Type Description <code>~/input</code> <code>sensor_msgs::msg::PointCloud2</code> input points <code>~/input/vector_map</code> <code>autoware_auto_mapping_msgs::msg::HADMapBin</code> vector map used for filtering points"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-inside-area-filter/#output","title":"Output","text":"Name Type Description <code>~/output</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-inside-area-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description <code>polygon_type</code> string polygon type to be filtered"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-inside-area-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/","title":"voxel_grid_outlier_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#voxel_grid_outlier_filter","title":"voxel_grid_outlier_filter","text":""},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#purpose","title":"Purpose","text":"<p>The purpose is to remove point cloud noise such as insects and rain.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Removing point cloud noise based on the number of points existing within a voxel. The radius_search_2d_outlier_filter is better for accuracy, but this method has the advantage of low calculation cost.</p> <p></p>"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>voxel_size_x</code> double 0.3 the voxel size along x-axis [m] <code>voxel_size_y</code> double 0.3 the voxel size along y-axis [m] <code>voxel_size_z</code> double 0.1 the voxel size along z-axis [m] <code>voxel_points_threshold</code> int 2 the minimum number of points in each voxel"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/radar_scan_to_pointcloud2/","title":"radar_scan_to_pointcloud2","text":""},{"location":"sensing/radar_scan_to_pointcloud2/#radar_scan_to_pointcloud2","title":"radar_scan_to_pointcloud2","text":""},{"location":"sensing/radar_scan_to_pointcloud2/#radar_scan_to_pointcloud2_node","title":"radar_scan_to_pointcloud2_node","text":"<ul> <li>Convert from <code>radar_msgs::msg::RadarScan</code> to <code>sensor_msgs::msg::PointCloud2</code></li> <li>Calculation cost O(n)<ul> <li>n: The number of radar return</li> </ul> </li> </ul>"},{"location":"sensing/radar_scan_to_pointcloud2/#input-topics","title":"Input topics","text":"Name Type Description input/radar radar_msgs::msg::RadarScan RadarScan"},{"location":"sensing/radar_scan_to_pointcloud2/#output-topics","title":"Output topics","text":"Name Type Description output/amplitude_pointcloud sensor_msgs::msg::PointCloud2 PointCloud2 radar pointcloud whose intensity is amplitude. output/doppler_pointcloud sensor_msgs::msg::PointCloud2 PointCloud2 radar pointcloud whose intensity is doppler velocity."},{"location":"sensing/radar_scan_to_pointcloud2/#parameters","title":"Parameters","text":"Name Type Description publish_amplitude_pointcloud bool Whether publish radar pointcloud whose intensity is amplitude. Default is <code>true</code>. publish_doppler_pointcloud bool Whether publish radar pointcloud whose intensity is doppler velocity. Default is <code>false</code>."},{"location":"sensing/radar_scan_to_pointcloud2/#how-to-launch","title":"How to launch","text":"<pre><code>ros2 launch radar_scan_to_pointcloud2 radar_scan_to_pointcloud2.launch.xml\n</code></pre>"},{"location":"sensing/radar_static_pointcloud_filter/","title":"radar_static_pointcloud_filter","text":""},{"location":"sensing/radar_static_pointcloud_filter/#radar_static_pointcloud_filter","title":"radar_static_pointcloud_filter","text":""},{"location":"sensing/radar_static_pointcloud_filter/#radar_static_pointcloud_filter_node","title":"radar_static_pointcloud_filter_node","text":"<p>Extract static/dynamic radar pointcloud by using doppler velocity and ego motion. Calculation cost is O(n). <code>n</code> is the number of radar pointcloud.</p>"},{"location":"sensing/radar_static_pointcloud_filter/#input-topics","title":"Input topics","text":"Name Type Description input/radar radar_msgs::msg::RadarScan RadarScan input/odometry nav_msgs::msg::Odometry Ego vehicle odometry topic"},{"location":"sensing/radar_static_pointcloud_filter/#output-topics","title":"Output topics","text":"Name Type Description output/static_radar_scan radar_msgs::msg::RadarScan static radar pointcloud output/dynamic_radar_scan radar_msgs::msg::RadarScan dynamic radar pointcloud"},{"location":"sensing/radar_static_pointcloud_filter/#parameters","title":"Parameters","text":"Name Type Description doppler_velocity_sd double Standard deviation for radar doppler velocity. [m/s]"},{"location":"sensing/radar_static_pointcloud_filter/#how-to-launch","title":"How to launch","text":"<pre><code>ros2 launch radar_static_pointcloud_filter radar_static_pointcloud_filter.launch\n</code></pre>"},{"location":"sensing/radar_static_pointcloud_filter/#algorithm","title":"Algorithm","text":""},{"location":"sensing/radar_threshold_filter/","title":"radar_threshold_filter","text":""},{"location":"sensing/radar_threshold_filter/#radar_threshold_filter","title":"radar_threshold_filter","text":""},{"location":"sensing/radar_threshold_filter/#radar_threshold_filter_node","title":"radar_threshold_filter_node","text":"<p>Remove noise from radar return by threshold.</p> <ul> <li>Amplitude filter: Low amplitude consider noise</li> <li>FOV filter: Pointcloud from radar's FOV edge occur perturbation</li> <li>Range filter: Too near pointcloud often occur noise</li> </ul> <p>Calculation cost is O(n). <code>n</code> is the number of radar return.</p>"},{"location":"sensing/radar_threshold_filter/#input-topics","title":"Input topics","text":"Name Type Description input/radar radar_msgs/msg/RadarScan.msg Radar pointcloud data"},{"location":"sensing/radar_threshold_filter/#output-topics","title":"Output topics","text":"Name Type Description output/radar radar_msgs/msg/RadarScan.msg Filtered radar pointcloud"},{"location":"sensing/radar_threshold_filter/#parameters","title":"Parameters","text":"<ul> <li>For node parameter</li> </ul> Name Type Description is_amplitude_filter bool if this parameter is true, apply amplitude filter (publish amplitude_min &lt; amplitude &lt; amplitude_max) amplitude_min double [dBm^2] amplitude_max double [dBm^2] is_range_filter bool if this parameter is true, apply range filter (publish range_min &lt; range &lt; range_max) range_min double [m] range_max double [m] is_azimuth_filter bool if this parameter is true, apply angle filter (publish azimuth_min &lt; range &lt; azimuth_max) azimuth_min double [rad] azimuth_max double [rad] is_z_filter bool if this parameter is true, apply z position filter (publish z_min &lt; z &lt; z_max) z_min double [m] z_max double [m]"},{"location":"sensing/radar_threshold_filter/#how-to-launch","title":"How to launch","text":"<pre><code>ros2 launch radar_threshold_filter radar_threshold_filter.launch.xml\n</code></pre>"},{"location":"sensing/radar_tracks_noise_filter/","title":"radar_tracks_noise_filter","text":""},{"location":"sensing/radar_tracks_noise_filter/#radar_tracks_noise_filter","title":"radar_tracks_noise_filter","text":"<p>This package contains a radar object filter module for <code>radar_msgs/msg/RadarTrack</code>. This package can filter noise objects in RadarTracks.</p>"},{"location":"sensing/radar_tracks_noise_filter/#algorithm","title":"Algorithm","text":"<p>The core algorithm of this package is <code>RadarTrackCrossingNoiseFilterNode::isNoise()</code> function. See the function and the parameters for details.</p> <ul> <li>Y-axis threshold</li> </ul> <p>Radar can detect x-axis velocity as doppler velocity, but cannot detect y-axis velocity. Some radar can estimate y-axis velocity inside the device, but it sometimes lack precision. In y-axis threshold filter, if y-axis velocity of RadarTrack is more than <code>velocity_y_threshold</code>, it treats as noise objects.</p>"},{"location":"sensing/radar_tracks_noise_filter/#input","title":"Input","text":"Name Type Description <code>~/input/tracks</code> radar_msgs/msg/RadarTracks.msg 3D detected tracks."},{"location":"sensing/radar_tracks_noise_filter/#output","title":"Output","text":"Name Type Description <code>~/output/noise_tracks</code> radar_msgs/msg/RadarTracks.msg Noise objects <code>~/output/filtered_tracks</code> radar_msgs/msg/RadarTracks.msg Filtered objects"},{"location":"sensing/radar_tracks_noise_filter/#parameters","title":"Parameters","text":"Name Type Description Default value <code>velocity_y_threshold</code> double Y-axis velocity threshold [m/s]. If y-axis velocity of RadarTrack is more than <code>velocity_y_threshold</code>, it treats as noise objects. 7.0"},{"location":"sensing/tier4_pcl_extensions/","title":"tier4_pcl_extensions","text":""},{"location":"sensing/tier4_pcl_extensions/#tier4_pcl_extensions","title":"tier4_pcl_extensions","text":""},{"location":"sensing/tier4_pcl_extensions/#purpose","title":"Purpose","text":"<p>The <code>tier4_pcl_extensions</code> is a pcl extension library. The voxel grid filter in this package works with a different algorithm than the original one.</p>"},{"location":"sensing/tier4_pcl_extensions/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/tier4_pcl_extensions/#original-algorithm-1","title":"Original Algorithm [1]","text":"<ol> <li>create a 3D voxel grid over the input pointcloud data</li> <li>calculate centroid in each voxel</li> <li>all the points are approximated with their centroid</li> </ol>"},{"location":"sensing/tier4_pcl_extensions/#extended-algorithm","title":"Extended Algorithm","text":"<ol> <li>create a 3D voxel grid over the input pointcloud data</li> <li>calculate centroid in each voxel</li> <li>all the points are approximated with the closest point to their centroid</li> </ol>"},{"location":"sensing/tier4_pcl_extensions/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/tier4_pcl_extensions/#parameters","title":"Parameters","text":""},{"location":"sensing/tier4_pcl_extensions/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/tier4_pcl_extensions/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/tier4_pcl_extensions/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/tier4_pcl_extensions/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>[1] https://pointclouds.org/documentation/tutorials/voxel_grid.html</p>"},{"location":"sensing/tier4_pcl_extensions/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/vehicle_velocity_converter/","title":"vehicle_velocity_converter","text":""},{"location":"sensing/vehicle_velocity_converter/#vehicle_velocity_converter","title":"vehicle_velocity_converter","text":""},{"location":"sensing/vehicle_velocity_converter/#purpose","title":"Purpose","text":"<p>This package converts autoware_auto_vehicle_msgs::msg::VehicleReport message to geometry_msgs::msg::TwistWithCovarianceStamped for gyro odometer node.</p>"},{"location":"sensing/vehicle_velocity_converter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/vehicle_velocity_converter/#input","title":"Input","text":"Name Type Description <code>velocity_status</code> <code>autoware_auto_vehicle_msgs::msg::VehicleReport</code> vehicle velocity"},{"location":"sensing/vehicle_velocity_converter/#output","title":"Output","text":"Name Type Description <code>twist_with_covariance</code> <code>geometry_msgs::msg::TwistWithCovarianceStamped</code> twist with covariance converted from VehicleReport"},{"location":"sensing/vehicle_velocity_converter/#parameters","title":"Parameters","text":"Name Type Description <code>speed_scale_factor</code> double speed scale factor (ideal value is 1.0) <code>frame_id</code> string frame id for output message <code>velocity_stddev_xx</code> double standard deviation for vx <code>angular_velocity_stddev_zz</code> double standard deviation for yaw rate"},{"location":"simulator/dummy_perception_publisher/","title":"dummy_perception_publisher","text":""},{"location":"simulator/dummy_perception_publisher/#dummy_perception_publisher","title":"dummy_perception_publisher","text":""},{"location":"simulator/dummy_perception_publisher/#purpose","title":"Purpose","text":"<p>This node publishes the result of the dummy detection with the type of perception.</p>"},{"location":"simulator/dummy_perception_publisher/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"simulator/dummy_perception_publisher/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"simulator/dummy_perception_publisher/#input","title":"Input","text":"Name Type Description <code>/tf</code> <code>tf2_msgs/TFMessage</code> TF (self-pose) <code>input/object</code> <code>dummy_perception_publisher::msg::Object</code> dummy detection objects"},{"location":"simulator/dummy_perception_publisher/#output","title":"Output","text":"Name Type Description <code>output/dynamic_object</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> dummy detection objects <code>output/points_raw</code> <code>sensor_msgs::msg::PointCloud2</code> point cloud of objects <code>output/debug/ground_truth_objects</code> <code>autoware_auto_perception_msgs::msg::TrackedObjects</code> ground truth objects"},{"location":"simulator/dummy_perception_publisher/#parameters","title":"Parameters","text":"Name Type Default Value Explanation <code>visible_range</code> double 100.0 sensor visible range [m] <code>detection_successful_rate</code> double 0.8 sensor detection rate. (min) 0.0 - 1.0(max) <code>enable_ray_tracing</code> bool true if True, use ray tracking <code>use_object_recognition</code> bool true if True, publish objects topic <code>use_base_link_z</code> bool true if True, node uses z coordinate of ego base_link <code>publish_ground_truth</code> bool false if True, publish ground truth objects <code>use_fixed_random_seed</code> bool false if True, use fixed random seed <code>random_seed</code> int 0 random seed"},{"location":"simulator/dummy_perception_publisher/#node-parameters","title":"Node Parameters","text":"<p>None.</p>"},{"location":"simulator/dummy_perception_publisher/#core-parameters","title":"Core Parameters","text":"<p>None.</p>"},{"location":"simulator/dummy_perception_publisher/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"simulator/fault_injection/","title":"fault_injection","text":""},{"location":"simulator/fault_injection/#fault_injection","title":"fault_injection","text":""},{"location":"simulator/fault_injection/#purpose","title":"Purpose","text":"<p>This package is used to convert pseudo system faults from PSim to Diagnostics and notify Autoware. The component diagram is as follows:</p> <p></p>"},{"location":"simulator/fault_injection/#test","title":"Test","text":"<pre><code>source install/setup.bash\ncd fault_injection\nlaunch_test test/test_fault_injection_node.test.py\n</code></pre>"},{"location":"simulator/fault_injection/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"simulator/fault_injection/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"simulator/fault_injection/#input","title":"Input","text":"Name Type Description <code>~/input/simulation_events</code> <code>tier4_simulation_msgs::msg::SimulationEvents</code> simulation events"},{"location":"simulator/fault_injection/#output","title":"Output","text":"<p>None.</p>"},{"location":"simulator/fault_injection/#parameters","title":"Parameters","text":"<p>None.</p>"},{"location":"simulator/fault_injection/#node-parameters","title":"Node Parameters","text":"<p>None.</p>"},{"location":"simulator/fault_injection/#core-parameters","title":"Core Parameters","text":"<p>None.</p>"},{"location":"simulator/fault_injection/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"simulator/simple_planning_simulator/","title":"simple_planning_simulator","text":""},{"location":"simulator/simple_planning_simulator/#simple_planning_simulator","title":"simple_planning_simulator","text":""},{"location":"simulator/simple_planning_simulator/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>This node simulates the vehicle motion for a vehicle command in 2D using a simple vehicle model.</p>"},{"location":"simulator/simple_planning_simulator/#design","title":"Design","text":"<p>The purpose of this simulator is for the integration test of planning and control modules. This does not simulate sensing or perception, but is implemented in pure c++ only and works without GPU.</p>"},{"location":"simulator/simple_planning_simulator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>It simulates only in 2D motion.</li> <li>It does not perform physical operations such as collision and sensing, but only calculates the integral results of vehicle dynamics.</li> </ul>"},{"location":"simulator/simple_planning_simulator/#inputs-outputs-api","title":"Inputs / Outputs / API","text":""},{"location":"simulator/simple_planning_simulator/#input","title":"input","text":"<ul> <li>input/initialpose [<code>geometry_msgs/msg/PoseWithCovarianceStamped</code>] : for initial pose</li> <li>input/ackermann_control_command [<code>autoware_auto_msgs/msg/AckermannControlCommand</code>] : target command to drive a vehicle</li> <li>input/manual_ackermann_control_command [<code>autoware_auto_msgs/msg/AckermannControlCommand</code>] : manual target command to drive a vehicle (used when control_mode_request = Manual)</li> <li>input/gear_command [<code>autoware_auto_vehicle_msgs/msg/GearCommand</code>] : target gear command.</li> <li>input/manual_gear_command [<code>autoware_auto_vehicle_msgs/msg/GearCommand</code>] : target gear command (used when control_mode_request = Manual)</li> <li>input/turn_indicators_command [<code>autoware_auto_vehicle_msgs/msg/TurnIndicatorsCommand</code>] : target turn indicator command</li> <li>input/hazard_lights_command [<code>autoware_auto_vehicle_msgs/msg/HazardLightsCommand</code>] : target hazard lights command</li> <li>input/control_mode_request [<code>tier4_vehicle_msgs::srv::ControlModeRequest</code>] : mode change for Auto/Manual driving</li> </ul>"},{"location":"simulator/simple_planning_simulator/#output","title":"output","text":"<ul> <li>/tf [<code>tf2_msgs/msg/TFMessage</code>] : simulated vehicle pose (base_link)</li> <li>/output/odometry [<code>nav_msgs/msg/Odometry</code>] : simulated vehicle pose and twist</li> <li>/output/steering [<code>autoware_auto_vehicle_msgs/msg/SteeringReport</code>] : simulated steering angle</li> <li>/output/control_mode_report [<code>autoware_auto_vehicle_msgs/msg/ControlModeReport</code>] : current control mode (Auto/Manual)</li> <li>/output/gear_report [<code>autoware_auto_vehicle_msgs/msg/ControlModeReport</code>] : simulated gear</li> <li>/output/turn_indicators_report [<code>autoware_auto_vehicle_msgs/msg/ControlModeReport</code>] : simulated turn indicator status</li> <li>/output/hazard_lights_report [<code>autoware_auto_vehicle_msgs/msg/ControlModeReport</code>] : simulated hazard lights status</li> </ul>"},{"location":"simulator/simple_planning_simulator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"simulator/simple_planning_simulator/#common-parameters","title":"Common Parameters","text":"Name Type Description Default value simulated_frame_id string set to the child_frame_id in output tf \"base_link\" origin_frame_id string set to the frame_id in output tf \"odom\" initialize_source string If \"ORIGIN\", the initial pose is set at (0,0,0). If \"INITIAL_POSE_TOPIC\", node will wait until the <code>input/initialpose</code> topic is published. \"INITIAL_POSE_TOPIC\" add_measurement_noise bool If true, the Gaussian noise is added to the simulated results. true pos_noise_stddev double Standard deviation for position noise 0.01 rpy_noise_stddev double Standard deviation for Euler angle noise 0.0001 vel_noise_stddev double Standard deviation for longitudinal velocity noise 0.0 angvel_noise_stddev double Standard deviation for angular velocity noise 0.0 steer_noise_stddev double Standard deviation for steering angle noise 0.0001 measurement_steer_bias double Measurement bias for steering angle 0.0"},{"location":"simulator/simple_planning_simulator/#vehicle-model-parameters","title":"Vehicle Model Parameters","text":""},{"location":"simulator/simple_planning_simulator/#vehicle_model_type-options","title":"vehicle_model_type options","text":"<ul> <li><code>IDEAL_STEER_VEL</code></li> <li><code>IDEAL_STEER_ACC</code></li> <li><code>IDEAL_STEER_ACC_GEARED</code></li> <li><code>DELAY_STEER_VEL</code></li> <li><code>DELAY_STEER_ACC</code></li> <li><code>DELAY_STEER_ACC_GEARED</code></li> <li><code>DELAY_STEER_MAP_ACC_GEARED</code>: applies 1D dynamics and time delay to the steering and acceleration commands. The simulated acceleration is determined by a value converted through the provided acceleration map. This model is valuable for an accurate simulation with acceleration deviations in a real vehicle.</li> </ul> <p>The <code>IDEAL</code> model moves ideally as commanded, while the <code>DELAY</code> model moves based on a 1st-order with time delay model. The <code>STEER</code> means the model receives the steer command. The <code>VEL</code> means the model receives the target velocity command, while the <code>ACC</code> model receives the target acceleration command. The <code>GEARED</code> suffix means that the motion will consider the gear command: the vehicle moves only one direction following the gear.</p> <p>The table below shows which models correspond to what parameters. The model names are written in abbreviated form (e.g. IDEAL_STEER_VEL = I_ST_V).</p> Name Type Description I_ST_V I_ST_A I_ST_A_G D_ST_V D_ST_A D_ST_A_G D_ST_M_ACC_G Default value unit acc_time_delay double dead time for the acceleration input x x x x o o o 0.1 [s] steer_time_delay double dead time for the steering input x x x o o o o 0.24 [s] vel_time_delay double dead time for the velocity input x x x o x x x 0.25 [s] acc_time_constant double time constant of the 1st-order acceleration dynamics x x x x o o o 0.1 [s] steer_time_constant double time constant of the 1st-order steering dynamics x x x o o o o 0.27 [s] steer_dead_band double dead band for steering angle x x x o o o x 0.0 [rad] vel_time_constant double time constant of the 1st-order velocity dynamics x x x o x x x 0.5 [s] vel_lim double limit of velocity x x x o o o o 50.0 [m/s] vel_rate_lim double limit of acceleration x x x o o o o 7.0 [m/ss] steer_lim double limit of steering angle x x x o o o o 1.0 [rad] steer_rate_lim double limit of steering angle change rate x x x o o o o 5.0 [rad/s] debug_acc_scaling_factor double scaling factor for accel command x x x x o o x 1.0 [-] debug_steer_scaling_factor double scaling factor for steer command x x x x o o x 1.0 [-] acceleration_map_path string path to csv file for acceleration map which converts velocity and ideal acceleration to actual acceleration x x x x x x o - [-] <p>The <code>acceleration_map</code> is used only for <code>DELAY_STEER_MAP_ACC_GEARED</code> and it shows the acceleration command on the vertical axis and the current velocity on the horizontal axis, with each cell representing the converted acceleration command that is actually used in the simulator's motion calculation. Values in between are linearly interpolated.</p> <p>Example of <code>acceleration_map.csv</code></p> <pre><code>default,  0.00,  1.39,  2.78,  4.17,  5.56,  6.94,  8.33,  9.72, 11.11, 12.50, 13.89, 15.28, 16.67\n-4.0,    -4.40, -4.36, -4.38, -4.12, -4.20, -3.94, -3.98, -3.80, -3.77, -3.76, -3.59, -3.50, -3.40\n-3.5,    -4.00, -3.91, -3.85, -3.64, -3.68, -3.55, -3.42, -3.24, -3.25, -3.00, -3.04, -2.93, -2.80\n-3.0,    -3.40, -3.37, -3.33, -3.00, -3.00, -2.90, -2.88, -2.65, -2.43, -2.44, -2.43, -2.39, -2.30\n-2.5,    -2.80, -2.72, -2.72, -2.62, -2.41, -2.43, -2.26, -2.18, -2.11, -2.03, -1.96, -1.91, -1.85\n-2.0,    -2.30, -2.24, -2.12, -2.02, -1.92, -1.81, -1.67, -1.58, -1.51, -1.49, -1.40, -1.35, -1.30\n-1.5,    -1.70, -1.61, -1.47, -1.46, -1.40, -1.37, -1.29, -1.24, -1.10, -0.99, -0.83, -0.80, -0.78\n-1.0,    -1.30, -1.28, -1.10, -1.09, -1.04, -1.02, -0.98, -0.89, -0.82, -0.61, -0.52, -0.54, -0.56\n-0.8,    -0.96, -0.90, -0.82, -0.74, -0.70, -0.65, -0.63, -0.59, -0.55, -0.44, -0.39, -0.39, -0.35\n-0.6,    -0.77, -0.71, -0.67, -0.65, -0.58, -0.52, -0.51, -0.50, -0.40, -0.33, -0.30, -0.31, -0.30\n-0.4,    -0.45, -0.40, -0.45, -0.44, -0.38, -0.35, -0.31, -0.30, -0.26, -0.30, -0.29, -0.31, -0.25\n-0.2,    -0.24, -0.24, -0.25, -0.22, -0.23, -0.25, -0.27, -0.29, -0.24, -0.22, -0.17, -0.18, -0.12\n 0.0,     0.00,  0.00, -0.05, -0.05, -0.05, -0.05, -0.08, -0.08, -0.08, -0.08, -0.10, -0.10, -0.10\n 0.2,     0.16,  0.12,  0.02,  0.02,  0.00,  0.00, -0.05, -0.05, -0.05, -0.05, -0.08, -0.08, -0.08\n 0.4,     0.38,  0.30,  0.22,  0.25,  0.24,  0.23,  0.20,  0.16,  0.16,  0.14,  0.10,  0.05,  0.05\n 0.6,     0.52,  0.52,  0.51,  0.49,  0.43,  0.40,  0.35,  0.33,  0.33,  0.33,  0.32,  0.34,  0.34\n 0.8,     0.82,  0.81,  0.78,  0.68,  0.63,  0.56,  0.53,  0.48,  0.43,  0.41,  0.37,  0.38,  0.40\n 1.0,     1.00,  1.08,  1.01,  0.88,  0.76,  0.69,  0.66,  0.58,  0.54,  0.49,  0.45,  0.40,  0.40\n 1.5,     1.52,  1.50,  1.38,  1.26,  1.14,  1.03,  0.91,  0.82,  0.67,  0.61,  0.51,  0.41,  0.41\n 2.0,     1.80,  1.80,  1.64,  1.43,  1.25,  1.11,  0.96,  0.81,  0.70,  0.59,  0.51,  0.42,  0.42\n</code></pre> <p></p> <p>Note: The steering/velocity/acceleration dynamics is modeled by a first order system with a deadtime in a delay model. The definition of the time constant is the time it takes for the step response to rise up to 63% of its final value. The deadtime is a delay in the response to a control input.</p>"},{"location":"simulator/simple_planning_simulator/#default-tf-configuration","title":"Default TF configuration","text":"<p>Since the vehicle outputs <code>odom</code>-&gt;<code>base_link</code> tf, this simulator outputs the tf with the same frame_id configuration. In the simple_planning_simulator.launch.py, the node that outputs the <code>map</code>-&gt;<code>odom</code> tf, that usually estimated by the localization module (e.g. NDT), will be launched as well. Since the tf output by this simulator module is an ideal value, <code>odom</code>-&gt;<code>map</code> will always be 0.</p>"},{"location":"simulator/simple_planning_simulator/#caveat-pitch-calculation","title":"(Caveat) Pitch calculation","text":"<p>Ego vehicle pitch angle is calculated in the following manner.</p> <p></p> <p>NOTE: driving against the line direction (as depicted in image's bottom row) is not supported and only shown for illustration purposes.</p>"},{"location":"simulator/simple_planning_simulator/#error-detection-and-handling","title":"Error detection and handling","text":"<p>The only validation on inputs being done is testing for a valid vehicle model type.</p>"},{"location":"simulator/simple_planning_simulator/#security-considerations","title":"Security considerations","text":""},{"location":"simulator/simple_planning_simulator/#references-external-links","title":"References / External links","text":"<p>This is originally developed in the Autoware.AI. See the link below.</p> <p>https://github.com/Autoware-AI/simulation/tree/master/wf_simulator</p>"},{"location":"simulator/simple_planning_simulator/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":"<ul> <li>Improving the accuracy of vehicle models (e.g., adding steering dead zones and slip behavior)</li> <li>Cooperation with modules that output pseudo pointcloud or pseudo perception results</li> </ul>"},{"location":"system/autoware_auto_msgs_adapter/","title":"autoware_auto_msgs_adapter","text":""},{"location":"system/autoware_auto_msgs_adapter/#autoware_auto_msgs_adapter","title":"autoware_auto_msgs_adapter","text":"<p>This package is used to convert <code>autoware_msgs</code> to <code>autoware_auto_msgs</code>.</p>"},{"location":"system/autoware_auto_msgs_adapter/#purpose","title":"Purpose","text":"<p>As we transition from <code>autoware_auto_msgs</code> to <code>autoware_msgs</code>, we wanted to provide flexibility and compatibility for users who are still using <code>autoware_auto_msgs</code>.</p> <p>This adapter package allows users to easily convert messages between the two formats.</p>"},{"location":"system/autoware_auto_msgs_adapter/#capabilities","title":"Capabilities","text":"<p>The <code>autoware_auto_msgs_adapter</code> package provides the following capabilities:</p> <ul> <li>Conversion of supported <code>autoware_msgs</code> messages to <code>autoware_auto_msgs</code> messages.</li> <li>Can be extended to support conversion for any message type pairs.</li> <li>Each instance is designed to convert from a single source message type to a single target message type.</li> <li>Multiple instances can be launched to convert multiple message types.</li> <li>Can be launched as a standalone node or as a component.</li> </ul>"},{"location":"system/autoware_auto_msgs_adapter/#usage","title":"Usage","text":"<p>Customize the adapter configuration by replicating and editing the <code>adapter_control.param.yaml</code> file located in the <code>autoware_auto_msgs_adapter/config</code> directory. Example configuration:</p> <pre><code>/**:\nros__parameters:\nmsg_type_target: \"autoware_auto_control_msgs/msg/AckermannControlCommand\"\ntopic_name_source: \"/control/command/control_cmd\"\ntopic_name_target: \"/control/command/control_cmd_auto\"\n</code></pre> <p>Set the <code>msg_type_target</code> parameter to the desired target message type from <code>autoware_auto_msgs</code>.</p> <p>Make sure that the <code>msg_type_target</code> has the correspondence in either:</p> <ul> <li>schema/autoware_auto_msgs_adapter.schema.json</li> <li>OR src/autoware_auto_msgs_adapter_core.cpp <code>AutowareAutoMsgsAdapterNode::create_adapter_map()</code> method.</li> </ul> <p>(If this package is maintained correctly, they should match each other.)</p> <p>Launch the adapter node by any of the following methods:</p>"},{"location":"system/autoware_auto_msgs_adapter/#ros2-launch","title":"<code>ros2 launch</code>","text":"<pre><code>ros2 launch autoware_auto_msgs_adapter autoware_auto_msgs_adapter.launch.xml param_path:='full_path_to_param_file'\n</code></pre> <p>Make sure to set the <code>param_path</code> argument to the full path of the parameter file.</p> <p>Alternatively,</p> <ul> <li>You can replicate and edit the launch file to suit to your needs.</li> <li>You can make use of the existing launch file in another launch file by providing the parameter file path as an   argument.</li> </ul>"},{"location":"system/autoware_auto_msgs_adapter/#ros2-run","title":"<code>ros2 run</code>","text":"<pre><code>ros2 run autoware_auto_msgs_adapter autoware_auto_msgs_adapter_exe --ros-args --params-file 'full_path_to_param_file'\n</code></pre> <p>Make sure to set the <code>param_path</code> argument to the full path of the parameter file.</p>"},{"location":"system/autoware_auto_msgs_adapter/#contributing","title":"Contributing","text":""},{"location":"system/autoware_auto_msgs_adapter/#current-implementation-details","title":"Current implementation details","text":"<p>The entry point for the adapter executable is created with <code>RCLCPP_COMPONENTS_REGISTER_NODE</code> the autoware_auto_msgs_adapter_core.cpp.</p> <p>This allows it to be launched as a component or as a standalone node.</p> <p>In the <code>AutowareAutoMsgsAdapterNode</code> constructor, the adapter is selected by the type string provided in the configuration file. The adapter is then initialized with the topic names provided.</p> <p>The constructors of the adapters are responsible for creating the publisher and subscriber (which makes use of the conversion method).</p>"},{"location":"system/autoware_auto_msgs_adapter/#adding-a-new-message-pair","title":"Adding a new message pair","text":"<p>To add a new message pair,</p> <ul> <li>Replicate and edit:<ul> <li>adapter_control.hpp.</li> <li>Add the new header file to the CMakeLists.txt.</li> </ul> </li> <li>Add a new entry to the returned map instance in the <code>AutowareAutoMsgsAdapterNode::create_adapter_map()</code> method of the adapter node:<ul> <li>autoware_auto_msgs_adapter_core.cpp</li> </ul> </li> <li>Add a new entry to the schema/autoware_auto_msgs_adapter.schema.json file in the <code>definitions:autoware_auto_msgs_adapter:properties:msg_type_target:enum</code> section.<ul> <li>Learn more about JSON schema usage in here.</li> </ul> </li> <li>Create a new config file by replicating and editing:<ul> <li>adapter_control.param.yaml</li> </ul> </li> <li>Add a new test file by replicating and editing:<ul> <li>test_msg_ackermann_control_command.cpp</li> <li>No need to edit the <code>CMakeLists.txt</code> file as it will automatically detect the new test file.</li> </ul> </li> </ul> <p>Also make sure to test the new adapter with:</p> <pre><code>colcon test --event-handlers console_cohesion+ --packages-select autoware_auto_msgs_adapter\n</code></pre>"},{"location":"system/bluetooth_monitor/","title":"bluetooth_monitor","text":""},{"location":"system/bluetooth_monitor/#macro-rendering-error","title":"Macro Rendering Error","text":"<p>File: <code>system/bluetooth_monitor/README.md</code></p> <p>FileNotFoundError: [Errno 2] No such file or directory: 'system/bluetooth_monitor/schema/bluetooth_monitor.schema.json'</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/mkdocs_macros/plugin.py\", line 527, in render\n    return md_template.render(**page_variables)\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/jinja2/environment.py\", line 1301, in render\n    self.environment.handle_exception()\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/jinja2/environment.py\", line 936, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 49, in top-level template code\n  File \"/home/runner/work/autoware.universe/autoware.universe/mkdocs_macros.py\", line 68, in json_to_markdown\n    with open(json_schema_file_path) as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'system/bluetooth_monitor/schema/bluetooth_monitor.schema.json'\n</code></pre>"},{"location":"system/component_state_monitor/","title":"component_state_monitor","text":""},{"location":"system/component_state_monitor/#component_state_monitor","title":"component_state_monitor","text":"<p>The component state monitor checks the state of each component using topic state monitor. This is an implementation for backward compatibility with the AD service state monitor. It will be replaced in the future using a diagnostics tree.</p>"},{"location":"system/default_ad_api/","title":"default_ad_api","text":""},{"location":"system/default_ad_api/#default_ad_api","title":"default_ad_api","text":""},{"location":"system/default_ad_api/#features","title":"Features","text":"<p>This package is a default implementation AD API.</p> <ul> <li>autoware state (backward compatibility)</li> <li>fail-safe</li> <li>interface</li> <li>localization</li> <li>motion</li> <li>operation mode</li> <li>routing</li> </ul>"},{"location":"system/default_ad_api/#web-server-script","title":"Web server script","text":"<p>This is a sample to call API using HTTP.</p>"},{"location":"system/default_ad_api/#guide-message-script","title":"Guide message script","text":"<p>This is a debug script to check the conditions for transition to autonomous mode.</p> <pre><code>$ ros2 run default_ad_api guide.py\n\nThe vehicle pose is not estimated. Please set an initial pose or check GNSS.\nThe route is not set. Please set a goal pose.\nThe topic rate error is detected. Please check [control,planning] components.\nThe vehicle is ready. Please change the operation mode to autonomous.\nThe vehicle is driving autonomously.\nThe vehicle has reached the goal of the route. Please reset a route.\n</code></pre>"},{"location":"system/default_ad_api/document/autoware-state/","title":"Autoware state compatibility","text":""},{"location":"system/default_ad_api/document/autoware-state/#autoware-state-compatibility","title":"Autoware state compatibility","text":""},{"location":"system/default_ad_api/document/autoware-state/#overview","title":"Overview","text":"<p>Since <code>/autoware/state</code> was so widely used, default_ad_api creates it from the states of AD API for backwards compatibility. The diagnostic checks that ad_service_state_monitor used to perform have been replaced by component_state_monitor. The service <code>/autoware/shutdown</code> to change autoware state to finalizing is also supported for compatibility.</p> <p></p>"},{"location":"system/default_ad_api/document/autoware-state/#conversion","title":"Conversion","text":"<p>This is the correspondence between AD API states and autoware states. The launch state is the data that default_ad_api node holds internally.</p> <p></p>"},{"location":"system/default_ad_api/document/fail-safe/","title":"Fail-safe API","text":""},{"location":"system/default_ad_api/document/fail-safe/#fail-safe-api","title":"Fail-safe API","text":""},{"location":"system/default_ad_api/document/fail-safe/#overview","title":"Overview","text":"<p>The fail-safe API simply relays the MRM state. See the autoware-documentation for AD API specifications.</p>"},{"location":"system/default_ad_api/document/interface/","title":"Interface API","text":""},{"location":"system/default_ad_api/document/interface/#interface-api","title":"Interface API","text":""},{"location":"system/default_ad_api/document/interface/#overview","title":"Overview","text":"<p>The interface API simply returns a version number. See the autoware-documentation for AD API specifications.</p>"},{"location":"system/default_ad_api/document/localization/","title":"Localization API","text":""},{"location":"system/default_ad_api/document/localization/#localization-api","title":"Localization API","text":""},{"location":"system/default_ad_api/document/localization/#overview","title":"Overview","text":"<p>Unify the location initialization method to the service. The topic <code>/initialpose</code> from rviz is now only subscribed to by adapter node and converted to API call. This API call is forwarded to the pose initializer node so it can centralize the state of pose initialization. For other nodes that require initialpose, pose initializer node publishes as <code>/initialpose3d</code>. See the autoware-documentation for AD API specifications.</p> <p></p>"},{"location":"system/default_ad_api/document/motion/","title":"Motion API","text":""},{"location":"system/default_ad_api/document/motion/#motion-api","title":"Motion API","text":""},{"location":"system/default_ad_api/document/motion/#overview","title":"Overview","text":"<p>Provides a hook for when the vehicle starts. It is typically used for announcements that call attention to the surroundings. Add a pause function to the vehicle_cmd_gate, and API will control it based on vehicle stopped and start requested. See the autoware-documentation for AD API specifications.</p> <p></p>"},{"location":"system/default_ad_api/document/motion/#states","title":"States","text":"<p>The implementation has more detailed state transitions to manage pause state synchronization. The correspondence with the AD API state is as follows.</p> <p></p>"},{"location":"system/default_ad_api/document/operation-mode/","title":"Operation mode API","text":""},{"location":"system/default_ad_api/document/operation-mode/#operation-mode-api","title":"Operation mode API","text":""},{"location":"system/default_ad_api/document/operation-mode/#overview","title":"Overview","text":"<p>Introduce operation mode. It handles autoware engage, gate_mode, external_cmd_selector and control_mode abstractly. When the mode is changed, it will be in-transition state, and if the transition completion condition to that mode is not satisfied, it will be returned to the previous mode. Also, currently, the condition for mode change is only <code>WaitingForEngage</code> in <code>/autoware/state</code>, and the engage state is shared between modes. After introducing the operation mode, each mode will have a transition available flag. See the autoware-documentation for AD API specifications.</p> <p></p>"},{"location":"system/default_ad_api/document/operation-mode/#states","title":"States","text":"<p>The operation mode has the following state transitions. Disabling autoware control and changing operation mode when autoware control is disabled can be done immediately. Otherwise, enabling autoware control and changing operation mode when autoware control is enabled causes the state will be transition state. If the mode change completion condition is not satisfied within the timeout in the transition state, it will return to the previous mode.</p> <p></p>"},{"location":"system/default_ad_api/document/operation-mode/#compatibility","title":"Compatibility","text":"<p>Ideally, vehicle_cmd_gate and external_cmd_selector should be merged so that the operation mode can be handled directly. However, currently the operation mode transition manager performs the following conversions to match the implementation. The transition manager monitors each topic in the previous interface and synchronizes the operation mode when it changes. When the operation mode is changed with the new interface, the transition manager disables synchronization and changes the operation mode using the previous interface.</p> <p></p>"},{"location":"system/default_ad_api/document/routing/","title":"Routing API","text":""},{"location":"system/default_ad_api/document/routing/#routing-api","title":"Routing API","text":""},{"location":"system/default_ad_api/document/routing/#overview","title":"Overview","text":"<p>Unify the route setting method to the service. This API supports two waypoint formats, poses and lanelet segments. The goal and checkpoint topics from rviz is only subscribed to by adapter node and converted to API call. This API call is forwarded to the mission planner node so it can centralize the state of routing. For other nodes that require route, mission planner node publishes as <code>/planning/mission_planning/route</code>. See the autoware-documentation for AD API specifications.</p> <p></p>"},{"location":"system/default_ad_api_helpers/ad_api_adaptors/","title":"ad_api_adaptors","text":""},{"location":"system/default_ad_api_helpers/ad_api_adaptors/#ad_api_adaptors","title":"ad_api_adaptors","text":""},{"location":"system/default_ad_api_helpers/ad_api_adaptors/#initial_pose_adaptor","title":"initial_pose_adaptor","text":"<p>This node makes it easy to use the localization AD API from RViz. When a initial pose topic is received, call the localization initialize API. This node depends on the map height fitter library. See here for more details.</p> Interface Local Name Global Name Description Subscription initialpose /initialpose The pose for localization initialization. Client - /api/localization/initialize The localization initialize API."},{"location":"system/default_ad_api_helpers/ad_api_adaptors/#routing_adaptor","title":"routing_adaptor","text":"<p>This node makes it easy to use the routing AD API from RViz. When a goal pose topic is received, reset the waypoints and call the API. When a waypoint pose topic is received, append it to the end of the waypoints to call the API. The clear API is called automatically before setting the route.</p> Interface Local Name Global Name Description Subscription - /api/routing/state The state of the routing API. Subscription ~/input/fixed_goal /planning/mission_planning/goal The goal pose of route. Disable goal modification. Subscription ~/input/rough_goal /rviz/routing/rough_goal The goal pose of route. Enable goal modification. Subscription ~/input/reroute /rviz/routing/reroute The goal pose of reroute. Subscription ~/input/waypoint /planning/mission_planning/checkpoint The waypoint pose of route. Client - /api/routing/clear_route The route clear API. Client - /api/routing/set_route_points The route points set API. Client - /api/routing/change_route_points The route points change API."},{"location":"system/default_ad_api_helpers/automatic_pose_initializer/","title":"automatic_pose_initializer","text":""},{"location":"system/default_ad_api_helpers/automatic_pose_initializer/#automatic_pose_initializer","title":"automatic_pose_initializer","text":""},{"location":"system/default_ad_api_helpers/automatic_pose_initializer/#automatic_pose_initializer_1","title":"automatic_pose_initializer","text":"<p>This node calls localization initialize API when the localization initialization state is uninitialized. Since the API uses GNSS pose when no pose is specified, initialization using GNSS can be performed automatically.</p> Interface Local Name Global Name Description Subscription - /api/localization/initialization_state The localization initialization state API. Client - /api/localization/initialize The localization initialize API."},{"location":"system/diagnostic_graph_aggregator/","title":"diagnostic_graph_aggregator","text":""},{"location":"system/diagnostic_graph_aggregator/#diagnostic_graph_aggregator","title":"diagnostic_graph_aggregator","text":""},{"location":"system/diagnostic_graph_aggregator/#overview","title":"Overview","text":"<p>The diagnostic graph aggregator node subscribes to diagnostic array and publishes aggregated diagnostic graph. As shown in the diagram below, this node introduces extra diagnostic status for intermediate functional unit. Diagnostic status dependencies will be directed acyclic graph (DAG).</p> <p></p>"},{"location":"system/diagnostic_graph_aggregator/#diagnostics-graph-message","title":"Diagnostics graph message","text":"<p>The diagnostics graph that this node outputs is a combination of diagnostic status and connections between them. This graph consists of an array of diagnostic nodes, and each node has a status and links. This link contains an index indicating the position of the node in the graph. Therefore, the graph can be reconstructed from the array of nodes using links. The following is an example of a message representing the graph in the overview section.</p> <p></p>"},{"location":"system/diagnostic_graph_aggregator/#operation-mode-availability","title":"Operation mode availability","text":"<p>For MRM, this node publishes the status of the top-level functional units in the dedicated message. Therefore, the diagnostic graph must contain functional units with the following names. This feature breaks the generality of the graph and may be changed to a plugin or another node in the future.</p> <ul> <li>/autoware/operation/stop</li> <li>/autoware/operation/autonomous</li> <li>/autoware/operation/local</li> <li>/autoware/operation/remote</li> <li>/autoware/operation/emergency-stop</li> <li>/autoware/operation/comfortable-stop</li> <li>/autoware/operation/pull-over</li> </ul>"},{"location":"system/diagnostic_graph_aggregator/#interfaces","title":"Interfaces","text":"Interface Type Interface Name Data Type Description subscription <code>/diagnostics</code> <code>diagnostic_msgs/msg/DiagnosticArray</code> Diagnostics input. publisher <code>/diagnostics_graph</code> <code>tier4_system_msgs/msg/DiagnosticGraph</code> Diagnostics graph. publisher <code>/system/operation_mode/availability</code> <code>tier4_system_msgs/msg/OperationModeAvailability</code> mode availability."},{"location":"system/diagnostic_graph_aggregator/#parameters","title":"Parameters","text":"Parameter Name Data Type Description <code>graph_file</code> <code>string</code> Path of the config file. <code>rate</code> <code>double</code> Rate of aggregation and topic publication. <code>input_qos_depth</code> <code>uint</code> QoS depth of input array topic. <code>graph_qos_depth</code> <code>uint</code> QoS depth of output graph topic. <code>use_operation_mode_availability</code> <code>bool</code> Use operation mode availability publisher. <code>use_debug_mode</code> <code>bool</code> Use debug output to stdout."},{"location":"system/diagnostic_graph_aggregator/#examples","title":"Examples","text":"<ul> <li>example_0.yaml</li> <li>example_1.yaml</li> <li>example_2.yaml</li> </ul> <pre><code>ros2 launch diagnostic_graph_aggregator example.launch.xml\n</code></pre>"},{"location":"system/diagnostic_graph_aggregator/#graph-file-format","title":"Graph file format","text":"<ul> <li>GraphFile</li> <li>Path</li> <li>Node<ul> <li>Diag</li> <li>Unit</li> <li>And</li> <li>Or</li> </ul> </li> </ul>"},{"location":"system/diagnostic_graph_aggregator/doc/format/and/","title":"Unit","text":""},{"location":"system/diagnostic_graph_aggregator/doc/format/and/#unit","title":"Unit","text":"<p>And is a node that is evaluated as the AND of the input nodes.</p>"},{"location":"system/diagnostic_graph_aggregator/doc/format/and/#format","title":"Format","text":"Name Type Required Description type string yes Specify <code>and</code> when using this object. name string yes Name of diagnostic status. list List&lt;Diag|Unit&gt; yes List of input node references."},{"location":"system/diagnostic_graph_aggregator/doc/format/diag/","title":"Diag","text":""},{"location":"system/diagnostic_graph_aggregator/doc/format/diag/#diag","title":"Diag","text":"<p>Diag is a node that refers to a source diagnostics.</p>"},{"location":"system/diagnostic_graph_aggregator/doc/format/diag/#format","title":"Format","text":"Name Type Required Description type string yes Specify <code>diag</code> when using this object. diag string yes Name of diagnostic status."},{"location":"system/diagnostic_graph_aggregator/doc/format/graph-file/","title":"GraphFile","text":""},{"location":"system/diagnostic_graph_aggregator/doc/format/graph-file/#graphfile","title":"GraphFile","text":"<p>GraphFile is the top level object that makes up the configuration file.</p>"},{"location":"system/diagnostic_graph_aggregator/doc/format/graph-file/#format","title":"Format","text":"Name Type Required Description files List&lt;Path&gt; no Paths of the files to include. nodes List&lt;Node&gt; no Nodes of the diagnostic graph."},{"location":"system/diagnostic_graph_aggregator/doc/format/node/","title":"Node","text":""},{"location":"system/diagnostic_graph_aggregator/doc/format/node/#node","title":"Node","text":"<p>Node is a base object that makes up the diagnostic graph.</p>"},{"location":"system/diagnostic_graph_aggregator/doc/format/node/#format","title":"Format","text":"Name Type Required Description type string yes Node type. See derived objects for details."},{"location":"system/diagnostic_graph_aggregator/doc/format/or/","title":"Unit","text":""},{"location":"system/diagnostic_graph_aggregator/doc/format/or/#unit","title":"Unit","text":"<p>Or is a node that is evaluated as the OR of the input nodes.</p>"},{"location":"system/diagnostic_graph_aggregator/doc/format/or/#format","title":"Format","text":"Name Type Required Description type string yes Specify <code>or</code> when using this object. name string yes Name of diagnostic status. list List&lt;Diag|Unit&gt; yes List of input node references."},{"location":"system/diagnostic_graph_aggregator/doc/format/path/","title":"Path","text":""},{"location":"system/diagnostic_graph_aggregator/doc/format/path/#path","title":"Path","text":"<p>Path is an object that indicates the path of the file to include.</p>"},{"location":"system/diagnostic_graph_aggregator/doc/format/path/#format","title":"Format","text":"Name Type Required Description package string yes Package name. path string yes Relative path in the package."},{"location":"system/diagnostic_graph_aggregator/doc/format/unit/","title":"Unit","text":""},{"location":"system/diagnostic_graph_aggregator/doc/format/unit/#unit","title":"Unit","text":"<p>Diag is a node that refers to a functional unit.</p>"},{"location":"system/diagnostic_graph_aggregator/doc/format/unit/#format","title":"Format","text":"Name Type Required Description type string yes Specify <code>unit</code> when using this object. name string yes Name of diagnostic status."},{"location":"system/dummy_diag_publisher/","title":"dummy_diag_publisher","text":""},{"location":"system/dummy_diag_publisher/#dummy_diag_publisher","title":"dummy_diag_publisher","text":""},{"location":"system/dummy_diag_publisher/#purpose","title":"Purpose","text":"<p>This package outputs a dummy diagnostic data for debugging and developing.</p>"},{"location":"system/dummy_diag_publisher/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/dummy_diag_publisher/#outputs","title":"Outputs","text":"Name Type Description <code>/diagnostics</code> <code>diagnostic_msgs::msgs::DiagnosticArray</code> Diagnostics outputs"},{"location":"system/dummy_diag_publisher/#parameters","title":"Parameters","text":""},{"location":"system/dummy_diag_publisher/#node-parameters","title":"Node Parameters","text":"<p>The parameter <code>DIAGNOSTIC_NAME</code> must be a name that exists in the parameter YAML file. If the parameter <code>status</code> is given from a command line, the parameter <code>is_active</code> is automatically set to <code>true</code>.</p> Name Type Default Value Explanation Reconfigurable <code>update_rate</code> int <code>10</code> Timer callback period [Hz] false <code>DIAGNOSTIC_NAME.is_active</code> bool <code>true</code> Force update or not true <code>DIAGNOSTIC_NAME.status</code> string <code>\"OK\"</code> diag status set by dummy diag publisher true"},{"location":"system/dummy_diag_publisher/#yaml-format-for-dummy_diag_publisher","title":"YAML format for dummy_diag_publisher","text":"<p>If the value is <code>default</code>, the default value will be set.</p> Key Type Default Value Explanation <code>required_diags.DIAGNOSTIC_NAME.is_active</code> bool <code>true</code> Force update or not <code>required_diags.DIAGNOSTIC_NAME.status</code> string <code>\"OK\"</code> diag status set by dummy diag publisher"},{"location":"system/dummy_diag_publisher/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/dummy_diag_publisher/#usage","title":"Usage","text":""},{"location":"system/dummy_diag_publisher/#launch","title":"launch","text":"<pre><code>ros2 launch dummy_diag_publisher dummy_diag_publisher.launch.xml\n</code></pre>"},{"location":"system/dummy_diag_publisher/#reconfigure","title":"reconfigure","text":"<pre><code>ros2 param set /dummy_diag_publisher velodyne_connection.status \"Warn\"\nros2 param set /dummy_diag_publisher velodyne_connection.is_active true\n</code></pre>"},{"location":"system/dummy_infrastructure/","title":"dummy_infrastructure","text":""},{"location":"system/dummy_infrastructure/#dummy_infrastructure","title":"dummy_infrastructure","text":"<p>This is a debug node for infrastructure communication.</p>"},{"location":"system/dummy_infrastructure/#usage","title":"Usage","text":"<pre><code>ros2 launch dummy_infrastructure dummy_infrastructure.launch.xml\nros2 run rqt_reconfigure rqt_reconfigure\n</code></pre>"},{"location":"system/dummy_infrastructure/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/dummy_infrastructure/#inputs","title":"Inputs","text":"Name Type Description <code>~/input/command_array</code> <code>tier4_v2x_msgs::msg::InfrastructureCommandArray</code> Infrastructure command"},{"location":"system/dummy_infrastructure/#outputs","title":"Outputs","text":"Name Type Description <code>~/output/state_array</code> <code>tier4_v2x_msgs::msg::VirtualTrafficLightStateArray</code> Virtual traffic light array"},{"location":"system/dummy_infrastructure/#parameters","title":"Parameters","text":""},{"location":"system/dummy_infrastructure/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Explanation <code>update_rate</code> int <code>10</code> Timer callback period [Hz] <code>use_first_command</code> bool <code>true</code> Consider instrument id or not <code>instrument_id</code> string `` Used as command id <code>approval</code> bool <code>false</code> set approval filed to ros param <code>is_finalized</code> bool <code>false</code> Stop at stop_line if finalization isn't completed"},{"location":"system/dummy_infrastructure/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/duplicated_node_checker/","title":"Duplicated Node Checker","text":""},{"location":"system/duplicated_node_checker/#duplicated-node-checker","title":"Duplicated Node Checker","text":""},{"location":"system/duplicated_node_checker/#purpose","title":"Purpose","text":"<p>This node monitors the ROS 2 environments and detect duplication of node names in the environment. The result is published as diagnostics.</p>"},{"location":"system/duplicated_node_checker/#standalone-startup","title":"Standalone Startup","text":"<pre><code>ros2 launch duplicated_node_checker duplicated_node_checker.launch.xml\n</code></pre>"},{"location":"system/duplicated_node_checker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The types of topic status and corresponding diagnostic status are following.</p> Duplication status Diagnostic status Description <code>OK</code> OK No duplication is detected <code>Duplicated Detected</code> ERROR Duplication is detected"},{"location":"system/duplicated_node_checker/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/duplicated_node_checker/#output","title":"Output","text":"Name Type Description <code>/diagnostics</code> <code>diagnostic_msgs/DiagnosticArray</code> Diagnostics outputs"},{"location":"system/duplicated_node_checker/#parameters","title":"Parameters","text":"Name Type Description Default Range update_rate float The scanning and update frequency of the checker. 10 &gt;2"},{"location":"system/duplicated_node_checker/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/emergency_handler/","title":"emergency_handler","text":""},{"location":"system/emergency_handler/#emergency_handler","title":"emergency_handler","text":""},{"location":"system/emergency_handler/#purpose","title":"Purpose","text":"<p>Emergency Handler is a node to select proper MRM from from system failure state contained in HazardStatus.</p>"},{"location":"system/emergency_handler/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"system/emergency_handler/#state-transitions","title":"State Transitions","text":""},{"location":"system/emergency_handler/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/emergency_handler/#input","title":"Input","text":"Name Type Description <code>/system/emergency/hazard_status</code> <code>autoware_auto_system_msgs::msg::HazardStatusStamped</code> Used to select proper MRM from system failure state contained in HazardStatus <code>/control/vehicle_cmd</code> <code>autoware_auto_control_msgs::msg::AckermannControlCommand</code> Used as reference when generate Emergency Control Command <code>/localization/kinematic_state</code> <code>nav_msgs::msg::Odometry</code> Used to decide whether vehicle is stopped or not <code>/vehicle/status/control_mode</code> <code>autoware_auto_vehicle_msgs::msg::ControlModeReport</code> Used to check vehicle mode: autonomous or manual <code>/system/api/mrm/comfortable_stop/status</code> <code>tier4_system_msgs::msg::MrmBehaviorStatus</code> Used to check if MRM comfortable stop operation is available <code>/system/api/mrm/emergency_stop/status</code> <code>tier4_system_msgs::msg::MrmBehaviorStatus</code> Used to check if MRM emergency stop operation is available"},{"location":"system/emergency_handler/#output","title":"Output","text":"Name Type Description <code>/system/emergency/shift_cmd</code> <code>autoware_auto_vehicle_msgs::msg::GearCommand</code> Required to execute proper MRM (send gear cmd) <code>/system/emergency/hazard_cmd</code> <code>autoware_auto_vehicle_msgs::msg::HazardLightsCommand</code> Required to execute proper MRM (send turn signal cmd) <code>/api/fail_safe/mrm_state</code> <code>autoware_adapi_v1_msgs::msg::MrmState</code> Inform MRM execution state and selected MRM behavior <code>/system/api/mrm/comfortable_stop/operate</code> <code>tier4_system_msgs::srv::OperateMrm</code> Execution order for MRM comfortable stop <code>/system/api/mrm/emergency_stop/operate</code> <code>tier4_system_msgs::srv::OperateMrm</code> Execution order for MRM emergency stop"},{"location":"system/emergency_handler/#parameters","title":"Parameters","text":"Name Type Description Default Range update_rate integer Timer callback period. 10 N/A timeout_hazard_status float If the input <code>hazard_status</code> topic cannot be received for more than <code>timeout_hazard_status</code>, vehicle will make an emergency stop. 0.5 N/A timeout_takeover_request float Transition to MRR_OPERATING if the time from the last takeover request exceeds <code>timeout_takeover_request</code>. 10.0 N/A use_takeover_request boolean If this parameter is true, the handler will record the time and make take over request to the driver when emergency state occurs. false N/A use_parking_after_stopped boolean If this parameter is true, it will publish PARKING shift command. false N/A use_comfortable_stop boolean If this parameter is true, operate comfortable stop when latent faults occur. false N/A turning_hazard_on.emergency boolean If this parameter is true, hazard lamps will be turned on during emergency state. true N/A"},{"location":"system/emergency_handler/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/mrm_comfortable_stop_operator/","title":"mrm_comfortable_stop_operator","text":""},{"location":"system/mrm_comfortable_stop_operator/#mrm_comfortable_stop_operator","title":"mrm_comfortable_stop_operator","text":""},{"location":"system/mrm_comfortable_stop_operator/#purpose","title":"Purpose","text":"<p>MRM comfortable stop operator is a node that generates comfortable stop commands according to the comfortable stop MRM order.</p>"},{"location":"system/mrm_comfortable_stop_operator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"system/mrm_comfortable_stop_operator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/mrm_comfortable_stop_operator/#input","title":"Input","text":"Name Type Description <code>~/input/mrm/comfortable_stop/operate</code> <code>tier4_system_msgs::srv::OperateMrm</code> MRM execution order"},{"location":"system/mrm_comfortable_stop_operator/#output","title":"Output","text":"Name Type Description <code>~/output/mrm/comfortable_stop/status</code> <code>tier4_system_msgs::msg::MrmBehaviorStatus</code> MRM execution status <code>~/output/velocity_limit</code> <code>tier4_planning_msgs::msg::VelocityLimit</code> Velocity limit command <code>~/output/velocity_limit/clear</code> <code>tier4_planning_msgs::msg::VelocityLimitClearCommand</code> Velocity limit clear command"},{"location":"system/mrm_comfortable_stop_operator/#parameters","title":"Parameters","text":""},{"location":"system/mrm_comfortable_stop_operator/#node-parameters","title":"Node Parameters","text":"Name Type Default value Explanation update_rate int <code>10</code> Timer callback frequency [Hz]"},{"location":"system/mrm_comfortable_stop_operator/#core-parameters","title":"Core Parameters","text":"Name Type Default value Explanation min_acceleration double <code>-1.0</code> Minimum acceleration for comfortable stop [m/s^2] max_jerk double <code>0.3</code> Maximum jerk for comfortable stop [m/s^3] min_jerk double <code>-0.3</code> Minimum jerk for comfortable stop [m/s^3]"},{"location":"system/mrm_comfortable_stop_operator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/mrm_emergency_stop_operator/","title":"mrm_emergency_stop_operator","text":""},{"location":"system/mrm_emergency_stop_operator/#mrm_emergency_stop_operator","title":"mrm_emergency_stop_operator","text":""},{"location":"system/mrm_emergency_stop_operator/#purpose","title":"Purpose","text":"<p>MRM emergency stop operator is a node that generates emergency stop commands according to the emergency stop MRM order.</p>"},{"location":"system/mrm_emergency_stop_operator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"system/mrm_emergency_stop_operator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/mrm_emergency_stop_operator/#input","title":"Input","text":"Name Type Description <code>~/input/mrm/emergency_stop/operate</code> <code>tier4_system_msgs::srv::OperateMrm</code> MRM execution order <code>~/input/control/control_cmd</code> <code>autoware_auto_control_msgs::msg::AckermannControlCommand</code> Control command output from the last node of the control component. Used for the initial value of the emergency stop command."},{"location":"system/mrm_emergency_stop_operator/#output","title":"Output","text":"Name Type Description <code>~/output/mrm/emergency_stop/status</code> <code>tier4_system_msgs::msg::MrmBehaviorStatus</code> MRM execution status <code>~/output/mrm/emergency_stop/control_cmd</code> <code>autoware_auto_control_msgs::msg::AckermannControlCommand</code> Emergency stop command"},{"location":"system/mrm_emergency_stop_operator/#parameters","title":"Parameters","text":""},{"location":"system/mrm_emergency_stop_operator/#node-parameters","title":"Node Parameters","text":"Name Type Default value Explanation update_rate int <code>30</code> Timer callback frequency [Hz]"},{"location":"system/mrm_emergency_stop_operator/#core-parameters","title":"Core Parameters","text":"Name Type Default value Explanation target_acceleration double <code>-2.5</code> Target acceleration for emergency stop [m/s^2] target_jerk double <code>-1.5</code> Target jerk for emergency stop [m/s^3]"},{"location":"system/mrm_emergency_stop_operator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/system_error_monitor/","title":"system_error_monitor","text":""},{"location":"system/system_error_monitor/#system_error_monitor","title":"system_error_monitor","text":""},{"location":"system/system_error_monitor/#purpose","title":"Purpose","text":"<p>Autoware Error Monitor has two main functions.</p> <ol> <li>It is to judge the system hazard level from the aggregated diagnostic information of each module of Autoware.</li> <li>It enables automatic recovery from the emergency state.</li> </ol>"},{"location":"system/system_error_monitor/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"system/system_error_monitor/#state-transition","title":"State Transition","text":""},{"location":"system/system_error_monitor/#updateemergencyholdingcondition-flow-chart","title":"updateEmergencyHoldingCondition Flow Chart","text":""},{"location":"system/system_error_monitor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/system_error_monitor/#input","title":"Input","text":"Name Type Description <code>/diagnostics_agg</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> Diagnostic information aggregated based diagnostic_aggregator setting is used to <code>/autoware/state</code> <code>autoware_auto_system_msgs::msg::AutowareState</code> Required to ignore error during Route, Planning and Finalizing. <code>/control/current_gate_mode</code> <code>tier4_control_msgs::msg::GateMode</code> Required to select the appropriate module from <code>autonomous_driving</code> or <code>external_control</code> <code>/vehicle/control_mode</code> <code>autoware_auto_vehicle_msgs::msg::ControlModeReport</code> Required to not hold emergency during manual driving"},{"location":"system/system_error_monitor/#output","title":"Output","text":"Name Type Description <code>/system/emergency/hazard_status</code> <code>autoware_auto_system_msgs::msg::HazardStatusStamped</code> HazardStatus contains system hazard level, emergency hold status and failure details <code>/diagnostics_err</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> This has the same contents as HazardStatus. This is used for visualization"},{"location":"system/system_error_monitor/#parameters","title":"Parameters","text":""},{"location":"system/system_error_monitor/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Explanation <code>ignore_missing_diagnostics</code> bool <code>false</code> If this parameter is turned off, it will be ignored if required modules have not been received. <code>add_leaf_diagnostics</code> bool <code>true</code> Required to use children diagnostics. <code>diag_timeout_sec</code> double <code>1.0</code> (sec) If required diagnostic is not received for a <code>diag_timeout_sec</code>, the diagnostic state become STALE state. <code>data_ready_timeout</code> double <code>30.0</code> If input topics required for system_error_monitor are not available for <code>data_ready_timeout</code> seconds, autoware_state will translate to emergency state. <code>data_heartbeat_timeout</code> double <code>1.0</code> If input topics required for system_error_monitor are not no longer subscribed for <code>data_heartbeat_timeout</code> seconds, autoware_state will translate to emergency state."},{"location":"system/system_error_monitor/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Explanation <code>hazard_recovery_timeout</code> double <code>5.0</code> The vehicle can recovery to normal driving if emergencies disappear during <code>hazard_recovery_timeout</code>. <code>use_emergency_hold</code> bool <code>false</code> If it is false, the vehicle will return to normal as soon as emergencies disappear. <code>use_emergency_hold_in_manual_driving</code> bool <code>false</code> If this parameter is turned off, emergencies will be ignored during manual driving. <code>emergency_hazard_level</code> int <code>2</code> If hazard_level is more than emergency_hazard_level, autoware state will translate to emergency state"},{"location":"system/system_error_monitor/#yaml-format-for-system_error_monitor","title":"YAML format for system_error_monitor","text":"<p>The parameter key should be filled with the hierarchical diagnostics output by diagnostic_aggregator. Parameters prefixed with <code>required_modules.autonomous_driving</code> are for autonomous driving. Parameters with the <code>required_modules.remote_control</code> prefix are for remote control. If the value is <code>default</code>, the default value will be set.</p> Key Type Default Value Explanation <code>required_modules.autonomous_driving.DIAGNOSTIC_NAME.sf_at</code> string <code>\"none\"</code> Diagnostic level where it becomes Safe Fault. Available options are <code>\"none\"</code>, <code>\"warn\"</code>, <code>\"error\"</code>. <code>required_modules.autonomous_driving.DIAGNOSTIC_NAME.lf_at</code> string <code>\"warn\"</code> Diagnostic level where it becomes Latent Fault. Available options are <code>\"none\"</code>, <code>\"warn\"</code>, <code>\"error\"</code>. <code>required_modules.autonomous_driving.DIAGNOSTIC_NAME.spf_at</code> string <code>\"error\"</code> Diagnostic level where it becomes Single Point Fault. Available options are <code>\"none\"</code>, <code>\"warn\"</code>, <code>\"error\"</code>. <code>required_modules.autonomous_driving.DIAGNOSTIC_NAME.auto_recovery</code> string <code>\"true\"</code> Determines whether the system will automatically recover when it recovers from an error. <code>required_modules.remote_control.DIAGNOSTIC_NAME.sf_at</code> string <code>\"none\"</code> Diagnostic level where it becomes Safe Fault. Available options are <code>\"none\"</code>, <code>\"warn\"</code>, <code>\"error\"</code>. <code>required_modules.remote_control.DIAGNOSTIC_NAME.lf_at</code> string <code>\"warn\"</code> Diagnostic level where it becomes Latent Fault. Available options are <code>\"none\"</code>, <code>\"warn\"</code>, <code>\"error\"</code>. <code>required_modules.remote_control.DIAGNOSTIC_NAME.spf_at</code> string <code>\"error\"</code> Diagnostic level where it becomes Single Point Fault. Available options are <code>\"none\"</code>, <code>\"warn\"</code>, <code>\"error\"</code>. <code>required_modules.remote_control.DIAGNOSTIC_NAME.auto_recovery</code> string <code>\"true\"</code> Determines whether the system will automatically recover when it recovers from an error."},{"location":"system/system_error_monitor/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/system_monitor/","title":"System Monitor for Autoware","text":""},{"location":"system/system_monitor/#system-monitor-for-autoware","title":"System Monitor for Autoware","text":"<p>Further improvement of system monitor functionality for Autoware.</p>"},{"location":"system/system_monitor/#description","title":"Description","text":"<p>This package provides the following nodes for monitoring system:</p> <ul> <li>CPU Monitor</li> <li>HDD Monitor</li> <li>Memory Monitor</li> <li>Network Monitor</li> <li>NTP Monitor</li> <li>Process Monitor</li> <li>GPU Monitor</li> <li>Voltage Monitor</li> </ul>"},{"location":"system/system_monitor/#supported-architecture","title":"Supported architecture","text":"<ul> <li>x86_64</li> <li>arm64v8/aarch64</li> </ul>"},{"location":"system/system_monitor/#operation-confirmed-platform","title":"Operation confirmed platform","text":"<ul> <li>PC system intel core i7</li> <li>NVIDIA Jetson AGX Xavier</li> <li>Raspberry Pi4 Model B</li> </ul>"},{"location":"system/system_monitor/#how-to-use","title":"How to use","text":"<p>Use colcon build and launch in the same way as other packages.</p> <pre><code>colcon build\nsource install/setup.bash\nros2 launch system_monitor system_monitor.launch.xml\n</code></pre> <p>CPU and GPU monitoring method differs depending on platform. CMake automatically chooses source to be built according to build environment. If you build this package on intel platform, CPU monitor and GPU monitor which run on intel platform are built.</p>"},{"location":"system/system_monitor/#ros-topics-published-by-system-monitor","title":"ROS topics published by system monitor","text":"<p>Every topic is published in 1 minute interval.</p> <ul> <li>CPU Monitor</li> <li>HDD Monitor</li> <li>Mem Monitor</li> <li>Net Monitor</li> <li>NTP Monitor</li> <li>Process Monitor</li> <li>GPU Monitor</li> <li>Voltage Monitor</li> </ul> <p>[Usage] \u2713\uff1aSupported, -\uff1aNot supported</p> Node Message Intel arm64(tegra) arm64(raspi) Notes CPU Monitor CPU Temperature \u2713 \u2713 \u2713 CPU Usage \u2713 \u2713 \u2713 CPU Load Average \u2713 \u2713 \u2713 CPU Thermal Throttling \u2713 - \u2713 CPU Frequency \u2713 \u2713 \u2713 Notification of frequency only, normally error not generated. HDD Monitor HDD Temperature \u2713 \u2713 \u2713 HDD PowerOnHours \u2713 \u2713 \u2713 HDD TotalDataWritten \u2713 \u2713 \u2713 HDD RecoveredError \u2713 \u2713 \u2713 HDD Usage \u2713 \u2713 \u2713 HDD ReadDataRate \u2713 \u2713 \u2713 HDD WriteDataRate \u2713 \u2713 \u2713 HDD ReadIOPS \u2713 \u2713 \u2713 HDD WriteIOPS \u2713 \u2713 \u2713 HDD Connection \u2713 \u2713 \u2713 Memory Monitor Memory Usage \u2713 \u2713 \u2713 Net Monitor Network Connection \u2713 \u2713 \u2713 Network Usage \u2713 \u2713 \u2713 Notification of usage only, normally error not generated. Network CRC Error \u2713 \u2713 \u2713 Warning occurs when the number of CRC errors in the period reaches the threshold value. The number of CRC errors that occur is the same as the value that can be confirmed with the ip command. IP Packet Reassembles Failed \u2713 \u2713 \u2713 NTP Monitor NTP Offset \u2713 \u2713 \u2713 Process Monitor Tasks Summary \u2713 \u2713 \u2713 High-load Proc[0-9] \u2713 \u2713 \u2713 High-mem Proc[0-9] \u2713 \u2713 \u2713 GPU Monitor GPU Temperature \u2713 \u2713 - GPU Usage \u2713 \u2713 - GPU Memory Usage \u2713 - - GPU Thermal Throttling \u2713 - - GPU Frequency \u2713 \u2713 - For Intel platform, monitor whether current GPU clock is supported by the GPU. Voltage Monitor CMOS Battery Status \u2713 - - Battery Health for RTC and BIOS -"},{"location":"system/system_monitor/#ros-parameters","title":"ROS parameters","text":"<p>See ROS parameters.</p>"},{"location":"system/system_monitor/#notes","title":"Notes","text":""},{"location":"system/system_monitor/#cpu-monitor-for-intel-platform","title":"CPU monitor for intel platform","text":"<p>Thermal throttling event can be monitored by reading contents of MSR(Model Specific Register), and accessing MSR is only allowed for root by default, so this package provides the following approach to minimize security risks as much as possible:</p> <ul> <li>Provide a small program named 'msr_reader' which accesses MSR and sends thermal throttling status to CPU monitor by using socket programming.</li> <li>Run 'msr_reader' as a specific user instead of root.</li> <li>CPU monitor is able to know the status as an unprivileged user since thermal throttling status is sent by socket communication.</li> </ul>"},{"location":"system/system_monitor/#instructions-before-starting","title":"Instructions before starting","text":"<ol> <li> <p>Create a user to run 'msr_reader'.</p> <pre><code>sudo adduser &lt;username&gt;\n</code></pre> </li> <li> <p>Load kernel module 'msr' into your target system.    The path '/dev/cpu/CPUNUM/msr' appears.</p> <pre><code>sudo modprobe msr\n</code></pre> </li> <li> <p>Allow user to access MSR with read-only privilege using the Access Control List (ACL).</p> <pre><code>sudo setfacl -m u:&lt;username&gt;:r /dev/cpu/*/msr\n</code></pre> </li> <li> <p>Assign capability to 'msr_reader' since msr kernel module requires rawio capability.</p> <pre><code>sudo setcap cap_sys_rawio=ep install/system_monitor/lib/system_monitor/msr_reader\n</code></pre> </li> <li> <p>Run 'msr_reader' as the user you created, and run system_monitor as a generic user.</p> <pre><code>su &lt;username&gt;\ninstall/system_monitor/lib/system_monitor/msr_reader\n</code></pre> </li> </ol>"},{"location":"system/system_monitor/#see-also","title":"See also","text":"<p>msr_reader</p>"},{"location":"system/system_monitor/#hdd-monitor","title":"HDD Monitor","text":"<p>Generally, S.M.A.R.T. information is used to monitor HDD temperature and life of HDD, and normally accessing disk device node is allowed for root user or disk group. As with the CPU monitor, this package provides an approach to minimize security risks as much as possible:</p> <ul> <li>Provide a small program named 'hdd_reader' which accesses S.M.A.R.T. information and sends some items of it to HDD monitor by using socket programming.</li> <li>Run 'hdd_reader' as a specific user.</li> <li>HDD monitor is able to know some items of S.M.A.R.T. information as an unprivileged user since those are sent by socket communication.</li> </ul>"},{"location":"system/system_monitor/#instructions-before-starting_1","title":"Instructions before starting","text":"<ol> <li> <p>Create a user to run 'hdd_reader'.</p> <pre><code>sudo adduser &lt;username&gt;\n</code></pre> </li> <li> <p>Add user to the disk group.</p> <pre><code>sudo usermod -a -G disk &lt;username&gt;\n</code></pre> </li> <li> <p>Assign capabilities to 'hdd_reader' since SCSI kernel module requires rawio capability to send ATA PASS-THROUGH (12) command and NVMe kernel module requires admin capability to send Admin Command.</p> <pre><code>sudo setcap 'cap_sys_rawio=ep cap_sys_admin=ep' install/system_monitor/lib/system_monitor/hdd_reader\n</code></pre> </li> <li> <p>Run 'hdd_reader' as the user you created, and run system_monitor as a generic user.</p> <pre><code>su &lt;username&gt;\ninstall/system_monitor/lib/system_monitor/hdd_reader\n</code></pre> </li> </ol>"},{"location":"system/system_monitor/#see-also_1","title":"See also","text":"<p>hdd_reader</p>"},{"location":"system/system_monitor/#gpu-monitor-for-intel-platform","title":"GPU Monitor for intel platform","text":"<p>Currently GPU monitor for intel platform only supports NVIDIA GPU whose information can be accessed by NVML API.</p> <p>Also you need to install CUDA libraries. For installation instructions for CUDA 10.0, see NVIDIA CUDA Installation Guide for Linux.</p>"},{"location":"system/system_monitor/#voltage-monitor-for-cmos-battery","title":"Voltage monitor for CMOS Battery","text":"<p>Some platforms have built-in batteries for the RTC and CMOS. This node determines the battery status from the result of executing cat /proc/driver/rtc. Also, if lm-sensors is installed, it is possible to use the results. However, the return value of sensors varies depending on the chipset, so it is necessary to set a string to extract the corresponding voltage. It is also necessary to set the voltage for warning and error. For example, if you want a warning when the voltage is less than 2.9V and an error when it is less than 2.7V. The execution result of sensors on the chipset nct6106 is as follows, and \"in7:\" is the voltage of the CMOS battery.</p> <pre><code>$ sensors\npch_cannonlake-virtual-0\nAdapter: Virtual device\ntemp1:        +42.0\u00b0C\n\nnct6106-isa-0a10\nAdapter: ISA adapter\nin0:           728.00 mV (min =  +0.00 V, max =  +1.74 V)\nin1:             1.01 V  (min =  +0.00 V, max =  +2.04 V)\nin2:             3.34 V  (min =  +0.00 V, max =  +4.08 V)\nin3:             3.34 V  (min =  +0.00 V, max =  +4.08 V)\nin4:             1.07 V  (min =  +0.00 V, max =  +2.04 V)\nin5:             1.05 V  (min =  +0.00 V, max =  +2.04 V)\nin6:             1.67 V  (min =  +0.00 V, max =  +2.04 V)\nin7:             3.06 V  (min =  +0.00 V, max =  +4.08 V)\nin8:             2.10 V  (min =  +0.00 V, max =  +4.08 V)\nfan1:          2789 RPM  (min =    0 RPM)\nfan2:             0 RPM  (min =    0 RPM)\n</code></pre> <p>The setting value of voltage_monitor.param.yaml is as follows.</p> <pre><code>/**:\nros__parameters:\ncmos_battery_warn: 2.90\ncmos_battery_error: 2.70\ncmos_battery_label: \"in7:\"\n</code></pre> <p>The above values of 2.7V and 2.90V are hypothetical. Depending on the motherboard and chipset, the value may vary. However, if the voltage of the lithium battery drops below 2.7V, it is recommended to replace it. In the above example, the message output to the topic /diagnostics is as follows. If the voltage &lt; 2.9V then:</p> <pre><code>  name: /autoware/system/resource_monitoring/voltage/cmos_battery\n  message: Warning\n  hardware_id: ''\n  values:\n  - key: 'voltage_monitor: CMOS Battery Status'\n    value: Low Battery\n</code></pre> <p>If the voltage &lt; 2.7V then:</p> <pre><code>  name: /autoware/system/resource_monitoring/voltage/cmos_battery\n  message: Warning\n  hardware_id: ''\n  values:\n  - key: 'voltage_monitor: CMOS Battery Status'\n    value: Battery Died\n</code></pre> <p>If neither, then:</p> <pre><code>  name: /autoware/system/resource_monitoring/voltage/cmos_battery\n  message: OK\n  hardware_id: ''\n  values:\n  - key: 'voltage_monitor: CMOS Battery Status'\n    value: OK\n</code></pre> <p>If the CMOS battery voltage drops less than voltage_error or voltage_warn,It will be a warning. If the battery runs out, the RTC will stop working when the power is turned off. However, since the vehicle can run, it is not an error. The vehicle will stop when an error occurs, but there is no need to stop immediately. It can be determined by the value of \"Low Battery\" or \"Battery Died\".</p>"},{"location":"system/system_monitor/#uml-diagrams","title":"UML diagrams","text":"<p>See Class diagrams. See Sequence diagrams.</p>"},{"location":"system/system_monitor/docs/class_diagrams/","title":"Class diagrams","text":""},{"location":"system/system_monitor/docs/class_diagrams/#class-diagrams","title":"Class diagrams","text":""},{"location":"system/system_monitor/docs/class_diagrams/#cpu-monitor","title":"CPU Monitor","text":""},{"location":"system/system_monitor/docs/class_diagrams/#hdd-monitor","title":"HDD Monitor","text":""},{"location":"system/system_monitor/docs/class_diagrams/#memory-monitor","title":"Memory Monitor","text":""},{"location":"system/system_monitor/docs/class_diagrams/#net-monitor","title":"Net Monitor","text":""},{"location":"system/system_monitor/docs/class_diagrams/#ntp-monitor","title":"NTP Monitor","text":""},{"location":"system/system_monitor/docs/class_diagrams/#process-monitor","title":"Process Monitor","text":""},{"location":"system/system_monitor/docs/class_diagrams/#gpu-monitor","title":"GPU Monitor","text":""},{"location":"system/system_monitor/docs/hdd_reader/","title":"hdd_reader","text":""},{"location":"system/system_monitor/docs/hdd_reader/#hdd_reader","title":"hdd_reader","text":""},{"location":"system/system_monitor/docs/hdd_reader/#name","title":"Name","text":"<p>hdd_reader - Read S.M.A.R.T. information for monitoring HDD temperature and life of HDD</p>"},{"location":"system/system_monitor/docs/hdd_reader/#synopsis","title":"Synopsis","text":"<p>hdd_reader [OPTION]</p>"},{"location":"system/system_monitor/docs/hdd_reader/#description","title":"Description","text":"<p>Read S.M.A.R.T. information for monitoring HDD temperature and life of HDD. This runs as a daemon process and listens to a TCP/IP port (7635 by default).</p> <p>Options: -h, --help \u00a0\u00a0\u00a0\u00a0Display help -p, --port # \u00a0\u00a0\u00a0\u00a0Port number to listen to</p> <p>Exit status: Returns 0 if OK; non-zero otherwise.</p>"},{"location":"system/system_monitor/docs/hdd_reader/#notes","title":"Notes","text":"<p>The 'hdd_reader' accesses minimal data enough to get Model number, Serial number, HDD temperature, and life of HDD. This is an approach to limit its functionality, however, the functionality can be expanded for further improvements and considerations in the future.</p>"},{"location":"system/system_monitor/docs/hdd_reader/#ata","title":"[ATA]","text":"Purpose Name Length Model number, Serial number IDENTIFY DEVICE data 256 words(512 bytes) HDD temperature, life of HDD SMART READ DATA 256 words(512 bytes) <p>For details please see the documents below.</p> <ul> <li>ATA Command Set - 4 (ACS-4)</li> <li>ATA/ATAPI Command Set - 3 (ACS-3)</li> <li>SMART Attribute Overview</li> <li>SMART Attribute Annex</li> </ul>"},{"location":"system/system_monitor/docs/hdd_reader/#nvme","title":"[NVMe]","text":"Purpose Name Length Model number, Serial number Identify Controller data structure 4096 bytes HDD temperature, life of HDD SMART / Health Information 36 Dword(144 bytes) <p>For details please see the documents below.</p> <ul> <li>NVM Express 1.2b</li> </ul>"},{"location":"system/system_monitor/docs/hdd_reader/#operation-confirmed-drives","title":"Operation confirmed drives","text":"<ul> <li>SAMSUNG MZVLB1T0HALR (SSD)</li> <li>Western Digital My Passport (Portable HDD)</li> </ul>"},{"location":"system/system_monitor/docs/msr_reader/","title":"msr_reader","text":""},{"location":"system/system_monitor/docs/msr_reader/#msr_reader","title":"msr_reader","text":""},{"location":"system/system_monitor/docs/msr_reader/#name","title":"Name","text":"<p>msr_reader - Read MSR register for monitoring thermal throttling event</p>"},{"location":"system/system_monitor/docs/msr_reader/#synopsis","title":"Synopsis","text":"<p>msr_reader [OPTION]</p>"},{"location":"system/system_monitor/docs/msr_reader/#description","title":"Description","text":"<p>Read MSR register for monitoring thermal throttling event. This runs as a daemon process and listens to a TCP/IP port (7634 by default).</p> <p>Options: -h, --help \u00a0\u00a0\u00a0\u00a0Display help -p, --port # \u00a0\u00a0\u00a0\u00a0Port number to listen to</p> <p>Exit status: Returns 0 if OK; non-zero otherwise.</p>"},{"location":"system/system_monitor/docs/msr_reader/#notes","title":"Notes","text":"<p>The 'msr_reader' accesses minimal data enough to get thermal throttling event. This is an approach to limit its functionality, however, the functionality can be expanded for further improvements and considerations in the future.</p> Register Address Name Length 1B1H IA32_PACKAGE_THERM_STATUS 64bit <p>For details please see the documents below.</p> <ul> <li>Intel\u00ae 64 and IA-32 ArchitecturesSoftware Developer\u2019s Manual</li> </ul>"},{"location":"system/system_monitor/docs/msr_reader/#operation-confirmed-platform","title":"Operation confirmed platform","text":"<ul> <li>PC system intel core i7</li> </ul>"},{"location":"system/system_monitor/docs/ros_parameters/","title":"ROS parameters","text":""},{"location":"system/system_monitor/docs/ros_parameters/#ros-parameters","title":"ROS parameters","text":""},{"location":"system/system_monitor/docs/ros_parameters/#cpu-monitor","title":"CPU Monitor","text":"<p>cpu_monitor:</p> Name Type Unit Default Notes temp_warn float DegC 90.0 Generates warning when CPU temperature reaches a specified value or higher. temp_error float DegC 95.0 Generates error when CPU temperature reaches a specified value or higher. usage_warn float %(1e-2) 0.90 Generates warning when CPU usage reaches a specified value or higher and last for usage_warn_count counts. usage_error float %(1e-2) 1.00 Generates error when CPU usage reaches a specified value or higher and last for usage_error_count counts. usage_warn_count int n/a 2 Generates warning when CPU usage reaches usage_warn value or higher and last for a specified counts. usage_error_count int n/a 2 Generates error when CPU usage reaches usage_error value or higher and last for a specified counts. load1_warn float %(1e-2) 0.90 Generates warning when load average 1min reaches a specified value or higher. load5_warn float %(1e-2) 0.80 Generates warning when load average 5min reaches a specified value or higher. msr_reader_port int n/a 7634 Port number to connect to msr_reader."},{"location":"system/system_monitor/docs/ros_parameters/#hdd-monitor","title":"HDD Monitor","text":"<p>hdd_monitor:</p> <p>\u00a0\u00a0disks:</p> Name Type Unit Default Notes name string n/a none The disk name to monitor temperature. (e.g. /dev/sda) temp_attribute_id int n/a 0xC2 S.M.A.R.T attribute ID of temperature. temp_warn float DegC 55.0 Generates warning when HDD temperature reaches a specified value or higher. temp_error float DegC 70.0 Generates error when HDD temperature reaches a specified value or higher. power_on_hours_attribute_id int n/a 0x09 S.M.A.R.T attribute ID of power-on hours. power_on_hours_warn int Hour 3000000 Generates warning when HDD power-on hours reaches a specified value or higher. total_data_written_attribute_id int n/a 0xF1 S.M.A.R.T attribute ID of total data written. total_data_written_warn int depends on device 4915200 Generates warning when HDD total data written reaches a specified value or higher. total_data_written_safety_factor int %(1e-2) 0.05 Safety factor of HDD total data written. recovered_error_attribute_id int n/a 0xC3 S.M.A.R.T attribute ID of recovered error. recovered_error_warn int n/a 1 Generates warning when HDD recovered error reaches a specified value or higher. read_data_rate_warn float MB/s 360.0 Generates warning when HDD read data rate reaches a specified value or higher. write_data_rate_warn float MB/s 103.5 Generates warning when HDD write data rate reaches a specified value or higher. read_iops_warn float IOPS 63360.0 Generates warning when HDD read IOPS reaches a specified value or higher. write_iops_warn float IOPS 24120.0 Generates warning when HDD write IOPS reaches a specified value or higher. <p>hdd_monitor:</p> Name Type Unit Default Notes hdd_reader_port int n/a 7635 Port number to connect to hdd_reader. usage_warn float %(1e-2) 0.95 Generates warning when disk usage reaches a specified value or higher. usage_error float %(1e-2) 0.99 Generates error when disk usage reaches a specified value or higher."},{"location":"system/system_monitor/docs/ros_parameters/#memory-monitor","title":"Memory Monitor","text":"<p>mem_monitor:</p> Name Type Unit Default Notes usage_warn float %(1e-2) 0.95 Generates warning when physical memory usage reaches a specified value or higher. usage_error float %(1e-2) 0.99 Generates error when physical memory usage reaches a specified value or higher."},{"location":"system/system_monitor/docs/ros_parameters/#net-monitor","title":"Net Monitor","text":"<p>net_monitor:</p> Name Type Unit Default Notes devices list[string] n/a none The name of network interface to monitor. (e.g. eth0, * for all network interfaces) monitor_program string n/a greengrass program name to be monitored by nethogs name. crc_error_check_duration int sec 1 CRC error check duration. crc_error_count_threshold int n/a 1 Generates warning when count of CRC errors during CRC error check duration reaches a specified value or higher. reassembles_failed_check_duration int sec 1 IP packet reassembles failed check duration. reassembles_failed_check_count int n/a 1 Generates warning when count of IP packet reassembles failed during IP packet reassembles failed check duration reaches a specified value or higher."},{"location":"system/system_monitor/docs/ros_parameters/#ntp-monitor","title":"NTP Monitor","text":"<p>ntp_monitor:</p> Name Type Unit Default Notes server string n/a ntp.ubuntu.com The name of NTP server to synchronize date and time. (e.g. ntp.nict.jp for Japan) offset_warn float sec 0.1 Generates warning when NTP offset reaches a specified value or higher. (default is 100ms) offset_error float sec 5.0 Generates warning when NTP offset reaches a specified value or higher. (default is 5sec)"},{"location":"system/system_monitor/docs/ros_parameters/#process-monitor","title":"Process Monitor","text":"<p>process_monitor:</p> Name Type Unit Default Notes num_of_procs int n/a 5 The number of processes to generate High-load Proc[0-9] and High-mem Proc[0-9]."},{"location":"system/system_monitor/docs/ros_parameters/#gpu-monitor","title":"GPU Monitor","text":"<p>gpu_monitor:</p> Name Type Unit Default Notes temp_warn float DegC 90.0 Generates warning when GPU temperature reaches a specified value or higher. temp_error float DegC 95.0 Generates error when GPU temperature reaches a specified value or higher. gpu_usage_warn float %(1e-2) 0.90 Generates warning when GPU usage reaches a specified value or higher. gpu_usage_error float %(1e-2) 1.00 Generates error when GPU usage reaches a specified value or higher. memory_usage_warn float %(1e-2) 0.90 Generates warning when GPU memory usage reaches a specified value or higher. memory_usage_error float %(1e-2) 1.00 Generates error when GPU memory usage reaches a specified value or higher."},{"location":"system/system_monitor/docs/ros_parameters/#voltage-monitor","title":"Voltage Monitor","text":"<p>voltage_monitor:</p> Name Type Unit Default Notes cmos_battery_warn float volt 2.9 Generates warning when voltage of CMOS Battery is lower. cmos_battery_error float volt 2.7 Generates error when voltage of CMOS Battery is lower. cmos_battery_label string n/a \"\" voltage string in sensors command outputs. if empty no voltage will be checked."},{"location":"system/system_monitor/docs/seq_diagrams/","title":"Sequence diagrams","text":""},{"location":"system/system_monitor/docs/seq_diagrams/#sequence-diagrams","title":"Sequence diagrams","text":""},{"location":"system/system_monitor/docs/seq_diagrams/#cpu-monitor","title":"CPU Monitor","text":""},{"location":"system/system_monitor/docs/seq_diagrams/#hdd-monitor","title":"HDD Monitor","text":""},{"location":"system/system_monitor/docs/seq_diagrams/#memory-monitor","title":"Memory Monitor","text":""},{"location":"system/system_monitor/docs/seq_diagrams/#net-monitor","title":"Net Monitor","text":""},{"location":"system/system_monitor/docs/seq_diagrams/#ntp-monitor","title":"NTP Monitor","text":""},{"location":"system/system_monitor/docs/seq_diagrams/#process-monitor","title":"Process Monitor","text":""},{"location":"system/system_monitor/docs/seq_diagrams/#gpu-monitor","title":"GPU Monitor","text":""},{"location":"system/system_monitor/docs/topics_cpu_monitor/","title":"ROS topics: CPU Monitor","text":""},{"location":"system/system_monitor/docs/topics_cpu_monitor/#ros-topics-cpu-monitor","title":"ROS topics: CPU Monitor","text":""},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-temperature","title":"CPU Temperature","text":"<p>/diagnostics/cpu_monitor: CPU Temperature</p> <p>[summary]</p> level message OK OK <p>[values]</p> key (example) value (example) Package id 0, Core [0-9], thermal_zone[0-9] 50.0 DegC <p>*key: thermal_zone[0-9] for ARM architecture.</p>"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-usage","title":"CPU Usage","text":"<p>/diagnostics/cpu_monitor: CPU Usage</p> <p>[summary]</p> level message OK OK WARN high load ERROR very high load <p>[values]</p> key value (example) CPU [all,0-9]: status OK / high load / very high load CPU [all,0-9]: usr 2.00% CPU [all,0-9]: nice 0.00% CPU [all,0-9]: sys 1.00% CPU [all,0-9]: idle 97.00%"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-load-average","title":"CPU Load Average","text":"<p>/diagnostics/cpu_monitor: CPU Load Average</p> <p>[summary]</p> level message OK OK WARN high load <p>[values]</p> key value (example) 1min 14.50% 5min 14.55% 15min 9.67%"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-thermal-throttling","title":"CPU Thermal Throttling","text":"<p>Intel and raspi platform only. Tegra platform not supported.</p> <p>/diagnostics/cpu_monitor: CPU Thermal Throttling</p> <p>[summary]</p> level message OK OK ERROR throttling <p>[values for intel platform]</p> key value (example) CPU [0-9]: Pkg Thermal Status OK / throttling <p>[values for raspi platform]</p> key value (example) status All clear / Currently throttled / Soft temperature limit active"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-frequency","title":"CPU Frequency","text":"<p>/diagnostics/cpu_monitor: CPU Frequency</p> <p>[summary]</p> level message OK OK <p>[values]</p> key value (example) CPU [0-9]: clock 2879MHz"},{"location":"system/system_monitor/docs/topics_gpu_monitor/","title":"ROS topics: GPU Monitor","text":""},{"location":"system/system_monitor/docs/topics_gpu_monitor/#ros-topics-gpu-monitor","title":"ROS topics: GPU Monitor","text":"<p>Intel and tegra platform only. Raspi platform not supported.</p>"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-temperature","title":"GPU Temperature","text":"<p>/diagnostics/gpu_monitor: GPU Temperature</p> <p>[summary]</p> level message OK OK WARN warm ERROR hot <p>[values]</p> key (example) value (example) GeForce GTX 1650, thermal_zone[0-9] 46.0 DegC <p>*key: thermal_zone[0-9] for ARM architecture.</p>"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-usage","title":"GPU Usage","text":"<p>/diagnostics/gpu_monitor: GPU Usage</p> <p>[summary]</p> level message OK OK WARN high load ERROR very high load <p>[values]</p> key value (example) GPU [0-9]: status OK / high load / very high load GPU [0-9]: name GeForce GTX 1650, gpu.[0-9] GPU [0-9]: usage 19.0% <p>*key: gpu.[0-9] for ARM architecture.</p>"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-memory-usage","title":"GPU Memory Usage","text":"<p>Intel platform only. There is no separate gpu memory in tegra. Both cpu and gpu uses cpu memory.</p> <p>/diagnostics/gpu_monitor: GPU Memory Usage</p> <p>[summary]</p> level message OK OK WARN high load ERROR very high load <p>[values]</p> key value (example) GPU [0-9]: status OK / high load / very high load GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: usage 13.0% GPU [0-9]: total 3G GPU [0-9]: used 1G GPU [0-9]: free 2G"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-thermal-throttling","title":"GPU Thermal Throttling","text":"<p>Intel platform only. Tegra platform not supported.</p> <p>/diagnostics/gpu_monitor: GPU Thermal Throttling</p> <p>[summary]</p> level message OK OK ERROR throttling <p>[values]</p> key value (example) GPU [0-9]: status OK / throttling GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: graphics clock 1020 MHz GPU [0-9]: reasons GpuIdle / SwThermalSlowdown etc."},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-frequency","title":"GPU Frequency","text":"<p>/diagnostics/gpu_monitor: GPU Frequency</p>"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#intel-platform","title":"Intel platform","text":"<p>[summary]</p> level message OK OK WARN unsupported clock <p>[values]</p> key value (example) GPU [0-9]: status OK / unsupported clock GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: graphics clock 1020 MHz"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#tegra-platform","title":"Tegra platform","text":"<p>[summary]</p> level message OK OK <p>[values]</p> key (example) value (example) GPU 17000000.gv11b: clock 318 MHz"},{"location":"system/system_monitor/docs/topics_hdd_monitor/","title":"ROS topics: HDD Monitor","text":""},{"location":"system/system_monitor/docs/topics_hdd_monitor/#ros-topics-hdd-monitor","title":"ROS topics: HDD Monitor","text":""},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-temperature","title":"HDD Temperature","text":"<p>/diagnostics/hdd_monitor: HDD Temperature</p> <p>[summary]</p> level message OK OK WARN hot ERROR critical hot <p>[values]</p> key value (example) HDD [0-9]: status OK / hot / critical hot HDD [0-9]: name /dev/nvme0 HDD [0-9]: model SAMSUNG MZVLB1T0HBLR-000L7 HDD [0-9]: serial S4EMNF0M820682 HDD [0-9]: temperature 37.0 DegC  not available"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-poweronhours","title":"HDD PowerOnHours","text":"<p>/diagnostics/hdd_monitor: HDD PowerOnHours</p> <p>[summary]</p> level message OK OK WARN lifetime limit <p>[values]</p> key value (example) HDD [0-9]: status OK / lifetime limit HDD [0-9]: name /dev/nvme0 HDD [0-9]: model PHISON PS5012-E12S-512G HDD [0-9]: serial FB590709182505050767 HDD [0-9]: power on hours 4834 Hours  not available"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-totaldatawritten","title":"HDD TotalDataWritten","text":"<p>/diagnostics/hdd_monitor: HDD TotalDataWritten</p> <p>[summary]</p> level message OK OK WARN warranty period <p>[values]</p> key value (example) HDD [0-9]: status OK / warranty period HDD [0-9]: name /dev/nvme0 HDD [0-9]: model PHISON PS5012-E12S-512G HDD [0-9]: serial FB590709182505050767 HDD [0-9]: total data written 146295330  not available"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-recoverederror","title":"HDD RecoveredError","text":"<p>/diagnostics/hdd_monitor: HDD RecoveredError</p> <p>[summary]</p> level message OK OK WARN high soft error rate <p>[values]</p> key value (example) HDD [0-9]: status OK / high soft error rate HDD [0-9]: name /dev/nvme0 HDD [0-9]: model PHISON PS5012-E12S-512G HDD [0-9]: serial FB590709182505050767 HDD [0-9]: recovered error 0  not available"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-usage","title":"HDD Usage","text":"<p>/diagnostics/hdd_monitor: HDD Usage</p> <p>[summary]</p> level message OK OK WARN low disk space ERROR very low disk space <p>[values]</p> key value (example) HDD [0-9]: status OK / low disk space / very low disk space HDD [0-9]: filesystem /dev/nvme0n1p4 HDD [0-9]: size 264G HDD [0-9]: used 172G HDD [0-9]: avail 749G HDD [0-9]: use 69% HDD [0-9]: mounted on /"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-readdatarate","title":"HDD ReadDataRate","text":"<p>/diagnostics/hdd_monitor: HDD ReadDataRate</p> <p>[summary]</p> level message OK OK WARN high data rate of read <p>[values]</p> key value (example) HDD [0-9]: status OK / high data rate of read HDD [0-9]: name /dev/nvme0 HDD [0-9]: data rate of read 0.00 MB/s"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-writedatarate","title":"HDD WriteDataRate","text":"<p>/diagnostics/hdd_monitor: HDD WriteDataRate</p> <p>[summary]</p> level message OK OK WARN high data rate of write <p>[values]</p> key value (example) HDD [0-9]: status OK / high data rate of write HDD [0-9]: name /dev/nvme0 HDD [0-9]: data rate of write 0.00 MB/s"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-readiops","title":"HDD ReadIOPS","text":"<p>/diagnostics/hdd_monitor: HDD ReadIOPS</p> <p>[summary]</p> level message OK OK WARN high IOPS of read <p>[values]</p> key value (example) HDD [0-9]: status OK / high IOPS of read HDD [0-9]: name /dev/nvme0 HDD [0-9]: IOPS of read 0.00 IOPS"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-writeiops","title":"HDD WriteIOPS","text":"<p>/diagnostics/hdd_monitor: HDD WriteIOPS</p> <p>[summary]</p> level message OK OK WARN high IOPS of write <p>[values]</p> key value (example) HDD [0-9]: status OK / high IOPS of write HDD [0-9]: name /dev/nvme0 HDD [0-9]: IOPS of write 0.00 IOPS"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-connection","title":"HDD Connection","text":"<p>/diagnostics/hdd_monitor: HDD Connection</p> <p>[summary]</p> level message OK OK WARN not connected <p>[values]</p> key value (example) HDD [0-9]: status OK / not connected HDD [0-9]: name /dev/nvme0 HDD [0-9]: mount point /"},{"location":"system/system_monitor/docs/topics_mem_monitor/","title":"ROS topics: Memory Monitor","text":""},{"location":"system/system_monitor/docs/topics_mem_monitor/#ros-topics-memory-monitor","title":"ROS topics: Memory Monitor","text":""},{"location":"system/system_monitor/docs/topics_mem_monitor/#memory-usage","title":"Memory Usage","text":"<p>/diagnostics/mem_monitor: Memory Usage</p> <p>[summary]</p> level message OK OK WARN high load ERROR very high load <p>[values]</p> key value (example) Mem: usage 29.72% Mem: total 31.2G Mem: used 6.0G Mem: free 20.7G Mem: shared 2.9G Mem: buff/cache 4.5G Mem: available 21.9G Swap: total 2.0G Swap: used 218M Swap: free 1.8G Total: total 33.2G Total: used 6.2G Total: free 22.5G Total: used+ 9.1G"},{"location":"system/system_monitor/docs/topics_net_monitor/","title":"ROS topics: Net Monitor","text":""},{"location":"system/system_monitor/docs/topics_net_monitor/#ros-topics-net-monitor","title":"ROS topics: Net Monitor","text":""},{"location":"system/system_monitor/docs/topics_net_monitor/#network-connection","title":"Network Connection","text":"<p>/diagnostics/net_monitor: Network Connection</p> <p>[summary]</p> level message OK OK WARN no such device <p>[values]</p> key value (example) Network [0-9]: status OK / no such device HDD [0-9]: name wlp82s0"},{"location":"system/system_monitor/docs/topics_net_monitor/#network-usage","title":"Network Usage","text":"<p>/diagnostics/net_monitor: Network Usage</p> <p>[summary]</p> level message OK OK <p>[values]</p> key value (example) Network [0-9]: status OK Network [0-9]: interface name wlp82s0 Network [0-9]: rx_usage 0.00% Network [0-9]: tx_usage 0.00% Network [0-9]: rx_traffic 0.00 MB/s Network [0-9]: tx_traffic 0.00 MB/s Network [0-9]: capacity 400.0 MB/s Network [0-9]: mtu 1500 Network [0-9]: rx_bytes 58455228 Network [0-9]: rx_errors 0 Network [0-9]: tx_bytes 11069136 Network [0-9]: tx_errors 0 Network [0-9]: collisions 0"},{"location":"system/system_monitor/docs/topics_net_monitor/#network-traffic","title":"Network Traffic","text":"<p>/diagnostics/net_monitor: Network Traffic</p> <p>[summary]</p> level message OK OK <p>[values when specified program is detected]</p> key value (example) nethogs [0-9]: program /lambda/greengrassSystemComponents/1384/999 nethogs [0-9]: sent (KB/Sec) 1.13574 nethogs [0-9]: received (KB/Sec) 0.261914 <p>[values when error is occurring]</p> key value (example) error execve failed: No such file or directory"},{"location":"system/system_monitor/docs/topics_net_monitor/#network-crc-error","title":"Network CRC Error","text":"<p>/diagnostics/net_monitor: Network CRC Error</p> <p>[summary]</p> level message OK OK WARN CRC error <p>[values]</p> key value (example) Network [0-9]: interface name wlp82s0 Network [0-9]: total rx_crc_errors 0 Network [0-9]: rx_crc_errors per unit time 0"},{"location":"system/system_monitor/docs/topics_net_monitor/#ip-packet-reassembles-failed","title":"IP Packet Reassembles Failed","text":"<p>/diagnostics/net_monitor: IP Packet Reassembles Failed</p> <p>[summary]</p> level message OK OK WARN reassembles failed <p>[values]</p> key value (example) total packet reassembles failed 0 packet reassembles failed per unit time 0"},{"location":"system/system_monitor/docs/topics_ntp_monitor/","title":"ROS topics: NTP Monitor","text":""},{"location":"system/system_monitor/docs/topics_ntp_monitor/#ros-topics-ntp-monitor","title":"ROS topics: NTP Monitor","text":""},{"location":"system/system_monitor/docs/topics_ntp_monitor/#ntp-offset","title":"NTP Offset","text":"<p>/diagnostics/ntp_monitor: NTP Offset</p> <p>[summary]</p> level message OK OK WARN high ERROR too high <p>[values]</p> key value (example) NTP Offset -0.013181 sec NTP Delay 0.053880 sec"},{"location":"system/system_monitor/docs/topics_process_monitor/","title":"ROS topics: Process Monitor","text":""},{"location":"system/system_monitor/docs/topics_process_monitor/#ros-topics-process-monitor","title":"ROS topics: Process Monitor","text":""},{"location":"system/system_monitor/docs/topics_process_monitor/#tasks-summary","title":"Tasks Summary","text":"<p>/diagnostics/process_monitor: Tasks Summary</p> <p>[summary]</p> level message OK OK <p>[values]</p> key value (example) total 409 running 2 sleeping 321 stopped 0 zombie 0"},{"location":"system/system_monitor/docs/topics_process_monitor/#high-load-proc0-9","title":"High-load Proc[0-9]","text":"<p>/diagnostics/process_monitor: High-load Proc[0-9]</p> <p>[summary]</p> level message OK OK <p>[values]</p> key value (example) COMMAND /usr/lib/firefox/firefox %CPU 37.5 %MEM 2.1 PID 14062 USER autoware PR 20 NI 0 VIRT 3461152 RES 669052 SHR 481208 S S TIME+ 23:57.49"},{"location":"system/system_monitor/docs/topics_process_monitor/#high-mem-proc0-9","title":"High-mem Proc[0-9]","text":"<p>/diagnostics/process_monitor: High-mem Proc[0-9]</p> <p>[summary]</p> level message OK OK <p>[values]</p> key value (example) COMMAND /snap/multipass/1784/usr/bin/qemu-system-x86_64 %CPU 0 %MEM 2.5 PID 1565 USER root PR 20 NI 0 VIRT 3722320 RES 812432 SHR 20340 S S TIME+ 0:22.84"},{"location":"system/system_monitor/docs/topics_voltage_monitor/","title":"ROS topics: Voltage Monitor","text":""},{"location":"system/system_monitor/docs/topics_voltage_monitor/#ros-topics-voltage-monitor","title":"ROS topics: Voltage Monitor","text":"<p>\"CMOS Battery Status\" and \"CMOS battery voltage\" are exclusive. Only one or the other is generated. Which one is generated depends on the value of cmos_battery_label.</p>"},{"location":"system/system_monitor/docs/topics_voltage_monitor/#cmos-battery-status","title":"CMOS Battery Status","text":"<p>/diagnostics/voltage_monitor: CMOS Battery Status</p> <p>[summary]</p> level message OK OK WARN Battery Dead <p>[values]</p> key (example) value (example) CMOS battery status OK / Battery Dead <p>*key: thermal_zone[0-9] for ARM architecture.</p>"},{"location":"system/system_monitor/docs/topics_voltage_monitor/#cmos-battery-voltage","title":"CMOS Battery Voltage","text":"<p>/diagnostics/voltage_monitor: CMOS battery voltage</p> <p>[summary]</p> level message OK OK WARN Low Battery WARN Battery Died <p>[values]</p> key value (example) CMOS battery voltage 3.06"},{"location":"system/system_monitor/docs/traffic_reader/","title":"traffic_reader","text":""},{"location":"system/system_monitor/docs/traffic_reader/#traffic_reader","title":"traffic_reader","text":""},{"location":"system/system_monitor/docs/traffic_reader/#name","title":"Name","text":"<p>traffic_reader - monitoring network traffic by process</p>"},{"location":"system/system_monitor/docs/traffic_reader/#synopsis","title":"Synopsis","text":"<p>traffic_reader [OPTION]</p>"},{"location":"system/system_monitor/docs/traffic_reader/#description","title":"Description","text":"<p>Monitoring network traffic by process. This runs as a daemon process and listens to a TCP/IP port (7636 by default).</p> <p>Options: -h, --help \u00a0\u00a0\u00a0\u00a0Display help -p, --port # \u00a0\u00a0\u00a0\u00a0Port number to listen to</p> <p>Exit status: Returns 0 if OK; non-zero otherwise.</p>"},{"location":"system/system_monitor/docs/traffic_reader/#notes","title":"Notes","text":"<p>The 'traffic_reader' requires nethogs command.</p>"},{"location":"system/system_monitor/docs/traffic_reader/#operation-confirmed-platform","title":"Operation confirmed platform","text":"<ul> <li>Ubuntu 20.04.3 LTS (GNU/Linux 5.11.0-40-generic x86_64)</li> </ul>"},{"location":"system/topic_state_monitor/","title":"topic_state_monitor","text":""},{"location":"system/topic_state_monitor/#topic_state_monitor","title":"topic_state_monitor","text":""},{"location":"system/topic_state_monitor/#purpose","title":"Purpose","text":"<p>This node monitors input topic for abnormalities such as timeout and low frequency. The result of topic status is published as diagnostics.</p>"},{"location":"system/topic_state_monitor/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The types of topic status and corresponding diagnostic status are following.</p> Topic status Diagnostic status Description <code>OK</code> OK The topic has no abnormalities <code>NotReceived</code> ERROR The topic has not been received yet <code>WarnRate</code> WARN The frequency of the topic is dropped <code>ErrorRate</code> ERROR The frequency of the topic is significantly dropped <code>Timeout</code> ERROR The topic subscription is stopped for a certain time"},{"location":"system/topic_state_monitor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/topic_state_monitor/#input","title":"Input","text":"Name Type Description any name any type Subscribe target topic to monitor"},{"location":"system/topic_state_monitor/#output","title":"Output","text":"Name Type Description <code>/diagnostics</code> <code>diagnostic_msgs/DiagnosticArray</code> Diagnostics outputs"},{"location":"system/topic_state_monitor/#parameters","title":"Parameters","text":""},{"location":"system/topic_state_monitor/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>topic</code> string - Name of target topic <code>topic_type</code> string - Type of target topic (used if the topic is not transform) <code>frame_id</code> string - Frame ID of transform parent (used if the topic is transform) <code>child_frame_id</code> string - Frame ID of transform child (used if the topic is transform) <code>transient_local</code> bool false QoS policy of topic subscription (Transient Local/Volatile) <code>best_effort</code> bool false QoS policy of topic subscription (Best Effort/Reliable) <code>diag_name</code> string - Name used for the diagnostics to publish <code>update_rate</code> double 10.0 Timer callback period [Hz]"},{"location":"system/topic_state_monitor/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>warn_rate</code> double 0.5 If the topic rate is lower than this value, the topic status becomes <code>WarnRate</code> <code>error_rate</code> double 0.1 If the topic rate is lower than this value, the topic status becomes <code>ErrorRate</code> <code>timeout</code> double 1.0 If the topic subscription is stopped for more than this time [s], the topic status becomes <code>Timeout</code> <code>window_size</code> int 10 Window size of target topic for calculating frequency"},{"location":"system/topic_state_monitor/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/velodyne_monitor/","title":"velodyne_monitor","text":""},{"location":"system/velodyne_monitor/#velodyne_monitor","title":"velodyne_monitor","text":""},{"location":"system/velodyne_monitor/#purpose","title":"Purpose","text":"<p>This node monitors the status of Velodyne LiDARs. The result of the status is published as diagnostics. Take care not to use this diagnostics to decide the lidar error. Please read Assumptions / Known limits for the detail reason.</p>"},{"location":"system/velodyne_monitor/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The status of Velodyne LiDAR can be retrieved from <code>http://[ip_address]/cgi/{info, settings, status, diag}.json</code>.</p> <p>The types of abnormal status and corresponding diagnostics status are following.</p> Abnormal status Diagnostic status No abnormality OK Top board temperature is too cold ERROR Top board temperature is cold WARN Top board temperature is too hot ERROR Top board temperature is hot WARN Bottom board temperature is too cold ERROR Bottom board temperature is cold WARN Bottom board temperature is too hot ERROR Bottom board temperature is hot WARN Rpm(Rotations per minute) of the motor is too low ERROR Rpm(Rotations per minute) of the motor is low WARN Connection error (cannot get Velodyne LiDAR status) ERROR"},{"location":"system/velodyne_monitor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/velodyne_monitor/#input","title":"Input","text":"<p>None</p>"},{"location":"system/velodyne_monitor/#output","title":"Output","text":"Name Type Description <code>/diagnostics</code> <code>diagnostic_msgs/DiagnosticArray</code> Diagnostics outputs"},{"location":"system/velodyne_monitor/#parameters","title":"Parameters","text":""},{"location":"system/velodyne_monitor/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>timeout</code> double 0.5 Timeout for HTTP request to get Velodyne LiDAR status [s]"},{"location":"system/velodyne_monitor/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>ip_address</code> string \"192.168.1.201\" IP address of target Velodyne LiDAR <code>temp_cold_warn</code> double -5.0 If the temperature of Velodyne LiDAR is lower than this value, the diagnostics status becomes WARN [\u00b0C] <code>temp_cold_error</code> double -10.0 If the temperature of Velodyne LiDAR is lower than this value, the diagnostics status becomes ERROR [\u00b0C] <code>temp_hot_warn</code> double 75.0 If the temperature of Velodyne LiDAR is higher than this value, the diagnostics status becomes WARN [\u00b0C] <code>temp_hot_error</code> double 80.0 If the temperature of Velodyne LiDAR is higher than this value, the diagnostics status becomes ERROR [\u00b0C] <code>rpm_ratio_warn</code> double 0.80 If the rpm rate of the motor (= current rpm / default rpm) is lower than this value, the diagnostics status becomes WARN <code>rpm_ratio_error</code> double 0.70 If the rpm rate of the motor (= current rpm / default rpm) is lower than this value, the diagnostics status becomes ERROR"},{"location":"system/velodyne_monitor/#config-files","title":"Config files","text":"<p>Config files for several velodyne models are prepared. The <code>temp_***</code> parameters are set with reference to the operational temperature from each datasheet. Moreover, the <code>temp_hot_***</code> of each model are set highly as 20 from operational temperature. Now, <code>VLP-16.param.yaml</code> is used as default argument because it is lowest spec.</p> Model Name Config name Operational Temperature [\u2103] VLP-16 VLP-16.param.yaml -10 to 60 VLP-32C VLP-32C.param.yaml -20 to 60 VLS-128 VLS-128.param.yaml -20 to 60 Velarray M1600 Velarray_M1600.param.yaml -40 to 85 HDL-32E HDL-32E.param.yaml -10 to 60"},{"location":"system/velodyne_monitor/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>This node uses the http_client and request results by GET method. It takes a few seconds to get results, or generate a timeout exception if it does not succeed the GET request. This occurs frequently and the diagnostics aggregator output STALE. Therefore I recommend to stop using this results to decide the lidar error, and only monitor it to confirm lidar status.</p>"},{"location":"tools/simulator_test/simulator_compatibility_test/","title":"simulator_compatibility_test","text":""},{"location":"tools/simulator_test/simulator_compatibility_test/#simulator_compatibility_test","title":"simulator_compatibility_test","text":""},{"location":"tools/simulator_test/simulator_compatibility_test/#purpose","title":"Purpose","text":"<p>Test procedures (e.g. test codes) to check whether a certain simulator is compatible with Autoware</p>"},{"location":"tools/simulator_test/simulator_compatibility_test/#overview-of-the-test-codes","title":"Overview of the test codes","text":"<p>File structure</p> <ul> <li>test_base</li> <li>test_sim_common_manual_testing</li> <li>test_morai_sim</li> </ul> <ol> <li>test_base provides shared methods for testing. Other test codes are created based on functions defined here.</li> <li>test_sim_common_manual_testing provides the most basic functions. Any simulator can be tested using codes here. However, to make these codes usable with any simulators, the codes do not include any features for test automation.</li> <li>test_morai_sim is an automated version of test_sim_common_manual_testing for MORAI SIM: Drive. Thus it includes 'MORAI SIM: Drive'-specific codes. Users of the other simulators may create similar version for their simulator of interest.</li> </ol>"},{"location":"tools/simulator_test/simulator_compatibility_test/#test-procedures-for-test_sim_common_manual_testing","title":"Test Procedures for test_sim_common_manual_testing","text":""},{"location":"tools/simulator_test/simulator_compatibility_test/#build-process-before-test","title":"Build process before test","text":"<pre><code>source install/setup.bash\ncolcon build --packages-select simulator_compatibility_test\ncd src/universe/autoware.universe/tools/simulator_test/simulator_compatibility_test/test_sim_common_manual_testing\n</code></pre> <p>To run each test case manually</p>"},{"location":"tools/simulator_test/simulator_compatibility_test/#test-case-1","title":"Test Case #1","text":"<ol> <li>Run your simulator</li> <li>Load a map and an ego vehicle for the test</li> <li> <p>Run the test using the following command</p> <pre><code>python -m pytest test_01_control_mode_and_report.py\n</code></pre> </li> <li> <p>Check if expected behavior is created within the simulator</p> <ul> <li>Ego vehicle control mode is changed into Manual (If the simulator has a GUI for this one, it should display the ego is in Manual)</li> <li>Ego vehicle control mode is changed into Auto (If the simulator has a GUI for this one, it should display the ego is in Auto)</li> </ul> </li> <li>Check if pytest output is passed or failure</li> </ol>"},{"location":"tools/simulator_test/simulator_compatibility_test/#test-case-2","title":"Test Case #2","text":"<ol> <li>Run your simulator (If the simulator is already running, skip this part)</li> <li>Load a map and an ego vehicle for the test (If a map and an ego are loaded already, skip this part)</li> <li> <p>Run the test using the following command</p> <pre><code>python -m pytest test_02_change_gear_and_report.py\n</code></pre> </li> <li> <p>Check if expected behavior is created within the simulator</p> <ul> <li>Ego vehicle gear mode is changed into \"P\" (If the simulator has a GUI for this one, it should display the gear mode is in \"P\")</li> <li>Ego vehicle gear mode is changed into \"N\" (If the simulator has a GUI for this one, it should display the gear mode is in \"N\")</li> <li>Ego vehicle gear mode is changed into \"R\" (If the simulator has a GUI for this one, it should display the gear mode is in \"R\")</li> <li>Ego vehicle gear mode is changed into \"D\" (If the simulator has a GUI for this one, it should display the gear mode is in \"D\")</li> </ul> </li> <li>Check if pytest output is passed or failure</li> </ol>"},{"location":"tools/simulator_test/simulator_compatibility_test/#test-case-3","title":"Test Case #3","text":"<ol> <li>Run your simulator (If the simulator is already running, skip this part)</li> <li>Load a map and an ego vehicle for the test (If a map and an ego are loaded already, skip this part)</li> <li> <p>Run the test using the following command</p> <pre><code>python -m pytest test_03_longitudinal_command_and_report.py\n</code></pre> </li> <li> <p>Check if expected behavior is created within the simulator</p> <ul> <li>Ego vehicle longitudinal velocity is greater than 10 kph (If the simulator has a GUI for this one, it should display the longitudinal velocity is greater than 10 kph)</li> <li>Ego vehicle longitudinal velocity is going below 10 kph. This is an ego vehicle initialize process to ensure the following acceleration is made by longitudinal.acceleration value (If the simulator has a GUI for this one, it should display the longitudinal velocity is less than 10 kph)</li> <li>Ego vehicle longitudinal velocity is greater than 10 kph (If the simulator has a GUI for this one, it should display the longitudinal velocity is greater than 10 kph)</li> <li>Ego vehicle longitudinal velocity is going below 10 kph. This is an ego vehicle reset process to tear down this test case.</li> </ul> </li> <li>Check if pytest output is passed or failure</li> </ol>"},{"location":"tools/simulator_test/simulator_compatibility_test/#test-case-4","title":"Test Case #4","text":"<ol> <li>Run your simulator (If the simulator is already running, skip this part)</li> <li>Load a map and an ego vehicle for the test (If a map and an ego are loaded already, skip this part)</li> <li> <p>Run the test using the following command</p> <pre><code>python -m pytest test_04_lateral_command_and_report.py\n</code></pre> </li> <li> <p>Check if expected behavior is created within the simulator</p> <ul> <li>Ego vehicle steering and/or tire value is greater than 0 degree (If the simulator has a GUI for this one, it should display the steering and/or tire is greater than 0 degree)</li> <li>Ego vehicle steering and/or tire value is 0 degree. This is a reset process. (If the simulator has a GUI for this one, it should display the steering and/or tire is 0 degree)</li> <li>Ego vehicle steering and/or tire value is less than 0 degree (If the simulator has a GUI for this one, it should display the steering and/or tire is less than 0 degree)</li> <li>Ego vehicle steering and/or tire value is 0 degree. This is a reset process. (If the simulator has a GUI for this one, it should display the steering and/or tire is 0 degree)</li> </ul> </li> <li>Check if pytest output is passed or failure</li> </ol>"},{"location":"tools/simulator_test/simulator_compatibility_test/#test-case-5","title":"Test Case #5","text":"<ol> <li>Run your simulator (If the simulator is already running, skip this part)</li> <li>Load a map and an ego vehicle for the test (If a map and an ego are loaded already, skip this part)</li> <li> <p>Run the test using the following command</p> <pre><code>python -m pytest test_05_turn_indicators_cmd_and_report.py\n</code></pre> </li> <li> <p>Check if expected behavior is created within the simulator</p> <ul> <li>Ego vehicle left turn indicator is turned on (If the simulator has a GUI for this one, it should display the left turn indicator is turned on)</li> <li>Ego vehicle right turn indicator is turned on (If the simulator has a GUI for this one, it should display the right turn indicator is turned on)</li> <li>Ego vehicle both turn indicators are turned off. This is a reset process. (If the simulator has a GUI for this one, it should display both left and right turn indicators are turned off)</li> </ul> </li> <li>Check if pytest output is passed or failure</li> </ol>"},{"location":"tools/simulator_test/simulator_compatibility_test/#test-case-6","title":"Test Case #6","text":"<ol> <li>Run your simulator (If the simulator is already running, skip this part)</li> <li>Load a map and an ego vehicle for the test (If a map and an ego are loaded already, skip this part)</li> <li> <p>Run the test using the following command</p> <pre><code>python -m pytest test_06_hazard_lights_cmd_and_report.py\n</code></pre> </li> <li> <p>Check if expected behavior is created within the simulator</p> <ul> <li>Ego vehicle hazard lights are turned on (If the simulator has a GUI for this one, it should display the hazard lights are turned on or blinking)</li> <li>Ego vehicle hazard lights are turned off. This is a reset process. (If the simulator has a GUI for this one, it should display the hazard lights are turned off)</li> </ul> </li> <li>Check if pytest output is passed or failure</li> </ol>"},{"location":"tools/simulator_test/simulator_compatibility_test/#test-procedures-for-test_morai_sim","title":"Test Procedures for test_morai_sim","text":""},{"location":"tools/simulator_test/simulator_compatibility_test/#build-process-before-test_1","title":"Build process before test","text":"<pre><code>source install/setup.bash\ncolcon build --packages-select simulator_compatibility_test\ncd src/universe/autoware.universe/tools/simulator_test/simulator_compatibility_test/test_morai_sim\n</code></pre> <p>Detailed process</p> <p>(WIP)</p>"},{"location":"tools/simulator_test/simulator_compatibility_test/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"tools/simulator_test/simulator_compatibility_test/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"tools/simulator_test/simulator_compatibility_test/#input","title":"Input","text":"Name Type Description <code>/vehicle/status/control_mode</code> <code>autoware_auto_vehicle_msgs::msg::ControlModeReport</code> for [Test Case #1] <code>/vehicle/status/gear_status</code> <code>autoware_auto_vehicle_msgs::msg::GearReport</code> for [Test Case #2] <code>/vehicle/status/velocity_status</code> <code>autoware_auto_vehicle_msgs::msg::VelocityReport</code> for [Test Case #3] <code>/vehicle/status/steering_status</code> <code>autoware_auto_vehicle_msgs::msg::SteeringReport</code> for [Test Case #4] <code>/vehicle/status/turn_indicators_status</code> <code>autoware_auto_vehicle_msgs::msg::TurnIndicatorsReport</code> for [Test Case #5] <code>/vehicle/status/hazard_lights_status</code> <code>autoware_auto_vehicle_msgs::msg::HazardLightsReport</code> for [Test Case #6]"},{"location":"tools/simulator_test/simulator_compatibility_test/#output","title":"Output","text":"Name Type Description <code>/control/command/control_mode_cmd</code> <code>autoware_auto_vehicle_msgs/ControlModeCommand</code> for [Test Case #1] <code>/control/command/gear_cmd</code> <code>autoware_auto_vehicle_msgs/GearCommand</code> for [Test Case #2] <code>/control/command/control_cmd</code> <code>autoware_auto_vehicle_msgs/AckermannControlCommand</code> for [Test Case #3, #4] <code>/vehicle/status/steering_status</code> <code>autoware_auto_vehicle_msgs/TurnIndicatorsCommand</code> for [Test Case #5] <code>/control/command/turn_indicators_cmd</code> <code>autoware_auto_vehicle_msgs/HazardLightsCommand</code> for [Test Case #6]"},{"location":"tools/simulator_test/simulator_compatibility_test/#parameters","title":"Parameters","text":"<p>None.</p>"},{"location":"tools/simulator_test/simulator_compatibility_test/#node-parameters","title":"Node Parameters","text":"<p>None.</p>"},{"location":"tools/simulator_test/simulator_compatibility_test/#core-parameters","title":"Core Parameters","text":"<p>None.</p>"},{"location":"tools/simulator_test/simulator_compatibility_test/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>None.</p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/","title":"accel_brake_map_calibrator","text":""},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#accel_brake_map_calibrator","title":"accel_brake_map_calibrator","text":"<p>The role of this node is to automatically calibrate <code>accel_map.csv</code> / <code>brake_map.csv</code> used in the <code>raw_vehicle_cmd_converter</code> node.</p> <p>The base map, which is lexus's one by default, is updated iteratively with the loaded driving data.</p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#how-to-calibrate","title":"How to calibrate","text":""},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#launch-calibrator","title":"Launch Calibrator","text":"<p>After launching Autoware, run the <code>accel_brake_map_calibrator</code> by the following command and then perform autonomous driving. Note: You can collect data with manual driving if it is possible to use the same vehicle interface as during autonomous driving (e.g. using a joystick).</p> <pre><code>ros2 launch accel_brake_map_calibrator accel_brake_map_calibrator.launch.xml rviz:=true\n</code></pre> <p>Or if you want to use rosbag files, run the following commands.</p> <pre><code>ros2 launch accel_brake_map_calibrator accel_brake_map_calibrator.launch.xml rviz:=true use_sim_time:=true\nros2 bag play &lt;rosbag_file&gt; --clock\n</code></pre> <p>During the calibration with setting the parameter <code>progress_file_output</code> to true, the log file is output in [directory of accel_brake_map_calibrator]/config/ . You can also see accel and brake maps in [directory of accel_brake_map_calibrator]/config/accel_map.csv and [directory of accel_brake_map_calibrator]/config/brake_map.csv after calibration.</p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#calibration-plugin","title":"Calibration plugin","text":"<p>The <code>rviz:=true</code> option displays the RViz with a calibration plugin as below.</p> <p> </p> <p>The current status (velocity and pedal) is shown in the plugin. The color on the current cell varies green/red depending on the current data is valid/invalid. The data that doesn't satisfy the following conditions are considered invalid and will not be used for estimation since aggressive data (e.g. when the pedal is moving fast) causes bad calibration accuracy.</p> <ul> <li>The velocity and pedal conditions are within certain ranges from the index values.</li> <li>The steer value, pedal speed, pitch value, etc. are less than corresponding thresholds.</li> <li>The velocity is higher than a threshold.</li> </ul> <p>The detailed parameters are described in the parameter section.</p> <p>Note: You don't need to worry about whether the current state is red or green during calibration. Just keep getting data until all the cells turn red.</p> <p>The value of each cell in the map is gray at first, and it changes from blue to red as the number of valid data in the cell accumulates. It is preferable to continue the calibration until each cell of the map becomes close to red. In particular, the performance near the stop depends strongly on the velocity of 0 ~ 6m/s range and the pedal value of +0.2 ~ -0.4, range so it is desirable to focus on those areas.</p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#diagnostics","title":"Diagnostics","text":"<p>The <code>accel brake map_calibrator</code> publishes diagnostics message depending on the calibration status. Diagnostic type <code>WARN</code> indicates that the current accel/brake map is estimated to be inaccurate. In this situation, it is strongly recommended to perform a re-calibration of the accel/brake map.</p> Status Diagnostics Type Diagnostics message Description No calibration required <code>OK</code> \"OK\" Calibration Required <code>WARN</code> \"Accel/brake map Calibration is required.\" The accuracy of current accel/brake map may be low. <p>This diagnostics status can be also checked on the following ROS topic.</p> <pre><code>ros2 topic echo /accel_brake_map_calibrator/output/update_suggest\n</code></pre> <p>When the diagnostics type is <code>WARN</code>, <code>True</code> is published on this topic and the update of the accel/brake map is suggested.</p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#evaluation-of-the-accel-brake-map-accuracy","title":"Evaluation of the accel / brake map accuracy","text":"<p>The accuracy of map is evaluated by the Root Mean Squared Error (RMSE) between the observed acceleration and predicted acceleration.</p> <p>TERMS:</p> <ul> <li><code>Observed acceleration</code>: the current vehicle acceleration which is calculated as a derivative value of the wheel speed.</li> </ul> <ul> <li><code>Predicted acceleration</code>: the output of the original accel/brake map, which the Autoware is expecting. The value is calculated using the current pedal and velocity.</li> </ul> <p>You can check additional error information with the following topics.</p> <ul> <li><code>/accel_brake_map_calibrator/output/current_map_error</code> : The error of the original map set in the <code>csv_path_accel/brake_map</code> path. The original map is not accurate if this value is large.</li> <li><code>/accel_brake_map_calibrator/output/updated_map_error</code> : The error of the map calibrated in this node. The calibration quality is low if this value is large.</li> <li><code>/accel_brake_map_calibrator/output/map_error_ratio</code> : The error ratio between the original map and updated map (ratio = updated / current). If this value is less than 1, it is desirable to update the map.</li> </ul>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#how-to-visualize-calibration-data","title":"How to visualize calibration data","text":"<p>The process of calibration can be visualized as below. Since these scripts need the log output of the calibration, the <code>pedal_accel_graph_output</code> parameter must be set to true while the calibration is running for the visualization.</p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#visualize-plot-of-relation-between-acceleration-and-pedal","title":"Visualize plot of relation between acceleration and pedal","text":"<p>The following command shows the plot of used data in the calibration. In each plot of velocity ranges, you can see the distribution of the relationship between pedal and acceleration, and raw data points with colors according to their pitch angles.</p> <pre><code>ros2 run accel_brake_map_calibrator view_plot.py\n</code></pre> <p></p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#visualize-statistics-about-accelerationvelocitypedal-data","title":"Visualize statistics about acceleration/velocity/pedal data","text":"<p>The following command shows the statistics of the calibration:</p> <ul> <li>mean value</li> <li>standard deviation</li> <li>number of data</li> </ul> <p>of all data in each map cell.</p> <pre><code>ros2 run accel_brake_map_calibrator view_statistics.py\n</code></pre> <p></p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#how-to-save-the-calibrated-accel-brake-map-anytime-you-want","title":"How to save the calibrated accel / brake map anytime you want","text":"<p>You can save accel and brake map anytime with the following command.</p> <pre><code>ros2 service call /accel_brake_map_calibrator/update_map_dir tier4_vehicle_msgs/srv/UpdateAccelBrakeMap \"path: '&lt;accel/brake map directory&gt;'\"\n</code></pre> <p>You can also save accel and brake map in the default directory where Autoware reads accel_map.csv/brake_map.csv using the RViz plugin (AccelBrakeMapCalibratorButtonPanel) as following.</p> <ol> <li> <p>Click Panels tab, and select AccelBrakeMapCalibratorButtonPanel.</p> <p></p> </li> <li> <p>Select the panel, and the button will appear at the bottom of RViz.</p> <p></p> </li> <li> <p>Press the button, and the accel / brake map will be saved.    (The button cannot be pressed in certain situations, such as when the calibrator node is not running.)</p> <p></p> </li> </ol>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#parameters","title":"Parameters","text":""},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#system-parameters","title":"System Parameters","text":"Name Type Description Default value update_method string you can select map calibration method. \"update_offset_each_cell\" calculates offsets for each grid cells on the map. \"update_offset_total\" calculates the total offset of the map. \"update_offset_each_cell\" get_pitch_method string \"tf\": get pitch from tf, \"none\": unable to perform pitch validation and pitch compensation \"tf\" pedal_accel_graph_output bool if true, it will output a log of the pedal accel graph. true progress_file_output bool if true, it will output a log and csv file of the update process. false default_map_dir str directory of default map [directory of raw_vehicle_cmd_converter]/data/default/ calibrated_map_dir str directory of calibrated map [directory of accel_brake_map_calibrator]/config/ update_hz double hz for update 10.0"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#algorithm-parameters","title":"Algorithm Parameters","text":"Name Type Description Default value initial_covariance double Covariance of initial acceleration map (larger covariance makes the update speed faster) 0.05 velocity_min_threshold double Speeds smaller than this are not used for updating. 0.1 velocity_diff_threshold double When the velocity data is more than this threshold away from the grid reference speed (center value), the associated data is not used for updating. 0.556 max_steer_threshold double If the steer angle is greater than this value, the associated data is not used for updating. 0.2 max_pitch_threshold double If the pitch angle is greater than this value, the associated data is not used for updating. 0.02 max_jerk_threshold double If the ego jerk calculated from ego acceleration is greater than this value, the associated data is not used for updating. 0.7 pedal_velocity_thresh double If the pedal moving speed is greater than this value, the associated data is not used for updating. 0.15 pedal_diff_threshold double If the current pedal value is more then this threshold away from the previous value, the associated data is not used for updating. 0.03 max_accel double Maximum value of acceleration calculated from velocity source. 5.0 min_accel double Minimum value of acceleration calculated from velocity source. -5.0 pedal_to_accel_delay double The delay time between actuation_cmd to acceleration, considered in the update logic. 0.3 update_suggest_thresh double threshold of RMSE ratio that update suggest flag becomes true. ( RMSE ratio: [RMSE of new map] / [RMSE of original map] ) 0.7 max_data_count int For visualization. When the data num of each grid gets this value, the grid color gets red. 100 accel_brake_value_source string Whether to use actuation_status or actuation_command as accel/brake sources. value status"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#test-utility-scripts","title":"Test utility scripts","text":""},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#constant-accelbrake-command-test","title":"Constant accel/brake command test","text":"<p>These scripts are useful to test for accel brake map calibration. These generate an <code>ActuationCmd</code> with a constant accel/brake value given interactively by a user through CLI.</p> <ul> <li>accel_tester.py</li> <li>brake_tester.py</li> <li>actuation_cmd_publisher.py</li> </ul> <p>The <code>accel/brake_tester.py</code> receives a target accel/brake command from CLI. It sends a target value to <code>actuation_cmd_publisher.py</code> which generates the <code>ActuationCmd</code>. You can run these scripts by the following commands in the different terminals, and it will be as in the screenshot below.</p> <pre><code>ros2 run accel_brake_map_calibrator accel_tester.py\nros2 run accel_brake_map_calibrator brake_tester.py\nros2 run accel_brake_map_calibrator actuation_cmd_publisher.py\n</code></pre> <p></p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#calibration-method","title":"Calibration Method","text":"<p>Two algorithms are selectable for the acceleration map update, update_offset_four_cell_around and update_offset_each_cell. Please see the link for details.</p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#data-preprocessing","title":"Data Preprocessing","text":"<p>Before calibration, missing or unusable data (e.g., too large handle angles) must first be eliminated. The following parameters are used to determine which data to remove.</p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#parameters_1","title":"Parameters","text":"Name Description Default Value velocity_min_threshold Exclude minimal velocity 0.1 max_steer_threshold Exclude large steering angle 0.2 max_pitch_threshold Exclude large pitch angle 0.02 max_jerk_threshold Exclude large jerk 0.7 pedal_velocity_thresh Exclude large pedaling speed 0.15"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#update_offset_each_cell","title":"update_offset_each_cell","text":"<p>Update by Recursive Least Squares(RLS) method using data close enough to each grid.</p> <p>Advantage : Only data close enough to each grid is used for calibration, allowing accurate updates at each point.</p> <p>Disadvantage : Calibration is time-consuming due to a large amount of data to be excluded.</p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#parameters_2","title":"Parameters","text":"<p>Data selection is determined by the following thresholds. | Name | Default Value | | ----------------------- | ------------- | | velocity_diff_threshold | 0.556 | | pedal_diff_threshold | 0.03 |</p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#update-formula","title":"Update formula","text":"\\[ \\begin{align}     \\theta[n]=&amp;     \\theta[n-1]+\\frac{p[n-1]x^{(n)}}{\\lambda+p[n-1]{(x^{(n)})}^2}(y^{(n)}-\\theta[n-1]x^{(n)})\\\\     p[n]=&amp;\\frac{p[n-1]}{\\lambda+p[n-1]{(x^{(n)})}^2} \\end{align} \\]"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#variables","title":"Variables","text":"Variable name Symbol covariance \\(p[n-1]\\) map_offset \\(\\theta[n]\\) forgettingfactor \\(\\lambda\\) phi \\(x(=1)\\) measured_acc \\(y\\)"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#update_offset_four_cell_around-1","title":"update_offset_four_cell_around [1]","text":"<p>Update the offsets by RLS in four grids around newly obtained data. By considering linear interpolation, the update takes into account appropriate weights. Therefore, there is no need to remove data by thresholding.</p> <p>Advantage : No data is wasted because updates are performed on the 4 grids around the data with appropriate weighting. Disadvantage : Accuracy may be degraded due to extreme bias of the data. For example, if data \\(z(k)\\) is biased near \\(Z_{RR}\\) in Fig. 2, updating is performed at the four surrounding points ( \\(Z_{RR}\\), \\(Z_{RL}\\), \\(Z_{LR}\\), and \\(Z_{LL}\\)), but accuracy at \\(Z_{LL}\\) is not expected.</p> <p> </p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#implementation","title":"Implementation","text":"<p>See eq.(7)-(10) in [1] for the updated formula. In addition, eq.(17),(18) from [1] are used for Anti-Windup.</p>"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#references","title":"References","text":"<p>[1] Gabrielle Lochrie, Michael Doljevic, Mario Nona, Yongsoon Yoon, Anti-Windup Recursive Least Squares Method for Adaptive Lookup Tables with Application to Automotive Powertrain Control Systems, IFAC-PapersOnLine, Volume 54, Issue 20, 2021, Pages 840-845</p>"},{"location":"vehicle/external_cmd_converter/","title":"external_cmd_converter","text":""},{"location":"vehicle/external_cmd_converter/#external_cmd_converter","title":"external_cmd_converter","text":"<p><code>external_cmd_converter</code> is a node that converts desired mechanical input to acceleration and velocity by using accel/brake map.</p>"},{"location":"vehicle/external_cmd_converter/#input-topics","title":"Input topics","text":"Name Type Description <code>~/in/external_control_cmd</code> tier4_external_api_msgs::msg::ControlCommand target <code>throttle/brake/steering_angle/steering_angle_velocity</code> is necessary to calculate desired control command. <code>~/input/shift_cmd\"</code> autoware_auto_vehicle_msgs::GearCommand current command of gear. <code>~/input/emergency_stop</code> tier4_external_api_msgs::msg::Heartbeat emergency heart beat for external command. <code>~/input/current_gate_mode</code> tier4_control_msgs::msg::GateMode topic for gate mode. <code>~/input/odometry</code> navigation_msgs::Odometry twist topic in odometry is used."},{"location":"vehicle/external_cmd_converter/#output-topics","title":"Output topics","text":"Name Type Description <code>~/out/control_cmd</code> autoware_auto_control_msgs::msg::AckermannControlCommand ackermann control command converted from selected external command"},{"location":"vehicle/external_cmd_converter/#parameters","title":"Parameters","text":"Parameter Type Description <code>timer_rate</code> double timer's update rate <code>wait_for_first_topic</code> double if time out check is done after receiving first topic <code>control_command_timeout</code> double time out check for control command <code>emergency_stop_timeout</code> double time out check for emergency stop command"},{"location":"vehicle/external_cmd_converter/#limitation","title":"Limitation","text":"<p>tbd.</p>"},{"location":"vehicle/raw_vehicle_cmd_converter/","title":"raw_vehicle_cmd_converter","text":""},{"location":"vehicle/raw_vehicle_cmd_converter/#raw_vehicle_cmd_converter","title":"raw_vehicle_cmd_converter","text":"<p><code>raw_vehicle_command_converter</code> is a node that converts desired acceleration and velocity to mechanical input by using feed forward + feed back control (optional).</p>"},{"location":"vehicle/raw_vehicle_cmd_converter/#input-topics","title":"Input topics","text":"Name Type Description <code>~/input/control_cmd</code> autoware_auto_control_msgs::msg::AckermannControlCommand target <code>velocity/acceleration/steering_angle/steering_angle_velocity</code> is necessary to calculate actuation command. <code>~/input/steering\"</code> autoware_auto_vehicle_msgs::SteeringReport current status of steering used for steering feed back control <code>~/input/twist</code> navigation_msgs::Odometry twist topic in odometry is used."},{"location":"vehicle/raw_vehicle_cmd_converter/#output-topics","title":"Output topics","text":"Name Type Description <code>~/output/actuation_cmd</code> tier4_vehicle_msgs::msg::ActuationCommandStamped actuation command for vehicle to apply mechanical input"},{"location":"vehicle/raw_vehicle_cmd_converter/#parameters","title":"Parameters","text":"Name Type Description Default Range csv_path_accel_map string path for acceleration map csv file $(find-pkg-share raw_vehicle_cmd_converter)/data/default/accel_map.csv N/A csv_path_brake_map string path for brake map csv file $(find-pkg-share raw_vehicle_cmd_converter)/data/default/brake_map.csv N/A csv_path_steer_map string path for steer map csv file $(find-pkg-share raw_vehicle_cmd_converter)/data/default/steer_map.csv N/A convert_accel_cmd boolean use accel or not true N/A convert_brake_cmd boolean use brake or not true N/A convert_steer_cmd boolean use steer or not true N/A use_steer_ff boolean steering steer controller using steer feed forward or not true N/A use_steer_fb boolean steering steer controller using steer feed back or not true N/A is_debugging boolean debugging mode or not false N/A max_throttle float maximum value of throttle 0.4 \u22650.0 max_brake float maximum value of brake 0.8 \u22650.0 max_steer float maximum value of steer 10.0 N/A min_steer float minimum value of steer -10.0 N/A steer_pid.kp float proportional coefficient value in PID control 150.0 N/A steer_pid.ki float integral coefficient value in PID control 15.0 &gt;0.0 steer_pid.kd float derivative coefficient value in PID control 0.0 N/A steer_pid.max float maximum value of PID 8.0 N/A steer_pid.min float minimum value of PID -8.0. N/A steer_pid.max_p float maximum value of Proportional in PID 8.0 N/A steer_pid.min_p float minimum value of Proportional in PID -8.0 N/A steer_pid.max_i float maximum value of Integral in PID 8.0 N/A steer_pid.min_i float minimum value of Integral in PID -8.0 N/A steer_pid.max_d float maximum value of Derivative in PID 0.0 N/A steer_pid.min_d float minimum value of Derivative in PID 0.0 N/A steer_pid.invalid_integration_decay float invalid integration decay value in PID control 0.97 &gt;0.0"},{"location":"vehicle/raw_vehicle_cmd_converter/#limitation","title":"Limitation","text":"<p>The current feed back implementation is only applied to steering control.</p>"},{"location":"vehicle/steer_offset_estimator/Readme/","title":"steer_offset_estimator","text":""},{"location":"vehicle/steer_offset_estimator/Readme/#steer_offset_estimator","title":"steer_offset_estimator","text":""},{"location":"vehicle/steer_offset_estimator/Readme/#purpose","title":"Purpose","text":"<p>The role of this node is to automatically calibrate <code>steer_offset</code> used in the <code>vehicle_interface</code> node.</p> <p>The base steer offset value is 0 by default, which is standard, is updated iteratively with the loaded driving data. This module is supposed to be used in below straight driving situation. </p>"},{"location":"vehicle/steer_offset_estimator/Readme/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Estimates sequential steering offsets from kinematic model and state observations.  Calculate yaw rate error and then calculate steering error recursively by least squared method, for more details see <code>updateSteeringOffset()</code> function.</p>"},{"location":"vehicle/steer_offset_estimator/Readme/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"vehicle/steer_offset_estimator/Readme/#input","title":"Input","text":"Name Type Description <code>~/input/twist</code> <code>geometry_msgs::msg::TwistStamped</code> vehicle twist <code>~/input/steer</code> <code>autoware_auto_vehicle_msgs::msg::SteeringReport</code> steering"},{"location":"vehicle/steer_offset_estimator/Readme/#output","title":"Output","text":"Name Type Description <code>~/output/steering_offset</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> steering offset <code>~/output/steering_offset_covariance</code> <code>tier4_debug_msgs::msg::Float32Stamped</code> covariance of steering offset"},{"location":"vehicle/steer_offset_estimator/Readme/#launch-calibrator","title":"Launch Calibrator","text":"<p>After launching Autoware, run the <code>steer_offset_estimator</code> by the following command and then perform autonomous driving. Note: You can collect data with manual driving if it is possible to use the same vehicle interface as during autonomous driving (e.g. using a joystick).</p> <pre><code>ros2 launch steer_offset_estimator steer_offset_estimator.launch.xml\n</code></pre> <p>Or if you want to use rosbag files, run the following commands.</p> <pre><code>ros2 param set /use_sim_time true\nros2 bag play &lt;rosbag_file&gt; --clock\n</code></pre>"},{"location":"vehicle/steer_offset_estimator/Readme/#parameters","title":"Parameters","text":"Name Type Description Default Range initial_covariance float steer offset is larger than tolerance 1000 N/A steer_update_hz float update hz of steer data 10 \u22650.0 forgetting_factor float weight of using previous value 0.999 \u22650.0 valid_min_velocity float velocity below this value is not used 5 \u22650.0 valid_max_steer float steer above this value is not used 0.05 N/A warn_steer_offset_deg float Warn if offset is above this value. ex. if absolute estimated offset is larger than 2.5[deg] =&gt; warning 2.5 N/A"},{"location":"vehicle/steer_offset_estimator/Readme/#diagnostics","title":"Diagnostics","text":"<p>The <code>steer_offset_estimator</code> publishes diagnostics message depending on the calibration status. Diagnostic type <code>WARN</code> indicates that the current steer_offset is estimated to be inaccurate. In this situation, it is strongly recommended to perform a re-calibration of the steer_offset.</p> Status Diagnostics Type Diagnostics message No calibration required <code>OK</code> \"Preparation\" Calibration Required <code>WARN</code> \"Steer offset is larger than tolerance\" <p>This diagnostics status can be also checked on the following ROS topic.</p> <pre><code>ros2 topic echo /vehicle/status/steering_offset\n</code></pre>"},{"location":"vehicle/vehicle_info_util/Readme/","title":"Vehicle Info Util","text":""},{"location":"vehicle/vehicle_info_util/Readme/#vehicle-info-util","title":"Vehicle Info Util","text":""},{"location":"vehicle/vehicle_info_util/Readme/#purpose","title":"Purpose","text":"<p>This package is to get vehicle info parameters.</p>"},{"location":"vehicle/vehicle_info_util/Readme/#description","title":"Description","text":"<p>In here, you can check the vehicle dimensions with more detail.</p>"},{"location":"vehicle/vehicle_info_util/Readme/#scripts","title":"Scripts","text":""},{"location":"vehicle/vehicle_info_util/Readme/#minimum-turning-radius","title":"Minimum turning radius","text":"<pre><code>$ ros2 run vehicle_info_util min_turning_radius_calculator.py\nyaml path is /home/autoware/pilot-auto/install/vehicle_info_util/share/vehicle_info_util/config/vehicle_info.param.yaml\nMinimum turning radius is 3.253042620027102 [m] for rear, 4.253220695862465 [m] for front.\n</code></pre> <p>You can designate yaml file with <code>-y</code> option as follows.</p> <pre><code>ros2 run vehicle_info_util min_turning_radius_calculator.py -y &lt;path-to-yaml&gt;\n</code></pre>"},{"location":"vehicle/vehicle_info_util/Readme/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"}]}